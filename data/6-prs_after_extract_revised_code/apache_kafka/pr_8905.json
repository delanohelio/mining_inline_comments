{"pr_number": 8905, "pr_title": "KAFKA-10173: Fix suppress changelog binary schema compatibility", "pr_createdAt": "2020-06-19T22:14:09Z", "pr_url": "https://github.com/apache/kafka/pull/8905", "timeline": [{"oid": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "url": "https://github.com/apache/kafka/commit/4c393332ac179d055d2e06a8f34170ed3d4ae628", "message": "KAFKA-10173: Directly use Arrays.equals for version comparison", "committedDate": "2020-06-19T21:26:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2MjU5NA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443062594", "bodyText": "On the side, I realized we can consolidate this check and perform it first, rather than after we're already written bad data into the buffer.", "author": "vvcephei", "createdAt": "2020-06-19T22:14:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -286,6 +289,15 @@ private void logTombstone(final Bytes key) {\n \n     private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch) {\n         for (final ConsumerRecord<byte[], byte[]> record : batch) {\n+            if (record.partition() != partition) {\n+                throw new IllegalStateException(\n+                    String.format(\n+                        \"record partition [%d] is being restored by the wrong suppress partition [%d]\",\n+                        record.partition(),\n+                        partition\n+                    )\n+                );\n+            }", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "63f1cc3ab18d301c085aeaecad98879a4d85e49b", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex d3fb8dfb97..89e8d049c9 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -261,29 +260,29 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n         final int sizeOfBufferTime = Long.BYTES;\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n-\n+        final byte[] array = buffer.array();\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                buffer.array(),\n-                V_2_CHANGELOG_HEADERS,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            array,\n+            CHANGELOG_HEADERS,\n+            partition,\n+            null,\n+            KEY_SERIALIZER,\n+            VALUE_SERIALIZER\n         );\n     }\n \n     private void logTombstone(final Bytes key) {\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                null,\n-                null,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            null,\n+            null,\n+            partition,\n+            null,\n+            KEY_SERIALIZER,\n+            VALUE_SERIALIZER\n         );\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2Mjk3MQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443062971", "bodyText": "This is the fix (although it was probably fine before). The implementation of Header.equals is not specified by any contract, so it's safer to perform a direct comparison on the header values. Just as before, I'm comparing byte arrays to avoid deserializing the value.", "author": "vvcephei", "createdAt": "2020-06-19T22:16:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -339,7 +341,7 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             recordContext\n                         )\n                     );\n-                } else if (V_1_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n+                } else if (Arrays.equals(record.headers().lastHeader(\"v\").value(), V_1_CHANGELOG_HEADER_VALUE)) {", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex d3fb8dfb97..0390777ff6 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -341,7 +344,7 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n                             recordContext\n                         )\n                     );\n-                } else if (Arrays.equals(record.headers().lastHeader(\"v\").value(), V_1_CHANGELOG_HEADER_VALUE)) {\n+                } else if (Arrays.equals(versionHeader.value(), V_1_CHANGELOG_HEADER_VALUE)) {\n                     // in this case, the changelog value is a serialized ContextualRecord\n                     final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n                     final long time = timeAndValue.getLong();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443137719", "bodyText": "my IDEA says this variable is never used.", "author": "chia7712", "createdAt": "2020-06-20T15:07:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -58,10 +59,12 @@\n public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrderedKeyValueBuffer<K, V> {\n     private static final BytesSerializer KEY_SERIALIZER = new BytesSerializer();\n     private static final ByteArraySerializer VALUE_SERIALIZER = new ByteArraySerializer();\n+    private static final byte[] V_1_CHANGELOG_HEADER_VALUE = {(byte) 1};\n     private static final RecordHeaders V_1_CHANGELOG_HEADERS =", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4MDc0MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443180740", "bodyText": "I saw it is used in line 342.", "author": "guozhangwang", "createdAt": "2020-06-21T04:33:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE4NDAyOQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443184029", "bodyText": "my bad. The unused variable is V_1_CHANGELOG_HEADERS rather than V_1_CHANGELOG_HEADER_VALUE", "author": "chia7712", "createdAt": "2020-06-21T05:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI1Mzc2MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443253760", "bodyText": "Ah, right. My mistake. Thanks for pointing it out.", "author": "vvcephei", "createdAt": "2020-06-21T20:24:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzcxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "7af7935a67c75ee263d05187a765bb53ed6030d7", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex d3fb8dfb97..ff86ffea20 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -60,8 +60,6 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n     private static final BytesSerializer KEY_SERIALIZER = new BytesSerializer();\n     private static final ByteArraySerializer VALUE_SERIALIZER = new ByteArraySerializer();\n     private static final byte[] V_1_CHANGELOG_HEADER_VALUE = {(byte) 1};\n-    private static final RecordHeaders V_1_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", V_1_CHANGELOG_HEADER_VALUE)});\n     private static final byte[] V_2_CHANGELOG_HEADER_VALUE = {(byte) 2};\n     private static final RecordHeaders V_2_CHANGELOG_HEADERS =\n         new RecordHeaders(new Header[] {new RecordHeader(\"v\", V_2_CHANGELOG_HEADER_VALUE)});\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzk0NQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443137945", "bodyText": "nit:\nWe seek the last header many times. Could we reuse the return value?", "author": "chia7712", "createdAt": "2020-06-20T15:10:31Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -299,16 +311,6 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                         minTimestamp = sortedMap.isEmpty() ? Long.MAX_VALUE : sortedMap.firstKey().time();\n                     }\n                 }\n-\n-                if (record.partition() != partition) {\n-                    throw new IllegalStateException(\n-                        String.format(\n-                            \"record partition [%d] is being restored by the wrong suppress partition [%d]\",\n-                            record.partition(),\n-                            partition\n-                        )\n-                    );\n-                }\n             } else {\n                 if (record.headers().lastHeader(\"v\") == null) {", "originalCommit": "4c393332ac179d055d2e06a8f34170ed3d4ae628", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI1Mzc5Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r443253796", "bodyText": "Sure!", "author": "vvcephei", "createdAt": "2020-06-21T20:24:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEzNzk0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex d3fb8dfb97..0390777ff6 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -312,7 +314,8 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n                     }\n                 }\n             } else {\n-                if (record.headers().lastHeader(\"v\") == null) {\n+                final Header versionHeader = record.headers().lastHeader(\"v\");\n+                if (versionHeader == null) {\n                     // in this case, the changelog value is just the serialized record value\n                     final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n                     final long time = timeAndValue.getLong();\n"}}, {"oid": "98786b338b41173a5232754d5d83c0f19392b908", "url": "https://github.com/apache/kafka/commit/98786b338b41173a5232754d5d83c0f19392b908", "message": "converting upgrade test to smoke test", "committedDate": "2020-06-24T15:25:01Z", "type": "commit"}, {"oid": "5ba0c59bd7706f18ef8e9604f699827e8f7e5f29", "url": "https://github.com/apache/kafka/commit/5ba0c59bd7706f18ef8e9604f699827e8f7e5f29", "message": "wip debugging suppress buffer", "committedDate": "2020-06-24T15:25:17Z", "type": "commit"}, {"oid": "f556a41f6ad96b4c2738df801196a2e5c3828a98", "url": "https://github.com/apache/kafka/commit/f556a41f6ad96b4c2738df801196a2e5c3828a98", "message": "asdf", "committedDate": "2020-06-24T22:15:09Z", "type": "commit"}, {"oid": "7af7935a67c75ee263d05187a765bb53ed6030d7", "url": "https://github.com/apache/kafka/commit/7af7935a67c75ee263d05187a765bb53ed6030d7", "message": "fix", "committedDate": "2020-06-24T22:47:46Z", "type": "commit"}, {"oid": "495aebc9609b09318f246cbe801e55662263d14e", "url": "https://github.com/apache/kafka/commit/495aebc9609b09318f246cbe801e55662263d14e", "message": "upgrade test passed (2.3.1 -> trunk)", "committedDate": "2020-06-25T03:29:42Z", "type": "commit"}, {"oid": "e29728f1312b1fdbdf3294ee12439b7f5414f23b", "url": "https://github.com/apache/kafka/commit/e29728f1312b1fdbdf3294ee12439b7f5414f23b", "message": "wip", "committedDate": "2020-06-25T04:09:08Z", "type": "commit"}, {"oid": "738eef739b53f395f4d7769f946fc8ba70ea89d7", "url": "https://github.com/apache/kafka/commit/738eef739b53f395f4d7769f946fc8ba70ea89d7", "message": "cleanup", "committedDate": "2020-06-25T04:21:34Z", "type": "commit"}, {"oid": "9cd927abc299fafa599324675900171e1f8b6360", "url": "https://github.com/apache/kafka/commit/9cd927abc299fafa599324675900171e1f8b6360", "message": "cleanup", "committedDate": "2020-06-25T04:27:17Z", "type": "commit"}, {"oid": "23964583a267948249ae300466f15c2de9b1b647", "url": "https://github.com/apache/kafka/commit/23964583a267948249ae300466f15c2de9b1b647", "message": "fix test", "committedDate": "2020-06-25T17:28:57Z", "type": "commit"}, {"oid": "29ab2070b0a7e8124de91a8cc2ce01b64ac6c1eb", "url": "https://github.com/apache/kafka/commit/29ab2070b0a7e8124de91a8cc2ce01b64ac6c1eb", "message": "direct encoding of restore test vx", "committedDate": "2020-06-25T19:29:29Z", "type": "commit"}, {"oid": "dab0fc1c157fb6183469056dbf715007f6721880", "url": "https://github.com/apache/kafka/commit/dab0fc1c157fb6183469056dbf715007f6721880", "message": "direct encoding of restore test v1", "committedDate": "2020-06-25T19:49:56Z", "type": "commit"}, {"oid": "776efb673d0b8b71f63d5bc92fc538d4bf9eeea2", "url": "https://github.com/apache/kafka/commit/776efb673d0b8b71f63d5bc92fc538d4bf9eeea2", "message": "add v2 and v3", "committedDate": "2020-06-25T20:19:11Z", "type": "commit"}, {"oid": "e6e0b48a32945c2ddaa16da98148b5ca3b7634f5", "url": "https://github.com/apache/kafka/commit/e6e0b48a32945c2ddaa16da98148b5ca3b7634f5", "message": "cleanup", "committedDate": "2020-06-25T20:31:19Z", "type": "commit"}, {"oid": "65a549f98d27010e43d07154b3c30e3a9e41f318", "url": "https://github.com/apache/kafka/commit/65a549f98d27010e43d07154b3c30e3a9e41f318", "message": "adding other old versions", "committedDate": "2020-06-26T02:00:45Z", "type": "commit"}, {"oid": "41a4db1a7c5bd2bb41a861e6f7889ddd9bf062a4", "url": "https://github.com/apache/kafka/commit/41a4db1a7c5bd2bb41a861e6f7889ddd9bf062a4", "message": "fallback to make 2.4 work", "committedDate": "2020-06-26T02:01:08Z", "type": "commit"}, {"oid": "63f1cc3ab18d301c085aeaecad98879a4d85e49b", "url": "https://github.com/apache/kafka/commit/63f1cc3ab18d301c085aeaecad98879a4d85e49b", "message": "remove 2.1. cf KAFKA-10203", "committedDate": "2020-06-26T03:07:58Z", "type": "commit"}, {"oid": "2465162c456560549b7678e3b13ed505023f8737", "url": "https://github.com/apache/kafka/commit/2465162c456560549b7678e3b13ed505023f8737", "message": "reduce cyclomatic complexity", "committedDate": "2020-06-26T03:24:27Z", "type": "commit"}, {"oid": "d513fe881aede6956c19dddae5827f9cc88a870e", "url": "https://github.com/apache/kafka/commit/d513fe881aede6956c19dddae5827f9cc88a870e", "message": "Remove ParallelGC jvm param\n\nI was getting this exception, and somehow, the parallel GC parameter was the culprit\n\n    java.lang.OutOfMemoryError: Java heap space\n        at org.apache.kafka.streams.kstream.internals.FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(FullChangeSerde.java:82)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:90)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2(TimeOrderedKeyValueBufferChangelogDeserializationHelper.java:61)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.restoreBatch(InMemoryTimeOrderedKeyValueBuffer.java:369)\n        at org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer$$Lambda$284/0x00000001002cb440.restoreBatch(Unknown Source)\n        at org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferTest.shouldRestoreV3FormatWithV2Header(TimeOrderedKeyValueBufferTest.java:742)", "committedDate": "2020-06-26T15:24:59Z", "type": "commit"}, {"oid": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "url": "https://github.com/apache/kafka/commit/7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "message": "cleanup", "committedDate": "2020-06-26T15:44:38Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NTgwNg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446265806", "bodyText": "Only used in the test now, so I moved it.", "author": "vvcephei", "createdAt": "2020-06-26T15:50:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/FullChangeSerde.java", "diffHunk": "@@ -68,33 +68,6 @@ private FullChangeSerde(final Serde<T> inner) {\n         return new Change<>(newValue, oldValue);\n     }\n \n-    /**\n-     * We used to serialize a Change into a single byte[]. Now, we don't anymore, but we still keep this logic here\n-     * so that we can produce the legacy format to test that we can still deserialize it.\n-     */\n-    public static byte[] mergeChangeArraysIntoSingleLegacyFormattedArray(final Change<byte[]> serialChange) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NjY1Nw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446266657", "bodyText": "This was correct before, since we check equality and enforce identity in the constructor, but Arrays.equals is extremely cheap when the arrays are identical, so explicitly doing an identity check instead of equality was a micro-optimization.", "author": "vvcephei", "createdAt": "2020-06-26T15:51:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/BufferValue.java", "diffHunk": "@@ -120,7 +120,7 @@ ByteBuffer serialize(final int endPadding) {\n \n         if (oldValue == null) {\n             buffer.putInt(NULL_VALUE_SENTINEL);\n-        } else if (priorValue == oldValue) {\n+        } else if (Arrays.equals(priorValue, oldValue)) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2NzI5MQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446267291", "bodyText": "We don't need to store the whole RecordHeaders for the old versions, just the actual version flag.", "author": "vvcephei", "createdAt": "2020-06-26T15:52:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -54,14 +56,17 @@\n import java.util.function.Supplier;\n \n import static java.util.Objects.requireNonNull;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV3;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2;\n \n public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrderedKeyValueBuffer<K, V> {\n     private static final BytesSerializer KEY_SERIALIZER = new BytesSerializer();\n     private static final ByteArraySerializer VALUE_SERIALIZER = new ByteArraySerializer();\n-    private static final RecordHeaders V_1_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 1})});\n-    private static final RecordHeaders V_2_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 2})});\n+    private static final byte[] V_1_CHANGELOG_HEADER_VALUE = {(byte) 1};\n+    private static final byte[] V_2_CHANGELOG_HEADER_VALUE = {(byte) 2};\n+    private static final byte[] V_3_CHANGELOG_HEADER_VALUE = {(byte) 3};\n+    static final RecordHeaders CHANGELOG_HEADERS =\n+        new RecordHeaders(new Header[] {new RecordHeader(\"v\", V_3_CHANGELOG_HEADER_VALUE)});", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "844321f187bb9cd4641ce087b632181761af1e5f", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex 0390777ff6..2909e2763f 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -56,6 +56,8 @@ import java.util.function.Consumer;\n import java.util.function.Supplier;\n \n import static java.util.Objects.requireNonNull;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV0;\n+import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV1;\n import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.deserializeV3;\n import static org.apache.kafka.streams.state.internals.TimeOrderedKeyValueBufferChangelogDeserializationHelper.duckTypeV2;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446269130", "bodyText": "See the comment on this method for why we need to duck-type version 2. I pulled these deserializations into a helper class because all the extra branches pushed our cyclomatic complexity over the limit.\nBut I kept the first two branches here because they aren't pure functions. They perform a lookup in the buffer itself as part of converting the old format.", "author": "vvcephei", "createdAt": "2020-06-26T15:55:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -361,26 +366,20 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             contextualRecord.recordContext()\n                         )\n                     );\n-                } else if (V_2_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n-                    // in this case, the changelog value is a serialized BufferValue\n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMzODMxOA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446338318", "bodyText": "Could you clarify which comment are you referring to? I did not see any comments for the \"restoreBatch\" method..", "author": "guozhangwang", "createdAt": "2020-06-26T18:15:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2Mzc5OA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446363798", "bodyText": "Sorry, the comments in duckTypeV2.\nBasically, because we released three versions that would write data in the \"v3\" format, but with the \"v2\" flag, when we see the v2 flag, the data might be in v2 format or v3 format. The only way to tell is to just try to deserialize it in v2 format, and if we get an exception, then to try with v3 format.", "author": "vvcephei", "createdAt": "2020-06-26T19:12:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2OTEzMA=="}], "type": "inlineReview", "revised_code": {"commit": "844321f187bb9cd4641ce087b632181761af1e5f", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex 0390777ff6..2909e2763f 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -316,66 +318,51 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n             } else {\n                 final Header versionHeader = record.headers().lastHeader(\"v\");\n                 if (versionHeader == null) {\n-                    // in this case, the changelog value is just the serialized record value\n-                    final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n-                    final long time = timeAndValue.getLong();\n-                    final byte[] changelogValue = new byte[record.value().length - 8];\n-                    timeAndValue.get(changelogValue);\n-\n-                    final Change<byte[]> change = requireNonNull(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(changelogValue));\n-\n-                    final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n-                        record.timestamp(),\n-                        record.offset(),\n-                        record.partition(),\n-                        record.topic(),\n-                        record.headers()\n-                    );\n-\n-                    cleanPut(\n-                        time,\n-                        key,\n-                        new BufferValue(\n-                            index.containsKey(key)\n-                                ? internalPriorValueForBuffered(key)\n-                                : change.oldValue,\n-                            change.oldValue,\n-                            change.newValue,\n-                            recordContext\n-                        )\n-                    );\n-                } else if (Arrays.equals(versionHeader.value(), V_1_CHANGELOG_HEADER_VALUE)) {\n-                    // in this case, the changelog value is a serialized ContextualRecord\n-                    final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n-                    final long time = timeAndValue.getLong();\n-                    final byte[] changelogValue = new byte[record.value().length - 8];\n-                    timeAndValue.get(changelogValue);\n-\n-                    final ContextualRecord contextualRecord = ContextualRecord.deserialize(ByteBuffer.wrap(changelogValue));\n-                    final Change<byte[]> change = requireNonNull(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(contextualRecord.value()));\n-\n-                    cleanPut(\n-                        time,\n-                        key,\n-                        new BufferValue(\n-                            index.containsKey(key)\n-                                ? internalPriorValueForBuffered(key)\n-                                : change.oldValue,\n-                            change.oldValue,\n-                            change.newValue,\n-                            contextualRecord.recordContext()\n-                        )\n-                    );\n-                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n-\n-                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    // Version 0:\n+                    // value:\n+                    //  - buffer time\n+                    //  - old value\n+                    //  - new value\n+                    final byte[] previousBufferedValue = index.containsKey(key)\n+                        ? internalPriorValueForBuffered(key)\n+                        : null;\n+                    final DeserializationResult deserializationResult = deserializeV0(record, key, previousBufferedValue);\n                     cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n-\n                 } else if (Arrays.equals(versionHeader.value(), V_3_CHANGELOG_HEADER_VALUE)) {\n-\n+                    // Version 3:\n+                    // value:\n+                    //  - record context\n+                    //  - prior value\n+                    //  - old value\n+                    //  - new value\n+                    //  - buffer time\n                     final DeserializationResult deserializationResult = deserializeV3(record, key);\n                     cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n \n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+                    // Version 2:\n+                    // value:\n+                    //  - record context\n+                    //  - old value\n+                    //  - new value\n+                    //  - prior value\n+                    //  - buffer time\n+                    // NOTE: 2.4.0, 2.4.1, and 2.5.0 actually encode Version 3 formatted data,\n+                    // but still set the Version 2 flag, so to deserialize, we have to duck type.\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n+                } else if (Arrays.equals(versionHeader.value(), V_1_CHANGELOG_HEADER_VALUE)) {\n+                    // Version 1:\n+                    // value:\n+                    //  - buffer time\n+                    //  - record context\n+                    //  - old value\n+                    //  - new value\n+                    final byte[] previousBufferedValue = index.containsKey(key)\n+                        ? internalPriorValueForBuffered(key)\n+                        : null;\n+                    final DeserializationResult deserializationResult = deserializeV1(record, key, previousBufferedValue);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n                 } else {\n                     throw new IllegalArgumentException(\"Restoring apparently invalid changelog record: \" + record);\n                 }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MDQ1MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446270450", "bodyText": "If you look a few lines up, you'll see that we just serialized the \"old value\", so we don't need to serialize it again here.", "author": "vvcephei", "createdAt": "2020-06-26T15:58:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -481,8 +480,7 @@ public void put(final long time,\n         final BufferValue buffered = getBuffered(serializedKey);\n         final byte[] serializedPriorValue;\n         if (buffered == null) {\n-            final V priorValue = value.oldValue;\n-            serializedPriorValue = (priorValue == null) ? null : valueSerde.innerSerde().serializer().serialize(changelogTopic, priorValue);\n+            serializedPriorValue = serialChange.oldValue;", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MjI3MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446272270", "bodyText": "Don't need this anymore because start blocks until it's \"started\" now.", "author": "vvcephei", "createdAt": "2020-06-26T16:01:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java", "diffHunk": "@@ -104,10 +104,6 @@ public void shouldWorkWithRebalance() throws InterruptedException {\n             clients.add(smokeTestClient);\n             smokeTestClient.start(props);\n \n-            while (!clients.get(clients.size() - 1).started()) {\n-                Thread.sleep(100);\n-            }\n-", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java\nindex 0e4e78ff68..c2bd7a5d24 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/integration/SmokeTestDriverIntegrationTest.java\n\n@@ -104,6 +104,10 @@ public class SmokeTestDriverIntegrationTest {\n             clients.add(smokeTestClient);\n             smokeTestClient.start(props);\n \n+            while (!clients.get(clients.size() - 1).started()) {\n+                Thread.sleep(100);\n+            }\n+\n             // let the oldest client die of \"natural causes\"\n             if (clients.size() >= 3) {\n                 final SmokeTestClient client = clients.remove(0);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MjczNg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446272736", "bodyText": "Moved from FullChangeSerde because it is only used in this test.", "author": "vvcephei", "createdAt": "2020-06-26T16:02:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/FullChangeSerdeTest.java", "diffHunk": "@@ -19,17 +19,46 @@\n import org.apache.kafka.common.serialization.Serdes;\n import org.junit.Test;\n \n+import java.nio.ByteBuffer;\n+\n import static org.hamcrest.CoreMatchers.nullValue;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.core.Is.is;\n \n public class FullChangeSerdeTest {\n     private final FullChangeSerde<String> serde = FullChangeSerde.wrap(Serdes.String());\n \n+    /**\n+     * We used to serialize a Change into a single byte[]. Now, we don't anymore, but we still keep this logic here\n+     * so that we can produce the legacy format to test that we can still deserialize it.\n+     */\n+    private static byte[] mergeChangeArraysIntoSingleLegacyFormattedArray(final Change<byte[]> serialChange) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MzIzMw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446273233", "bodyText": "imported the headers from the production code, so that it'll stay current.", "author": "vvcephei", "createdAt": "2020-06-26T16:03:09Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java", "diffHunk": "@@ -56,14 +55,13 @@\n import static java.nio.charset.StandardCharsets.UTF_8;\n import static java.util.Arrays.asList;\n import static java.util.Collections.singletonList;\n+import static org.apache.kafka.streams.state.internals.InMemoryTimeOrderedKeyValueBuffer.CHANGELOG_HEADERS;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.Matchers.is;\n import static org.junit.Assert.fail;\n \n @RunWith(Parameterized.class)\n public class TimeOrderedKeyValueBufferTest<B extends TimeOrderedKeyValueBuffer<String, String>> {\n-    private static final RecordHeaders V_2_CHANGELOG_HEADERS =\n-        new RecordHeaders(new Header[] {new RecordHeader(\"v\", new byte[] {(byte) 2})});", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTE5Mw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446275193", "bodyText": "This was one of my major findings in KAFKA-10173. Because the test was serializing the \"old versions\" using code shared with the current logic, we could not detect when we accidentally changed the current serialization logic without bumping the version number.\nBy instead testing against fixed pre-serialized data, we should be a lot safer.\nI took inspiration from the way that Karsten reported the observed serialized data in the bug report. Hex-encoding the binary data makes the tests more readable than a long array of byte literals.", "author": "vvcephei", "createdAt": "2020-06-26T16:06:41Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferTest.java", "diffHunk": "@@ -372,12 +370,14 @@ public void shouldRestoreOldFormat() {\n \n         context.setRecordContext(new ProcessorRecordContext(0, 0, 0, \"\", null));\n \n-        final FullChangeSerde<String> serializer = FullChangeSerde.wrap(Serdes.String());\n+        // These serialized formats were captured by running version 2.1 code.\n+        // They verify that an upgrade from 2.1 will work.\n+        // Do not change them.\n+        final String toDeleteBinaryValue = \"0000000000000000FFFFFFFF00000006646F6F6D6564\";\n+        final String asdfBinaryValue = \"0000000000000002FFFFFFFF0000000471776572\";\n+        final String zxcvBinaryValue1 = \"00000000000000010000000870726576696F757300000005656F34696D\";\n+        final String zxcvBinaryValue2 = \"000000000000000100000005656F34696D000000046E657874\";", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTkzNQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446275935", "bodyText": "This is the version bump we should have done in 2.4.0. I'll backport this fix to the 2.4 branch.", "author": "vvcephei", "createdAt": "2020-06-26T16:08:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -361,26 +366,20 @@ private void restoreBatch(final Collection<ConsumerRecord<byte[], byte[]>> batch\n                             contextualRecord.recordContext()\n                         )\n                     );\n-                } else if (V_2_CHANGELOG_HEADERS.lastHeader(\"v\").equals(record.headers().lastHeader(\"v\"))) {\n-                    // in this case, the changelog value is a serialized BufferValue\n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n+\n+                } else if (Arrays.equals(versionHeader.value(), V_3_CHANGELOG_HEADER_VALUE)) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "844321f187bb9cd4641ce087b632181761af1e5f", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\nindex 0390777ff6..2909e2763f 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java\n\n@@ -316,66 +318,51 @@ public final class InMemoryTimeOrderedKeyValueBuffer<K, V> implements TimeOrdere\n             } else {\n                 final Header versionHeader = record.headers().lastHeader(\"v\");\n                 if (versionHeader == null) {\n-                    // in this case, the changelog value is just the serialized record value\n-                    final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n-                    final long time = timeAndValue.getLong();\n-                    final byte[] changelogValue = new byte[record.value().length - 8];\n-                    timeAndValue.get(changelogValue);\n-\n-                    final Change<byte[]> change = requireNonNull(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(changelogValue));\n-\n-                    final ProcessorRecordContext recordContext = new ProcessorRecordContext(\n-                        record.timestamp(),\n-                        record.offset(),\n-                        record.partition(),\n-                        record.topic(),\n-                        record.headers()\n-                    );\n-\n-                    cleanPut(\n-                        time,\n-                        key,\n-                        new BufferValue(\n-                            index.containsKey(key)\n-                                ? internalPriorValueForBuffered(key)\n-                                : change.oldValue,\n-                            change.oldValue,\n-                            change.newValue,\n-                            recordContext\n-                        )\n-                    );\n-                } else if (Arrays.equals(versionHeader.value(), V_1_CHANGELOG_HEADER_VALUE)) {\n-                    // in this case, the changelog value is a serialized ContextualRecord\n-                    final ByteBuffer timeAndValue = ByteBuffer.wrap(record.value());\n-                    final long time = timeAndValue.getLong();\n-                    final byte[] changelogValue = new byte[record.value().length - 8];\n-                    timeAndValue.get(changelogValue);\n-\n-                    final ContextualRecord contextualRecord = ContextualRecord.deserialize(ByteBuffer.wrap(changelogValue));\n-                    final Change<byte[]> change = requireNonNull(FullChangeSerde.decomposeLegacyFormattedArrayIntoChangeArrays(contextualRecord.value()));\n-\n-                    cleanPut(\n-                        time,\n-                        key,\n-                        new BufferValue(\n-                            index.containsKey(key)\n-                                ? internalPriorValueForBuffered(key)\n-                                : change.oldValue,\n-                            change.oldValue,\n-                            change.newValue,\n-                            contextualRecord.recordContext()\n-                        )\n-                    );\n-                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n-\n-                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    // Version 0:\n+                    // value:\n+                    //  - buffer time\n+                    //  - old value\n+                    //  - new value\n+                    final byte[] previousBufferedValue = index.containsKey(key)\n+                        ? internalPriorValueForBuffered(key)\n+                        : null;\n+                    final DeserializationResult deserializationResult = deserializeV0(record, key, previousBufferedValue);\n                     cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n-\n                 } else if (Arrays.equals(versionHeader.value(), V_3_CHANGELOG_HEADER_VALUE)) {\n-\n+                    // Version 3:\n+                    // value:\n+                    //  - record context\n+                    //  - prior value\n+                    //  - old value\n+                    //  - new value\n+                    //  - buffer time\n                     final DeserializationResult deserializationResult = deserializeV3(record, key);\n                     cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n \n+                } else if (Arrays.equals(versionHeader.value(), V_2_CHANGELOG_HEADER_VALUE)) {\n+                    // Version 2:\n+                    // value:\n+                    //  - record context\n+                    //  - old value\n+                    //  - new value\n+                    //  - prior value\n+                    //  - buffer time\n+                    // NOTE: 2.4.0, 2.4.1, and 2.5.0 actually encode Version 3 formatted data,\n+                    // but still set the Version 2 flag, so to deserialize, we have to duck type.\n+                    final DeserializationResult deserializationResult = duckTypeV2(record, key);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n+                } else if (Arrays.equals(versionHeader.value(), V_1_CHANGELOG_HEADER_VALUE)) {\n+                    // Version 1:\n+                    // value:\n+                    //  - buffer time\n+                    //  - record context\n+                    //  - old value\n+                    //  - new value\n+                    final byte[] previousBufferedValue = index.containsKey(key)\n+                        ? internalPriorValueForBuffered(key)\n+                        : null;\n+                    final DeserializationResult deserializationResult = deserializeV1(record, key, previousBufferedValue);\n+                    cleanPut(deserializationResult.time(), deserializationResult.key(), deserializationResult.bufferValue());\n                 } else {\n                     throw new IllegalArgumentException(\"Restoring apparently invalid changelog record: \" + record);\n                 }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NjY5Mg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446276692", "bodyText": "I inlined these utilities to make this class more \"portable\". I.e., so that we can copy-paste it into the upgrade-test modules without dragging in a bunch of extra dependencies.", "author": "vvcephei", "createdAt": "2020-06-26T16:09:34Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3ODMyNw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446278327", "bodyText": "This was another bug I happened to notice while scrutinizing this system test. createKafkaStreams was registering a state listener and exception handler. But the next line here was overriding the exception handler, so the one registered in createKafkaStreams was getting ignored. I noticed it because I registered a state listener here, which also caused the one registered in createKafkaStreams to get ignored.\nInlining solves this problem, and since createKafkaStreams had only one usage, it was needless anyway.", "author": "vvcephei", "createdAt": "2020-06-26T16:12:42Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3ODcyMA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446278720", "bodyText": "A new message we can look for to wait until the instance has completed joining the group.", "author": "vvcephei", "createdAt": "2020-06-26T16:13:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTQxMQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279411", "bodyText": "Found a missed condition, if the close timed out, there wouldn't be an exception, just a false return value.", "author": "vvcephei", "createdAt": "2020-06-26T16:14:52Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTczNw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279737", "bodyText": "I moved all these to the system test propFile() definition.", "author": "vvcephei", "createdAt": "2020-06-26T16:15:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-CLOSED\");\n-        }\n-        try {\n-            thread.join();\n-        } catch (final Exception ex) {\n-            // do not remove these printouts since they are needed for health scripts\n+        } else if (wasClosed) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-            // ignore\n+        } else {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't close\");\n         }\n     }\n \n     private Properties getStreamsConfig(final Properties props) {\n         final Properties fullProps = new Properties(props);\n         fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, \"SmokeTest\");\n         fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, \"SmokeTest-\" + name);\n-        fullProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);\n-        fullProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n-        fullProps.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);\n-        fullProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n-        fullProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        fullProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n-        fullProps.put(ProducerConfig.ACKS_CONFIG, \"all\");", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3OTg1Nw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446279857", "bodyText": "inlined above.", "author": "vvcephei", "createdAt": "2020-06-26T16:15:44Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -38,107 +37,128 @@\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n-import org.apache.kafka.test.TestUtils;\n \n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n public class SmokeTestClient extends SmokeTestUtil {\n \n     private final String name;\n \n-    private Thread thread;\n     private KafkaStreams streams;\n     private boolean uncaughtException = false;\n-    private boolean started;\n-    private boolean closed;\n+    private volatile boolean closed;\n \n-    public SmokeTestClient(final String name) {\n-        super();\n-        this.name = name;\n+    private static void addShutdownHook(final String name, final Runnable runnable) {\n+        if (name != null) {\n+            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n+        } else {\n+            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n+        }\n     }\n \n-    public boolean started() {\n-        return started;\n+    private static File tempDirectory() {\n+        final String prefix = \"kafka-\";\n+        final File file;\n+        try {\n+            file = Files.createTempDirectory(prefix).toFile();\n+        } catch (final IOException ex) {\n+            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n+        }\n+        file.deleteOnExit();\n+\n+        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n+            try {\n+                Utils.delete(file);\n+            } catch (final IOException e) {\n+                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n+                e.printStackTrace(System.out);\n+            }\n+        });\n+\n+        return file;\n+    }\n+\n+    public SmokeTestClient(final String name) {\n+        this.name = name;\n     }\n \n     public boolean closed() {\n         return closed;\n     }\n \n     public void start(final Properties streamsProperties) {\n-        streams = createKafkaStreams(streamsProperties);\n+        final Topology build = getTopology();\n+        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n+\n+        final CountDownLatch countDownLatch = new CountDownLatch(1);\n+        streams.setStateListener((newState, oldState) -> {\n+            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n+            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n+                countDownLatch.countDown();\n+            }\n+\n+            if (newState == KafkaStreams.State.NOT_RUNNING) {\n+                closed = true;\n+            }\n+        });\n+\n         streams.setUncaughtExceptionHandler((t, e) -> {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n+            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n+            e.printStackTrace(System.out);\n             uncaughtException = true;\n-            e.printStackTrace();\n+            streams.close(Duration.ofSeconds(30));\n         });\n \n-        Exit.addShutdownHook(\"streams-shutdown-hook\", () -> close());\n+        addShutdownHook(\"streams-shutdown-hook\", this::close);\n \n-        thread = new Thread(() -> streams.start());\n-        thread.start();\n+        streams.start();\n+        try {\n+            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n+                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n+            }\n+        } catch (final InterruptedException e) {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n+            e.printStackTrace(System.out);\n+        }\n+        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n+        System.out.println(name + \" started at \" + Instant.now());\n     }\n \n     public void closeAsync() {\n         streams.close(Duration.ZERO);\n     }\n \n     public void close() {\n-        streams.close(Duration.ofSeconds(5));\n-        // do not remove these printouts since they are needed for health scripts\n-        if (!uncaughtException) {\n+        final boolean wasClosed = streams.close(Duration.ofMinutes(1));\n+\n+        if (wasClosed && !uncaughtException) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-CLOSED\");\n-        }\n-        try {\n-            thread.join();\n-        } catch (final Exception ex) {\n-            // do not remove these printouts since they are needed for health scripts\n+        } else if (wasClosed) {\n             System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-            // ignore\n+        } else {\n+            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't close\");\n         }\n     }\n \n     private Properties getStreamsConfig(final Properties props) {\n         final Properties fullProps = new Properties(props);\n         fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, \"SmokeTest\");\n         fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, \"SmokeTest-\" + name);\n-        fullProps.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);\n-        fullProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n-        fullProps.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);\n-        fullProps.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);\n-        fullProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        fullProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n-        fullProps.put(ProducerConfig.ACKS_CONFIG, \"all\");\n-        fullProps.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());\n+        fullProps.put(StreamsConfig.STATE_DIR_CONFIG, tempDirectory().getAbsolutePath());\n         fullProps.putAll(props);\n         return fullProps;\n     }\n \n-    private KafkaStreams createKafkaStreams(final Properties props) {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\nindex c9d7bc749f..db243fdd84 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n\n@@ -37,15 +38,11 @@ import org.apache.kafka.streams.kstream.TimeWindows;\n import org.apache.kafka.streams.kstream.Windowed;\n import org.apache.kafka.streams.state.Stores;\n import org.apache.kafka.streams.state.WindowStore;\n+import org.apache.kafka.test.TestUtils;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n import java.time.Duration;\n import java.time.Instant;\n import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n \n import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MDMyMw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446280323", "bodyText": "Oops! I noticed the lack of a newline in the output. It didn't matter for the tests because the greps aren't bounded by line.", "author": "vvcephei", "createdAt": "2020-06-26T16:16:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -75,7 +75,7 @@ public void process(final Object key, final Object value) {\n \n                     @Override\n                     public void close() {\n-                        System.out.printf(\"Close processor for task %s\", context().taskId());\n+                        System.out.printf(\"Close processor for task %s%n\", context().taskId());", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java\nindex e8ec04c23c..990bfdb73f 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java\n\n@@ -75,7 +75,7 @@ public class SmokeTestUtil {\n \n                     @Override\n                     public void close() {\n-                        System.out.printf(\"Close processor for task %s%n\", context().taskId());\n+                        System.out.printf(\"Close processor for task %s\", context().taskId());\n                         System.out.println(\"processed \" + numRecordsProcessed + \" records\");\n                         final long processed;\n                         if (largestOffset >= smallestOffset) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI4MDg1Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446280856", "bodyText": "All of these are just copy-pasted from the main module.", "author": "vvcephei", "createdAt": "2020-06-26T16:17:40Z", "path": "streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java", "diffHunk": "@@ -0,0 +1,298 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.tests;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.common.utils.KafkaThread;\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.kstream.Consumed;\n+import org.apache.kafka.streams.kstream.Grouped;\n+import org.apache.kafka.streams.kstream.KGroupedStream;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Suppressed.BufferConfig;\n+import org.apache.kafka.streams.kstream.TimeWindows;\n+import org.apache.kafka.streams.kstream.Windowed;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.state.WindowStore;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n+\n+public class SmokeTestClient extends SmokeTestUtil {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java b/streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\ndeleted file mode 100644\nindex ced1369af3..0000000000\n--- a/streams/upgrade-system-tests-22/src/test/java/org/apache/kafka/streams/tests/SmokeTestClient.java\n+++ /dev/null\n\n@@ -1,298 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.tests;\n-\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.common.utils.KafkaThread;\n-import org.apache.kafka.common.utils.Utils;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KeyValue;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.Topology;\n-import org.apache.kafka.streams.kstream.Consumed;\n-import org.apache.kafka.streams.kstream.Grouped;\n-import org.apache.kafka.streams.kstream.KGroupedStream;\n-import org.apache.kafka.streams.kstream.KStream;\n-import org.apache.kafka.streams.kstream.KTable;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.kstream.Produced;\n-import org.apache.kafka.streams.kstream.Suppressed.BufferConfig;\n-import org.apache.kafka.streams.kstream.TimeWindows;\n-import org.apache.kafka.streams.kstream.Windowed;\n-import org.apache.kafka.streams.state.Stores;\n-import org.apache.kafka.streams.state.WindowStore;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.nio.file.Files;\n-import java.time.Duration;\n-import java.time.Instant;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.TimeUnit;\n-\n-import static org.apache.kafka.streams.kstream.Suppressed.untilWindowCloses;\n-\n-public class SmokeTestClient extends SmokeTestUtil {\n-\n-    private final String name;\n-\n-    private KafkaStreams streams;\n-    private boolean uncaughtException = false;\n-    private boolean started;\n-    private volatile boolean closed;\n-\n-    private static void addShutdownHook(final String name, final Runnable runnable) {\n-        if (name != null) {\n-            Runtime.getRuntime().addShutdownHook(KafkaThread.nonDaemon(name, runnable));\n-        } else {\n-            Runtime.getRuntime().addShutdownHook(new Thread(runnable));\n-        }\n-    }\n-\n-    private static File tempDirectory() {\n-        final String prefix = \"kafka-\";\n-        final File file;\n-        try {\n-            file = Files.createTempDirectory(prefix).toFile();\n-        } catch (final IOException ex) {\n-            throw new RuntimeException(\"Failed to create a temp dir\", ex);\n-        }\n-        file.deleteOnExit();\n-\n-        addShutdownHook(\"delete-temp-file-shutdown-hook\", () -> {\n-            try {\n-                Utils.delete(file);\n-            } catch (final IOException e) {\n-                System.out.println(\"Error deleting \" + file.getAbsolutePath());\n-                e.printStackTrace(System.out);\n-            }\n-        });\n-\n-        return file;\n-    }\n-\n-    public SmokeTestClient(final String name) {\n-        this.name = name;\n-    }\n-\n-    public boolean started() {\n-        return started;\n-    }\n-\n-    public boolean closed() {\n-        return closed;\n-    }\n-\n-    public void start(final Properties streamsProperties) {\n-        final Topology build = getTopology();\n-        streams = new KafkaStreams(build, getStreamsConfig(streamsProperties));\n-\n-        final CountDownLatch countDownLatch = new CountDownLatch(1);\n-        streams.setStateListener((newState, oldState) -> {\n-            System.out.printf(\"%s %s: %s -> %s%n\", name, Instant.now(), oldState, newState);\n-            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {\n-                started = true;\n-                countDownLatch.countDown();\n-            }\n-\n-            if (newState == KafkaStreams.State.NOT_RUNNING) {\n-                closed = true;\n-            }\n-        });\n-\n-        streams.setUncaughtExceptionHandler((t, e) -> {\n-            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-            System.out.println(name + \": FATAL: An unexpected exception is encountered on thread \" + t + \": \" + e);\n-            e.printStackTrace(System.out);\n-            uncaughtException = true;\n-            streams.close(Duration.ofSeconds(30));\n-        });\n-\n-        addShutdownHook(\"streams-shutdown-hook\", this::close);\n-\n-        streams.start();\n-        try {\n-            if (!countDownLatch.await(1, TimeUnit.MINUTES)) {\n-                System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't start in one minute\");\n-            }\n-        } catch (final InterruptedException e) {\n-            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: \" + e);\n-            e.printStackTrace(System.out);\n-        }\n-        System.out.println(name + \": SMOKE-TEST-CLIENT-STARTED\");\n-        System.out.println(name + \" started at \" + Instant.now());\n-    }\n-\n-    public void closeAsync() {\n-        streams.close(Duration.ZERO);\n-    }\n-\n-    public void close() {\n-        final boolean closed = streams.close(Duration.ofMinutes(1));\n-\n-        if (closed && !uncaughtException) {\n-            System.out.println(name + \": SMOKE-TEST-CLIENT-CLOSED\");\n-        } else if (closed) {\n-            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION\");\n-        } else {\n-            System.out.println(name + \": SMOKE-TEST-CLIENT-EXCEPTION: Didn't close\");\n-        }\n-    }\n-\n-    private Properties getStreamsConfig(final Properties props) {\n-        final Properties fullProps = new Properties(props);\n-        fullProps.put(StreamsConfig.APPLICATION_ID_CONFIG, \"SmokeTest\");\n-        fullProps.put(StreamsConfig.CLIENT_ID_CONFIG, \"SmokeTest-\" + name);\n-        fullProps.put(StreamsConfig.STATE_DIR_CONFIG, tempDirectory().getAbsolutePath());\n-        fullProps.putAll(props);\n-        return fullProps;\n-    }\n-\n-    public Topology getTopology() {\n-        final StreamsBuilder builder = new StreamsBuilder();\n-        final Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);\n-        final KStream<String, Integer> source = builder.stream(\"data\", stringIntConsumed);\n-        source.filterNot((k, v) -> k.equals(\"flush\"))\n-              .to(\"echo\", Produced.with(stringSerde, intSerde));\n-        final KStream<String, Integer> data = source.filter((key, value) -> value == null || value != END);\n-        data.process(SmokeTestUtil.printProcessorSupplier(\"data\", name));\n-\n-        // min\n-        final KGroupedStream<String, Integer> groupedData = data.groupByKey(Grouped.with(stringSerde, intSerde));\n-\n-        final KTable<Windowed<String>, Integer> minAggregation = groupedData\n-            .windowedBy(TimeWindows.of(Duration.ofDays(1)).grace(Duration.ofMinutes(1)))\n-            .aggregate(\n-                () -> Integer.MAX_VALUE,\n-                (aggKey, value, aggregate) -> (value < aggregate) ? value : aggregate,\n-                Materialized\n-                    .<String, Integer, WindowStore<Bytes, byte[]>>as(\"uwin-min\")\n-                    .withValueSerde(intSerde)\n-                    .withRetention(Duration.ofHours(25))\n-            );\n-\n-        streamify(minAggregation, \"min-raw\");\n-\n-        streamify(minAggregation.suppress(untilWindowCloses(BufferConfig.unbounded())), \"min-suppressed\");\n-\n-        minAggregation\n-            .toStream(new Unwindow<>())\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"min\", Produced.with(stringSerde, intSerde));\n-\n-        final KTable<Windowed<String>, Integer> smallWindowSum = groupedData\n-            .windowedBy(TimeWindows.of(Duration.ofSeconds(2)).advanceBy(Duration.ofSeconds(1)).grace(Duration.ofSeconds(30)))\n-            .reduce((l, r) -> l + r);\n-\n-        streamify(smallWindowSum, \"sws-raw\");\n-        streamify(smallWindowSum.suppress(untilWindowCloses(BufferConfig.unbounded())), \"sws-suppressed\");\n-\n-        final KTable<String, Integer> minTable = builder.table(\n-            \"min\",\n-            Consumed.with(stringSerde, intSerde),\n-            Materialized.as(\"minStoreName\"));\n-\n-        minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(\"min\", name));\n-\n-        // max\n-        groupedData\n-            .windowedBy(TimeWindows.of(Duration.ofDays(2)))\n-            .aggregate(\n-                () -> Integer.MIN_VALUE,\n-                (aggKey, value, aggregate) -> (value > aggregate) ? value : aggregate,\n-                Materialized.<String, Integer, WindowStore<Bytes, byte[]>>as(\"uwin-max\").withValueSerde(intSerde))\n-            .toStream(new Unwindow<>())\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"max\", Produced.with(stringSerde, intSerde));\n-\n-        final KTable<String, Integer> maxTable = builder.table(\n-            \"max\",\n-            Consumed.with(stringSerde, intSerde),\n-            Materialized.as(\"maxStoreName\"));\n-        maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(\"max\", name));\n-\n-        // sum\n-        groupedData\n-            .windowedBy(TimeWindows.of(Duration.ofDays(2)))\n-            .aggregate(\n-                () -> 0L,\n-                (aggKey, value, aggregate) -> (long) value + aggregate,\n-                Materialized.<String, Long, WindowStore<Bytes, byte[]>>as(\"win-sum\").withValueSerde(longSerde))\n-            .toStream(new Unwindow<>())\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"sum\", Produced.with(stringSerde, longSerde));\n-\n-        final Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);\n-        final KTable<String, Long> sumTable = builder.table(\"sum\", stringLongConsumed);\n-        sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(\"sum\", name));\n-\n-        // cnt\n-        groupedData\n-            .windowedBy(TimeWindows.of(Duration.ofDays(2)))\n-            .count(Materialized.as(\"uwin-cnt\"))\n-            .toStream(new Unwindow<>())\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"cnt\", Produced.with(stringSerde, longSerde));\n-\n-        final KTable<String, Long> cntTable = builder.table(\n-            \"cnt\",\n-            Consumed.with(stringSerde, longSerde),\n-            Materialized.as(\"cntStoreName\"));\n-        cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(\"cnt\", name));\n-\n-        // dif\n-        maxTable\n-            .join(\n-                minTable,\n-                (value1, value2) -> value1 - value2)\n-            .toStream()\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"dif\", Produced.with(stringSerde, intSerde));\n-\n-        // avg\n-        sumTable\n-            .join(\n-                cntTable,\n-                (value1, value2) -> (double) value1 / (double) value2)\n-            .toStream()\n-            .filterNot((k, v) -> k.equals(\"flush\"))\n-            .to(\"avg\", Produced.with(stringSerde, doubleSerde));\n-\n-        // test repartition\n-        final Agg agg = new Agg();\n-        cntTable.groupBy(agg.selector(), Grouped.with(stringSerde, longSerde))\n-                .aggregate(agg.init(), agg.adder(), agg.remover(),\n-                           Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(\"cntByCnt\"))\n-                               .withKeySerde(Serdes.String())\n-                               .withValueSerde(Serdes.Long()))\n-                .toStream()\n-                .to(\"tagg\", Produced.with(stringSerde, longSerde));\n-\n-        return builder.build();\n-    }\n-\n-    private static void streamify(final KTable<Windowed<String>, Integer> windowedTable, final String topic) {\n-        windowedTable\n-            .toStream()\n-            .filterNot((k, v) -> k.key().equals(\"flush\"))\n-            .map((key, value) -> new KeyValue<>(key.toString(), value))\n-            .to(topic, Produced.with(stringSerde, intSerde));\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDc1NQ==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446340755", "bodyText": "I'm assuming 22..25 client / drive code are all copy-pastes here so I skipped reviewing them. LMK if they aren't.", "author": "guozhangwang", "createdAt": "2020-06-26T18:20:21Z", "path": "streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.tests;\n+\n+import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.StreamsConfig;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+import static org.apache.kafka.streams.tests.SmokeTestDriver.generate;\n+import static org.apache.kafka.streams.tests.SmokeTestDriver.generatePerpetually;\n+\n+public class StreamsSmokeTest {", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NDE5Ng==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446364196", "bodyText": "That's correct.", "author": "vvcephei", "createdAt": "2020-06-26T19:13:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0MDc1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "c4200998e5361bc2500c66cfac4fd64045e20add", "chunk": "diff --git a/streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java b/streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java\ndeleted file mode 100644\nindex 07c7d5d82e..0000000000\n--- a/streams/upgrade-system-tests-25/src/test/java/org/apache/kafka/streams/tests/StreamsSmokeTest.java\n+++ /dev/null\n\n@@ -1,99 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.tests;\n-\n-import org.apache.kafka.common.utils.Utils;\n-import org.apache.kafka.streams.StreamsConfig;\n-\n-import java.io.IOException;\n-import java.time.Duration;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.Set;\n-import java.util.UUID;\n-\n-import static org.apache.kafka.streams.tests.SmokeTestDriver.generate;\n-import static org.apache.kafka.streams.tests.SmokeTestDriver.generatePerpetually;\n-\n-public class StreamsSmokeTest {\n-\n-    /**\n-     *  args ::= kafka propFileName command disableAutoTerminate\n-     *  command := \"run\" | \"process\"\n-     *\n-     * @param args\n-     */\n-    public static void main(final String[] args) throws IOException {\n-        if (args.length < 2) {\n-            System.err.println(\"StreamsSmokeTest are expecting two parameters: propFile, command; but only see \" + args.length + \" parameter\");\n-            System.exit(1);\n-        }\n-\n-        final String propFileName = args[0];\n-        final String command = args[1];\n-        final boolean disableAutoTerminate = args.length > 2;\n-\n-        final Properties streamsProperties = Utils.loadProps(propFileName);\n-        final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n-        final String processingGuarantee = streamsProperties.getProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG);\n-\n-        if (kafka == null) {\n-            System.err.println(\"No bootstrap kafka servers specified in \" + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n-            System.exit(1);\n-        }\n-\n-        if (\"process\".equals(command)) {\n-            if (!StreamsConfig.AT_LEAST_ONCE.equals(processingGuarantee) &&\n-                !StreamsConfig.EXACTLY_ONCE.equals(processingGuarantee)) {\n-\n-                System.err.println(\"processingGuarantee must be either \" + StreamsConfig.AT_LEAST_ONCE + \" or \" +\n-                    StreamsConfig.EXACTLY_ONCE);\n-\n-                System.exit(1);\n-            }\n-        }\n-\n-        System.out.println(\"StreamsTest instance started (StreamsSmokeTest)\");\n-        System.out.println(\"command=\" + command);\n-        System.out.println(\"props=\" + streamsProperties);\n-        System.out.println(\"disableAutoTerminate=\" + disableAutoTerminate);\n-\n-        switch (command) {\n-            case \"run\":\n-                // this starts the driver (data generation and result verification)\n-                final int numKeys = 10;\n-                final int maxRecordsPerKey = 500;\n-                if (disableAutoTerminate) {\n-                    generatePerpetually(kafka, numKeys, maxRecordsPerKey);\n-                } else {\n-                    // slow down data production to span 30 seconds so that system tests have time to\n-                    // do their bounces, etc.\n-                    final Map<String, Set<Integer>> allData =\n-                        generate(kafka, numKeys, maxRecordsPerKey, Duration.ofSeconds(30));\n-                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);\n-                }\n-                break;\n-            case \"process\":\n-                // this starts the stream processing app\n-                new SmokeTestClient(UUID.randomUUID().toString()).start(streamsProperties);\n-                break;\n-            default:\n-                System.out.println(\"unknown command: \" + command);\n-        }\n-    }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NDQ0Mw==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446344443", "bodyText": "Some docs, either here or directly inside InMemoryTimeOrderedKeyValueBuffer.java explaining the format difference would help a lot. You can see some examples like object GroupMetadataManager", "author": "guozhangwang", "createdAt": "2020-06-26T18:28:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.kstream.internals.Change;\n+import org.apache.kafka.streams.kstream.internals.FullChangeSerde;\n+\n+import java.nio.ByteBuffer;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+final class TimeOrderedKeyValueBufferChangelogDeserializationHelper {\n+    private TimeOrderedKeyValueBufferChangelogDeserializationHelper() {}\n+\n+    static final class DeserializationResult {\n+        private final long time;\n+        private final Bytes key;\n+        private final BufferValue bufferValue;\n+\n+        private DeserializationResult(final long time, final Bytes key, final BufferValue bufferValue) {\n+            this.time = time;\n+            this.key = key;\n+            this.bufferValue = bufferValue;\n+        }\n+\n+        long time() {\n+            return time;\n+        }\n+\n+        Bytes key() {\n+            return key;\n+        }\n+\n+        BufferValue bufferValue() {\n+            return bufferValue;\n+        }\n+    }\n+\n+\n+    static DeserializationResult duckTypeV2(final ConsumerRecord<byte[], byte[]> record, final Bytes key) {\n+        DeserializationResult deserializationResult = null;\n+        RuntimeException v2DeserializationException = null;\n+        RuntimeException v3DeserializationException = null;\n+        try {\n+            deserializationResult = deserializeV2(record, key);\n+        } catch (final RuntimeException e) {\n+            v2DeserializationException = e;\n+        }\n+        // versions 2.4.0, 2.4.1, and 2.5.0 would have erroneously encoded a V3 record with the\n+        // V2 header, so we'll try duck-typing to see if this is decodable as V3\n+        if (deserializationResult == null) {\n+            try {\n+                deserializationResult = deserializeV3(record, key);\n+            } catch (final RuntimeException e) {\n+                v3DeserializationException = e;\n+            }\n+        }\n+\n+        if (deserializationResult == null) {\n+            // ok, it wasn't V3 either. Throw both exceptions:\n+            final RuntimeException exception =\n+                new RuntimeException(\"Couldn't deserialize record as v2 or v3: \" + record,\n+                                     v2DeserializationException);\n+            exception.addSuppressed(v3DeserializationException);\n+            throw exception;\n+        }\n+        return deserializationResult;\n+    }\n+\n+    private static DeserializationResult deserializeV2(final ConsumerRecord<byte[], byte[]> record,", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NDM2NA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446364364", "bodyText": "sure thing!", "author": "vvcephei", "createdAt": "2020-06-26T19:14:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NDQ0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "844321f187bb9cd4641ce087b632181761af1e5f", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java\nindex 14ec7a01ce..74489c230b 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/TimeOrderedKeyValueBufferChangelogDeserializationHelper.java\n\n@@ -18,8 +18,10 @@ package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.common.utils.Utils;\n import org.apache.kafka.streams.kstream.internals.Change;\n import org.apache.kafka.streams.kstream.internals.FullChangeSerde;\n+import org.apache.kafka.streams.processor.internals.ProcessorRecordContext;\n \n import java.nio.ByteBuffer;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446345790", "bodyText": "I'm just thinking, maybe we should encode headers to tombstones too in case in the future we changed the semantics of tombstones?", "author": "guozhangwang", "createdAt": "2020-06-26T18:31:59Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryTimeOrderedKeyValueBuffer.java", "diffHunk": "@@ -258,34 +263,43 @@ private void logValue(final Bytes key, final BufferKey bufferKey, final BufferVa\n         final int sizeOfBufferTime = Long.BYTES;\n         final ByteBuffer buffer = value.serialize(sizeOfBufferTime);\n         buffer.putLong(bufferKey.time());\n-\n+        final byte[] array = buffer.array();\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                buffer.array(),\n-                V_2_CHANGELOG_HEADERS,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            array,\n+            CHANGELOG_HEADERS,\n+            partition,\n+            null,\n+            KEY_SERIALIZER,\n+            VALUE_SERIALIZER\n         );\n     }\n \n     private void logTombstone(final Bytes key) {\n         ((RecordCollector.Supplier) context).recordCollector().send(\n-                changelogTopic,\n-                key,\n-                null,\n-                null,\n-                partition,\n-                null,\n-                KEY_SERIALIZER,\n-                VALUE_SERIALIZER\n+            changelogTopic,\n+            key,\n+            null,\n+            null,", "originalCommit": "7b8deb8d98fbf435886cbbcc9093016ee8dd2004", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NTk5Mg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446365992", "bodyText": "I remember considering this when I added the first version header. The reason I didn't is that, since the initial version didn't have any headers, even if we change the tombstone format in the future, we'll always have to interpret a \"no header, null value\" record as being a \"legacy format\" tombstone, just like we have to interpret a \"no header, non-null value\" as being a \"legacy format\" data record.\nYou can think of \"no header\" as indicating \"version 0\". Since we haven't changed the format of tombstones yet, there's no value in adding a \"version 1\" flag. We should just wait until we do need to make such a change (if ever).", "author": "vvcephei", "createdAt": "2020-06-26T19:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ2MTgxMg==", "url": "https://github.com/apache/kafka/pull/8905#discussion_r446461812", "bodyText": "SG", "author": "guozhangwang", "createdAt": "2020-06-27T00:37:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0NTc5MA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "844321f187bb9cd4641ce087b632181761af1e5f", "url": "https://github.com/apache/kafka/commit/844321f187bb9cd4641ce087b632181761af1e5f", "message": "fix attempt to allocate arbitrary sized array", "committedDate": "2020-06-26T21:24:24Z", "type": "commit"}, {"oid": "c4200998e5361bc2500c66cfac4fd64045e20add", "url": "https://github.com/apache/kafka/commit/c4200998e5361bc2500c66cfac4fd64045e20add", "message": "factor out system test changes", "committedDate": "2020-06-26T21:59:20Z", "type": "commit"}, {"oid": "f5cc0b72f12cb3ee2efb207913240539b1ec1d61", "url": "https://github.com/apache/kafka/commit/f5cc0b72f12cb3ee2efb207913240539b1ec1d61", "message": "style", "committedDate": "2020-06-26T23:56:42Z", "type": "commit"}]}