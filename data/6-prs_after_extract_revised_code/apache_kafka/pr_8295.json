{"pr_number": 8295, "pr_title": "KAFKA-9627: Replace ListOffset request/response with automated protocol", "pr_createdAt": "2020-03-13T11:56:28Z", "pr_url": "https://github.com/apache/kafka/pull/8295", "timeline": [{"oid": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "url": "https://github.com/apache/kafka/commit/65ac2f1501fd6ee863a873cd93b64a1534d09461", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-07-02T16:07:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NTU1MA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449575550", "bodyText": "I just noticed that we don't ensure that all futures of the current broker are completed. It would be great to ensure it by using completeUnrealizedFutures method if retryTopicPartitionOffsets is empty. We already do this in alterReplicaLogDirs() if you want to see an example.", "author": "dajac", "createdAt": "2020-07-03T13:09:39Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3883,21 +3886,24 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (Entry<TopicPartition, PartitionData> result : response.responseData().entrySet()) {\n-                        TopicPartition tp = result.getKey();\n-                        PartitionData partitionData = result.getValue();\n-\n-                        KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n-                        Errors error = partitionData.error;\n-                        OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n-                        if (offsetRequestSpec == null) {\n-                            future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n-                        } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n-                            retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(new ListOffsetsResultInfo(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch));\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                        for (ListOffsetPartitionResponse partition : topic.partitions()) {\n+                            TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                            KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n+                            Errors error = Errors.forCode(partition.errorCode());\n+                            OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n+                            if (offsetRequestSpec == null) {\n+                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n+                            } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n+                                retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n+                            } else if (error == Errors.NONE) {\n+                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH) \n+                                        ? Optional.empty() \n+                                        : Optional.of(partition.leaderEpoch());\n+                                future.complete(new ListOffsetsResultInfo(partition.offset(), partition.timestamp(), leaderEpoch));\n+                            } else {\n+                                future.completeExceptionally(error.exception());\n+                            }\n                         }\n                     }", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\nindex 8100c2e036..9acd341250 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n\n@@ -3886,19 +3989,19 @@ public class KafkaAdminClient extends AdminClient {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                    for (ListOffsetTopicResponse topic : response.topics()) {\n                         for (ListOffsetPartitionResponse partition : topic.partitions()) {\n                             TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n                             KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n                             Errors error = Errors.forCode(partition.errorCode());\n                             OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n                             if (offsetRequestSpec == null) {\n-                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n+                                log.warn(\"Server response mentioned unknown topic partition {}\", tp);\n                             } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n                                 retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n                             } else if (error == Errors.NONE) {\n-                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH) \n-                                        ? Optional.empty() \n+                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH)\n+                                        ? Optional.empty()\n                                         : Optional.of(partition.leaderEpoch());\n                                 future.complete(new ListOffsetsResultInfo(partition.offset(), partition.timestamp(), leaderEpoch));\n                             } else {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU3NzYzNg==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449577636", "bodyText": "This is not related to your PR at all. It seems that if offsetRequestSpec is null here, future will be null as well cause futures is initialised based on topicPartitionOffsets. If it turns out to be correct, it may be better to just log a warning here like we do in createTopics().", "author": "dajac", "createdAt": "2020-07-03T13:14:27Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -3883,21 +3886,24 @@ void handleResponse(AbstractResponse abstractResponse) {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (Entry<TopicPartition, PartitionData> result : response.responseData().entrySet()) {\n-                        TopicPartition tp = result.getKey();\n-                        PartitionData partitionData = result.getValue();\n-\n-                        KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n-                        Errors error = partitionData.error;\n-                        OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n-                        if (offsetRequestSpec == null) {\n-                            future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n-                        } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n-                            retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n-                        } else if (error == Errors.NONE) {\n-                            future.complete(new ListOffsetsResultInfo(partitionData.offset, partitionData.timestamp, partitionData.leaderEpoch));\n-                        } else {\n-                            future.completeExceptionally(error.exception());\n+                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                        for (ListOffsetPartitionResponse partition : topic.partitions()) {\n+                            TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                            KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n+                            Errors error = Errors.forCode(partition.errorCode());\n+                            OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n+                            if (offsetRequestSpec == null) {\n+                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\nindex 8100c2e036..9acd341250 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n\n@@ -3886,19 +3989,19 @@ public class KafkaAdminClient extends AdminClient {\n                     ListOffsetResponse response = (ListOffsetResponse) abstractResponse;\n                     Map<TopicPartition, OffsetSpec> retryTopicPartitionOffsets = new HashMap<>();\n \n-                    for (ListOffsetTopicResponse topic : response.responseData()) {\n+                    for (ListOffsetTopicResponse topic : response.topics()) {\n                         for (ListOffsetPartitionResponse partition : topic.partitions()) {\n                             TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n                             KafkaFutureImpl<ListOffsetsResultInfo> future = futures.get(tp);\n                             Errors error = Errors.forCode(partition.errorCode());\n                             OffsetSpec offsetRequestSpec = topicPartitionOffsets.get(tp);\n                             if (offsetRequestSpec == null) {\n-                                future.completeExceptionally(new KafkaException(\"Unexpected topic partition \" + tp + \" in broker response!\"));\n+                                log.warn(\"Server response mentioned unknown topic partition {}\", tp);\n                             } else if (MetadataOperationContext.shouldRefreshMetadata(error)) {\n                                 retryTopicPartitionOffsets.put(tp, offsetRequestSpec);\n                             } else if (error == Errors.NONE) {\n-                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH) \n-                                        ? Optional.empty() \n+                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH)\n+                                        ? Optional.empty()\n                                         : Optional.of(partition.leaderEpoch());\n                                 future.complete(new ListOffsetsResultInfo(partition.offset(), partition.timestamp(), leaderEpoch));\n                             } else {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449588910", "bodyText": "This conversion is a bit unfortunate as we have to traverse all the partitions again to build the List<ListOffsetTopic>. Instead, we could compute it directly within groupListOffsetRequests and could receive Map<Node, List<ListOffsetTopic> directly here. That seems doable but I may have missed something.", "author": "dajac", "createdAt": "2020-07-03T13:39:28Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -965,11 +994,11 @@ public void onFailure(RuntimeException e) {\n      * @return A response which can be polled to obtain the corresponding timestamps and offsets.\n      */\n     private RequestFuture<ListOffsetResult> sendListOffsetRequest(final Node node,\n-                                                                  final Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch,\n+                                                                  final Map<TopicPartition, ListOffsetPartition> timestampsToSearch,\n                                                                   boolean requireTimestamp) {\n         ListOffsetRequest.Builder builder = ListOffsetRequest.Builder\n                 .forConsumer(requireTimestamp, isolationLevel)\n-                .setTargetTimes(timestampsToSearch);\n+                .setTargetTimes(toListOffsetTopics(timestampsToSearch));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjMyNDA2OQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r452324069", "bodyText": "I initially tried to do that but there's a couple of intermediate collections using TopicPartition in these methods and it makes it really hard to update them. For example:\n\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L735\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L880\nhttps://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java#L887", "author": "mimaison", "createdAt": "2020-07-09T15:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg0OTM1NQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r454849355", "bodyText": "I had a look at this and your are right. It seems that keeping TopicPartition is better and difficult to change. In this case, have you considered pushing the conversion to the Builder by providing an overload of setTargetTimes which accepts a Map<TopicPartition, ListOffsetPartition>? That could make the code in the Fetcher a bit cleaner.", "author": "dajac", "createdAt": "2020-07-15T07:30:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU4ODkxMA=="}], "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\nindex 1323b3bb5c..5581f25b5f 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n\n@@ -998,7 +979,7 @@ public class Fetcher<K, V> implements Closeable {\n                                                                   boolean requireTimestamp) {\n         ListOffsetRequest.Builder builder = ListOffsetRequest.Builder\n                 .forConsumer(requireTimestamp, isolationLevel)\n-                .setTargetTimes(toListOffsetTopics(timestampsToSearch));\n+                .setTargetTimes(ListOffsetRequest.toListOffsetTopics(timestampsToSearch));\n \n         log.debug(\"Sending ListOffsetRequest {} to broker {}\", builder, node);\n         return client.send(node, builder)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449597967", "bodyText": "I wonder if grouping by TopicPartition is really necessary here. We iterate over timestampsToSearch to get the ListOffsetPartitionResponse for the current TopicPartition but we could also iterate over the response set directly and thus avoid grouping. Moreover, we always assume that the result set contains the TopicPartition that we are interested in so it would not change the semantic. Am I missing something? What do you think?", "author": "dajac", "createdAt": "2020-07-03T13:59:48Z", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -994,30 +1023,29 @@ public void onSuccess(ClientResponse response, RequestFuture<ListOffsetResult> f\n      *               value of each partition may be null only for v0. In v1 and later the ListOffset API would not\n      *               return a null timestamp (-1 is returned instead when necessary).\n      */\n-    private void handleListOffsetResponse(Map<TopicPartition, ListOffsetRequest.PartitionData> timestampsToSearch,\n+    private void handleListOffsetResponse(Map<TopicPartition, ListOffsetPartition> timestampsToSearch,\n                                           ListOffsetResponse listOffsetResponse,\n                                           RequestFuture<ListOffsetResult> future) {\n         Map<TopicPartition, ListOffsetData> fetchedOffsets = new HashMap<>();\n         Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         Set<String> unauthorizedTopics = new HashSet<>();\n \n-        for (Map.Entry<TopicPartition, ListOffsetRequest.PartitionData> entry : timestampsToSearch.entrySet()) {\n+        Map<TopicPartition, ListOffsetPartitionResponse> partitionsData = byTopicPartitions(listOffsetResponse.responseData());", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM3NDkyNQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r452374925", "bodyText": "We now added logic in the AdminClient to handle partial responses from brokers (based on #8295 (comment)). Shouldn't we do the same here instead of assuming the response is always complete? I'm not even sure if we should retry if a resource is missing from the response but we could at least log it instead of hitting a NPE.", "author": "mimaison", "createdAt": "2020-07-09T17:23:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg1MzQxOA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r454853418", "bodyText": "I agree that we should at minimum avoid hitting a NPE.", "author": "dajac", "createdAt": "2020-07-15T07:38:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTY3MTE0MA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r455671140", "bodyText": "I actually switched logic to loop on the response as you initially suggested", "author": "mimaison", "createdAt": "2020-07-16T10:00:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTU5Nzk2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\nindex 1323b3bb5c..5581f25b5f 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java\n\n@@ -1023,79 +1003,78 @@ public class Fetcher<K, V> implements Closeable {\n      *               value of each partition may be null only for v0. In v1 and later the ListOffset API would not\n      *               return a null timestamp (-1 is returned instead when necessary).\n      */\n-    private void handleListOffsetResponse(Map<TopicPartition, ListOffsetPartition> timestampsToSearch,\n-                                          ListOffsetResponse listOffsetResponse,\n+    private void handleListOffsetResponse(ListOffsetResponse listOffsetResponse,\n                                           RequestFuture<ListOffsetResult> future) {\n         Map<TopicPartition, ListOffsetData> fetchedOffsets = new HashMap<>();\n         Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         Set<String> unauthorizedTopics = new HashSet<>();\n \n-        Map<TopicPartition, ListOffsetPartitionResponse> partitionsData = byTopicPartitions(listOffsetResponse.responseData());\n-        for (Map.Entry<TopicPartition, ListOffsetPartition> entry : timestampsToSearch.entrySet()) {\n-            TopicPartition topicPartition = entry.getKey();\n-            ListOffsetPartitionResponse partitionData = partitionsData.get(topicPartition);\n-            Errors error = Errors.forCode(partitionData.errorCode());\n-            switch (error) {\n-                case NONE:\n-                    if (!partitionData.oldStyleOffsets().isEmpty()) {\n-                        // Handle v0 response with offsets\n-                        long offset;\n-                        if (partitionData.oldStyleOffsets().size() > 1) {\n-                            future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n-                                                                       partitionData.oldStyleOffsets().size()));\n-                            return;\n+        for (ListOffsetTopicResponse topic : listOffsetResponse.topics()) {\n+            for (ListOffsetPartitionResponse partition : topic.partitions()) {\n+                TopicPartition topicPartition = new TopicPartition(topic.name(), partition.partitionIndex());\n+                Errors error = Errors.forCode(partition.errorCode());\n+                switch (error) {\n+                    case NONE:\n+                        if (!partition.oldStyleOffsets().isEmpty()) {\n+                            // Handle v0 response with offsets\n+                            long offset;\n+                            if (partition.oldStyleOffsets().size() > 1) {\n+                                future.raise(new IllegalStateException(\"Unexpected partitionData response of length \" +\n+                                        partition.oldStyleOffsets().size()));\n+                                return;\n+                            } else {\n+                                offset = partition.oldStyleOffsets().get(0);\n+                            }\n+                            log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n+                                topicPartition, offset);\n+                            if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                                ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n+                                fetchedOffsets.put(topicPartition, offsetData);\n+                            }\n                         } else {\n-                            offset = partitionData.oldStyleOffsets().get(0);\n-                        }\n-                        log.debug(\"Handling v0 ListOffsetResponse response for {}. Fetched offset {}\",\n-                            topicPartition, offset);\n-                        if (offset != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                            ListOffsetData offsetData = new ListOffsetData(offset, null, Optional.empty());\n-                            fetchedOffsets.put(topicPartition, offsetData);\n-                        }\n-                    } else {\n-                        // Handle v1 and later response or v0 without offsets\n-                        log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n-                            topicPartition, partitionData.offset(), partitionData.timestamp());\n-                        if (partitionData.offset() != ListOffsetResponse.UNKNOWN_OFFSET) {\n-                            Optional<Integer> leaderEpoch = (partitionData.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH)\n-                                    ? Optional.empty()\n-                                    : Optional.of(partitionData.leaderEpoch());\n-                            ListOffsetData offsetData = new ListOffsetData(partitionData.offset(), partitionData.timestamp(),\n-                                leaderEpoch);\n-                            fetchedOffsets.put(topicPartition, offsetData);\n+                            // Handle v1 and later response or v0 without offsets\n+                            log.debug(\"Handling ListOffsetResponse response for {}. Fetched offset {}, timestamp {}\",\n+                                topicPartition, partition.offset(), partition.timestamp());\n+                            if (partition.offset() != ListOffsetResponse.UNKNOWN_OFFSET) {\n+                                Optional<Integer> leaderEpoch = (partition.leaderEpoch() == ListOffsetResponse.UNKNOWN_EPOCH)\n+                                        ? Optional.empty()\n+                                        : Optional.of(partition.leaderEpoch());\n+                                ListOffsetData offsetData = new ListOffsetData(partition.offset(), partition.timestamp(),\n+                                    leaderEpoch);\n+                                fetchedOffsets.put(topicPartition, offsetData);\n+                            }\n                         }\n-                    }\n-                    break;\n-                case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n-                    // The message format on the broker side is before 0.10.0, which means it does not\n-                    // support timestamps. We treat this case the same as if we weren't able to find an\n-                    // offset corresponding to the requested timestamp and leave it out of the result.\n-                    log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n-                                  \"is before 0.10.0\", topicPartition);\n-                    break;\n-                case NOT_LEADER_FOR_PARTITION:\n-                case REPLICA_NOT_AVAILABLE:\n-                case KAFKA_STORAGE_ERROR:\n-                case OFFSET_NOT_AVAILABLE:\n-                case LEADER_NOT_AVAILABLE:\n-                case FENCED_LEADER_EPOCH:\n-                case UNKNOWN_LEADER_EPOCH:\n-                    log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n-                        topicPartition, error);\n-                    partitionsToRetry.add(topicPartition);\n-                    break;\n-                case UNKNOWN_TOPIC_OR_PARTITION:\n-                    log.warn(\"Received unknown topic or partition error in ListOffset request for partition {}\", topicPartition);\n-                    partitionsToRetry.add(topicPartition);\n-                    break;\n-                case TOPIC_AUTHORIZATION_FAILED:\n-                    unauthorizedTopics.add(topicPartition.topic());\n-                    break;\n-                default:\n-                    log.warn(\"Attempt to fetch offsets for partition {} failed due to unexpected exception: {}, retrying.\",\n-                        topicPartition, error.message());\n-                    partitionsToRetry.add(topicPartition);\n+                        break;\n+                    case UNSUPPORTED_FOR_MESSAGE_FORMAT:\n+                        // The message format on the broker side is before 0.10.0, which means it does not\n+                        // support timestamps. We treat this case the same as if we weren't able to find an\n+                        // offset corresponding to the requested timestamp and leave it out of the result.\n+                        log.debug(\"Cannot search by timestamp for partition {} because the message format version \" +\n+                                      \"is before 0.10.0\", topicPartition);\n+                        break;\n+                    case NOT_LEADER_OR_FOLLOWER:\n+                    case REPLICA_NOT_AVAILABLE:\n+                    case KAFKA_STORAGE_ERROR:\n+                    case OFFSET_NOT_AVAILABLE:\n+                    case LEADER_NOT_AVAILABLE:\n+                    case FENCED_LEADER_EPOCH:\n+                    case UNKNOWN_LEADER_EPOCH:\n+                        log.debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",\n+                            topicPartition, error);\n+                        partitionsToRetry.add(topicPartition);\n+                        break;\n+                    case UNKNOWN_TOPIC_OR_PARTITION:\n+                        log.warn(\"Received unknown topic or partition error in ListOffset request for partition {}\", topicPartition);\n+                        partitionsToRetry.add(topicPartition);\n+                        break;\n+                    case TOPIC_AUTHORIZATION_FAILED:\n+                        unauthorizedTopics.add(topicPartition.topic());\n+                        break;\n+                    default:\n+                        log.warn(\"Attempt to fetch offsets for partition {} failed due to unexpected exception: {}, retrying.\",\n+                            topicPartition, error.message());\n+                        partitionsToRetry.add(topicPartition);\n+                }\n             }\n         }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwMzY4NA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449603684", "bodyText": "It would be great if we could add unit tests for this method and perhaps others as well.", "author": "dajac", "createdAt": "2020-07-03T14:12:19Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java", "diffHunk": "@@ -156,145 +66,98 @@ private Builder(short oldestAllowedVersion,\n                         int replicaId,\n                         IsolationLevel isolationLevel) {\n             super(ApiKeys.LIST_OFFSETS, oldestAllowedVersion, latestAllowedVersion);\n-            this.replicaId = replicaId;\n-            this.isolationLevel = isolationLevel;\n+            data = new ListOffsetRequestData()\n+                    .setIsolationLevel(isolationLevel.id())\n+                    .setReplicaId(replicaId);\n         }\n \n-        public Builder setTargetTimes(Map<TopicPartition, PartitionData> partitionTimestamps) {\n-            this.partitionTimestamps = partitionTimestamps;\n+        public Builder setTargetTimes(List<ListOffsetTopic> topics) {\n+            data.setTopics(topics);\n             return this;\n         }\n \n         @Override\n         public ListOffsetRequest build(short version) {\n-            return new ListOffsetRequest(replicaId, partitionTimestamps, isolationLevel, version);\n+            return new ListOffsetRequest(version, data);\n         }\n \n         @Override\n         public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"(type=ListOffsetRequest\")\n-               .append(\", replicaId=\").append(replicaId);\n-            if (partitionTimestamps != null) {\n-                bld.append(\", partitionTimestamps=\").append(partitionTimestamps);\n-            }\n-            bld.append(\", isolationLevel=\").append(isolationLevel);\n-            bld.append(\")\");\n-            return bld.toString();\n-        }\n-    }\n-\n-    public static final class PartitionData {\n-        public final long timestamp;\n-        public final int maxNumOffsets; // only supported in v0\n-        public final Optional<Integer> currentLeaderEpoch;\n-\n-        private PartitionData(long timestamp, int maxNumOffsets, Optional<Integer> currentLeaderEpoch) {\n-            this.timestamp = timestamp;\n-            this.maxNumOffsets = maxNumOffsets;\n-            this.currentLeaderEpoch = currentLeaderEpoch;\n-        }\n-\n-        // For V0\n-        public PartitionData(long timestamp, int maxNumOffsets) {\n-            this(timestamp, maxNumOffsets, Optional.empty());\n-        }\n-\n-        public PartitionData(long timestamp, Optional<Integer> currentLeaderEpoch) {\n-            this(timestamp, 1, currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public boolean equals(Object obj) {\n-            if (!(obj instanceof PartitionData)) return false;\n-            PartitionData other = (PartitionData) obj;\n-            return this.timestamp == other.timestamp &&\n-                this.currentLeaderEpoch.equals(other.currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            return Objects.hash(timestamp, currentLeaderEpoch);\n-        }\n-\n-        @Override\n-        public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"{timestamp: \").append(timestamp).\n-                    append(\", maxNumOffsets: \").append(maxNumOffsets).\n-                    append(\", currentLeaderEpoch: \").append(currentLeaderEpoch).\n-                    append(\"}\");\n-            return bld.toString();\n+            return data.toString();\n         }\n     }\n \n     /**\n      * Private constructor with a specified version.\n      */\n-    private ListOffsetRequest(int replicaId,\n-                              Map<TopicPartition, PartitionData> targetTimes,\n-                              IsolationLevel isolationLevel,\n-                              short version) {\n+    private ListOffsetRequest(short version, ListOffsetRequestData data) {\n         super(ApiKeys.LIST_OFFSETS, version);\n-        this.replicaId = replicaId;\n-        this.isolationLevel = isolationLevel;\n-        this.partitionTimestamps = targetTimes;\n+        this.data = data;\n         this.duplicatePartitions = Collections.emptySet();\n     }\n \n     public ListOffsetRequest(Struct struct, short version) {\n         super(ApiKeys.LIST_OFFSETS, version);\n-        Set<TopicPartition> duplicatePartitions = new HashSet<>();\n-        replicaId = struct.get(REPLICA_ID);\n-        isolationLevel = struct.hasField(ISOLATION_LEVEL) ?\n-                IsolationLevel.forId(struct.get(ISOLATION_LEVEL)) :\n-                IsolationLevel.READ_UNCOMMITTED;\n-        partitionTimestamps = new HashMap<>();\n-        for (Object topicResponseObj : struct.get(TOPICS)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                long timestamp = partitionResponse.get(TIMESTAMP);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-\n-                int maxNumOffsets = partitionResponse.getOrElse(MAX_NUM_OFFSETS, 1);\n-                Optional<Integer> currentLeaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, CURRENT_LEADER_EPOCH);\n-                PartitionData partitionData = new PartitionData(timestamp, maxNumOffsets, currentLeaderEpoch);\n-                if (partitionTimestamps.put(tp, partitionData) != null)\n+        data = new ListOffsetRequestData(struct, version);\n+        duplicatePartitions = new HashSet<>();\n+        Set<TopicPartition> partitions = new HashSet<>();\n+        for (ListOffsetTopic topic : data.topics()) {\n+            for (ListOffsetPartition partition : topic.partitions()) {\n+                TopicPartition tp = new TopicPartition(topic.name(), partition.partitionIndex());\n+                if (!partitions.add(tp)) {\n                     duplicatePartitions.add(tp);\n+                }\n             }\n         }\n-        this.duplicatePartitions = duplicatePartitions;\n     }\n \n     @Override\n-    @SuppressWarnings(\"deprecation\")\n     public AbstractResponse getErrorResponse(int throttleTimeMs, Throwable e) {", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java\nindex 3bab5724a0..07f2998a4b 100644\n--- a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java\n+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetRequest.java\n\n@@ -67,8 +69,8 @@ public class ListOffsetRequest extends AbstractRequest {\n                         IsolationLevel isolationLevel) {\n             super(ApiKeys.LIST_OFFSETS, oldestAllowedVersion, latestAllowedVersion);\n             data = new ListOffsetRequestData()\n-                    .setIsolationLevel(isolationLevel.id())\n-                    .setReplicaId(replicaId);\n+                      .setIsolationLevel(isolationLevel.id())\n+                      .setReplicaId(replicaId);\n         }\n \n         public Builder setTargetTimes(List<ListOffsetTopic> topics) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwMzk0NQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449603945", "bodyText": "I would call this one topics() as you did already in the request.", "author": "dajac", "createdAt": "2020-07-03T14:12:57Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java", "diffHunk": "@@ -58,239 +47,54 @@\n public class ListOffsetResponse extends AbstractResponse {\n     public static final long UNKNOWN_TIMESTAMP = -1L;\n     public static final long UNKNOWN_OFFSET = -1L;\n+    public static final int UNKNOWN_EPOCH = RecordBatch.NO_PARTITION_LEADER_EPOCH;\n \n-    // top level fields\n-    private static final Field.ComplexArray TOPICS = new Field.ComplexArray(\"responses\",\n-            \"The listed offsets by topic\");\n-\n-    // topic level fields\n-    private static final Field.ComplexArray PARTITIONS = new Field.ComplexArray(\"partition_responses\",\n-            \"The listed offsets by partition\");\n-\n-    // partition level fields\n-    // This key is only used by ListOffsetResponse v0\n-    @Deprecated\n-    private static final Field.Array OFFSETS = new Field.Array(\"offsets\", INT64, \"A list of offsets.\");\n-    private static final Field.Int64 TIMESTAMP = new Field.Int64(\"timestamp\",\n-            \"The timestamp associated with the returned offset\");\n-    private static final Field.Int64 OFFSET = new Field.Int64(\"offset\",\n-            \"The offset found\");\n-\n-    private static final Field PARTITIONS_V0 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            OFFSETS);\n-\n-    private static final Field TOPICS_V0 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V0);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V0 = new Schema(\n-            TOPICS_V0);\n-\n-    // V1 bumped for the removal of the offsets array\n-    private static final Field PARTITIONS_V1 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            TIMESTAMP,\n-            OFFSET);\n-\n-    private static final Field TOPICS_V1 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V1);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V1 = new Schema(\n-            TOPICS_V1);\n-\n-    // V2 bumped for the addition of the throttle time\n-    private static final Schema LIST_OFFSET_RESPONSE_V2 = new Schema(\n-            THROTTLE_TIME_MS,\n-            TOPICS_V1);\n-\n-    // V3 bumped to indicate that on quota violation brokers send out responses before throttling.\n-    private static final Schema LIST_OFFSET_RESPONSE_V3 = LIST_OFFSET_RESPONSE_V2;\n-\n-    // V4 bumped for the addition of the current leader epoch in the request schema and the\n-    // leader epoch in the response partition data\n-    private static final Field PARTITIONS_V4 = PARTITIONS.withFields(\n-            PARTITION_ID,\n-            ERROR_CODE,\n-            TIMESTAMP,\n-            OFFSET,\n-            LEADER_EPOCH);\n+    private final ListOffsetResponseData data;\n \n-    private static final Field TOPICS_V4 = TOPICS.withFields(\n-            TOPIC_NAME,\n-            PARTITIONS_V4);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V4 = new Schema(\n-            THROTTLE_TIME_MS,\n-            TOPICS_V4);\n-\n-    private static final Schema LIST_OFFSET_RESPONSE_V5 = LIST_OFFSET_RESPONSE_V4;\n-\n-    public static Schema[] schemaVersions() {\n-        return new Schema[] {LIST_OFFSET_RESPONSE_V0, LIST_OFFSET_RESPONSE_V1, LIST_OFFSET_RESPONSE_V2,\n-            LIST_OFFSET_RESPONSE_V3, LIST_OFFSET_RESPONSE_V4, LIST_OFFSET_RESPONSE_V5};\n-    }\n-\n-    public static final class PartitionData {\n-        public final Errors error;\n-        // The offsets list is only used in ListOffsetResponse v0.\n-        public final List<Long> offsets;\n-        public final Long timestamp;\n-        public final Long offset;\n-        public final Optional<Integer> leaderEpoch;\n-\n-        /**\n-         * Constructor for ListOffsetResponse v0\n-         */\n-        public PartitionData(Errors error, List<Long> offsets) {\n-            this.error = error;\n-            this.offsets = offsets;\n-            this.timestamp = null;\n-            this.offset = null;\n-            this.leaderEpoch = Optional.empty();\n-        }\n-\n-        /**\n-         * Constructor for ListOffsetResponse v1\n-         */\n-        public PartitionData(Errors error, long timestamp, long offset, Optional<Integer> leaderEpoch) {\n-            this.error = error;\n-            this.timestamp = timestamp;\n-            this.offset = offset;\n-            this.offsets = null;\n-            this.leaderEpoch = leaderEpoch;\n-        }\n-\n-        @Override\n-        public String toString() {\n-            StringBuilder bld = new StringBuilder();\n-            bld.append(\"PartitionData(\").\n-                    append(\"errorCode: \").append(error.code());\n-\n-            if (offsets == null) {\n-                bld.append(\", timestamp: \").append(timestamp).\n-                        append(\", offset: \").append(offset).\n-                        append(\", leaderEpoch: \").append(leaderEpoch);\n-            } else {\n-                bld.append(\", offsets: \").\n-                        append(\"[\").\n-                        append(Utils.join(this.offsets, \",\")).\n-                        append(\"]\");\n-            }\n-            bld.append(\")\");\n-            return bld.toString();\n-        }\n+    public ListOffsetResponse(ListOffsetResponseData data) {\n+        this.data = data;\n     }\n \n-    private final int throttleTimeMs;\n-    private final Map<TopicPartition, PartitionData> responseData;\n-\n-    /**\n-     * Constructor for all versions without throttle time\n-     */\n-    public ListOffsetResponse(Map<TopicPartition, PartitionData> responseData) {\n-        this(DEFAULT_THROTTLE_TIME, responseData);\n-    }\n-\n-    public ListOffsetResponse(int throttleTimeMs, Map<TopicPartition, PartitionData> responseData) {\n-        this.throttleTimeMs = throttleTimeMs;\n-        this.responseData = responseData;\n-    }\n-\n-    public ListOffsetResponse(Struct struct) {\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n-        responseData = new HashMap<>();\n-        for (Object topicResponseObj : struct.get(TOPICS)) {\n-            Struct topicResponse = (Struct) topicResponseObj;\n-            String topic = topicResponse.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicResponse.get(PARTITIONS)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partitionResponse.get(ERROR_CODE));\n-                PartitionData partitionData;\n-                if (partitionResponse.hasField(OFFSETS)) {\n-                    Object[] offsets = partitionResponse.get(OFFSETS);\n-                    List<Long> offsetsList = new ArrayList<>();\n-                    for (Object offset : offsets)\n-                        offsetsList.add((Long) offset);\n-                    partitionData = new PartitionData(error, offsetsList);\n-                } else {\n-                    long timestamp = partitionResponse.get(TIMESTAMP);\n-                    long offset = partitionResponse.get(OFFSET);\n-                    Optional<Integer> leaderEpoch = RequestUtils.getLeaderEpoch(partitionResponse, LEADER_EPOCH);\n-                    partitionData = new PartitionData(error, timestamp, offset, leaderEpoch);\n-                }\n-                responseData.put(new TopicPartition(topic, partition), partitionData);\n-            }\n-        }\n+    public ListOffsetResponse(Struct struct, short version) {\n+        data = new ListOffsetResponseData(struct, version);\n     }\n \n     @Override\n     public int throttleTimeMs() {\n-        return throttleTimeMs;\n+        return data.throttleTimeMs();\n     }\n \n-    public Map<TopicPartition, PartitionData> responseData() {\n-        return responseData;\n+    public ListOffsetResponseData data() {\n+        return data;\n+    }\n+\n+    public List<ListOffsetTopicResponse> responseData() {", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java\nindex ceb230e24c..dd941d742a 100644\n--- a/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java\n+++ b/clients/src/main/java/org/apache/kafka/common/requests/ListOffsetResponse.java\n\n@@ -68,14 +71,14 @@ public class ListOffsetResponse extends AbstractResponse {\n         return data;\n     }\n \n-    public List<ListOffsetTopicResponse> responseData() {\n+    public List<ListOffsetTopicResponse> topics() {\n         return data.topics();\n     }\n \n     @Override\n     public Map<Errors, Integer> errorCounts() {\n         Map<Errors, Integer> errorCounts = new HashMap<>();\n-        responseData().forEach(topic ->\n+        topics().forEach(topic ->\n             topic.partitions().forEach(partition ->\n                 updateErrorCounts(errorCounts, Errors.forCode(partition.errorCode()))\n             )\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYwNzA3OA==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449607078", "bodyText": "nit: What about creating a small helper to create a ListOffsetTopicResponse for a given TopicPartition & co? That would reduce the boilerplate code.", "author": "dajac", "createdAt": "2020-07-03T14:20:14Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -3220,12 +3286,30 @@ public void testListOffsetsMetadataRetriableErrors() throws Exception {\n             env.kafkaClient().prepareResponse(prepareMetadataResponse(cluster, Errors.NONE));\n \n             // listoffsets response from broker 0\n-            Map<TopicPartition, PartitionData> responseData = new HashMap<>();\n-            responseData.put(tp0, new PartitionData(Errors.NONE, -1L, 345L, Optional.of(543)));\n+            ListOffsetTopicResponse t0 = new ListOffsetTopicResponse()\n+                    .setName(tp0.topic())\n+                    .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n+                            .setPartitionIndex(tp0.partition())\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setTimestamp(-1L)\n+                            .setOffset(345L)\n+                            .setLeaderEpoch(543)));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex eaac3ed188..80d52947cd 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -3286,31 +3918,17 @@ public class KafkaAdminClientTest {\n             env.kafkaClient().prepareResponse(prepareMetadataResponse(cluster, Errors.NONE));\n \n             // listoffsets response from broker 0\n-            ListOffsetTopicResponse t0 = new ListOffsetTopicResponse()\n-                    .setName(tp0.topic())\n-                    .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n-                            .setPartitionIndex(tp0.partition())\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setTimestamp(-1L)\n-                            .setOffset(345L)\n-                            .setLeaderEpoch(543)));\n+            ListOffsetTopicResponse t0 = ListOffsetResponse.singletonListOffsetTopicResponse(tp0, Errors.NONE, -1L, 345L, 543);\n             ListOffsetResponseData responseData = new ListOffsetResponseData()\n                     .setThrottleTimeMs(0)\n                     .setTopics(Arrays.asList(t0));\n-            env.kafkaClient().prepareResponse(new ListOffsetResponse(responseData));\n+            env.kafkaClient().prepareResponseFrom(new ListOffsetResponse(responseData), node0);\n             // listoffsets response from broker 1\n-            ListOffsetTopicResponse t1 = new ListOffsetTopicResponse()\n-                    .setName(tp1.topic())\n-                    .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n-                            .setPartitionIndex(tp1.partition())\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setTimestamp(-1L)\n-                            .setOffset(789L)\n-                            .setLeaderEpoch(987)));\n+            ListOffsetTopicResponse t1 = ListOffsetResponse.singletonListOffsetTopicResponse(tp1, Errors.NONE, -1L, 789L, 987);\n             responseData = new ListOffsetResponseData()\n                     .setThrottleTimeMs(0)\n                     .setTopics(Arrays.asList(t1));\n-            env.kafkaClient().prepareResponse(new ListOffsetResponse(responseData));\n+            env.kafkaClient().prepareResponseFrom(new ListOffsetResponse(responseData), node1);\n \n             Map<TopicPartition, OffsetSpec> partitions = new HashMap<>();\n             partitions.put(tp0, OffsetSpec.latest());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTYxMTkzMQ==", "url": "https://github.com/apache/kafka/pull/8295#discussion_r449611931", "bodyText": "Can we remove these two?", "author": "dajac", "createdAt": "2020-07-03T14:31:39Z", "path": "clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java", "diffHunk": "@@ -1581,8 +1582,19 @@ SaslClient createSaslClient() {\n \n     @Test\n     public void testConvertListOffsetResponseToSaslHandshakeResponse() {\n-        ListOffsetResponse response = new ListOffsetResponse(0, Collections.singletonMap(new TopicPartition(\"topic\", 0),\n-            new ListOffsetResponse.PartitionData(Errors.NONE, 0, 0, Optional.empty())));\n+        ListOffsetResponseData data = new ListOffsetResponseData()\n+                .setThrottleTimeMs(0)\n+                .setTopics(Collections.singletonList(new ListOffsetTopicResponse()\n+                        .setName(\"topic\")\n+                        .setPartitions(Collections.singletonList(new ListOffsetPartitionResponse()\n+                                .setErrorCode(Errors.NONE.code())\n+                                .setLeaderEpoch(ListOffsetResponse.UNKNOWN_EPOCH)\n+                                .setPartitionIndex(0)\n+                                .setOffset(0)\n+                                .setTimestamp(0)))));\n+        ListOffsetResponse response = new ListOffsetResponse(data);\n+//        ListOffsetResponse response = new ListOffsetResponse(0, Collections.singletonMap(new TopicPartition(\"topic\", 0),\n+//            new ListOffsetResponse.PartitionData(Errors.NONE, 0, 0, Optional.empty())));", "originalCommit": "65ac2f1501fd6ee863a873cd93b64a1534d09461", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java b/clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java\nindex b5aa666f1c..ed922b18f6 100644\n--- a/clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java\n+++ b/clients/src/test/java/org/apache/kafka/common/security/authenticator/SaslAuthenticatorTest.java\n\n@@ -1593,8 +1593,6 @@ public class SaslAuthenticatorTest {\n                                 .setOffset(0)\n                                 .setTimestamp(0)))));\n         ListOffsetResponse response = new ListOffsetResponse(data);\n-//        ListOffsetResponse response = new ListOffsetResponse(0, Collections.singletonMap(new TopicPartition(\"topic\", 0),\n-//            new ListOffsetResponse.PartitionData(Errors.NONE, 0, 0, Optional.empty())));\n         ByteBuffer buffer = response.serialize(ApiKeys.LIST_OFFSETS, LIST_OFFSETS.latestVersion(), 0);\n         final RequestHeader header0 = new RequestHeader(LIST_OFFSETS, LIST_OFFSETS.latestVersion(), \"id\", SaslClientAuthenticator.MIN_RESERVED_CORRELATION_ID);\n         Assert.assertThrows(SchemaException.class, () -> NetworkClient.parseResponse(buffer.duplicate(), header0));\n"}}, {"oid": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "url": "https://github.com/apache/kafka/commit/78cb96fc13e5c337372b24b5ea50d7ade30485fc", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-09-24T08:51:31Z", "type": "commit"}, {"oid": "78cb96fc13e5c337372b24b5ea50d7ade30485fc", "url": "https://github.com/apache/kafka/commit/78cb96fc13e5c337372b24b5ea50d7ade30485fc", "message": "KAFKA-9627: Replace ListOffset request/response with automated protocol\n\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>", "committedDate": "2020-09-24T08:51:31Z", "type": "forcePushed"}]}