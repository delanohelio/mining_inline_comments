{"pr_number": 8829, "pr_title": "KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter", "pr_createdAt": "2020-06-07T22:57:58Z", "pr_url": "https://github.com/apache/kafka/pull/8829", "timeline": [{"oid": "c2867f70bc491f87329736487aaf68663c50f4a5", "url": "https://github.com/apache/kafka/commit/c2867f70bc491f87329736487aaf68663c50f4a5", "message": "KAFKA-10115: Incorporate errors.tolerance with the Errant Record Reporter", "committedDate": "2020-06-08T21:14:11Z", "type": "commit"}, {"oid": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "url": "https://github.com/apache/kafka/commit/0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "message": "addressed comments", "committedDate": "2020-06-10T22:21:21Z", "type": "commit"}, {"oid": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "url": "https://github.com/apache/kafka/commit/0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "message": "addressed comments", "committedDate": "2020-06-10T22:21:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ1MjE1MQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438452151", "bodyText": "instead, you can just check:\n        if (retryWithToleranceOperator.failed()) {\n            throw retryWithToleranceOperator.error();\n        }\n\nbecause we are already storing the error in the processing context. you can expose that through the operator.", "author": "wicknicks", "createdAt": "2020-06-10T22:58:33Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -556,6 +556,9 @@ private void deliverMessages() {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n+            if (workerErrantRecordReporter != null && workerErrantRecordReporter.mustThrowException()) {\n+                throw workerErrantRecordReporter.getExceptionToThrow();\n+            }", "originalCommit": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\nindex d057176f29..44a1139057 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n\n@@ -556,8 +556,9 @@ class WorkerSinkTask extends WorkerTask {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n-            if (workerErrantRecordReporter != null && workerErrantRecordReporter.mustThrowException()) {\n-                throw workerErrantRecordReporter.getExceptionToThrow();\n+            if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {\n+                throw new ConnectException(\"Tolerance exceeded in error handler\",\n+                    retryWithToleranceOperator.error());\n             }\n             recordBatch(messageBatch.size());\n             sinkTaskMetricsGroup.recordPut(time.milliseconds() - start);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ1MjY5Ng==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438452696", "bodyText": "we don't need these vars, the errors are already stored in the ProcessingContext. look at comment above.", "author": "wicknicks", "createdAt": "2020-06-10T23:00:11Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -99,8 +102,15 @@ public WorkerErrantRecordReporter(\n                 valLength, key, value, headers);\n         }\n \n-        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n-            SinkTask.class, consumerRecord, error);\n+        Future<Void> future;\n+        try {\n+            future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+                SinkTask.class, consumerRecord, error);\n+        } catch (ConnectException e) {\n+            mustThrowException = true;\n+            exceptionToThrow = e;\n+            throw e;\n+        }", "originalCommit": "0ce4daf669d95320c9ab096b7dbc207786aaeeb8", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex c483318540..b709e28c5e 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -102,15 +99,7 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n                 valLength, key, value, headers);\n         }\n \n-        Future<Void> future;\n-        try {\n-            future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n-                SinkTask.class, consumerRecord, error);\n-        } catch (ConnectException e) {\n-            mustThrowException = true;\n-            exceptionToThrow = e;\n-            throw e;\n-        }\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT, SinkTask.class, consumerRecord, error);\n \n         if (!future.isDone()) {\n             futures.add(future);\n"}}, {"oid": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "url": "https://github.com/apache/kafka/commit/603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "message": "addressed comments", "committedDate": "2020-06-10T23:55:34Z", "type": "commit"}, {"oid": "aefe73ffc3e6173a2427bd3dfb957d776d4088fd", "url": "https://github.com/apache/kafka/commit/aefe73ffc3e6173a2427bd3dfb957d776d4088fd", "message": "moved errant record reporter test", "committedDate": "2020-06-11T00:20:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NTM1NQ==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438475355", "bodyText": "minor: we should move this test to ErrorHandlingIntegrationTest. this class was meant to be an example of how to do integration tests.", "author": "wicknicks", "createdAt": "2020-06-11T00:17:22Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -237,6 +239,7 @@ public void testErrantRecordReporter() throws Exception {\n         props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n         props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+        props.put(ERRORS_TOLERANCE_CONFIG, ToleranceType.ALL.value());", "originalCommit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aefe73ffc3e6173a2427bd3dfb957d776d4088fd", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\nindex 0177bb429f..8538fb401d 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\n\n@@ -225,72 +219,6 @@ public class ExampleConnectIntegrationTest {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n-    @Test\n-    public void testErrantRecordReporter() throws Exception {\n-        connect.kafka().createTopic(DLQ_TOPIC, 1);\n-        // create test topic\n-        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n-\n-        // setup up props for the sink connector\n-        Map<String, String> props = new HashMap<>();\n-        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n-        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n-        props.put(TOPICS_CONFIG, \"test-topic\");\n-        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n-        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n-        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n-        props.put(ERRORS_TOLERANCE_CONFIG, ToleranceType.ALL.value());\n-\n-        // expect all records to be consumed by the connector\n-        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n-\n-        // expect all records to be consumed by the connector\n-        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n-\n-        // validate the intended connector configuration, a config that errors\n-        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n-            \"Validating connector configuration produced an unexpected number or errors.\");\n-\n-        // add missing configuration to make the config valid\n-        props.put(\"name\", CONNECTOR_NAME);\n-\n-        // validate the intended connector configuration, a valid config\n-        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n-            \"Validating connector configuration produced an unexpected number or errors.\");\n-\n-        // start a sink connector\n-        connect.configureConnector(CONNECTOR_NAME, props);\n-\n-        waitForCondition(this::checkForPartitionAssignment,\n-            CONNECTOR_SETUP_DURATION_MS,\n-            \"Connector tasks were not assigned a partition each.\");\n-\n-        // produce some messages into source topic partitions\n-        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n-            connect.kafka().produce(\"test-topic\", i % NUM_TOPIC_PARTITIONS, \"key\", \"simple-message-value-\" + i);\n-        }\n-\n-        // consume all records from the source topic or fail, to ensure that they were correctly produced.\n-        assertEquals(\"Unexpected number of records consumed\", NUM_RECORDS_PRODUCED,\n-            connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic\").count());\n-\n-        // wait for the connector tasks to consume all records.\n-        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);\n-\n-        // wait for the connector tasks to commit all records.\n-        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);\n-\n-        // consume all records from the dlq topic or fail, to ensure that they were correctly produced\n-        int recordNum = connect.kafka().consume(\n-            NUM_RECORDS_PRODUCED,\n-            RECORD_TRANSFER_DURATION_MS,\n-            DLQ_TOPIC\n-        ).count();\n-\n-        // delete connector\n-        connect.deleteConnector(CONNECTOR_NAME);\n-    }\n-\n     /**\n      * Check if a partition was assigned to each task. This method swallows exceptions since it is invoked from a\n      * {@link org.apache.kafka.test.TestUtils#waitForCondition} that will throw an error if this method continued\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ3NjcyMg==", "url": "https://github.com/apache/kafka/pull/8829#discussion_r438476722", "bodyText": "let's add a small comment saying why we need to do this: specifically, that if the errors raised from the operator were swallowed by the task implementation, then here we need to kill the task, and if they were not swallowed, we would not get here.", "author": "wicknicks", "createdAt": "2020-06-11T00:22:32Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -556,6 +556,10 @@ private void deliverMessages() {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n+            if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {", "originalCommit": "603217f1ed0e68295b5a6cfd0bc2fd09ba9f681d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c3324cce90f8379c69c8538b594edd29e83f7fe1", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\nindex 44a1139057..267ffd0361 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n\n@@ -556,6 +556,8 @@ class WorkerSinkTask extends WorkerTask {\n             log.trace(\"{} Delivering batch of {} messages to task\", this, messageBatch.size());\n             long start = time.milliseconds();\n             task.put(new ArrayList<>(messageBatch));\n+            // if errors raised from the operator were swallowed by the task implementation, an\n+            // exception needs to be thrown to kill the task indicating the tolerance was exceeded\n             if (retryWithToleranceOperator.failed() && !retryWithToleranceOperator.withinToleranceLimits()) {\n                 throw new ConnectException(\"Tolerance exceeded in error handler\",\n                     retryWithToleranceOperator.error());\n"}}, {"oid": "c3324cce90f8379c69c8538b594edd29e83f7fe1", "url": "https://github.com/apache/kafka/commit/c3324cce90f8379c69c8538b594edd29e83f7fe1", "message": "more comments", "committedDate": "2020-06-11T00:44:15Z", "type": "commit"}]}