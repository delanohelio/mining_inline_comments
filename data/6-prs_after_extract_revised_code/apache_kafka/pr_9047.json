{"pr_number": 9047, "pr_title": "KAFKA-9274: Remove `retries` for global task", "pr_createdAt": "2020-07-21T07:21:50Z", "pr_url": "https://github.com/apache/kafka/pull/9047", "timeline": [{"oid": "89ae568004fdc04b3d216cec53585fadf55641cc", "url": "https://github.com/apache/kafka/commit/89ae568004fdc04b3d216cec53585fadf55641cc", "message": "KAFKA-9274: remove `retries` for global tasks\n - part of KIP-572\n - removed the usage of `retries` in `GlobalStateManger`\n - instead of retries the new `task.timeout.ms` config is used", "committedDate": "2020-07-28T04:05:45Z", "type": "commit"}, {"oid": "7cecee3b990da57dd0a57ec5e497d1c8c2550955", "url": "https://github.com/apache/kafka/commit/7cecee3b990da57dd0a57ec5e497d1c8c2550955", "message": "Rebased\n\nUpdated poll timeout", "committedDate": "2020-07-28T04:39:47Z", "type": "commit"}, {"oid": "7cecee3b990da57dd0a57ec5e497d1c8c2550955", "url": "https://github.com/apache/kafka/commit/7cecee3b990da57dd0a57ec5e497d1c8c2550955", "message": "Rebased\n\nUpdated poll timeout", "committedDate": "2020-07-28T04:39:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA0NzA2OA==", "url": "https://github.com/apache/kafka/pull/9047#discussion_r462047068", "bodyText": "Could we just throw here?", "author": "abbccdda", "createdAt": "2020-07-29T05:28:49Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "diffHunk": "@@ -274,30 +252,74 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n                               final RecordConverter recordConverter) {\n         for (final TopicPartition topicPartition : topicPartitions) {\n             globalConsumer.assign(Collections.singletonList(topicPartition));\n+            long offset;\n             final Long checkpoint = checkpointFileCache.get(topicPartition);\n             if (checkpoint != null) {\n                 globalConsumer.seek(topicPartition, checkpoint);\n+                offset = checkpoint;\n             } else {\n                 globalConsumer.seekToBeginning(Collections.singletonList(topicPartition));\n+                offset = retryUntilSuccessOrThrowOnTaskTimeout(\n+                    () -> globalConsumer.position(topicPartition),\n+                    String.format(\n+                        \"Failed to get position for partition %s. The broker may be transiently unavailable at the moment.\",\n+                        topicPartition\n+                    )\n+                );\n             }\n \n-            long offset = globalConsumer.position(topicPartition);\n             final Long highWatermark = highWatermarks.get(topicPartition);\n             final RecordBatchingStateRestoreCallback stateRestoreAdapter =\n                 StateRestoreCallbackAdapter.adapt(stateRestoreCallback);\n \n             stateRestoreListener.onRestoreStart(topicPartition, storeName, offset, highWatermark);\n             long restoreCount = 0L;\n \n-            while (offset < highWatermark) {\n-                final ConsumerRecords<byte[], byte[]> records = globalConsumer.poll(pollTime);\n+            while (offset < highWatermark) { // when we \"fix\" this loop (KAFKA-7380 / KAFKA-10317)\n+                                             // we should update the `poll()` timeout below\n+\n+                // we ignore `poll.ms` config during bootstrapping phase and\n+                // apply `request.timeout.ms` plus `task.timeout.ms` instead\n+                //\n+                // the reason is, that `poll.ms` might be too short to give a fetch request a fair chance\n+                // to actually complete and we don't want to start `task.timeout.ms` too early\n+                //\n+                // we also pass `task.timeout.ms` into `poll()` directly right now as it simplifies our own code:\n+                // if we don't pass it in, we would just track the timeout ourselves and call `poll()` again\n+                // in our own retry loop; by passing the timeout we can reuse the consumer's internal retry loop instead\n+                //\n+                // note that using `request.timeout.ms` provides a conservative upper bound for the timeout;\n+                // this implies that we might start `task.timeout.ms` \"delayed\" -- however, starting the timeout\n+                // delayed is preferable (as it's more robust) than starting it too early\n+                //\n+                // TODO https://issues.apache.org/jira/browse/KAFKA-10315\n+                //   -> do a more precise timeout handling if `poll` would throw an exception if a fetch request fails\n+                //      (instead of letting the consumer retry fetch requests silently)\n+                //\n+                // TODO https://issues.apache.org/jira/browse/KAFKA-10317 and\n+                //      https://issues.apache.org/jira/browse/KAFKA-7380\n+                //  -> don't pass in `task.timeout.ms` to stay responsive if `KafkaStreams#close` gets called\n+                final ConsumerRecords<byte[], byte[]> records = globalConsumer.poll(requestTimeoutPlusTaskTimeout);\n+                if (records.isEmpty()) {\n+                    // this will always throw\n+                    maybeUpdateDeadlineOrThrow(time.milliseconds());", "originalCommit": "7cecee3b990da57dd0a57ec5e497d1c8c2550955", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc1NjY0Ng==", "url": "https://github.com/apache/kafka/pull/9047#discussion_r464756646", "bodyText": "We could, but this implies redundant code to \"assemble\" the error message, and I prefer to reuse the existing code for it.", "author": "mjsax", "createdAt": "2020-08-04T02:07:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA0NzA2OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA0ODg3OQ==", "url": "https://github.com/apache/kafka/pull/9047#discussion_r462048879", "bodyText": "nit: we could just use Map for startOffsets and endOffsets", "author": "abbccdda", "createdAt": "2020-07-29T05:35:04Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java", "diffHunk": "@@ -671,19 +1211,21 @@ private void writeCorruptCheckpoint() throws IOException {\n         }\n     }\n \n-    private void initializeConsumer(final long numRecords, final long startOffset, final TopicPartition topicPartition) {\n+    private void initializeConsumer(final long numRecords, final long startOffset, final TopicPartition... topicPartitions) {\n+        consumer.assign(Arrays.asList(topicPartitions));\n+\n         final HashMap<TopicPartition, Long> startOffsets = new HashMap<>();\n-        startOffsets.put(topicPartition, startOffset);\n         final HashMap<TopicPartition, Long> endOffsets = new HashMap<>();\n-        endOffsets.put(topicPartition, startOffset + numRecords);\n-        consumer.updatePartitions(topicPartition.topic(), Collections.singletonList(new PartitionInfo(topicPartition.topic(), topicPartition.partition(), null, null, null)));\n-        consumer.assign(Collections.singletonList(topicPartition));\n+        for (final TopicPartition topicPartition : topicPartitions) {", "originalCommit": "7cecee3b990da57dd0a57ec5e497d1c8c2550955", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf6ee1e774a3a635cb8ac9c134ff3269edaa7357", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java b/streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java\nindex be2135c836..2b630a109f 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImplTest.java\n\n@@ -1214,8 +1214,8 @@ public class GlobalStateManagerImplTest {\n     private void initializeConsumer(final long numRecords, final long startOffset, final TopicPartition... topicPartitions) {\n         consumer.assign(Arrays.asList(topicPartitions));\n \n-        final HashMap<TopicPartition, Long> startOffsets = new HashMap<>();\n-        final HashMap<TopicPartition, Long> endOffsets = new HashMap<>();\n+        final Map<TopicPartition, Long> startOffsets = new HashMap<>();\n+        final Map<TopicPartition, Long> endOffsets = new HashMap<>();\n         for (final TopicPartition topicPartition : topicPartitions) {\n             startOffsets.put(topicPartition, startOffset);\n             endOffsets.put(topicPartition, startOffset + numRecords);\n"}}, {"oid": "cf6ee1e774a3a635cb8ac9c134ff3269edaa7357", "url": "https://github.com/apache/kafka/commit/cf6ee1e774a3a635cb8ac9c134ff3269edaa7357", "message": "Github comments", "committedDate": "2020-08-04T02:12:18Z", "type": "commit"}]}