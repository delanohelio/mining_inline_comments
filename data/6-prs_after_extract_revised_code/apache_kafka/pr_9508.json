{"pr_number": 9508, "pr_title": "KAFKA-10648: Add Prefix Scan support to State Stores", "pr_createdAt": "2020-10-27T05:18:06Z", "pr_url": "https://github.com/apache/kafka/pull/9508", "timeline": [{"oid": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "url": "https://github.com/apache/kafka/commit/0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2020-12-01T13:03:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQxMTcwMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544411701", "bodyText": "This is not strictly necessary since you test the mock result you provide which has nothing to do with the code under test.", "author": "cadonna", "createdAt": "2020-12-16T15:53:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        assertFalse(iterator.hasNext());", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 9479b15f8c..37f7fa9603 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -438,16 +438,18 @@ public class MeteredKeyValueStoreTest {\n     @Test\n     public void shouldGetRecordsWithPrefixKey() {\n         final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n         expect(inner.prefixScan(KEY, stringSerializer))\n-                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n         init();\n \n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);\n         final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n         assertThat(iterator.next().value, equalTo(VALUE));\n-        assertFalse(iterator.hasNext());\n         iterator.close();\n \n-        final KafkaMetric metric = metric(\"prefixScan-rate\");\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n         assertTrue((Double) metric.metricValue() > 0);\n         verify(inner);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwMTQ1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544501455", "bodyText": "Please change prefixScan to prefix-scan. For metrics name we do not use camel case. See put-all or put-if-absent for examples.", "author": "cadonna", "createdAt": "2020-12-16T17:50:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefixScan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefixScan\";", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\nindex de76bb4d0a..eb6825fa8d 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n\n@@ -110,8 +110,8 @@ public class StateStoreMetrics {\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n-    private static final String PREFIX_SCAN = \"prefixScan\";\n-    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefixScan\";\n+    private static final String PREFIX_SCAN = \"prefix-scan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n     private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;\n     private static final String PREFIX_SCAN_RATE_DESCRIPTION =\n             RATE_DESCRIPTION_PREFIX + PREFIX_SCAN_DESCRIPTION + RATE_DESCRIPTION_SUFFIX;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNDc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544504791", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(\n          \n          \n            \n                        wrapped().prefixScan(prefix, prefixKeySerializer),\n          \n          \n            \n                        prefixScanSensor\n          \n          \n            \n                    );\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2020-12-16T17:55:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,15 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(\n+            wrapped().prefixScan(prefix, prefixKeySerializer),\n+            prefixScanSensor\n+        );", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\nindex cedc65ba7d..4f8b5a491d 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\n\n@@ -235,10 +235,8 @@ public class MeteredKeyValueStore<K, V>\n     @Override\n     public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n \n-        return new MeteredKeyValueIterator(\n-            wrapped().prefixScan(prefix, prefixKeySerializer),\n-            prefixScanSensor\n-        );\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n+            prefixKeySerializer), prefixScanSensor);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUwNTc1OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544505758", "bodyText": "nit: Please use 4 instead of 8 spaces indentation.", "author": "cadonna", "createdAt": "2020-12-16T17:56:59Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,23 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 9479b15f8c..37f7fa9603 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -438,16 +438,18 @@ public class MeteredKeyValueStoreTest {\n     @Test\n     public void shouldGetRecordsWithPrefixKey() {\n         final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n         expect(inner.prefixScan(KEY, stringSerializer))\n-                .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n         init();\n \n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);\n         final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n         assertThat(iterator.next().value, equalTo(VALUE));\n-        assertFalse(iterator.hasNext());\n         iterator.close();\n \n-        final KafkaMetric metric = metric(\"prefixScan-rate\");\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n         assertTrue((Double) metric.metricValue() > 0);\n         verify(inner);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDUxNzAxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r544517012", "bodyText": "Sorry, if I haven't noticed it before. Could you move this method up before approximateNumEntries() to have all operations in one block?", "author": "cadonna", "createdAt": "2020-12-16T18:13:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -627,6 +645,11 @@ public void addToBatch(final byte[] key,\n         public void close() {\n             columnFamily.close();\n         }\n+\n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            return new RocksDBPrefixIterator(name, db.newIterator(columnFamily), openIterators, prefix);\n+        }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\nindex 339a27f616..4f16b90e95 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n\n@@ -645,11 +663,6 @@ public class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingS\n         public void close() {\n             columnFamily.close();\n         }\n-\n-        @Override\n-        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n-            return new RocksDBPrefixIterator(name, db.newIterator(columnFamily), openIterators, prefix);\n-        }\n     }\n \n     // not private for testing\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzMzk5Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545033996", "bodyText": "nit: I would just add parameter toInclusive to the existing method instead of creating an overload.", "author": "cadonna", "createdAt": "2020-12-17T11:56:54Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "diffHunk": "@@ -284,6 +284,10 @@ public boolean isEmpty() {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n     }\n \n+    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n+        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n+    }", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java\nindex e07099bc6c..e4b054b8e7 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java\n\n@@ -280,10 +280,6 @@ class NamedCache {\n         return cache.isEmpty();\n     }\n \n-    synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to) {\n-        return keySetIterator(cache.navigableKeySet().subSet(from, true, to, true), true);\n-    }\n-\n     synchronized Iterator<Bytes> keyRange(final Bytes from, final Bytes to, final boolean toInclusive) {\n         return keySetIterator(cache.navigableKeySet().subSet(from, true, to, toInclusive), true);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545040031", "bodyText": "I actually do not completely understand why we need a specific iterator for the prefix scan. We could just as good extend RocksDBRangeIterator to consider or not consider the end result of the range. We can do that because those iterator implementations are internal and the public API does not care which iterator is used as long as it implements interface KeyValueIterator and the behavior is correct. Extending and re-using RocksDBRangeIterator would lead to less code to maintain. Note, I agree that we need the public method prefixScan(), but what the implementation uses internally is not relevant for the KIP as long as it is correct. Did I miss something that imposes the implementation for a separate iterator?", "author": "cadonna", "createdAt": "2020-12-17T12:07:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.state.KeyValueIterator;\n+import org.rocksdb.RocksIterator;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Set;\n+\n+class RocksDBPrefixIterator extends RocksDbIterator {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk1ODk3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545958971", "bodyText": "Well the only reason i chose to add a separate iterator is that for prefix scan there is no end key which could be known upfront. We could pass a null end key but that's prohibited in RocksDBRangeIterator constructor and i don't think we should be changing that condition or we could pass in the same value or from and to and add a condition in makeNext() for prefix scan. i thought having a separate iterator might be cleaner. You have some other approach in mind?", "author": "vamossagar12", "createdAt": "2020-12-18T16:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NjIwNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545986204", "bodyText": "Could you not use the same technique as you use in the in-memory key-value state store and the cache?\n        final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n        final Bytes to = Bytes.increment(from);", "author": "cadonna", "createdAt": "2020-12-18T17:39:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjY0MzI3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r546643272", "bodyText": "I tested with that approach. It worked for all cases apart from the abcd/abce case that you mentioned below. So, we will need a way to signal to the Iterator that for the case of range iteration for prefixScan, it should ignore the last key. I suppose we will have to add another flag similar to the ones used in in memory key-value store.\nBTW, the other reason why I created a separate prefix iterator was that there was an old unused prefix iterator, So, I thought of repurposing it :)", "author": "vamossagar12", "createdAt": "2020-12-21T11:02:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzQ4MzQxOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r553483419", "bodyText": "hey @cadonna , did you get a chance to look at the comment?", "author": "vamossagar12", "createdAt": "2021-01-07T17:45:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEyNDY3NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555124675", "bodyText": "Sorry for the late reply but I was on holidays.\n\nI suppose we will have to add another flag similar to the ones used in in memory key-value store.\n\nThis is what I meant with \"extend RocksDBRangeIterator\".\nI think less code is easier maintainable. In addition, I think the code is easier to follow if cache, in-memory, and RocksDB use the same technique for the prefix scan unless we have a technique with better performance that is specific to one of those components. So I would be in favor of just using RocksDBRangeIterator.", "author": "cadonna", "createdAt": "2021-01-11T15:25:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEzNDg2Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555134866", "bodyText": "Thanks for the reply. Sure, I would make the relevant changes.", "author": "vamossagar12", "createdAt": "2021-01-11T15:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTE0MDg3Mg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r555140872", "bodyText": "Awesome!", "author": "cadonna", "createdAt": "2021-01-11T15:48:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ5Nzk2NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556497964", "bodyText": "As discussed, I have extended RocksDBRangeIterator to include the toInclusive flag.\n1 point to note here is that with this change, RocksDBPrefixIterator is no longer needed so I have removed it and it's relevant test case.\nAlso, the unit tests for prefix scan, i have a separate PR 9717 where i am writing the unit tests for RangeIterator. I plan to add the prefix scan cases there. would that be ok?", "author": "vamossagar12", "createdAt": "2021-01-13T12:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0MDAzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java\ndeleted file mode 100644\nindex 5e8b490fed..0000000000\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBPrefixIterator.java\n+++ /dev/null\n\n@@ -1,66 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.state.internals;\n-\n-import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.streams.KeyValue;\n-import org.apache.kafka.streams.state.KeyValueIterator;\n-import org.rocksdb.RocksIterator;\n-\n-import java.nio.ByteBuffer;\n-import java.util.Set;\n-\n-class RocksDBPrefixIterator extends RocksDbIterator {\n-\n-    private final byte[] rawPrefix;\n-\n-    RocksDBPrefixIterator(final String name,\n-                          final RocksIterator newIterator,\n-                          final Set<KeyValueIterator<Bytes, byte[]>> openIterators,\n-                          final Bytes prefix) {\n-        super(name, newIterator, openIterators, true);\n-        this.rawPrefix = prefix.get();\n-        newIterator.seek(rawPrefix);\n-    }\n-\n-    private boolean prefixEquals(final byte[] prefix) {\n-\n-        if (this.getRawPrefix().length > prefix.length)\n-            return false;\n-\n-        final ByteBuffer prefixSlice = ByteBuffer.wrap(prefix, 0, this.getRawPrefix().length);\n-        return ByteBuffer.wrap(this.getRawPrefix()).equals(prefixSlice);\n-    }\n-\n-    public KeyValue<Bytes, byte[]> getNext() {\n-        return super.makeNext();\n-    }\n-\n-    public byte[] getRawPrefix() {\n-        return this.rawPrefix;\n-    }\n-\n-    @Override\n-    public KeyValue<Bytes, byte[]> makeNext() {\n-        final KeyValue<Bytes, byte[]> next = getNext();\n-        if (next == null || !prefixEquals(next.key.get())) {\n-            return allDone();\n-        } else {\n-            return next;\n-        }\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDQwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545044402", "bodyText": "If you consider my comment in NamedCache, you could also just call range() here. Or -- if you want to be more descriptive -- having prefixScan() and range() in this class calling a private overload of range() with a flag that excludes or includes the end of the range. That would deduplicate code.", "author": "cadonna", "createdAt": "2020-12-17T12:14:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -201,6 +201,14 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n+        final NamedCache cache = getCache(namespace);\n+        if (cache == null) {\n+            return new MemoryLRUCacheBytesIterator(Collections.emptyIterator(), new NamedCache(namespace, this.metrics));\n+        }\n+        return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to, false), cache);", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\nindex 1cf385eb8f..84e6c5fcce 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\n\n@@ -202,11 +223,7 @@ public class ThreadCache {\n     }\n \n     public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n-        final NamedCache cache = getCache(namespace);\n-        if (cache == null) {\n-            return new MemoryLRUCacheBytesIterator(Collections.emptyIterator(), new NamedCache(namespace, this.metrics));\n-        }\n-        return new MemoryLRUCacheBytesIterator(cache.keyRange(from, to, false), cache);\n+        return range(namespace, from, to, false);\n     }\n \n     public long size() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1MzI5OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545053299", "bodyText": "For new sensors like this, we only need to consider built-in metrics version LATEST. Hence, you should not call throughputAndLatencySensor(), but only call the parts that are relevant for LATEST and not for FROM_0100_TO_24. You also need to adapt the corresponding unit test for that. See also KIP-444 for more details.", "author": "cadonna", "createdAt": "2020-12-17T12:30:03Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,26 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String threadId,\n+                                     final String taskId,\n+                                     final String storeType,\n+                                     final String storeName,\n+                                     final StreamsMetricsImpl streamsMetrics) {\n+        return throughputAndLatencySensor(", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\nindex de76bb4d0a..eb6825fa8d 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n\n@@ -315,24 +315,32 @@ public class StateStoreMetrics {\n         );\n     }\n \n-    public static Sensor prefixScanSensor(final String threadId,\n-                                     final String taskId,\n-                                     final String storeType,\n-                                     final String storeName,\n-                                     final StreamsMetricsImpl streamsMetrics) {\n-        return throughputAndLatencySensor(\n-            threadId,\n-            taskId,\n-            storeType,\n-            storeName,\n+    public static Sensor prefixScanSensor(final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+\n+        final String latencyMetricName = PREFIX_SCAN + LATENCY_SUFFIX;\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(taskId, storeType, storeName);\n+\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(taskId, storeName, PREFIX_SCAN, RecordingLevel.DEBUG);\n+        addInvocationRateToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n             PREFIX_SCAN,\n-            PREFIX_SCAN_RATE_DESCRIPTION,\n-            PREFIX_SCAN_TOTAL_DESCRIPTION,\n+            PREFIX_SCAN_RATE_DESCRIPTION\n+        );\n+\n+        addAvgAndMaxToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n+            latencyMetricName,\n             PREFIX_SCAN_AVG_LATENCY_DESCRIPTION,\n-            PREFIX_SCAN_MAX_LATENCY_DESCRIPTION,\n-            RecordingLevel.DEBUG,\n-            streamsMetrics\n+            PREFIX_SCAN_MAX_LATENCY_DESCRIPTION\n         );\n+        return sensor;\n     }\n \n     public static Sensor flushSensor(final String threadId,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r545055794", "bodyText": "In all the tests for the prefix scan you should also verify boundary conditions, e.g., if you have a prefix abcd, you should verify that abce is not matched since this is the first key that should not be matched.", "author": "cadonna", "createdAt": "2020-12-17T12:34:18Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java", "diffHunk": "@@ -359,6 +361,34 @@ public void shouldReverseIterateOverRange() {\n         ), results);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {", "originalCommit": "0e45602d3b4dfb78e72d3ad25c374ef44cf69da5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjUwMzE0OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r556503148", "bodyText": "i have added wherever the boundary conditions were not getting tested.", "author": "vamossagar12", "createdAt": "2021-01-13T13:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTc5NA=="}], "type": "inlineReview", "revised_code": {"commit": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java\nindex b946472d95..66b13c1ad4 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/CachingInMemoryKeyValueStoreTest.java\n\n@@ -389,6 +389,32 @@ public class CachingInMemoryKeyValueStoreTest extends AbstractKeyValueStoreTest\n         assertThat(values, is(Arrays.asList(\"2\", \"2\")));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKeyExcludingNextLargestKey() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(bytesKey(\"abcd\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"abcdd\"), bytesValue(\"1\")));\n+        entries.add(new KeyValue<>(bytesKey(\"abce\"), bytesValue(\"2\")));\n+        entries.add(new KeyValue<>(bytesKey(\"abc\"), bytesValue(\"2\")));\n+\n+        store.putAll(entries);\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"abcd\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(2));\n+        assertThat(keys, is(Arrays.asList(\"abcd\", \"abcdd\")));\n+        assertThat(values, is(Arrays.asList(\"2\", \"1\")));\n+    }\n+\n     @Test\n     public void shouldDeleteItemsFromCache() {\n         store.put(bytesKey(\"a\"), bytesValue(\"a\"));\n"}}, {"oid": "1210e2475f23648bbcdd338fc9a7beb38541c0c4", "url": "https://github.com/apache/kafka/commit/1210e2475f23648bbcdd338fc9a7beb38541c0c4", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T12:47:23Z", "type": "forcePushed"}, {"oid": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "url": "https://github.com/apache/kafka/commit/822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-13T13:01:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIwMTEwOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558201109", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n          \n          \n            \n                        prefixKeySerializer), prefixScanSensor);\n          \n          \n            \n                    return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);", "author": "cadonna", "createdAt": "2021-01-15T10:21:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -229,6 +232,13 @@ public V delete(final K key) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n+\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n+            prefixKeySerializer), prefixScanSensor);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\nindex 4f8b5a491d..51e6dd02e9 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java\n\n@@ -235,8 +235,7 @@ public class MeteredKeyValueStore<K, V>\n     @Override\n     public <PS extends Serializer<P>, P> KeyValueIterator<K, V> prefixScan(final P prefix, final PS prefixKeySerializer) {\n \n-        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix,\n-            prefixKeySerializer), prefixScanSensor);\n+        return new MeteredKeyValueIterator(wrapped().prefixScan(prefix, prefixKeySerializer), prefixScanSensor);\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzEyMQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217121", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false);\n          \n          \n            \n                        return new RocksDBRangeIterator(\n          \n          \n            \n                            name,\n          \n          \n            \n                            db.newIterator(columnFamily),\n          \n          \n            \n                            openIterators,\n          \n          \n            \n                            prefix,\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:33:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\nindex 4f16b90e95..12ba4eb5c3 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n\n@@ -626,8 +624,8 @@ public class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingS\n                 prefix,\n                 to,\n                 true,\n-                false);\n-\n+                false\n+            );\n         }\n \n         @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIxNzM1NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558217355", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:33:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -594,6 +616,20 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDbIterator(name, innerIterWithTimestamp, openIterators, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBRangeIterator(\n+                name,\n+                db.newIterator(columnFamily),\n+                openIterators,\n+                prefix,\n+                to,\n+                true,\n+                false);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\nindex 4f16b90e95..12ba4eb5c3 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n\n@@ -626,8 +624,8 @@ public class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingS\n                 prefix,\n                 to,\n                 true,\n-                false);\n-\n+                false\n+            );\n         }\n \n         @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMDE3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558220179", "bodyText": "I guess you forgot to remove this. \ud83d\ude42", "author": "cadonna", "createdAt": "2021-01-15T10:37:23Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -308,6 +309,24 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix,\n+                                                                                    final PS prefixKeySerializer) {\n+        Objects.requireNonNull(prefix, \"prefix cannot be null\");\n+        Objects.requireNonNull(prefixKeySerializer, \"prefixKeySerializer cannot be null\");\n+\n+        validateStoreOpen();\n+        final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+\n+        /*final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n+        final Bytes to = Bytes.increment(from);*/", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\nindex 4f16b90e95..12ba4eb5c3 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n\n@@ -318,9 +318,6 @@ public class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingS\n         validateStoreOpen();\n         final Bytes prefixBytes = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n \n-        /*final Bytes from = Bytes.wrap(prefixKeySerializer.serialize(null, prefix));\n-        final Bytes to = Bytes.increment(from);*/\n-\n         final KeyValueIterator<Bytes, byte[]> rocksDbPrefixSeekIterator = dbAccessor.prefixScan(prefixBytes);\n         openIterators.add(rocksDbPrefixSeekIterator);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMTk3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558221974", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid2)),\n+            stringSerializer.serialize(null, \"b\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(prefix, stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(valuesWithPrefix.get(0), is(\"a\"));\n+    }\n+\n+    @Test\n+    public void shouldReturnNoKeys() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"a\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"b\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"c\")),\n+            stringSerializer.serialize(null, \"e\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\nindex 87965a5f32..0568df2d56 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n\n@@ -383,18 +383,6 @@ public class RocksDBStoreTest {\n             new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n             stringSerializer.serialize(null, \"f\")));\n \n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abc\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abce\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n         rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n         rocksDBStore.putAll(entries);\n         rocksDBStore.flush();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMjA2Nw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558222067", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:40:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k1\")),\n+            stringSerializer.serialize(null, \"a\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_3\")),\n+            stringSerializer.serialize(null, \"b\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k2\")),\n+            stringSerializer.serialize(null, \"c\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_2\")),\n+            stringSerializer.serialize(null, \"d\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"k3\")),\n+            stringSerializer.serialize(null, \"e\")));\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abc\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        entries.add(new KeyValue<>(\n+            new Bytes(stringSerializer.serialize(null, \"abce\")),\n+            stringSerializer.serialize(null, \"f\")));\n+\n+        rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n+        rocksDBStore.putAll(entries);\n+        rocksDBStore.flush();\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = rocksDBStore.prefixScan(\"prefix\", stringSerializer);\n+        final List<String> valuesWithPrefix = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            valuesWithPrefix.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(3));\n+        assertThat(valuesWithPrefix.get(0), is(\"f\"));\n+        assertThat(valuesWithPrefix.get(1), is(\"d\"));\n+        assertThat(valuesWithPrefix.get(2), is(\"b\"));\n+\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefixAsabcd = rocksDBStore.prefixScan(\"abcd\", stringSerializer);\n+        numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefixAsabcd.hasNext()) {\n+            keysWithPrefixAsabcd.next().key.get();\n+            numberOfKeysReturned++;\n+        }\n+\n+        assertThat(numberOfKeysReturned, is(1));\n+\n+    }\n+\n+    @Test\n+    public void shouldReturnUUIDsWithStringPrefix() {\n+        final List<KeyValue<Bytes, byte[]>> entries = new ArrayList<>();\n+        final Serializer<UUID> uuidSerializer = Serdes.UUID().serializer();\n+        final UUID uuid1 = UUID.randomUUID();\n+        final UUID uuid2 = UUID.randomUUID();\n+        final String prefix = uuid1.toString().substring(0, 4);\n+        entries.add(new KeyValue<>(\n+            new Bytes(uuidSerializer.serialize(null, uuid1)),\n+            stringSerializer.serialize(null, \"a\")));\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\nindex 87965a5f32..0568df2d56 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n\n@@ -383,18 +383,6 @@ public class RocksDBStoreTest {\n             new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n             stringSerializer.serialize(null, \"f\")));\n \n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abc\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abce\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n         rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n         rocksDBStore.putAll(entries);\n         rocksDBStore.flush();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223002", "bodyText": "Could you split up this unit test into two, one for prefix prefix and one for prefix abcd? The former tests the general case whereas the latter tests the corner case where the the end of the range should not be returned.", "author": "cadonna", "createdAt": "2021-01-15T10:42:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java", "diffHunk": "@@ -360,6 +361,130 @@ public void shouldPutAll() {\n                 rocksDBStore.get(new Bytes(stringSerializer.serialize(null, \"3\")))));\n     }\n \n+    @Test\n+    public void shouldReturnKeysWithGivenPrefix() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\nindex 87965a5f32..0568df2d56 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBStoreTest.java\n\n@@ -383,18 +383,6 @@ public class RocksDBStoreTest {\n             new Bytes(stringSerializer.serialize(null, \"prefix_1\")),\n             stringSerializer.serialize(null, \"f\")));\n \n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abc\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abcd\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n-        entries.add(new KeyValue<>(\n-            new Bytes(stringSerializer.serialize(null, \"abce\")),\n-            stringSerializer.serialize(null, \"f\")));\n-\n         rocksDBStore.init((StateStoreContext) context, rocksDBStore);\n         rocksDBStore.putAll(entries);\n         rocksDBStore.flush();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyMzcwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558223708", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            true);\n          \n          \n            \n                            true\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:43:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java", "diffHunk": "@@ -580,7 +601,8 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n                 openIterators,\n                 from,\n                 to,\n-                forward);\n+                forward,\n+                true);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\nindex 4f16b90e95..12ba4eb5c3 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBStore.java\n\n@@ -602,7 +599,8 @@ public class RocksDBStore implements KeyValueStore<Bytes, byte[]>, BatchWritingS\n                 from,\n                 to,\n                 forward,\n-                true);\n+                true\n+            );\n         }\n \n         @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNTAxNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558225014", "bodyText": "I think here IntelliJ's formatting confused you. \ud83d\ude42\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            to,\n          \n          \n            \n                    true,\n          \n          \n            \n                  false);\n          \n          \n            \n                            to,\n          \n          \n            \n                            true,\n          \n          \n            \n                            false\n          \n          \n            \n                        );", "author": "cadonna", "createdAt": "2021-01-15T10:46:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java", "diffHunk": "@@ -218,6 +219,19 @@ public void prepareBatch(final List<KeyValue<Bytes, byte[]>> entries,\n             return new RocksDBDualCFIterator(name, innerIterWithTimestamp, innerIterNoTimestamp, forward);\n         }\n \n+        @Override\n+        public KeyValueIterator<Bytes, byte[]> prefixScan(final Bytes prefix) {\n+            final Bytes to = Bytes.increment(prefix);\n+            return new RocksDBDualCFRangeIterator(\n+                name,\n+                db.newIterator(newColumnFamily),\n+                db.newIterator(oldColumnFamily),\n+                prefix,\n+                to,\n+        true,\n+      false);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java\nindex 63881e6b4f..db3d17560b 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStore.java\n\n@@ -228,8 +228,9 @@ public class RocksDBTimestampedStore extends RocksDBStore implements Timestamped\n                 db.newIterator(oldColumnFamily),\n                 prefix,\n                 to,\n-        true,\n-      false);\n+                true,\n+                false\n+            );\n         }\n \n         @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIyNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558226883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it =\n          \n          \n            \n                                 rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n          \n          \n            \n                    try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "author": "cadonna", "createdAt": "2021-01-15T10:49:21Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java", "diffHunk": "@@ -337,6 +341,22 @@ private void iteratorsShouldNotMigrateData() {\n             }\n             assertFalse(it.hasNext());\n         }\n+\n+        try (final KeyValueIterator<Bytes, byte[]> it =\n+                     rocksDBStore.prefixScan(\"key1\", stringSerializer)) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java\nindex 504ea251ee..a1d511ae1c 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/RocksDBTimestampedStoreTest.java\n\n@@ -342,8 +342,7 @@ public class RocksDBTimestampedStoreTest extends RocksDBStoreTest {\n             assertFalse(it.hasNext());\n         }\n \n-        try (final KeyValueIterator<Bytes, byte[]> it =\n-                     rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n+        try (final KeyValueIterator<Bytes, byte[]> it = rocksDBStore.prefixScan(\"key1\", stringSerializer)) {\n             {\n                 final KeyValue<Bytes, byte[]> keyValue = it.next();\n                 assertArrayEquals(\"key1\".getBytes(), keyValue.key.get());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMTk0NQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558231945", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T10:58:55Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -307,6 +315,34 @@ public static Sensor rangeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor prefixScanSensor(final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+\n+        final String latencyMetricName = PREFIX_SCAN + LATENCY_SUFFIX;\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(taskId, storeType, storeName);\n+\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(taskId, storeName, PREFIX_SCAN, RecordingLevel.DEBUG);\n+        addInvocationRateToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,\n+            tagMap,\n+            PREFIX_SCAN,\n+            PREFIX_SCAN_RATE_DESCRIPTION\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\nindex eb6825fa8d..06402f4ab4 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n\n@@ -331,7 +330,6 @@ public class StateStoreMetrics {\n             PREFIX_SCAN,\n             PREFIX_SCAN_RATE_DESCRIPTION\n         );\n-\n         addAvgAndMaxToSensor(\n             sensor,\n             STATE_STORE_LEVEL_GROUP,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzMjIyMA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558232220", "bodyText": "This description is not needed since there is no total metric.", "author": "cadonna", "createdAt": "2021-01-15T10:59:24Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -110,6 +110,14 @@ private StateStoreMetrics() {}\n     private static final String RANGE_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n     private static final String RANGE_MAX_LATENCY_DESCRIPTION = MAX_LATENCY_DESCRIPTION_PREFIX + RANGE_DESCRIPTION;\n \n+    private static final String PREFIX_SCAN = \"prefix-scan\";\n+    private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n+    private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\nindex eb6825fa8d..06402f4ab4 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java\n\n@@ -112,7 +112,6 @@ public class StateStoreMetrics {\n \n     private static final String PREFIX_SCAN = \"prefix-scan\";\n     private static final String PREFIX_SCAN_DESCRIPTION = \"calls to prefix-scan\";\n-    private static final String PREFIX_SCAN_TOTAL_DESCRIPTION = TOTAL_DESCRIPTION + PREFIX_SCAN_DESCRIPTION;\n     private static final String PREFIX_SCAN_RATE_DESCRIPTION =\n             RATE_DESCRIPTION_PREFIX + PREFIX_SCAN_DESCRIPTION + RATE_DESCRIPTION_SUFFIX;\n     private static final String PREFIX_SCAN_AVG_LATENCY_DESCRIPTION = AVG_LATENCY_DESCRIPTION_PREFIX + PREFIX_SCAN_DESCRIPTION;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDYzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234637", "bodyText": "I do not think you need to put an entry if you use mocks.", "author": "cadonna", "createdAt": "2021-01-15T11:04:08Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 37f7fa9603..88b72df468 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -438,13 +438,10 @@ public class MeteredKeyValueStoreTest {\n     @Test\n     public void shouldGetRecordsWithPrefixKey() {\n         final StringSerializer stringSerializer = new StringSerializer();\n-        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n-        expectLastCall();\n         expect(inner.prefixScan(KEY, stringSerializer))\n             .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n         init();\n \n-        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);\n         final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n         assertThat(iterator.next().value, equalTo(VALUE));\n         iterator.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODIzNDg3NA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558234874", "bodyText": "This line is not needed in this case. A method call without a return value is expected on the mock if you simply call the method on the mock in the replay phase.", "author": "cadonna", "createdAt": "2021-01-15T11:04:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,25 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n+        expectLastCall();", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 37f7fa9603..88b72df468 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -438,13 +438,10 @@ public class MeteredKeyValueStoreTest {\n     @Test\n     public void shouldGetRecordsWithPrefixKey() {\n         final StringSerializer stringSerializer = new StringSerializer();\n-        inner.put(eq(Bytes.increment(KEY_BYTES)), aryEq(VALUE_BYTES));\n-        expectLastCall();\n         expect(inner.prefixScan(KEY, stringSerializer))\n             .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n         init();\n \n-        metered.put(Bytes.increment(KEY_BYTES).toString(), VALUE);\n         final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n         assertThat(iterator.next().value, equalTo(VALUE));\n         iterator.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNDAwMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558304002", "bodyText": "I think we do not need this method. We can just call range() in CachingKeyValueStore.", "author": "cadonna", "createdAt": "2021-01-15T13:25:15Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -218,6 +222,10 @@ public MemoryLRUCacheBytesIterator reverseAll(final String namespace) {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n+    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java b/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\nindex 84e6c5fcce..fce7875835 100644\n--- a/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\n+++ b/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java\n\n@@ -222,10 +222,6 @@ public class ThreadCache {\n         return new MemoryLRUCacheBytesIterator(cache.reverseAllKeys(), cache);\n     }\n \n-    public MemoryLRUCacheBytesIterator prefixScan(final String namespace, final Bytes from, final Bytes to) {\n-        return range(namespace, from, to, false);\n-    }\n-\n     public long size() {\n         long size = 0;\n         for (final NamedCache cache : caches.values()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzIyOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307228", "bodyText": "If we remove prefixScan(), you can remove this test, but we need a test for range() that excludes the end limit.", "author": "cadonna", "createdAt": "2021-01-15T13:30:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java", "diffHunk": "@@ -285,6 +285,25 @@ public void shouldPeekAndIterateOverRange() {\n         assertEquals(5, bytesIndex);\n     }\n \n+    @Test\n+    public void testPrefixScan() {", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java\nindex 8201454f7b..afd5449afd 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ThreadCacheTest.java\n\n@@ -286,13 +286,13 @@ public class ThreadCacheTest {\n     }\n \n     @Test\n-    public void testPrefixScan() {\n+    public void shouldSkipToEntryWhentoInclusiveIsFalseInRange() {\n         final ThreadCache cache = new ThreadCache(logContext, 10000L, new MockStreamsMetrics(new Metrics()));\n         final byte[][] bytes = {{0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}, {10}};\n         for (final byte[] aByte : bytes) {\n             cache.put(namespace, Bytes.wrap(aByte), dirtyEntry(aByte));\n         }\n-        final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.prefixScan(namespace, Bytes.wrap(new byte[]{1}), Bytes.wrap(new byte[]{4}));\n+        final ThreadCache.MemoryLRUCacheBytesIterator iterator = cache.range(namespace, Bytes.wrap(new byte[]{1}), Bytes.wrap(new byte[]{4}), false);\n         int bytesIndex = 1;\n         while (iterator.hasNext()) {\n             final Bytes peekedKey = iterator.peekNextKey();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc1Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307756", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:32Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\nindex e4732fe244..4972dd4686 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\n@@ -210,11 +210,9 @@ public class StateStoreMetricsTest {\n         final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n         final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n         final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n-\n         expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n             .andReturn(expectedSensor);\n         expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n-\n         StreamsMetricsImpl.addInvocationRateToSensor(\n             expectedSensor,\n             STORE_LEVEL_GROUP,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzc5MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307791", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\nindex e4732fe244..4972dd4686 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\n@@ -210,11 +210,9 @@ public class StateStoreMetricsTest {\n         final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n         final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n         final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n-\n         expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n             .andReturn(expectedSensor);\n         expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n-\n         StreamsMetricsImpl.addInvocationRateToSensor(\n             expectedSensor,\n             STORE_LEVEL_GROUP,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODMwNzkyMw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r558307923", "bodyText": "Please remove empty line.", "author": "cadonna", "createdAt": "2021-01-15T13:31:56Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,42 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+", "originalCommit": "822edfe01fa4e19ce5896e7671d374f7b3b4c99e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "17be91a37214bf77430c65d9300a5120e4348df9", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\nindex e4732fe244..4972dd4686 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\n@@ -210,11 +210,9 @@ public class StateStoreMetricsTest {\n         final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n         final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n         final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n-\n         expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n             .andReturn(expectedSensor);\n         expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n-\n         StreamsMetricsImpl.addInvocationRateToSensor(\n             expectedSensor,\n             STORE_LEVEL_GROUP,\n"}}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "commit"}, {"oid": "17be91a37214bf77430c65d9300a5120e4348df9", "url": "https://github.com/apache/kafka/commit/17be91a37214bf77430c65d9300a5120e4348df9", "message": "KAFKA-10648: Add Prefix Scan support to State Stores", "committedDate": "2021-01-20T12:27:29Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564394476", "bodyText": "This method needs unit testing. Try to use a mock for the cache in the test.", "author": "cadonna", "createdAt": "2021-01-26T10:11:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "diffHunk": "@@ -291,6 +292,16 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         return new MergedSortedCacheKeyValueBytesStoreIterator(cacheIterator, storeIterator, true);\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzNDAzNA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566234034", "bodyText": "Similar comment as below. Unit tests are in CachingInMemoryKeyValueStoreTest which already extends AbstractKeyValueStoreTest and creates an in memory cache store.", "author": "vamossagar12", "createdAt": "2021-01-28T16:31:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3ODg3MQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567478871", "bodyText": "Oh, I see. I missed those. Sorry! That is fine then, although I think unit tests with mocks would be better.", "author": "cadonna", "createdAt": "2021-01-31T20:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1NzcyNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570157727", "bodyText": "Actually, I had created another ticket to streamline tests for CachingKVStore: https://issues.apache.org/jira/browse/KAFKA-10788. @rohitrmd  had volunteered to take this up.", "author": "vamossagar12", "createdAt": "2021-02-04T11:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDM5NDQ3Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNTMwOA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564405308", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n          \n          \n            \n                    final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n          \n      \n    \n    \n  \n\nIn such a way, you can reuse variable hi and there. Similar is true for my suggestions below.", "author": "cadonna", "createdAt": "2021-01-26T10:27:55Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\nindex ed3963509f..c538959658 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\n@@ -205,7 +205,7 @@ public class ChangeLoggingKeyValueBytesStoreTest {\n     public void shouldGetRecordsWithPrefixKey() {\n         store.put(hi, there);\n         store.put(Bytes.increment(hi), world);\n-        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n         final List<String> keys = new ArrayList<>();\n         final List<String> values = new ArrayList<>();\n         int numberOfKeysReturned = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjI2OA==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406268", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final List<String> keys = new ArrayList<>();\n          \n          \n            \n                    final List<String> values = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> keys = new ArrayList<>();\n          \n          \n            \n                    final List<Bytes> values = new ArrayList<>();", "author": "cadonna", "createdAt": "2021-01-26T10:29:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\nindex ed3963509f..c538959658 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\n@@ -205,7 +205,7 @@ public class ChangeLoggingKeyValueBytesStoreTest {\n     public void shouldGetRecordsWithPrefixKey() {\n         store.put(hi, there);\n         store.put(Bytes.increment(hi), world);\n-        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n         final List<String> keys = new ArrayList<>();\n         final List<String> values = new ArrayList<>();\n         int numberOfKeysReturned = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjUxMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406512", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        keys.add(next.key.toString());\n          \n          \n            \n                        values.add(new String(next.value));\n          \n          \n            \n                        keys.add(next.key);\n          \n          \n            \n                        values.add(Bytes.wrap(next.value));", "author": "cadonna", "createdAt": "2021-01-26T10:29:53Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\nindex ed3963509f..c538959658 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\n@@ -205,7 +205,7 @@ public class ChangeLoggingKeyValueBytesStoreTest {\n     public void shouldGetRecordsWithPrefixKey() {\n         store.put(hi, there);\n         store.put(Bytes.increment(hi), world);\n-        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n         final List<String> keys = new ArrayList<>();\n         final List<String> values = new ArrayList<>();\n         int numberOfKeysReturned = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwNjg4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564406883", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertThat(keys, is(Collections.singletonList(\"hi\")));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(\"there\")));\n          \n          \n            \n                    assertThat(keys, is(Collections.singletonList(hi)));\n          \n          \n            \n                    assertThat(values, is(Collections.singletonList(Bytes.wrap(there))));", "author": "cadonna", "createdAt": "2021-01-26T10:30:22Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java", "diffHunk": "@@ -196,6 +201,26 @@ public void shouldReturnValueOnGetWhenExists() {\n         assertThat(store.get(hello), equalTo(world));\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        store.put(hi, there);\n+        store.put(Bytes.increment(hi), world);\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final List<String> keys = new ArrayList<>();\n+        final List<String> values = new ArrayList<>();\n+        int numberOfKeysReturned = 0;\n+\n+        while (keysWithPrefix.hasNext()) {\n+            final KeyValue<Bytes, byte[]> next = keysWithPrefix.next();\n+            keys.add(next.key.toString());\n+            values.add(new String(next.value));\n+            numberOfKeysReturned++;\n+        }\n+        assertThat(numberOfKeysReturned, is(1));\n+        assertThat(keys, is(Collections.singletonList(\"hi\")));\n+        assertThat(values, is(Collections.singletonList(\"there\")));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\nindex ed3963509f..c538959658 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\n@@ -205,7 +205,7 @@ public class ChangeLoggingKeyValueBytesStoreTest {\n     public void shouldGetRecordsWithPrefixKey() {\n         store.put(hi, there);\n         store.put(Bytes.increment(hi), world);\n-        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(\"hi\", new StringSerializer());\n+        final KeyValueIterator<Bytes, byte[]> keysWithPrefix = store.prefixScan(hi.toString(), new StringSerializer());\n         final List<String> keys = new ArrayList<>();\n         final List<String> values = new ArrayList<>();\n         int numberOfKeysReturned = 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564409937", "bodyText": "@vamossagar12 I can still not find the unit test for this method.", "author": "cadonna", "createdAt": "2021-01-26T10:35:35Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStore.java", "diffHunk": "@@ -103,6 +105,20 @@ public void putAll(final List<KeyValue<Bytes, byte[]>> entries) {\n         }\n     }\n \n+    @Override\n+    public <PS extends Serializer<P>, P> KeyValueIterator<Bytes, byte[]> prefixScan(final P prefix, final PS prefixKeySerializer) {", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NjIzMzAzOQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r566233039", "bodyText": "For this, do you want me to add the test cases here?https://github.com/apache/kafka/blob/17be91a37214bf77430c65d9300a5120e4348df9/streams/src/test/java/org/apache/kafka/streams/state/internals/InMemoryKeyValueStoreTest.java\nThere are tests in CachingInMemoryKeyValueStoreTest, which is where the tests for other methods like range etc have been added.", "author": "vamossagar12", "createdAt": "2021-01-28T16:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzQ3OTU0Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r567479543", "bodyText": "I think those tests never call the prefixScan() on the underlying in-memory state store because all entries fit into the cache. You would need to add another test that flushes the cache before you call prefixScan(). I would prefer a test that directly tests the in-memory store without any cache in between.", "author": "cadonna", "createdAt": "2021-01-31T20:27:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDE1MzEyMg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r570153122", "bodyText": "Here is the new ticket: https://issues.apache.org/jira/browse/KAFKA-12289 and the PR for the ticket:\n#10052", "author": "vamossagar12", "createdAt": "2021-02-04T11:33:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQwOTkzNw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQxNTY4Mw==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564415683", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");", "author": "cadonna", "createdAt": "2021-01-26T10:44:39Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "dddad17ad5102e937150bd7115c215b92807e734", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 88b72df468..5ad9d79288 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -446,7 +446,7 @@ public class MeteredKeyValueStoreTest {\n         assertThat(iterator.next().value, equalTo(VALUE));\n         iterator.close();\n \n-        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n+        final KafkaMetric metric = metric(\"prefix-scan-rate\");\n         assertTrue((Double) metric.metricValue() > 0);\n         verify(inner);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NDQyMTkzNg==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r564421936", "bodyText": "nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final Sensor sensor =\n          \n          \n            \n                            StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n          \n          \n            \n                    final Sensor sensor = StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "author": "cadonna", "createdAt": "2021-01-26T10:55:02Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -204,6 +204,39 @@ public void shouldGetRangeSensor() {\n         );\n     }\n \n+    @Test\n+    public void shouldGetPrefixScanSensor() {\n+        final String metricName = \"prefix-scan\";\n+        final String descriptionOfRate = \"The average number of calls to prefix-scan per second\";\n+        final String descriptionOfAvg = \"The average latency of calls to prefix-scan\";\n+        final String descriptionOfMax = \"The maximum latency of calls to prefix-scan\";\n+        expect(streamsMetrics.storeLevelSensor(TASK_ID, STORE_NAME, metricName, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addInvocationRateToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            metricName,\n+            descriptionOfRate\n+        );\n+        StreamsMetricsImpl.addAvgAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,\n+            storeTagMap,\n+            latencyMetricName(metricName),\n+            descriptionOfAvg,\n+            descriptionOfMax\n+        );\n+        replay(StreamsMetricsImpl.class, streamsMetrics);\n+\n+        final Sensor sensor =\n+                StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);", "originalCommit": "17be91a37214bf77430c65d9300a5120e4348df9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\nindex 4972dd4686..4e4d6c8706 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\n@@ -230,8 +230,7 @@ public class StateStoreMetricsTest {\n         );\n         replay(StreamsMetricsImpl.class, streamsMetrics);\n \n-        final Sensor sensor =\n-                StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n+        final Sensor sensor = StateStoreMetrics.prefixScanSensor(TASK_ID, STORE_TYPE, STORE_NAME, streamsMetrics);\n \n         verify(StreamsMetricsImpl.class, streamsMetrics);\n         assertThat(sensor, is(expectedSensor));\n"}}, {"oid": "4a41206daa0ffbc7516059d29a7ddda109f64b5e", "url": "https://github.com/apache/kafka/commit/4a41206daa0ffbc7516059d29a7ddda109f64b5e", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:16:52Z", "type": "commit"}, {"oid": "33be9113c6225063a1af489c5eca62f7645250ab", "url": "https://github.com/apache/kafka/commit/33be9113c6225063a1af489c5eca62f7645250ab", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:12Z", "type": "commit"}, {"oid": "25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "url": "https://github.com/apache/kafka/commit/25980a0b3e6fdedf2fe707f78591dd5c9ba840c9", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:21Z", "type": "commit"}, {"oid": "a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "url": "https://github.com/apache/kafka/commit/a2ea51336e4ea2010f1d93dd87d4b1526281cadb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueBytesStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:32Z", "type": "commit"}, {"oid": "dddad17ad5102e937150bd7115c215b92807e734", "url": "https://github.com/apache/kafka/commit/dddad17ad5102e937150bd7115c215b92807e734", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:17:49Z", "type": "commit"}, {"oid": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "url": "https://github.com/apache/kafka/commit/d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-01-28T16:18:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODUwODA3OQ==", "url": "https://github.com/apache/kafka/pull/9508#discussion_r568508079", "bodyText": "To get rid of the test failure, you need to change this:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final KafkaMetric metric = metric(\"prefix-scan-rate\");\n          \n          \n            \n                    final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n          \n      \n    \n    \n  \n\nSorry, the failure of the test is my bad. I missed the issue with the different metrics versions when I requested to change this in a previous review.", "author": "cadonna", "createdAt": "2021-02-02T10:55:13Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "diffHunk": "@@ -434,6 +435,22 @@ public void shouldRemoveMetricsEvenIfWrappedStoreThrowsOnClose() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetRecordsWithPrefixKey() {\n+        final StringSerializer stringSerializer = new StringSerializer();\n+        expect(inner.prefixScan(KEY, stringSerializer))\n+            .andReturn(new KeyValueIteratorStub<>(Collections.singletonList(BYTE_KEY_VALUE_PAIR).iterator()));\n+        init();\n+\n+        final KeyValueIterator<String, String> iterator = metered.prefixScan(KEY, stringSerializer);\n+        assertThat(iterator.next().value, equalTo(VALUE));\n+        iterator.close();\n+\n+        final KafkaMetric metric = metric(\"prefix-scan-rate\");", "originalCommit": "d2479a41c4d90e44c7dacb8028368cfa4a846cbb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8eca3c9c2852172896001178f8e7a115fd392aeb", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\nindex 5ad9d79288..88b72df468 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\n@@ -446,7 +446,7 @@ public class MeteredKeyValueStoreTest {\n         assertThat(iterator.next().value, equalTo(VALUE));\n         iterator.close();\n \n-        final KafkaMetric metric = metric(\"prefix-scan-rate\");\n+        final KafkaMetric metric = metrics.metric(new MetricName(\"prefix-scan-rate\", STORE_LEVEL_GROUP, \"\", tags));\n         assertTrue((Double) metric.metricValue() > 0);\n         verify(inner);\n     }\n"}}, {"oid": "8eca3c9c2852172896001178f8e7a115fd392aeb", "url": "https://github.com/apache/kafka/commit/8eca3c9c2852172896001178f8e7a115fd392aeb", "message": "Update streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java\n\nCo-authored-by: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2021-02-03T10:49:01Z", "type": "commit"}]}