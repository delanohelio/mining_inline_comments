{"pr_number": 8646, "pr_title": "KAFKA-9974: Fix flaky test by removing unneeded asserts", "pr_createdAt": "2020-05-11T08:53:22Z", "pr_url": "https://github.com/apache/kafka/pull/8646", "timeline": [{"oid": "95ed0d7fb1202024a66864ce888f750a664f2315", "url": "https://github.com/apache/kafka/commit/95ed0d7fb1202024a66864ce888f750a664f2315", "message": "KAFKA-9974: fix flaky test by removing unneeded asserts", "committedDate": "2020-05-11T08:24:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA==", "url": "https://github.com/apache/kafka/pull/8646#discussion_r424078394", "bodyText": "I am not sure if this is the right fix. Note that \"restoring\" and \"standby task update\" are two different things, and a standby task should. never use the \"restore\" code path what this assertion verifies.\nWhat could explain a restore is the migration of the active task from one instance to the other. However, this should actually not happen either. Could you reproduce the issue locally? We recently worked on rebalancing so maybe the issue is with regard to that.", "author": "mjsax", "createdAt": "2020-05-12T22:51:43Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "originalCommit": "95ed0d7fb1202024a66864ce888f750a664f2315", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUxOTEwMw==", "url": "https://github.com/apache/kafka/pull/8646#discussion_r428519103", "bodyText": "Sorry @mjsax , I tried to trace the code to find out why that would happen, but I still can't figure it out. Do you have any suggestion for it? If not, I think we can at least have more clear message output when this assert failure happen again. I'm not sure if the info is enough if the error happened again. And not sure if you have any thoughts about it?\nIt'll look like this when assert failure.\n\nThanks.", "author": "showuon", "createdAt": "2020-05-21T08:33:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMTc1OQ==", "url": "https://github.com/apache/kafka/pull/8646#discussion_r429501759", "bodyText": "I am not 100% sure about the root cause of the issue... If you have a good suggestion for a better error message, that would be great. I have not idea how we could improve it atm.", "author": "mjsax", "createdAt": "2020-05-23T01:24:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM1OTAwMA==", "url": "https://github.com/apache/kafka/pull/8646#discussion_r438359000", "bodyText": "I think one possible explanation is that when we start the two instances, the rebalances took not just once but twice (once for the first instance, and another time for the second). In between the task may already be processed a bit on the first instance, and then after the second rebalance it was migrated and hence was restored a bit causing the listener to be triggered.\nI'd agree with @showuon here that we do not need to strictly forbids restoration not happening when starting the first two instances, just making sure after closing one instance we can restore the other up to the first batch is enough (which is already validated). So I'm fine with removing this listener all together.", "author": "guozhangwang", "createdAt": "2020-06-10T19:29:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM1OTUxNA==", "url": "https://github.com/apache/kafka/pull/8646#discussion_r438359514", "bodyText": "BTW there's another report that line 159 can also fail:\njava.lang.AssertionError: Expected: is <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:159)\n\nWhich is a bit mystery to me, since I cannot really think of a way how that could happen.\nANyways, for now removing that listener all together seems good to me.", "author": "guozhangwang", "createdAt": "2020-06-10T19:30:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}], "type": "inlineReview", "revised_code": {"commit": "8f26e431bbd320106236379e2af821aa9dc0f17f", "chunk": "diff --git a/streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java b/streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java\nindex 03348a4801..20539c805e 100644\n--- a/streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java\n+++ b/streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java\n\n@@ -133,6 +133,17 @@ public class OptimizedKTableIntegrationTest {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n+        final String unexpectedMsg = \"We had an unexpected restore, current status is: \\n\"\n+                + \"keyQueryMetadata: %s \\n\"\n+                + \"kafkaStreams1 state: %s \\n\"\n+                + \"kafkaStreams1 state: %s \\n\";\n+\n+        // Assert that no restore has occurred, ensures that when we check later that the restore\n+        // notification actually came from after the rebalance.\n+        assertThat(String.format(unexpectedMsg, keyQueryMetadata, kafkaStreams1.state(), kafkaStreams2.state()),\n+                listener.startOffset, is(equalTo(0L)));\n+        assertThat(listener.totalNumRestored, is(equalTo(0L)));\n+\n         // Assert that the current value in store reflects all messages being processed\n         assertThat(kafkaStreams1WasFirstActive ? store1.get(key) : store2.get(key), is(equalTo(batch1NumMessages - 1)));\n \n"}}, {"oid": "28c7b7d5dc542b688eefdac2d7215081d7d8ba2d", "url": "https://github.com/apache/kafka/commit/28c7b7d5dc542b688eefdac2d7215081d7d8ba2d", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KAFKA-9974", "committedDate": "2020-05-21T08:11:21Z", "type": "commit"}, {"oid": "8f26e431bbd320106236379e2af821aa9dc0f17f", "url": "https://github.com/apache/kafka/commit/8f26e431bbd320106236379e2af821aa9dc0f17f", "message": "add more info when failed", "committedDate": "2020-05-21T08:23:40Z", "type": "commit"}, {"oid": "fd53b0c97bf7ad140b32329b203315df23576753", "url": "https://github.com/apache/kafka/commit/fd53b0c97bf7ad140b32329b203315df23576753", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KAFKA-9974", "committedDate": "2020-06-15T09:49:52Z", "type": "commit"}, {"oid": "e82aa8808cb781c5367a1ca5fada4c964a98943e", "url": "https://github.com/apache/kafka/commit/e82aa8808cb781c5367a1ca5fada4c964a98943e", "message": "KAFKA-9974: remove the listener", "committedDate": "2020-06-15T14:18:04Z", "type": "commit"}, {"oid": "e82aa8808cb781c5367a1ca5fada4c964a98943e", "url": "https://github.com/apache/kafka/commit/e82aa8808cb781c5367a1ca5fada4c964a98943e", "message": "KAFKA-9974: remove the listener", "committedDate": "2020-06-15T14:18:04Z", "type": "forcePushed"}]}