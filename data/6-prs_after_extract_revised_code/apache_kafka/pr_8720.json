{"pr_number": 8720, "pr_title": "KAFKA-9971: Error Reporting in Sink Connectors (KIP-610)", "pr_createdAt": "2020-05-23T01:00:41Z", "pr_url": "https://github.com/apache/kafka/pull/8720", "timeline": [{"oid": "26034508ea5c799768484c70df2a06fe0f8c53c9", "url": "https://github.com/apache/kafka/commit/26034508ea5c799768484c70df2a06fe0f8c53c9", "message": "KAFKA-9971: Error Reporting in Sink Connectors\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "1134e990335001d732678c4def54738a4d71473d", "url": "https://github.com/apache/kafka/commit/1134e990335001d732678c4def54738a4d71473d", "message": "addressed comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "url": "https://github.com/apache/kafka/commit/cd043161ad3a430f4c341fb86b4a4f31aded0e5b", "message": "addressed more comments\n\nSigned-off-by: Aakash Shah <ashah@confluent.io>", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "message": "addressed some more comments", "committedDate": "2020-05-27T19:26:35Z", "type": "commit"}, {"oid": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "url": "https://github.com/apache/kafka/commit/b91a8f989aa3dcfccf9d3498717c0517b013c677", "message": "addressed some more comments", "committedDate": "2020-05-27T19:26:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NTU3OQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431395579", "bodyText": "We should not make this method a default method, since both implementations of the interface define this method.", "author": "rhauch", "createdAt": "2020-05-27T19:34:37Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "diffHunk": "@@ -16,17 +16,25 @@\n  */\n package org.apache.kafka.connect.runtime.errors;\n \n+import org.apache.kafka.clients.producer.RecordMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+\n /**\n  * Report an error using the information contained in the {@link ProcessingContext}.\n  */\n public interface ErrorReporter extends AutoCloseable {\n \n     /**\n-     * Report an error.\n+     * Report an error and return the producer future.\n      *\n      * @param context the processing context (cannot be null).\n+     * @return future result from the producer sending a record to Kafka.\n      */\n-    void report(ProcessingContext context);\n+    default Future<RecordMetadata> report(ProcessingContext context) {\n+        return CompletableFuture.completedFuture(null);\n+    }", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java\nindex fd68d7f42e..f9bc2f2360 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java\n\n@@ -18,7 +18,6 @@ package org.apache.kafka.connect.runtime.errors;\n \n import org.apache.kafka.clients.producer.RecordMetadata;\n \n-import java.util.concurrent.CompletableFuture;\n import java.util.concurrent.Future;\n \n /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM5NzA1NA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431397054", "bodyText": "What about creating:\n    private static final Future<RecordMetadata> COMPLETED = CompletableFuture.completedFuture(null);\n\nand then returning that instance in all of these places. Since it's already completed, immutable, and we don't allow cancellation, it should be fine to reuse in this LogReporter.", "author": "rhauch", "createdAt": "2020-05-27T19:37:27Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java", "diffHunk": "@@ -50,17 +53,18 @@ public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandling\n      * @param context the processing context.\n      */\n     @Override\n-    public void report(ProcessingContext context) {\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (!connConfig.enableErrorLog()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         if (!context.failed()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         log.error(message(context), context.error());\n         errorHandlingMetrics.recordErrorLogged();\n+        return CompletableFuture.completedFuture(null);", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java\nindex 462ddbc7db..cf9db2c44d 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java\n\n@@ -55,16 +56,16 @@ public class LogReporter implements ErrorReporter {\n     @Override\n     public Future<RecordMetadata> report(ProcessingContext context) {\n         if (!connConfig.enableErrorLog()) {\n-            return CompletableFuture.completedFuture(null);\n+            return COMPLETED;\n         }\n \n         if (!context.failed()) {\n-            return CompletableFuture.completedFuture(null);\n+            return COMPLETED;\n         }\n \n         log.error(message(context), context.error());\n         errorHandlingMetrics.recordErrorLogged();\n-        return CompletableFuture.completedFuture(null);\n+        return COMPLETED;\n     }\n \n     // Visible for testing\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMDc4Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431400787", "bodyText": "How about clarifying this a bit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        String topic = record.topic();\n          \n          \n            \n                        // Generate a new consumer record from the modified sink record. We prefer\n          \n          \n            \n                        // to send the original consumer record (pre-transformed) to the DLQ, \n          \n          \n            \n                        // but in this case we don't have one and send the potentially transformed\n          \n          \n            \n                        // record instead\n          \n          \n            \n                        String topic = record.topic();", "author": "rhauch", "createdAt": "2020-05-27T19:44:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 1ac5b2e032..5bd9ace33f 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -72,6 +72,10 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n         if (record instanceof InternalSinkRecord) {\n             consumerRecord = ((InternalSinkRecord) record).originalRecord();\n         } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n             String topic = record.topic();\n             byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n             byte[] value = valueConverter.fromConnectData(topic,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwMjEyMw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431402123", "bodyText": "We should use the length of the key and value in the record:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n          \n          \n            \n                            -1, key, value, headers);\n          \n          \n            \n                        int keyLength = key != null ? key.length : -1;\n          \n          \n            \n                        int valLength = value != null ? value.length : -1;\n          \n          \n            \n                        consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n          \n          \n            \n                            record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n          \n          \n            \n                            valLength, key, value, headers);", "author": "rhauch", "createdAt": "2020-05-27T19:47:20Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, -1,\n+                -1, key, value, headers);", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 1ac5b2e032..5bd9ace33f 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -72,6 +72,10 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n         if (record instanceof InternalSinkRecord) {\n             consumerRecord = ((InternalSinkRecord) record).originalRecord();\n         } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n             String topic = record.topic();\n             byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n             byte[] value = valueConverter.fromConnectData(topic,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQwNzg1NA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431407854", "bodyText": "Once again, please add trace log messages before an after this line.", "author": "rhauch", "createdAt": "2020-05-27T19:58:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -360,6 +364,10 @@ private void doCommit(Map<TopicPartition, OffsetAndMetadata> offsets, boolean cl\n     }\n \n     private void commitOffsets(long now, boolean closing) {\n+        if (workerErrantRecordReporter != null) {\n+            workerErrantRecordReporter.awaitAllFutures();", "originalCommit": "b91a8f989aa3dcfccf9d3498717c0517b013c677", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\nindex 42614c9025..0d339ea524 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n\n@@ -365,7 +365,9 @@ class WorkerSinkTask extends WorkerTask {\n \n     private void commitOffsets(long now, boolean closing) {\n         if (workerErrantRecordReporter != null) {\n+            log.trace(\"Awaiting all reported errors to be completed\");\n             workerErrantRecordReporter.awaitAllFutures();\n+            log.trace(\"Completed all reported errors\");\n         }\n \n         if (currentOffsets.isEmpty())\n"}}, {"oid": "ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "url": "https://github.com/apache/kafka/commit/ab1af2bd15f4a05974ec55bcae34ad39ece1aad3", "message": "KAFKA-9971: Added trace logging", "committedDate": "2020-05-27T20:02:55Z", "type": "commit"}, {"oid": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "url": "https://github.com/apache/kafka/commit/0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "message": "more comments", "committedDate": "2020-05-27T21:24:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDExNg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474116", "bodyText": "Hmm, let's just have this delegate to the super method. It's internal, so we need not include the original record details.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return \"InternalSinkRecord{\" +\n          \n          \n            \n                            \"consumerRecord=\" + originalRecord.toString() +\n          \n          \n            \n                            \"} \" + super.toString();\n          \n          \n            \n                    return super.toString();", "author": "rhauch", "createdAt": "2020-05-27T22:16:37Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"InternalSinkRecord{\" +\n+                \"consumerRecord=\" + originalRecord.toString() +\n+                \"} \" + super.toString();", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\nindex e9046747ef..c9229c4cee 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n\n@@ -34,7 +34,7 @@ public class InternalSinkRecord extends SinkRecord {\n         this.originalRecord = originalRecord;\n     }\n \n-    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n                               int partition, Schema keySchema, Object key, Schema valueSchema,\n                               Object value, long kafkaOffset, Long timestamp,\n                               TimestampType timestampType, Iterable<Header> headers) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NDI5Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431474297", "bodyText": "Let's make this protected.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n          \n          \n            \n                protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "author": "rhauch", "createdAt": "2020-05-27T22:17:06Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\nindex e9046747ef..c9229c4cee 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n\n@@ -34,7 +34,7 @@ public class InternalSinkRecord extends SinkRecord {\n         this.originalRecord = originalRecord;\n     }\n \n-    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n                               int partition, Schema keySchema, Object key, Schema valueSchema,\n                               Object value, long kafkaOffset, Long timestamp,\n                               TimestampType timestampType, Iterable<Header> headers) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NTg0Ng==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431475846", "bodyText": "These can be final.", "author": "rhauch", "createdAt": "2020-05-27T22:21:14Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 5bd9ace33f..8bafffbaaa 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -42,10 +42,10 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n \n     private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n \n-    private RetryWithToleranceOperator retryWithToleranceOperator;\n-    private Converter keyConverter;\n-    private Converter valueConverter;\n-    private HeaderConverter headerConverter;\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n \n     // Visible for testing\n     final LinkedList<Future<Void>> futures;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ3NjI3MA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431476270", "bodyText": "Nit: new line is unnecessary, and there's a misspelling:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's \" +\n          \n          \n            \n                                \"completition.\");\n          \n          \n            \n                            log.error(\"Encountered an error while awaiting an errant record future's completion.\");", "author": "rhauch", "createdAt": "2020-05-27T22:22:19Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private RetryWithToleranceOperator retryWithToleranceOperator;\n+    private Converter keyConverter;\n+    private Converter valueConverter;\n+    private HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's \" +\n+                    \"completition.\");", "originalCommit": "0e40408c23c1086dddf01d741a5e27a86fc2ab9a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 5bd9ace33f..8bafffbaaa 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -42,10 +42,10 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n \n     private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n \n-    private RetryWithToleranceOperator retryWithToleranceOperator;\n-    private Converter keyConverter;\n-    private Converter valueConverter;\n-    private HeaderConverter headerConverter;\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n \n     // Visible for testing\n     final LinkedList<Future<Void>> futures;\n"}}, {"oid": "126b04c568381dc108db87da27ddd4862a5e0d2b", "url": "https://github.com/apache/kafka/commit/126b04c568381dc108db87da27ddd4862a5e0d2b", "message": "more comments again", "committedDate": "2020-05-27T22:58:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxMjAxOQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431512019", "bodyText": "nit: indentation is a bit off here.", "author": "kkonstantine", "createdAt": "2020-05-28T00:10:55Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\nindex c9229c4cee..69554ffb30 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n\n@@ -23,6 +23,11 @@ import org.apache.kafka.connect.data.Schema;\n import org.apache.kafka.connect.header.Header;\n import org.apache.kafka.connect.sink.SinkRecord;\n \n+/**\n+ * A specialization of {@link SinkRecord} that allows a {@link WorkerSinkTask} to track the\n+ * original {@link ConsumerRecord} for each {@link SinkRecord}. It is used internally and not\n+ * exposed to connectors.\n+ */\n public class InternalSinkRecord extends SinkRecord {\n \n     private final ConsumerRecord<byte[], byte[]> originalRecord;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514015", "bodyText": "these overrides don't seem to add much.", "author": "kkonstantine", "createdAt": "2020-05-28T00:17:44Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {\n+\n+    private final ConsumerRecord<byte[], byte[]> originalRecord;\n+\n+    public InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, SinkRecord record) {\n+        super(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(),\n+            record.valueSchema(), record.value(), record.kafkaOffset(), record.timestamp(),\n+            record.timestampType(), record.headers());\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    protected InternalSinkRecord(ConsumerRecord<byte[], byte[]> originalRecord, String topic,\n+                              int partition, Schema keySchema, Object key, Schema valueSchema,\n+                              Object value, long kafkaOffset, Long timestamp,\n+                              TimestampType timestampType, Iterable<Header> headers) {\n+        super(topic, partition, keySchema, key, valueSchema, value, kafkaOffset, timestamp, timestampType, headers);\n+        this.originalRecord = originalRecord;\n+    }\n+\n+    @Override\n+    public SinkRecord newRecord(String topic, Integer kafkaPartition, Schema keySchema, Object key,\n+                                Schema valueSchema, Object value, Long timestamp,\n+                                Iterable<Header> headers) {\n+        return new InternalSinkRecord(originalRecord, topic, kafkaPartition, keySchema, key,\n+            valueSchema, value, kafkaOffset(), timestamp, timestampType(), headers());\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        return super.equals(o);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return super.hashCode();\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return super.toString();\n+    }", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0NDg0Mg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431544842", "bodyText": "IIUC, spotbugs complained if these were not here.", "author": "rhauch", "createdAt": "2020-05-28T02:17:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU1NzcxNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431557717", "bodyText": "never mind then. I'll leave this to AI.", "author": "kkonstantine", "createdAt": "2020-05-28T03:11:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDAxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\nindex c9229c4cee..69554ffb30 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n\n@@ -23,6 +23,11 @@ import org.apache.kafka.connect.data.Schema;\n import org.apache.kafka.connect.header.Header;\n import org.apache.kafka.connect.sink.SinkRecord;\n \n+/**\n+ * A specialization of {@link SinkRecord} that allows a {@link WorkerSinkTask} to track the\n+ * original {@link ConsumerRecord} for each {@link SinkRecord}. It is used internally and not\n+ * exposed to connectors.\n+ */\n public class InternalSinkRecord extends SinkRecord {\n \n     private final ConsumerRecord<byte[], byte[]> originalRecord;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNDMzNg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431514336", "bodyText": "nit: I think we always keep = with the left operand.", "author": "kkonstantine", "createdAt": "2020-05-28T00:18:41Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -553,13 +554,16 @@ private WorkerTask buildWorkerTask(ClusterConfigState configState,\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n+            WorkerErrantRecordReporter workerErrantRecordReporter\n+                = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java\nindex 1275079bef..b890e691c3 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java\n\n@@ -554,8 +554,7 @@ public class Worker {\n             log.info(\"Initializing: {}\", transformationChain);\n             SinkConnectorConfig sinkConfig = new SinkConnectorConfig(plugins, connConfig.originalsStrings());\n             retryWithToleranceOperator.reporters(sinkTaskReporters(id, sinkConfig, errorHandlingMetrics, connectorClass));\n-            WorkerErrantRecordReporter workerErrantRecordReporter\n-                = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,\n+            WorkerErrantRecordReporter workerErrantRecordReporter = createWorkerErrantRecordReporter(sinkConfig, retryWithToleranceOperator,\n                     keyConverter, valueConverter, headerConverter);\n \n             Map<String, Object> consumerProps = consumerConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUxNjcyNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431516724", "bodyText": "should be final right?", "author": "kkonstantine", "createdAt": "2020-05-28T00:27:30Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -94,6 +95,7 @@\n     private int commitFailures;\n     private boolean pausedForRedelivery;\n     private boolean committing;\n+    private WorkerErrantRecordReporter workerErrantRecordReporter;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\nindex 0d339ea524..8c4a496271 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n\n@@ -95,7 +95,7 @@ class WorkerSinkTask extends WorkerTask {\n     private int commitFailures;\n     private boolean pausedForRedelivery;\n     private boolean committing;\n-    private WorkerErrantRecordReporter workerErrantRecordReporter;\n+    private final WorkerErrantRecordReporter workerErrantRecordReporter;\n \n     public WorkerSinkTask(ConnectorTaskId id,\n                           SinkTask task,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjUzNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522534", "bodyText": "let's add protected here to be symmetric to what other fields that are accessed by the context have as scope", "author": "kkonstantine", "createdAt": "2020-05-28T00:49:31Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java", "diffHunk": "@@ -518,6 +535,10 @@ private Headers convertHeadersFor(ConsumerRecord<byte[], byte[]> record) {\n         return result;\n     }\n \n+    WorkerErrantRecordReporter workerErrantRecordReporter() {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\nindex 0d339ea524..8c4a496271 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java\n\n@@ -535,7 +535,7 @@ class WorkerSinkTask extends WorkerTask {\n         return result;\n     }\n \n-    WorkerErrantRecordReporter workerErrantRecordReporter() {\n+    protected WorkerErrantRecordReporter workerErrantRecordReporter() {\n         return workerErrantRecordReporter;\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMjkwOQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431522909", "bodyText": "typo. Not sure how you want to say it.", "author": "kkonstantine", "createdAt": "2020-05-28T00:50:57Z", "path": "connect/api/src/main/java/org/apache/kafka/connect/sink/ErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.sink;\n+\n+import java.util.concurrent.Future;\n+import org.apache.kafka.connect.errors.ConnectException;\n+\n+/**\n+ * Component that the sink task can use as it {@link SinkTask#put(java.util.Collection)}.", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUyMzYxOA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431523618", "bodyText": "A class javadoc would be helpful, in order to understand why this is introduced.", "author": "kkonstantine", "createdAt": "2020-05-28T00:53:45Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+public class InternalSinkRecord extends SinkRecord {", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\nindex c9229c4cee..69554ffb30 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/InternalSinkRecord.java\n\n@@ -23,6 +23,11 @@ import org.apache.kafka.connect.data.Schema;\n import org.apache.kafka.connect.header.Header;\n import org.apache.kafka.connect.sink.SinkRecord;\n \n+/**\n+ * A specialization of {@link SinkRecord} that allows a {@link WorkerSinkTask} to track the\n+ * original {@link ConsumerRecord} for each {@link SinkRecord}. It is used internally and not\n+ * exposed to connectors.\n+ */\n public class InternalSinkRecord extends SinkRecord {\n \n     private final ConsumerRecord<byte[], byte[]> originalRecord;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDgyNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534827", "bodyText": "nit: it was correct before", "author": "kkonstantine", "createdAt": "2020-05-28T01:38:01Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\nindex ac0b652852..e07a5d0118 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\n\n@@ -139,10 +139,10 @@ public class DeadLetterQueueReporter implements ErrorReporter {\n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         } else {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         }\n \n         if (connConfig.isDlqContextHeadersEnabled()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNDg0Mw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431534843", "bodyText": "nit: it was correct before", "author": "kkonstantine", "createdAt": "2020-05-28T01:38:07Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "diffHunk": "@@ -111,41 +115,41 @@ public static DeadLetterQueueReporter createAndSetup(Map<String, Object> adminPr\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n         this.errorHandlingMetrics = errorHandlingMetrics;\n+        this.dlqTopicName = connConfig.dlqTopicName().trim();\n     }\n \n     /**\n-     * Write the raw records into a Kafka topic.\n+     * Write the raw records into a Kafka topic and return the producer future.\n      *\n      * @param context processing context containing the raw record at {@link ProcessingContext#consumerRecord()}.\n+     * @return the future associated with the writing of this record; never null\n      */\n-    public void report(ProcessingContext context) {\n-        final String dlqTopicName = connConfig.dlqTopicName();\n+    public Future<RecordMetadata> report(ProcessingContext context) {\n         if (dlqTopicName.isEmpty()) {\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n-\n         errorHandlingMetrics.recordDeadLetterQueueProduceRequest();\n \n         ConsumerRecord<byte[], byte[]> originalMessage = context.consumerRecord();\n         if (originalMessage == null) {\n             errorHandlingMetrics.recordDeadLetterQueueProduceFailed();\n-            return;\n+            return CompletableFuture.completedFuture(null);\n         }\n \n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         } else {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                    originalMessage.key(), originalMessage.value(), originalMessage.headers());", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\nindex ac0b652852..e07a5d0118 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java\n\n@@ -139,10 +139,10 @@ public class DeadLetterQueueReporter implements ErrorReporter {\n         ProducerRecord<byte[], byte[]> producerRecord;\n         if (originalMessage.timestamp() == RecordBatch.NO_TIMESTAMP) {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null,\n-                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         } else {\n             producerRecord = new ProducerRecord<>(dlqTopicName, null, originalMessage.timestamp(),\n-                originalMessage.key(), originalMessage.value(), originalMessage.headers());\n+                    originalMessage.key(), originalMessage.value(), originalMessage.headers());\n         }\n \n         if (connConfig.isDlqContextHeadersEnabled()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431535467", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final LinkedList<Future<Void>> futures;\n          \n          \n            \n                protected final List<Future<Void>> futures;", "author": "kkonstantine", "createdAt": "2020-05-28T01:40:39Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU2NDY3MQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431564671", "bodyText": "Unfortunately, this doesn't let for poll() to be recognized.", "author": "aakashnshah", "createdAt": "2020-05-28T03:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTYxMjczNw==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431612737", "bodyText": "Is that for tests? Anyway, we can revisit in a cleanup in the future.", "author": "kkonstantine", "createdAt": "2020-05-28T06:39:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzNTQ2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 8bafffbaaa..6e1fa53164 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -48,7 +48,7 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n     private final HeaderConverter headerConverter;\n \n     // Visible for testing\n-    final LinkedList<Future<Void>> futures;\n+    protected final LinkedList<Future<Void>> futures;\n \n     public WorkerErrantRecordReporter(\n         RetryWithToleranceOperator retryWithToleranceOperator,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzODgzMg==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431538832", "bodyText": "Suggestion (can't add because of the deleted line):\n        List<Future<RecordMetadata>> futures = reporters.stream()\n                .map(r -> r.report(this))\n                .filter(Future::isDone)\n                .collect(Collectors.toCollection(LinkedList::new));", "author": "kkonstantine", "createdAt": "2020-05-28T01:53:49Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java", "diffHunk": "@@ -132,11 +138,25 @@ public void currentContext(Stage stage, Class<?> klass) {\n \n     /**\n      * Report errors. Should be called only if an error was encountered while executing the operation.\n+     *\n+     * @return a errant record future that potentially aggregates the producer futures\n      */\n-    public void report() {\n+    public Future<Void> report() {\n+        if (reporters.size() == 1) {\n+            return new ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(this)));\n+        }\n+\n+        List<Future<RecordMetadata>> futures = new LinkedList<>();", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java\nindex e926fd66dc..0ddf894e57 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ProcessingContext.java\n\n@@ -146,13 +147,10 @@ class ProcessingContext implements AutoCloseable {\n             return new ErrantRecordFuture(Collections.singletonList(reporters.iterator().next().report(this)));\n         }\n \n-        List<Future<RecordMetadata>> futures = new LinkedList<>();\n-        for (ErrorReporter reporter: reporters) {\n-            Future<RecordMetadata> future = reporter.report(this);\n-            if (!future.isDone()) {\n-                futures.add(future);\n-            }\n-        }\n+        List<Future<RecordMetadata>> futures = reporters.stream()\n+                .map(r -> r.report(this))\n+                .filter(Future::isDone)\n+                .collect(Collectors.toCollection(LinkedList::new));\n         if (futures.isEmpty()) {\n             return CompletableFuture.completedFuture(null);\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTUzOTQxNA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431539414", "bodyText": "nit: initialization is not required", "author": "kkonstantine", "createdAt": "2020-05-28T01:55:58Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 8bafffbaaa..6e1fa53164 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -48,7 +48,7 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n     private final HeaderConverter headerConverter;\n \n     // Visible for testing\n-    final LinkedList<Future<Void>> futures;\n+    protected final LinkedList<Future<Void>> futures;\n \n     public WorkerErrantRecordReporter(\n         RetryWithToleranceOperator retryWithToleranceOperator,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDI3Ng==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540276", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        for (Future<RecordMetadata> future: futures) {\n          \n          \n            \n                            if (!future.isDone()) {\n          \n          \n            \n                                return false;\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return true;\n          \n          \n            \n                        return futures.stream().allMatch(Future::isDone);", "author": "kkonstantine", "createdAt": "2020-05-28T01:59:32Z", "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime.errors;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.header.Header;\n+import org.apache.kafka.connect.runtime.InternalSinkRecord;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+\n+public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n+\n+    private static final Logger log = LoggerFactory.getLogger(WorkerErrantRecordReporter.class);\n+\n+    private final RetryWithToleranceOperator retryWithToleranceOperator;\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+    private final HeaderConverter headerConverter;\n+\n+    // Visible for testing\n+    final LinkedList<Future<Void>> futures;\n+\n+    public WorkerErrantRecordReporter(\n+        RetryWithToleranceOperator retryWithToleranceOperator,\n+        Converter keyConverter,\n+        Converter valueConverter,\n+        HeaderConverter headerConverter\n+    ) {\n+        this.retryWithToleranceOperator = retryWithToleranceOperator;\n+        this.keyConverter = keyConverter;\n+        this.valueConverter = valueConverter;\n+        this.headerConverter = headerConverter;\n+        this.futures = new LinkedList<>();\n+    }\n+\n+    @Override\n+    public Future<Void> report(SinkRecord record, Throwable error) {\n+        ConsumerRecord<byte[], byte[]> consumerRecord;\n+\n+        // Most of the records will be an internal sink record, but the task could potentially\n+        // report modified or new records, so handle both cases\n+        if (record instanceof InternalSinkRecord) {\n+            consumerRecord = ((InternalSinkRecord) record).originalRecord();\n+        } else {\n+            // Generate a new consumer record from the modified sink record. We prefer\n+            // to send the original consumer record (pre-transformed) to the DLQ,\n+            // but in this case we don't have one and send the potentially transformed\n+            // record instead\n+            String topic = record.topic();\n+            byte[] key = keyConverter.fromConnectData(topic, record.keySchema(), record.key());\n+            byte[] value = valueConverter.fromConnectData(topic,\n+                record.valueSchema(), record.value());\n+\n+            RecordHeaders headers = new RecordHeaders();\n+            if (record.headers() != null) {\n+                for (Header header : record.headers()) {\n+                    String headerKey = header.key();\n+                    byte[] rawHeader = headerConverter.fromConnectHeader(topic, headerKey,\n+                        header.schema(), header.value());\n+                    headers.add(headerKey, rawHeader);\n+                }\n+            }\n+\n+            int keyLength = key != null ? key.length : -1;\n+            int valLength = value != null ? value.length : -1;\n+\n+            consumerRecord = new ConsumerRecord<>(record.topic(), record.kafkaPartition(),\n+                record.kafkaOffset(), record.timestamp(), record.timestampType(), -1L, keyLength,\n+                valLength, key, value, headers);\n+        }\n+\n+        Future<Void> future = retryWithToleranceOperator.executeFailed(Stage.TASK_PUT,\n+            SinkTask.class, consumerRecord, error);\n+\n+        if (!future.isDone()) {\n+            futures.add(future);\n+        }\n+        return future;\n+    }\n+\n+    /**\n+     * Gets all futures returned by the sink records sent to Kafka by the errant\n+     * record reporter. This function is intended to be used to block on all the errant record\n+     * futures.\n+     */\n+    public void awaitAllFutures() {\n+        Future<?> future = null;\n+        while ((future = futures.poll()) != null) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException | ExecutionException e) {\n+                log.error(\"Encountered an error while awaiting an errant record future's completion.\");\n+                throw new ConnectException(e);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Wrapper class to aggregate producer futures and abstract away the record metadata from the\n+     * Connect user.\n+     */\n+    public static class ErrantRecordFuture implements Future<Void> {\n+\n+        private final List<Future<RecordMetadata>> futures;\n+\n+        public ErrantRecordFuture(List<Future<RecordMetadata>> producerFutures) {\n+            futures = producerFutures;\n+        }\n+\n+        public boolean cancel(boolean mayInterruptIfRunning) {\n+            throw new UnsupportedOperationException(\"Reporting an errant record cannot be cancelled.\");\n+        }\n+\n+        public boolean isCancelled() {\n+            return false;\n+        }\n+\n+        public boolean isDone() {\n+            for (Future<RecordMetadata> future: futures) {\n+                if (!future.isDone()) {\n+                    return false;\n+                }\n+            }\n+            return true;", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\nindex 8bafffbaaa..6e1fa53164 100644\n--- a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n+++ b/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/WorkerErrantRecordReporter.java\n\n@@ -48,7 +48,7 @@ public class WorkerErrantRecordReporter implements ErrantRecordReporter {\n     private final HeaderConverter headerConverter;\n \n     // Visible for testing\n-    final LinkedList<Future<Void>> futures;\n+    protected final LinkedList<Future<Void>> futures;\n \n     public WorkerErrantRecordReporter(\n         RetryWithToleranceOperator retryWithToleranceOperator,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MDk3MA==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431540970", "bodyText": "nit: extra blank line\n\n  \n    \n      \n        Suggested change", "author": "kkonstantine", "createdAt": "2020-05-28T02:02:04Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.sink.ErrantRecordReporter;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n+\n+    @Override\n+    public Class<? extends Task> taskClass() {\n+        return ErrantRecordSinkTask.class;\n+    }\n+\n+    public static class ErrantRecordSinkTask extends MonitorableSinkTask {\n+        private ErrantRecordReporter reporter;\n+\n+        public ErrantRecordSinkTask() {\n+            super();\n+        }\n+\n+        @Override\n+        public void start(Map<String, String> props) {\n+            super.start(props);\n+            reporter = context.errantRecordReporter();\n+        }\n+\n+        @Override\n+        public void put(Collection<SinkRecord> records) {\n+", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java\nindex 0815c52a50..0fe2f88083 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ErrantRecordSinkConnector.java\n\n@@ -48,7 +48,6 @@ public class ErrantRecordSinkConnector extends MonitorableSinkConnector {\n \n         @Override\n         public void put(Collection<SinkRecord> records) {\n-\n             for (SinkRecord rec : records) {\n                 taskHandle.record();\n                 TopicPartition tp = cachedTopicPartitions\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTU0MTI5MQ==", "url": "https://github.com/apache/kafka/pull/8720#discussion_r431541291", "bodyText": "Suggested change", "author": "kkonstantine", "createdAt": "2020-05-28T02:03:22Z", "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java", "diffHunk": "@@ -219,6 +223,72 @@ public void testSourceConnector() throws Exception {\n         connect.deleteConnector(CONNECTOR_NAME);\n     }\n \n+    @Test\n+    public void testErrantRecordReporter() throws Exception {\n+        connect.kafka().createTopic(DLQ_TOPIC, 1);\n+        // create test topic\n+        connect.kafka().createTopic(\"test-topic\", NUM_TOPIC_PARTITIONS);\n+\n+        // setup up props for the sink connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(CONNECTOR_CLASS_CONFIG, ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME);\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(NUM_TASKS));\n+        props.put(TOPICS_CONFIG, \"test-topic\");\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedRecords(NUM_RECORDS_PRODUCED);\n+\n+        // expect all records to be consumed by the connector\n+        connectorHandle.expectedCommits(NUM_RECORDS_PRODUCED);\n+\n+        // validate the intended connector configuration, a config that errors\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 1,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // add missing configuration to make the config valid\n+        props.put(\"name\", CONNECTOR_NAME);\n+\n+        // validate the intended connector configuration, a valid config\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(ERRANT_RECORD_SINK_CONNECTOR_CLASS_NAME, props, 0,\n+            \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        // start a sink connector\n+        connect.configureConnector(CONNECTOR_NAME, props);\n+\n+        waitForCondition(this::checkForPartitionAssignment,\n+            CONNECTOR_SETUP_DURATION_MS,\n+            \"Connector tasks were not assigned a partition each.\");\n+\n+        // produce some messages into source topic partitions\n+        for (int i = 0; i < NUM_RECORDS_PRODUCED; i++) {\n+            connect.kafka().produce(\"test-topic\", i % NUM_TOPIC_PARTITIONS, \"key\", \"simple-message-value-\" + i);\n+        }\n+\n+        // consume all records from the source topic or fail, to ensure that they were correctly produced.\n+        assertEquals(\"Unexpected number of records consumed\", NUM_RECORDS_PRODUCED,\n+            connect.kafka().consume(NUM_RECORDS_PRODUCED, RECORD_TRANSFER_DURATION_MS, \"test-topic\").count());\n+\n+        // wait for the connector tasks to consume all records.\n+        connectorHandle.awaitRecords(RECORD_TRANSFER_DURATION_MS);\n+\n+        // wait for the connector tasks to commit all records.\n+        connectorHandle.awaitCommits(RECORD_TRANSFER_DURATION_MS);\n+\n+        // consume all records from the dlq topic or fail, to ensure that they were correctly produced\n+        int recordNum = connect.kafka().consume(\n+            NUM_RECORDS_PRODUCED,\n+            RECORD_TRANSFER_DURATION_MS,\n+            DLQ_TOPIC\n+        ).count();\n+\n+        // delete connector\n+        connect.deleteConnector(CONNECTOR_NAME);\n+", "originalCommit": "126b04c568381dc108db87da27ddd4862a5e0d2b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1c4442fffdc3b163458bf42b7af29183062c636f", "chunk": "diff --git a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\nindex 47a3b1af42..a2b40e547e 100644\n--- a/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\n+++ b/connect/runtime/src/test/java/org/apache/kafka/connect/integration/ExampleConnectIntegrationTest.java\n\n@@ -286,7 +286,6 @@ public class ExampleConnectIntegrationTest {\n \n         // delete connector\n         connect.deleteConnector(CONNECTOR_NAME);\n-\n     }\n \n     /**\n"}}, {"oid": "1c4442fffdc3b163458bf42b7af29183062c636f", "url": "https://github.com/apache/kafka/commit/1c4442fffdc3b163458bf42b7af29183062c636f", "message": "addressed some more comments", "committedDate": "2020-05-28T03:55:43Z", "type": "commit"}]}