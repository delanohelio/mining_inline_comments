{"pr_number": 9007, "pr_title": "KAFKA-10120: Deprecate DescribeLogDirsResult.all() and .values()", "pr_createdAt": "2020-07-10T13:54:39Z", "pr_url": "https://github.com/apache/kafka/pull/9007", "timeline": [{"oid": "dcb214ca84c27ec9f691392c12532017d7b5785c", "url": "https://github.com/apache/kafka/commit/dcb214ca84c27ec9f691392c12532017d7b5785c", "message": "KAFKA-10120: Deprecate and replace DescribeLogDirsResult.all() and .values()\n\nAs per KIP-621. Also added some tests in KafkaAdminClientTest", "committedDate": "2020-07-14T15:18:02Z", "type": "commit"}, {"oid": "05c63fbd74f313f9ee48e1b96821e60f868c1757", "url": "https://github.com/apache/kafka/commit/05c63fbd74f313f9ee48e1b96821e60f868c1757", "message": "Review comments", "committedDate": "2020-07-14T15:18:02Z", "type": "commit"}, {"oid": "f2232b7e08b00509505cf36dec1bb83618448f0d", "url": "https://github.com/apache/kafka/commit/f2232b7e08b00509505cf36dec1bb83618448f0d", "message": "Review comments", "committedDate": "2020-07-14T15:18:02Z", "type": "commit"}, {"oid": "ba2160ca834c6ebdca26d708500560972b4baebc", "url": "https://github.com/apache/kafka/commit/ba2160ca834c6ebdca26d708500560972b4baebc", "message": "Review comment", "committedDate": "2020-07-14T15:20:57Z", "type": "commit"}, {"oid": "ba2160ca834c6ebdca26d708500560972b4baebc", "url": "https://github.com/apache/kafka/commit/ba2160ca834c6ebdca26d708500560972b4baebc", "message": "Review comment", "committedDate": "2020-07-14T15:20:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ4MDc1MQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454480751", "bodyText": "Is it better to have package-private visibility?", "author": "chia7712", "createdAt": "2020-07-14T16:22:41Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.clients.admin;\n+\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.ApiException;\n+\n+import java.util.Map;\n+\n+import static java.util.Collections.unmodifiableMap;\n+\n+/**\n+ * A description of a log directory on a particular broker.\n+ */\n+public class LogDirDescription {\n+    private final Map<TopicPartition, ReplicaInfo> replicaInfos;\n+    private final ApiException error;\n+\n+    public LogDirDescription(ApiException error, Map<TopicPartition, ReplicaInfo> replicaInfos) {", "originalCommit": "ba2160ca834c6ebdca26d708500560972b4baebc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ4NDE0Nw==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454484147", "bodyText": "I initially thought the same, but TopicDescription, for example (as well as other classes accessible from *Results classes) have a public constructors.", "author": "tombentley", "createdAt": "2020-07-14T16:27:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ4MDc1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU2OTAwMw==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454569003", "bodyText": "That's great point. At the moment, I think that we are not consistent about this. Some are package private and some are not. The advantage of keeping it public is that it allows to use the class in unit tests which resides in other packages.", "author": "dajac", "createdAt": "2020-07-14T18:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ4MDc1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "7fb3f1201e0803300c9d4d5a2aed949069761dd8", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java b/clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java\nindex d7288ea574..a9bcdabfee 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/LogDirDescription.java\n\n@@ -36,7 +36,8 @@ public class LogDirDescription {\n     }\n \n     /**\n-     * <p>Returns `ApiException` if the log directory is offline or an error occurred, otherwise returns null.</p>\n+     * Returns `ApiException` if the log directory is offline or an error occurred, otherwise returns null.\n+     * <p>\n      * <ul>\n      * <li> KafkaStorageException - The log directory is offline.\n      * <li> UnknownServerException - The server experienced an unexpected error when processing the request.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQ4NzE0NA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454487144", "bodyText": "Does it need comment to describe the replacement? for example\n@deprecated Deprecated Since Kafka 2.7. Use {@link LogDirDescription}.", "author": "chia7712", "createdAt": "2020-07-14T16:32:14Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java", "diffHunk": "@@ -95,6 +76,7 @@ public static DescribeLogDirsResponse parse(ByteBuffer buffer, short version) {\n      * KAFKA_STORAGE_ERROR (56)\n      * UNKNOWN (-1)\n      */\n+    @Deprecated", "originalCommit": "ba2160ca834c6ebdca26d708500560972b4baebc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java\nindex 35b16da3d8..e26fc554dc 100644\n--- a/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java\n+++ b/clients/src/main/java/org/apache/kafka/common/requests/DescribeLogDirsResponse.java\n\n@@ -75,6 +75,11 @@ public class DescribeLogDirsResponse extends AbstractResponse {\n      *\n      * KAFKA_STORAGE_ERROR (56)\n      * UNKNOWN (-1)\n+     *\n+     * @deprecated Deprecated Since Kafka 2.7.\n+     * Use {@link org.apache.kafka.clients.admin.DescribeLogDirsResult#descriptions()}\n+     * and {@link org.apache.kafka.clients.admin.DescribeLogDirsResult#allDescriptions()} to access the replacement\n+     * class {@link org.apache.kafka.clients.admin.LogDirDescription}.\n      */\n     @Deprecated\n     static public class LogDirInfo {\n"}}, {"oid": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "url": "https://github.com/apache/kafka/commit/590d9e57ad0c308b3f851d31a8563f46777aa54d", "message": "Review comment", "committedDate": "2020-07-14T17:41:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU2NzIyOA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454567228", "bodyText": "nit: shall we remove response/?", "author": "dajac", "createdAt": "2020-07-14T18:43:12Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2286,13 +2288,15 @@ public DescribeLogDirsResult describeLogDirs(Collection<Integer> brokers, Descri\n                     return new DescribeLogDirsRequest.Builder(new DescribeLogDirsRequestData().setTopics(null));\n                 }\n \n+                @SuppressWarnings(\"deprecation\")\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     DescribeLogDirsResponse response = (DescribeLogDirsResponse) abstractResponse;\n-                    if (response.logDirInfos().size() > 0) {\n-                        future.complete(response.logDirInfos());\n+                    Map<String, LogDirDescription> descriptions = logDirDescriptions(response);\n+                    if (descriptions.size() > 0) {\n+                        future.complete(descriptions);\n                     } else {\n-                        // response.logDirInfos() will be empty if and only if the user is not authorized to describe clsuter resource.\n+                        // response/descriptions will be empty if and only if the user is not authorized to describe cluster resource.", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\nindex a09520a462..89f5cf135d 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n\n@@ -2296,7 +2296,7 @@ public class KafkaAdminClient extends AdminClient {\n                     if (descriptions.size() > 0) {\n                         future.complete(descriptions);\n                     } else {\n-                        // response/descriptions will be empty if and only if the user is not authorized to describe cluster resource.\n+                        // descriptions will be empty if and only if the user is not authorized to describe cluster resource.\n                         future.completeExceptionally(Errors.CLUSTER_AUTHORIZATION_FAILED.exception());\n                     }\n                 }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU2NzgyOQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454567829", "bodyText": "I suggest to add a unit test to cover this change. I think that the previous behaviour was a bug so it would be great to not reintroduce it in the future.", "author": "dajac", "createdAt": "2020-07-14T18:44:19Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2354,32 +2374,31 @@ public DescribeReplicaLogDirsResult describeReplicaLogDirs(Collection<TopicParti\n                 @Override\n                 public void handleResponse(AbstractResponse abstractResponse) {\n                     DescribeLogDirsResponse response = (DescribeLogDirsResponse) abstractResponse;\n-                    for (Map.Entry<String, DescribeLogDirsResponse.LogDirInfo> responseEntry: response.logDirInfos().entrySet()) {\n+                    for (Map.Entry<String, LogDirDescription> responseEntry: logDirDescriptions(response).entrySet()) {\n                         String logDir = responseEntry.getKey();\n-                        DescribeLogDirsResponse.LogDirInfo logDirInfo = responseEntry.getValue();\n+                        LogDirDescription logDirInfo = responseEntry.getValue();\n \n                         // No replica info will be provided if the log directory is offline\n-                        if (logDirInfo.error == Errors.KAFKA_STORAGE_ERROR)\n+                        if (logDirInfo.error() instanceof KafkaStorageException)\n                             continue;\n-                        if (logDirInfo.error != Errors.NONE)\n+                        if (logDirInfo.error() != null)\n                             handleFailure(new IllegalStateException(\n-                                \"The error \" + logDirInfo.error + \" for log directory \" + logDir + \" in the response from broker \" + brokerId + \" is illegal\"));\n+                                \"The error \" + logDirInfo.error().getClass().getName() + \" for log directory \" + logDir + \" in the response from broker \" + brokerId + \" is illegal\"));\n \n-                        for (Map.Entry<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> replicaInfoEntry: logDirInfo.replicaInfos.entrySet()) {\n+                        for (Map.Entry<TopicPartition, ReplicaInfo> replicaInfoEntry: logDirInfo.replicaInfos().entrySet()) {\n                             TopicPartition tp = replicaInfoEntry.getKey();\n-                            DescribeLogDirsResponse.ReplicaInfo replicaInfo = replicaInfoEntry.getValue();\n+                            ReplicaInfo replicaInfo = replicaInfoEntry.getValue();\n                             ReplicaLogDirInfo replicaLogDirInfo = replicaDirInfoByPartition.get(tp);\n                             if (replicaLogDirInfo == null) {\n-                                handleFailure(new IllegalStateException(\n-                                    \"The partition \" + tp + \" in the response from broker \" + brokerId + \" is not in the request\"));\n-                            } else if (replicaInfo.isFuture) {\n+                                log.warn(\"Server response from broker {} mentioned unknown partition {}\", brokerId, tp);", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3MjAwOA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454572008", "bodyText": "nit: What about extracting the construction in a small helper method prepareDescribeLogDirsResponse that create a response for one LogDir and TopicPartition? It seems that the same block of code is used in many tests.", "author": "dajac", "createdAt": "2020-07-14T18:51:34Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,205 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 256ff88574..e8b0910922 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,202 +1059,222 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n     @Test\n     public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n-            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n-            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n-            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n-            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n-            assertNotNull(allDescriptions.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n-            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n-            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n-            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n-            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n         }\n     }\n \n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n-            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n-                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n-            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n-            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n-            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n-            assertNotNull(deprecatedAll.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n-            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n-                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n-            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n-            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n-            assertFalse(allReplicaInfos.get(tp).isFuture);\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n         }\n     }\n \n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,\n+                                           String logDir, TopicPartition tp, Errors error,\n+                                           int offsetLag, long partitionSize) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertEquals(error, descriptionsMap.get(logDir).error);\n+        Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                descriptionsMap.get(logDir).replicaInfos;\n+        assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+        assertEquals(partitionSize, allReplicaInfos.get(tp).size);\n+        assertEquals(offsetLag, allReplicaInfos.get(tp).offsetLag);\n+        assertFalse(allReplicaInfos.get(tp).isFuture);\n+    }\n+\n     @Test\n     public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n             Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+            assertEquals(error.exception().getClass(), descriptionsMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(logDir).replicaInfos().keySet());\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertEquals(brokers, allDescriptions.keySet());\n             Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error.exception().getClass(), allMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos().keySet());\n         }\n     }\n \n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n             Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), valuesMap.keySet());\n+            assertEquals(error, valuesMap.get(logDir).error);\n+            assertEquals(emptySet(), valuesMap.get(logDir).replicaInfos.keySet());\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertEquals(brokers, deprecatedAll.keySet());\n             Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error, allMap.get(logDir).error);\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos.keySet());\n         }\n     }\n \n     @Test\n     public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {\n         TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 12, 1);\n+        TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 12, 2);\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka0\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tpr1.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tpr1.partition())\n-                                            .setPartitionSize(987654321)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(12))))),\n-                            new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                                    .setErrorCode(Errors.NONE.code())\n-                                    .setLogDir(\"/var/data/kafka1\")\n-                                    .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                            .setName(tpr1.topic())\n-                                            .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                                    .setPartitionIndex(tpr1.partition())\n-                                                    .setPartitionSize(123456789)\n-                                                    .setIsFutureKey(true)\n-                                                    .setOffsetLag(4321)))))\n-                    ))), env.cluster().nodeById(tpr1.brokerId()));\n-\n-            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr1));\n+            String broker1log0 = \"/var/data/kafka0\";\n+            String broker1log1 = \"/var/data/kafka1\";\n+            String broker2log0 = \"/var/data/kafka2\";\n+            env.kafkaClient().prepareResponseFrom(\n+                    new DescribeLogDirsResponse(\n+                            new DescribeLogDirsResponseData().setResults(asList(\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log0)\n+                                            .setTopics(prepareDescribeLogDirsTopics(987654321, 12, tpr1.topic(), tpr1.partition(), false)),\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log1)\n+                                            .setTopics(prepareDescribeLogDirsTopics(123456789, 4321, tpr1.topic(), tpr1.partition(), true))))),\n+                    env.cluster().nodeById(tpr1.brokerId()));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.KAFKA_STORAGE_ERROR, broker2log0),\n+                    env.cluster().nodeById(tpr2.brokerId()));\n+\n+            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr2));\n+\n             Map<TopicPartitionReplica, KafkaFuture<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> values = result.values();\n-            assertEquals(TestUtils.toSet(singletonList(tpr1)), values.keySet());\n+            assertEquals(TestUtils.toSet(asList(tpr1, tpr2)), values.keySet());\n \n             assertNotNull(values.get(tpr1));\n-            assertEquals(\"/var/data/kafka0\", values.get(tpr1).get().getCurrentReplicaLogDir());\n+            assertEquals(broker1log0, values.get(tpr1).get().getCurrentReplicaLogDir());\n             assertEquals(12, values.get(tpr1).get().getCurrentReplicaOffsetLag());\n-            assertEquals(\"/var/data/kafka1\", values.get(tpr1).get().getFutureReplicaLogDir());\n+            assertEquals(broker1log1, values.get(tpr1).get().getFutureReplicaLogDir());\n             assertEquals(4321, values.get(tpr1).get().getFutureReplicaOffsetLag());\n+\n+            assertNotNull(values.get(tpr2));\n+            assertNull(values.get(tpr2).get().getCurrentReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getCurrentReplicaOffsetLag());\n+            assertNull(values.get(tpr2).get().getFutureReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getFutureReplicaOffsetLag());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3NTkyNg==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454575926", "bodyText": "nit: In some of the other tests, you have an empty line after calling the method of the admin client. Shall we add one everywhere in order to be consistent? I personally like to have one before and after to separate blocks of code. I leave this up to you.", "author": "dajac", "createdAt": "2020-07-14T18:58:34Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,205 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n+                                    .setName(tp.topic())\n+                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                                            .setPartitionIndex(tp.partition())\n+                                            .setPartitionSize(1234567890)\n+                                            .setIsFutureKey(false)\n+                                            .setOffsetLag(0)))))\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 256ff88574..e8b0910922 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,202 +1059,222 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n     @Test\n     public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n-            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n-            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n-            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n-            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n-            assertNotNull(allDescriptions.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n-            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n-            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n-            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n-            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n         }\n     }\n \n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n-            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n-                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n-            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n-            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n-            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n-            assertNotNull(deprecatedAll.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n-            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n-                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n-            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n-            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n-            assertFalse(allReplicaInfos.get(tp).isFuture);\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n         }\n     }\n \n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,\n+                                           String logDir, TopicPartition tp, Errors error,\n+                                           int offsetLag, long partitionSize) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertEquals(error, descriptionsMap.get(logDir).error);\n+        Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                descriptionsMap.get(logDir).replicaInfos;\n+        assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+        assertEquals(partitionSize, allReplicaInfos.get(tp).size);\n+        assertEquals(offsetLag, allReplicaInfos.get(tp).offsetLag);\n+        assertFalse(allReplicaInfos.get(tp).isFuture);\n+    }\n+\n     @Test\n     public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n             Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+            assertEquals(error.exception().getClass(), descriptionsMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(logDir).replicaInfos().keySet());\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertEquals(brokers, allDescriptions.keySet());\n             Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error.exception().getClass(), allMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos().keySet());\n         }\n     }\n \n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n             Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), valuesMap.keySet());\n+            assertEquals(error, valuesMap.get(logDir).error);\n+            assertEquals(emptySet(), valuesMap.get(logDir).replicaInfos.keySet());\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertEquals(brokers, deprecatedAll.keySet());\n             Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error, allMap.get(logDir).error);\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos.keySet());\n         }\n     }\n \n     @Test\n     public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {\n         TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 12, 1);\n+        TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 12, 2);\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka0\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tpr1.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tpr1.partition())\n-                                            .setPartitionSize(987654321)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(12))))),\n-                            new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                                    .setErrorCode(Errors.NONE.code())\n-                                    .setLogDir(\"/var/data/kafka1\")\n-                                    .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                            .setName(tpr1.topic())\n-                                            .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                                    .setPartitionIndex(tpr1.partition())\n-                                                    .setPartitionSize(123456789)\n-                                                    .setIsFutureKey(true)\n-                                                    .setOffsetLag(4321)))))\n-                    ))), env.cluster().nodeById(tpr1.brokerId()));\n-\n-            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr1));\n+            String broker1log0 = \"/var/data/kafka0\";\n+            String broker1log1 = \"/var/data/kafka1\";\n+            String broker2log0 = \"/var/data/kafka2\";\n+            env.kafkaClient().prepareResponseFrom(\n+                    new DescribeLogDirsResponse(\n+                            new DescribeLogDirsResponseData().setResults(asList(\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log0)\n+                                            .setTopics(prepareDescribeLogDirsTopics(987654321, 12, tpr1.topic(), tpr1.partition(), false)),\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log1)\n+                                            .setTopics(prepareDescribeLogDirsTopics(123456789, 4321, tpr1.topic(), tpr1.partition(), true))))),\n+                    env.cluster().nodeById(tpr1.brokerId()));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.KAFKA_STORAGE_ERROR, broker2log0),\n+                    env.cluster().nodeById(tpr2.brokerId()));\n+\n+            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr2));\n+\n             Map<TopicPartitionReplica, KafkaFuture<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> values = result.values();\n-            assertEquals(TestUtils.toSet(singletonList(tpr1)), values.keySet());\n+            assertEquals(TestUtils.toSet(asList(tpr1, tpr2)), values.keySet());\n \n             assertNotNull(values.get(tpr1));\n-            assertEquals(\"/var/data/kafka0\", values.get(tpr1).get().getCurrentReplicaLogDir());\n+            assertEquals(broker1log0, values.get(tpr1).get().getCurrentReplicaLogDir());\n             assertEquals(12, values.get(tpr1).get().getCurrentReplicaOffsetLag());\n-            assertEquals(\"/var/data/kafka1\", values.get(tpr1).get().getFutureReplicaLogDir());\n+            assertEquals(broker1log1, values.get(tpr1).get().getFutureReplicaLogDir());\n             assertEquals(4321, values.get(tpr1).get().getFutureReplicaOffsetLag());\n+\n+            assertNotNull(values.get(tpr2));\n+            assertNull(values.get(tpr2).get().getCurrentReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getCurrentReplicaOffsetLag());\n+            assertNull(values.get(tpr2).get().getFutureReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getFutureReplicaOffsetLag());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU3OTYxNA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454579614", "bodyText": "nit: You can reuse brokers here. Would it make sense to allow extract the other constants in local variables?", "author": "dajac", "createdAt": "2020-07-14T19:05:22Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,205 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n+                                    .setName(tp.topic())\n+                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                                            .setPartitionIndex(tp.partition())\n+                                            .setPartitionSize(1234567890)\n+                                            .setIsFutureKey(false)\n+                                            .setOffsetLag(0)))))\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(Collections.singleton(0), descriptions.keySet());", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 256ff88574..e8b0910922 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,202 +1059,222 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n     @Test\n     public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n-            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n-            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n-            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n-            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n-            assertNotNull(allDescriptions.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n-            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n-            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n-            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n-            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n         }\n     }\n \n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n-            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n-                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n-            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n-            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n-            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n-            assertNotNull(deprecatedAll.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n-            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n-                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n-            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n-            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n-            assertFalse(allReplicaInfos.get(tp).isFuture);\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n         }\n     }\n \n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,\n+                                           String logDir, TopicPartition tp, Errors error,\n+                                           int offsetLag, long partitionSize) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertEquals(error, descriptionsMap.get(logDir).error);\n+        Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                descriptionsMap.get(logDir).replicaInfos;\n+        assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+        assertEquals(partitionSize, allReplicaInfos.get(tp).size);\n+        assertEquals(offsetLag, allReplicaInfos.get(tp).offsetLag);\n+        assertFalse(allReplicaInfos.get(tp).isFuture);\n+    }\n+\n     @Test\n     public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n             Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+            assertEquals(error.exception().getClass(), descriptionsMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(logDir).replicaInfos().keySet());\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertEquals(brokers, allDescriptions.keySet());\n             Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error.exception().getClass(), allMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos().keySet());\n         }\n     }\n \n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n             Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), valuesMap.keySet());\n+            assertEquals(error, valuesMap.get(logDir).error);\n+            assertEquals(emptySet(), valuesMap.get(logDir).replicaInfos.keySet());\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertEquals(brokers, deprecatedAll.keySet());\n             Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error, allMap.get(logDir).error);\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos.keySet());\n         }\n     }\n \n     @Test\n     public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {\n         TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 12, 1);\n+        TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 12, 2);\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka0\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tpr1.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tpr1.partition())\n-                                            .setPartitionSize(987654321)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(12))))),\n-                            new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                                    .setErrorCode(Errors.NONE.code())\n-                                    .setLogDir(\"/var/data/kafka1\")\n-                                    .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                            .setName(tpr1.topic())\n-                                            .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                                    .setPartitionIndex(tpr1.partition())\n-                                                    .setPartitionSize(123456789)\n-                                                    .setIsFutureKey(true)\n-                                                    .setOffsetLag(4321)))))\n-                    ))), env.cluster().nodeById(tpr1.brokerId()));\n-\n-            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr1));\n+            String broker1log0 = \"/var/data/kafka0\";\n+            String broker1log1 = \"/var/data/kafka1\";\n+            String broker2log0 = \"/var/data/kafka2\";\n+            env.kafkaClient().prepareResponseFrom(\n+                    new DescribeLogDirsResponse(\n+                            new DescribeLogDirsResponseData().setResults(asList(\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log0)\n+                                            .setTopics(prepareDescribeLogDirsTopics(987654321, 12, tpr1.topic(), tpr1.partition(), false)),\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log1)\n+                                            .setTopics(prepareDescribeLogDirsTopics(123456789, 4321, tpr1.topic(), tpr1.partition(), true))))),\n+                    env.cluster().nodeById(tpr1.brokerId()));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.KAFKA_STORAGE_ERROR, broker2log0),\n+                    env.cluster().nodeById(tpr2.brokerId()));\n+\n+            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr2));\n+\n             Map<TopicPartitionReplica, KafkaFuture<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> values = result.values();\n-            assertEquals(TestUtils.toSet(singletonList(tpr1)), values.keySet());\n+            assertEquals(TestUtils.toSet(asList(tpr1, tpr2)), values.keySet());\n \n             assertNotNull(values.get(tpr1));\n-            assertEquals(\"/var/data/kafka0\", values.get(tpr1).get().getCurrentReplicaLogDir());\n+            assertEquals(broker1log0, values.get(tpr1).get().getCurrentReplicaLogDir());\n             assertEquals(12, values.get(tpr1).get().getCurrentReplicaOffsetLag());\n-            assertEquals(\"/var/data/kafka1\", values.get(tpr1).get().getFutureReplicaLogDir());\n+            assertEquals(broker1log1, values.get(tpr1).get().getFutureReplicaLogDir());\n             assertEquals(4321, values.get(tpr1).get().getFutureReplicaOffsetLag());\n+\n+            assertNotNull(values.get(tpr2));\n+            assertNull(values.get(tpr2).get().getCurrentReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getCurrentReplicaOffsetLag());\n+            assertNull(values.get(tpr2).get().getFutureReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getFutureReplicaOffsetLag());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4MzkyOQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454583929", "bodyText": "This block of assertions is used multiple times. Would it make sense to extract it in a helper method, say assertDescriptions, that verifies a descriptions map contains the information about a single log dir/topic partition?\nSomething like assertDescriptionContains(descriptionsMap, logDir, tp, size, offsetLag, isFuture).", "author": "dajac", "createdAt": "2020-07-14T19:13:23Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,205 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n+                                    .setName(tp.topic())\n+                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                                            .setPartitionIndex(tp.partition())\n+                                            .setPartitionSize(1234567890)\n+                                            .setIsFutureKey(false)\n+                                            .setOffsetLag(0)))))\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertNotNull(descriptions.get(0));\n+            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n+            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n+            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n+            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n+            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n+            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk0MTM0OA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454941348", "bodyText": "This might have less mileage than you expected because the different types mean we need two methods each with two call sites, rather than 4 call sites for a single method, but I've done it anyway.", "author": "tombentley", "createdAt": "2020-07-15T10:09:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4MzkyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 256ff88574..e8b0910922 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,202 +1059,222 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n     @Test\n     public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n-            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n-            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n-            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n-            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n-            assertNotNull(allDescriptions.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n-            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n-            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n-            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n-            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n         }\n     }\n \n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n-            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n-                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n-            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n-            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n-            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n-            assertNotNull(deprecatedAll.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n-            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n-                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n-            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n-            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n-            assertFalse(allReplicaInfos.get(tp).isFuture);\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n         }\n     }\n \n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,\n+                                           String logDir, TopicPartition tp, Errors error,\n+                                           int offsetLag, long partitionSize) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertEquals(error, descriptionsMap.get(logDir).error);\n+        Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                descriptionsMap.get(logDir).replicaInfos;\n+        assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+        assertEquals(partitionSize, allReplicaInfos.get(tp).size);\n+        assertEquals(offsetLag, allReplicaInfos.get(tp).offsetLag);\n+        assertFalse(allReplicaInfos.get(tp).isFuture);\n+    }\n+\n     @Test\n     public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n             Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+            assertEquals(error.exception().getClass(), descriptionsMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(logDir).replicaInfos().keySet());\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertEquals(brokers, allDescriptions.keySet());\n             Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error.exception().getClass(), allMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos().keySet());\n         }\n     }\n \n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n             Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), valuesMap.keySet());\n+            assertEquals(error, valuesMap.get(logDir).error);\n+            assertEquals(emptySet(), valuesMap.get(logDir).replicaInfos.keySet());\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertEquals(brokers, deprecatedAll.keySet());\n             Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error, allMap.get(logDir).error);\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos.keySet());\n         }\n     }\n \n     @Test\n     public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {\n         TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 12, 1);\n+        TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 12, 2);\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka0\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tpr1.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tpr1.partition())\n-                                            .setPartitionSize(987654321)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(12))))),\n-                            new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                                    .setErrorCode(Errors.NONE.code())\n-                                    .setLogDir(\"/var/data/kafka1\")\n-                                    .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                            .setName(tpr1.topic())\n-                                            .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                                    .setPartitionIndex(tpr1.partition())\n-                                                    .setPartitionSize(123456789)\n-                                                    .setIsFutureKey(true)\n-                                                    .setOffsetLag(4321)))))\n-                    ))), env.cluster().nodeById(tpr1.brokerId()));\n-\n-            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr1));\n+            String broker1log0 = \"/var/data/kafka0\";\n+            String broker1log1 = \"/var/data/kafka1\";\n+            String broker2log0 = \"/var/data/kafka2\";\n+            env.kafkaClient().prepareResponseFrom(\n+                    new DescribeLogDirsResponse(\n+                            new DescribeLogDirsResponseData().setResults(asList(\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log0)\n+                                            .setTopics(prepareDescribeLogDirsTopics(987654321, 12, tpr1.topic(), tpr1.partition(), false)),\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log1)\n+                                            .setTopics(prepareDescribeLogDirsTopics(123456789, 4321, tpr1.topic(), tpr1.partition(), true))))),\n+                    env.cluster().nodeById(tpr1.brokerId()));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.KAFKA_STORAGE_ERROR, broker2log0),\n+                    env.cluster().nodeById(tpr2.brokerId()));\n+\n+            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr2));\n+\n             Map<TopicPartitionReplica, KafkaFuture<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> values = result.values();\n-            assertEquals(TestUtils.toSet(singletonList(tpr1)), values.keySet());\n+            assertEquals(TestUtils.toSet(asList(tpr1, tpr2)), values.keySet());\n \n             assertNotNull(values.get(tpr1));\n-            assertEquals(\"/var/data/kafka0\", values.get(tpr1).get().getCurrentReplicaLogDir());\n+            assertEquals(broker1log0, values.get(tpr1).get().getCurrentReplicaLogDir());\n             assertEquals(12, values.get(tpr1).get().getCurrentReplicaOffsetLag());\n-            assertEquals(\"/var/data/kafka1\", values.get(tpr1).get().getFutureReplicaLogDir());\n+            assertEquals(broker1log1, values.get(tpr1).get().getFutureReplicaLogDir());\n             assertEquals(4321, values.get(tpr1).get().getFutureReplicaOffsetLag());\n+\n+            assertNotNull(values.get(tpr2));\n+            assertNull(values.get(tpr2).get().getCurrentReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getCurrentReplicaOffsetLag());\n+            assertNull(values.get(tpr2).get().getFutureReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getFutureReplicaOffsetLag());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NTM3NQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454585375", "bodyText": "This is not due to your PR but shall we add a unit test which uses multiple brokers?", "author": "dajac", "createdAt": "2020-07-14T19:15:56Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,205 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n+                                    .setName(tp.topic())\n+                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                                            .setPartitionIndex(tp.partition())\n+                                            .setPartitionSize(1234567890)\n+                                            .setIsFutureKey(false)\n+                                            .setOffsetLag(0)))))\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertNotNull(descriptions.get(0));\n+            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n+            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n+            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n+            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n+            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n+            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+\n+            Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n+            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertNotNull(allDescriptions.get(0));\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n+            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n+            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n+            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n+            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n+            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n+            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    @Test\n+    public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.NONE.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n+                                    .setName(tp.topic())\n+                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                                            .setPartitionIndex(tp.partition())\n+                                            .setPartitionSize(1234567890)\n+                                            .setIsFutureKey(false)\n+                                            .setOffsetLag(0)))))\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n+            Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n+            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertNotNull(deprecatedValues.get(0));\n+            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n+            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n+            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n+                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n+            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n+            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n+            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n+            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+\n+            Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n+            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertNotNull(deprecatedAll.get(0));\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n+            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n+            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n+            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n+            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n+            assertFalse(allReplicaInfos.get(tp).isFuture);\n+        }\n+    }\n+\n+    @Test\n+    public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(emptyList())\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertNotNull(descriptions.get(0));\n+            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n+            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+\n+            Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n+            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n+            assertNotNull(allMap);\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n+            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n+            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+        }\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    @Test\n+    public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n+        List<Integer> brokers = singletonList(0);\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n+                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n+                            .setLogDir(\"/var/data/kafka\")\n+                            .setTopics(emptyList())\n+                    ))), env.cluster().nodeById(0));\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n+            Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n+            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertNotNull(deprecatedValues.get(0));\n+            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n+            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n+            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+\n+            Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n+            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n+            assertNotNull(allMap);\n+            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n+            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n+            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+        }\n+    }\n+\n+    @Test\n+    public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {", "originalCommit": "590d9e57ad0c308b3f851d31a8563f46777aa54d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk0MTM4NA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r454941384", "bodyText": "I added it to the existing test. Due to the new helper methods I felt this didn't really complicate the test very much and is also allows us to cover the case where the RPC returns STORAGE_ERROR.", "author": "tombentley", "createdAt": "2020-07-15T10:09:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU4NTM3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "af5224e02573157c183e7c5d5c9609b9592d5763", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 256ff88574..e8b0910922 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,202 +1059,222 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n     @Test\n     public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n-            Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertNull(descriptionsMap.get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n-            assertEquals(1234567890, descriptionsReplicaInfos.get(tp).size());\n-            assertEquals(0, descriptionsReplicaInfos.get(tp).offsetLag());\n-            assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n-            assertNotNull(allDescriptions.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allDescriptions.get(0).keySet());\n-            assertNull(allDescriptions.get(0).get(\"/var/data/kafka\").error());\n-            Map<TopicPartition, ReplicaInfo> allDescriptionsReplicInfos = allDescriptions.get(0).get(\"/var/data/kafka\").replicaInfos();\n-            assertEquals(Collections.singleton(tp), allDescriptionsReplicInfos.keySet());\n-            assertEquals(1234567890, allDescriptionsReplicInfos.get(tp).size());\n-            assertEquals(0, allDescriptionsReplicInfos.get(tp).offsetLag());\n-            assertFalse(allDescriptionsReplicInfos.get(tp).isFuture());\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n         }\n     }\n \n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n         TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tp.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tp.partition())\n-                                            .setPartitionSize(1234567890)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(0)))))\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n-            Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.NONE, valuesMap.get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> valuesReplicaInfos =\n-                    valuesMap.get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), valuesReplicaInfos.keySet());\n-            assertEquals(1234567890, valuesReplicaInfos.get(tp).size);\n-            assertEquals(0, valuesReplicaInfos.get(tp).offsetLag);\n-            assertFalse(valuesReplicaInfos.get(tp).isFuture);\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n-            assertNotNull(deprecatedAll.get(0));\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), deprecatedAll.get(0).keySet());\n-            assertEquals(Errors.NONE, deprecatedAll.get(0).get(\"/var/data/kafka\").error);\n-            Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n-                    deprecatedAll.get(0).get(\"/var/data/kafka\").replicaInfos;\n-            assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n-            assertEquals(1234567890, allReplicaInfos.get(tp).size);\n-            assertEquals(0, allReplicaInfos.get(tp).offsetLag);\n-            assertFalse(allReplicaInfos.get(tp).isFuture);\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n         }\n     }\n \n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,\n+                                           String logDir, TopicPartition tp, Errors error,\n+                                           int offsetLag, long partitionSize) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertEquals(error, descriptionsMap.get(logDir).error);\n+        Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> allReplicaInfos =\n+                descriptionsMap.get(logDir).replicaInfos;\n+        assertEquals(Collections.singleton(tp), allReplicaInfos.keySet());\n+        assertEquals(partitionSize, allReplicaInfos.get(tp).size);\n+        assertEquals(offsetLag, allReplicaInfos.get(tp).offsetLag);\n+        assertFalse(allReplicaInfos.get(tp).isFuture);\n+    }\n+\n     @Test\n     public void testDescribeLogDirsOfflineDir() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n             Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n-            assertEquals(Collections.singleton(0), descriptions.keySet());\n+            assertEquals(brokers, descriptions.keySet());\n             assertNotNull(descriptions.get(0));\n             Map<String, LogDirDescription> descriptionsMap = descriptions.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), descriptionsMap.keySet());\n-            assertEquals(KafkaStorageException.class, descriptionsMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), descriptionsMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+            assertEquals(error.exception().getClass(), descriptionsMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), descriptionsMap.get(logDir).replicaInfos().keySet());\n \n             Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n-            assertEquals(Collections.singleton(0), allDescriptions.keySet());\n+            assertEquals(brokers, allDescriptions.keySet());\n             Map<String, LogDirDescription> allMap = allDescriptions.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(KafkaStorageException.class, allMap.get(\"/var/data/kafka\").error().getClass());\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos().keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error.exception().getClass(), allMap.get(logDir).error().getClass());\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos().keySet());\n         }\n     }\n \n     @SuppressWarnings(\"deprecation\")\n     @Test\n     public void testDescribeLogDirsOfflineDirDeprecated() throws ExecutionException, InterruptedException {\n-        List<Integer> brokers = singletonList(0);\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.KAFKA_STORAGE_ERROR;\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.KAFKA_STORAGE_ERROR.code())\n-                            .setLogDir(\"/var/data/kafka\")\n-                            .setTopics(emptyList())\n-                    ))), env.cluster().nodeById(0));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, emptyList()),\n+                    env.cluster().nodeById(0));\n+\n             DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n \n             Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n-            assertEquals(Collections.singleton(0), deprecatedValues.keySet());\n+            assertEquals(brokers, deprecatedValues.keySet());\n             assertNotNull(deprecatedValues.get(0));\n             Map<String, DescribeLogDirsResponse.LogDirInfo> valuesMap = deprecatedValues.get(0).get();\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), valuesMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, valuesMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), valuesMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), valuesMap.keySet());\n+            assertEquals(error, valuesMap.get(logDir).error);\n+            assertEquals(emptySet(), valuesMap.get(logDir).replicaInfos.keySet());\n \n             Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n-            assertEquals(Collections.singleton(0), deprecatedAll.keySet());\n+            assertEquals(brokers, deprecatedAll.keySet());\n             Map<String, DescribeLogDirsResponse.LogDirInfo> allMap = deprecatedAll.get(0);\n             assertNotNull(allMap);\n-            assertEquals(Collections.singleton(\"/var/data/kafka\"), allMap.keySet());\n-            assertEquals(Errors.KAFKA_STORAGE_ERROR, allMap.get(\"/var/data/kafka\").error);\n-            assertEquals(emptySet(), allMap.get(\"/var/data/kafka\").replicaInfos.keySet());\n+            assertEquals(Collections.singleton(logDir), allMap.keySet());\n+            assertEquals(error, allMap.get(logDir).error);\n+            assertEquals(emptySet(), allMap.get(logDir).replicaInfos.keySet());\n         }\n     }\n \n     @Test\n     public void testDescribeReplicaLogDirs() throws ExecutionException, InterruptedException {\n         TopicPartitionReplica tpr1 = new TopicPartitionReplica(\"topic\", 12, 1);\n+        TopicPartitionReplica tpr2 = new TopicPartitionReplica(\"topic\", 12, 2);\n \n         try (AdminClientUnitTestEnv env = mockClientEnv()) {\n             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n-            env.kafkaClient().prepareResponseFrom(new DescribeLogDirsResponse(\n-                    new DescribeLogDirsResponseData().setResults(asList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                            .setErrorCode(Errors.NONE.code())\n-                            .setLogDir(\"/var/data/kafka0\")\n-                            .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                    .setName(tpr1.topic())\n-                                    .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                            .setPartitionIndex(tpr1.partition())\n-                                            .setPartitionSize(987654321)\n-                                            .setIsFutureKey(false)\n-                                            .setOffsetLag(12))))),\n-                            new DescribeLogDirsResponseData.DescribeLogDirsResult()\n-                                    .setErrorCode(Errors.NONE.code())\n-                                    .setLogDir(\"/var/data/kafka1\")\n-                                    .setTopics(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsTopic()\n-                                            .setName(tpr1.topic())\n-                                            .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n-                                                    .setPartitionIndex(tpr1.partition())\n-                                                    .setPartitionSize(123456789)\n-                                                    .setIsFutureKey(true)\n-                                                    .setOffsetLag(4321)))))\n-                    ))), env.cluster().nodeById(tpr1.brokerId()));\n-\n-            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr1));\n+            String broker1log0 = \"/var/data/kafka0\";\n+            String broker1log1 = \"/var/data/kafka1\";\n+            String broker2log0 = \"/var/data/kafka2\";\n+            env.kafkaClient().prepareResponseFrom(\n+                    new DescribeLogDirsResponse(\n+                            new DescribeLogDirsResponseData().setResults(asList(\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log0)\n+                                            .setTopics(prepareDescribeLogDirsTopics(987654321, 12, tpr1.topic(), tpr1.partition(), false)),\n+                                    new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                                            .setErrorCode(Errors.NONE.code())\n+                                            .setLogDir(broker1log1)\n+                                            .setTopics(prepareDescribeLogDirsTopics(123456789, 4321, tpr1.topic(), tpr1.partition(), true))))),\n+                    env.cluster().nodeById(tpr1.brokerId()));\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.KAFKA_STORAGE_ERROR, broker2log0),\n+                    env.cluster().nodeById(tpr2.brokerId()));\n+\n+            DescribeReplicaLogDirsResult result = env.adminClient().describeReplicaLogDirs(asList(tpr1, tpr2));\n+\n             Map<TopicPartitionReplica, KafkaFuture<DescribeReplicaLogDirsResult.ReplicaLogDirInfo>> values = result.values();\n-            assertEquals(TestUtils.toSet(singletonList(tpr1)), values.keySet());\n+            assertEquals(TestUtils.toSet(asList(tpr1, tpr2)), values.keySet());\n \n             assertNotNull(values.get(tpr1));\n-            assertEquals(\"/var/data/kafka0\", values.get(tpr1).get().getCurrentReplicaLogDir());\n+            assertEquals(broker1log0, values.get(tpr1).get().getCurrentReplicaLogDir());\n             assertEquals(12, values.get(tpr1).get().getCurrentReplicaOffsetLag());\n-            assertEquals(\"/var/data/kafka1\", values.get(tpr1).get().getFutureReplicaLogDir());\n+            assertEquals(broker1log1, values.get(tpr1).get().getFutureReplicaLogDir());\n             assertEquals(4321, values.get(tpr1).get().getFutureReplicaOffsetLag());\n+\n+            assertNotNull(values.get(tpr2));\n+            assertNull(values.get(tpr2).get().getCurrentReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getCurrentReplicaOffsetLag());\n+            assertNull(values.get(tpr2).get().getFutureReplicaLogDir());\n+            assertEquals(-1, values.get(tpr2).get().getFutureReplicaOffsetLag());\n         }\n     }\n \n"}}, {"oid": "af5224e02573157c183e7c5d5c9609b9592d5763", "url": "https://github.com/apache/kafka/commit/af5224e02573157c183e7c5d5c9609b9592d5763", "message": "Code review", "committedDate": "2020-07-15T09:35:44Z", "type": "commit"}, {"oid": "cad6e6347ea76177446c3423e04aec39fd49d36b", "url": "https://github.com/apache/kafka/commit/cad6e6347ea76177446c3423e04aec39fd49d36b", "message": "Review comment", "committedDate": "2020-07-15T10:08:06Z", "type": "commit"}, {"oid": "7fb3f1201e0803300c9d4d5a2aed949069761dd8", "url": "https://github.com/apache/kafka/commit/7fb3f1201e0803300c9d4d5a2aed949069761dd8", "message": "Review comment", "committedDate": "2020-07-15T12:46:34Z", "type": "commit"}, {"oid": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "url": "https://github.com/apache/kafka/commit/637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "message": "Review comment", "committedDate": "2020-07-15T13:50:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0MDczMw==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459540733", "bodyText": "This can be static. Also should we keep it in DescribeLogDirsResponse?", "author": "mimaison", "createdAt": "2020-07-23T15:34:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2306,6 +2310,22 @@ void handleFailure(Throwable throwable) {\n         return new DescribeLogDirsResult(new HashMap<>(futures));\n     }\n \n+    private Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\nindex 89f5cf135d..d083a8694f 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n\n@@ -2310,8 +2310,8 @@ public class KafkaAdminClient extends AdminClient {\n         return new DescribeLogDirsResult(new HashMap<>(futures));\n     }\n \n-    private Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {\n-        HashMap<String, LogDirDescription> result = new HashMap<>(response.data().results().size());\n+    private static Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {\n+        Map<String, LogDirDescription> result = new HashMap<>(response.data().results().size());\n         for (DescribeLogDirsResponseData.DescribeLogDirsResult logDirResult : response.data().results()) {\n             Map<TopicPartition, ReplicaInfo> replicaInfoMap = new HashMap<>();\n             for (DescribeLogDirsResponseData.DescribeLogDirsTopic t : logDirResult.topics()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU0MDk0NA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459540944", "bodyText": "The left side can be Map", "author": "mimaison", "createdAt": "2020-07-23T15:34:24Z", "path": "clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java", "diffHunk": "@@ -2306,6 +2310,22 @@ void handleFailure(Throwable throwable) {\n         return new DescribeLogDirsResult(new HashMap<>(futures));\n     }\n \n+    private Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {\n+        HashMap<String, LogDirDescription> result = new HashMap<>(response.data().results().size());", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\nindex 89f5cf135d..d083a8694f 100644\n--- a/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n+++ b/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java\n\n@@ -2310,8 +2310,8 @@ public class KafkaAdminClient extends AdminClient {\n         return new DescribeLogDirsResult(new HashMap<>(futures));\n     }\n \n-    private Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {\n-        HashMap<String, LogDirDescription> result = new HashMap<>(response.data().results().size());\n+    private static Map<String, LogDirDescription> logDirDescriptions(DescribeLogDirsResponse response) {\n+        Map<String, LogDirDescription> result = new HashMap<>(response.data().results().size());\n         for (DescribeLogDirsResponseData.DescribeLogDirsResult logDirResult : response.data().results()) {\n             Map<TopicPartition, ReplicaInfo> replicaInfoMap = new HashMap<>();\n             for (DescribeLogDirsResponseData.DescribeLogDirsTopic t : logDirResult.topics()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1ODA3Mw==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459558073", "bodyText": "This can be static", "author": "mimaison", "createdAt": "2020-07-23T15:59:23Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,263 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 994606d56a..3972df81e0 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,12 +1059,12 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n-    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+    private static DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n         return prepareDescribeLogDirsResponse(error, logDir,\n                 prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n     }\n \n-    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+    private static List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n             long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n         return singletonList(new DescribeLogDirsTopic()\n                 .setName(topic)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1ODE1MA==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459558150", "bodyText": "This can be static", "author": "mimaison", "createdAt": "2020-07-23T15:59:33Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,263 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 994606d56a..3972df81e0 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,12 +1059,12 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n-    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+    private static DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n         return prepareDescribeLogDirsResponse(error, logDir,\n                 prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n     }\n \n-    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+    private static List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n             long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n         return singletonList(new DescribeLogDirsTopic()\n                 .setName(topic)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU1ODY2OQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459558669", "bodyText": "This can be static", "author": "mimaison", "createdAt": "2020-07-23T16:00:17Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,263 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 994606d56a..3972df81e0 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,12 +1059,12 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n-    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+    private static DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n         return prepareDescribeLogDirsResponse(error, logDir,\n                 prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n     }\n \n-    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+    private static List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n             long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n         return singletonList(new DescribeLogDirsTopic()\n                 .setName(topic)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2MTY4OQ==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459561689", "bodyText": "This can be static", "author": "mimaison", "createdAt": "2020-07-23T16:04:57Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,263 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(brokers, descriptions.keySet());\n+            assertNotNull(descriptions.get(0));\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n+\n+            Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n+        }\n+    }\n+\n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 994606d56a..3972df81e0 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,12 +1059,12 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n-    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+    private static DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n         return prepareDescribeLogDirsResponse(error, logDir,\n                 prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n     }\n \n-    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+    private static List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n             long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n         return singletonList(new DescribeLogDirsTopic()\n                 .setName(topic)\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTU2MTc3Mg==", "url": "https://github.com/apache/kafka/pull/9007#discussion_r459561772", "bodyText": "This can be static", "author": "mimaison", "createdAt": "2020-07-23T16:05:09Z", "path": "clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java", "diffHunk": "@@ -1057,6 +1059,263 @@ public void testDescribeConfigsUnrequested() throws Exception {\n         }\n     }\n \n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+        return prepareDescribeLogDirsResponse(error, logDir,\n+                prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n+    }\n+\n+    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+            long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n+        return singletonList(new DescribeLogDirsTopic()\n+                .setName(topic)\n+                .setPartitions(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsPartition()\n+                        .setPartitionIndex(partition)\n+                        .setPartitionSize(partitionSize)\n+                        .setIsFutureKey(isFuture)\n+                        .setOffsetLag(offsetLag))));\n+    }\n+\n+    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir,\n+                                                                   List<DescribeLogDirsTopic> topics) {\n+        return new DescribeLogDirsResponse(\n+                new DescribeLogDirsResponseData().setResults(singletonList(new DescribeLogDirsResponseData.DescribeLogDirsResult()\n+                        .setErrorCode(error.code())\n+                        .setLogDir(logDir)\n+                        .setTopics(topics)\n+                )));\n+    }\n+\n+    @Test\n+    public void testDescribeLogDirs() throws ExecutionException, InterruptedException {\n+        Set<Integer> brokers = Collections.singleton(0);\n+        String logDir = \"/var/data/kafka\";\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        long partitionSize = 1234567890;\n+        long offsetLag = 24;\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(Errors.NONE, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n+            Map<Integer, KafkaFuture<Map<String, LogDirDescription>>> descriptions = result.descriptions();\n+            assertEquals(brokers, descriptions.keySet());\n+            assertNotNull(descriptions.get(0));\n+            assertDescriptionContains(descriptions.get(0).get(), logDir, tp, partitionSize, offsetLag);\n+\n+            Map<Integer, Map<String, LogDirDescription>> allDescriptions = result.allDescriptions().get();\n+            assertEquals(brokers, allDescriptions.keySet());\n+            assertDescriptionContains(allDescriptions.get(0), logDir, tp, partitionSize, offsetLag);\n+        }\n+    }\n+\n+    private void assertDescriptionContains(Map<String, LogDirDescription> descriptionsMap, String logDir,\n+                                           TopicPartition tp, long partitionSize, long offsetLag) {\n+        assertNotNull(descriptionsMap);\n+        assertEquals(Collections.singleton(logDir), descriptionsMap.keySet());\n+        assertNull(descriptionsMap.get(logDir).error());\n+        Map<TopicPartition, ReplicaInfo> descriptionsReplicaInfos = descriptionsMap.get(logDir).replicaInfos();\n+        assertEquals(Collections.singleton(tp), descriptionsReplicaInfos.keySet());\n+        assertEquals(partitionSize, descriptionsReplicaInfos.get(tp).size());\n+        assertEquals(offsetLag, descriptionsReplicaInfos.get(tp).offsetLag());\n+        assertFalse(descriptionsReplicaInfos.get(tp).isFuture());\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    @Test\n+    public void testDescribeLogDirsDeprecated() throws ExecutionException, InterruptedException {\n+        Set<Integer> brokers = Collections.singleton(0);\n+        TopicPartition tp = new TopicPartition(\"topic\", 12);\n+        String logDir = \"/var/data/kafka\";\n+        Errors error = Errors.NONE;\n+        int offsetLag = 24;\n+        long partitionSize = 1234567890;\n+\n+        try (AdminClientUnitTestEnv env = mockClientEnv()) {\n+            env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());\n+            env.kafkaClient().prepareResponseFrom(\n+                    prepareDescribeLogDirsResponse(error, logDir, tp, partitionSize, offsetLag),\n+                    env.cluster().nodeById(0));\n+\n+            DescribeLogDirsResult result = env.adminClient().describeLogDirs(brokers);\n+\n+            Map<Integer, KafkaFuture<Map<String, DescribeLogDirsResponse.LogDirInfo>>> deprecatedValues = result.values();\n+            assertEquals(brokers, deprecatedValues.keySet());\n+            assertNotNull(deprecatedValues.get(0));\n+            assertDescriptionContains(deprecatedValues.get(0).get(), logDir, tp, error, offsetLag, partitionSize);\n+\n+            Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> deprecatedAll = result.all().get();\n+            assertEquals(brokers, deprecatedAll.keySet());\n+            assertDescriptionContains(deprecatedAll.get(0), logDir, tp, error, offsetLag, partitionSize);\n+        }\n+    }\n+\n+    @SuppressWarnings(\"deprecation\")\n+    private void assertDescriptionContains(Map<String, DescribeLogDirsResponse.LogDirInfo> descriptionsMap,", "originalCommit": "637e7dbe9d888f41d9cbe9ff97de179eab5584a2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d17d02f31f30ed64acc8609e0339cb9883a19316", "chunk": "diff --git a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\nindex 994606d56a..3972df81e0 100644\n--- a/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n+++ b/clients/src/test/java/org/apache/kafka/clients/admin/KafkaAdminClientTest.java\n\n@@ -1059,12 +1059,12 @@ public class KafkaAdminClientTest {\n         }\n     }\n \n-    private DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n+    private static DescribeLogDirsResponse prepareDescribeLogDirsResponse(Errors error, String logDir, TopicPartition tp, long partitionSize, long offsetLag) {\n         return prepareDescribeLogDirsResponse(error, logDir,\n                 prepareDescribeLogDirsTopics(partitionSize, offsetLag, tp.topic(), tp.partition(), false));\n     }\n \n-    private List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n+    private static List<DescribeLogDirsTopic> prepareDescribeLogDirsTopics(\n             long partitionSize, long offsetLag, String topic, int partition, boolean isFuture) {\n         return singletonList(new DescribeLogDirsTopic()\n                 .setName(topic)\n"}}, {"oid": "d17d02f31f30ed64acc8609e0339cb9883a19316", "url": "https://github.com/apache/kafka/commit/d17d02f31f30ed64acc8609e0339cb9883a19316", "message": "Review comments", "committedDate": "2020-07-28T07:41:11Z", "type": "commit"}]}