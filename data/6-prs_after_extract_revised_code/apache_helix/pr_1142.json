{"pr_number": 1142, "pr_title": "Respect Maximum Number Of Attempts for the tasks", "pr_createdAt": "2020-07-07T20:15:21Z", "pr_url": "https://github.com/apache/helix/pull/1142", "timeline": [{"oid": "4d6e98ec814f4e87536a8ff81fb98cf52cac57b8", "url": "https://github.com/apache/helix/commit/4d6e98ec814f4e87536a8ff81fb98cf52cac57b8", "message": "Respect Maximum Number Of Attempts for the tasks\n\nIn this commit, several scheduling parts have been changed which\nenforces the scheduler to respect maximum number of attempts for\nthe tasks.\n\nAlso, it has been observed that when a task being dropped and\nscheduled again, max number of attempts is not being respected.\nin this commit, further checks is added to not schedule the\ntasks again once we reach its maximum number of attempts.\n\nSeveral of the tests have been changed. Scpecially the once with\nstrict MaxNumberOfAttempts. These tests need to be changed\nbecause they are mostly related to the targetted jobs. Once\nwe start the participants and targetted partition bounces between\nthe participants, the tasks needs to be reassigned and number\nof attempts for the task will increase.", "committedDate": "2020-07-08T17:58:55Z", "type": "forcePushed"}, {"oid": "be5497b0d1da02fe8fcbab12444947c91442e047", "url": "https://github.com/apache/helix/commit/be5497b0d1da02fe8fcbab12444947c91442e047", "message": "Respect Maximum Number Of Attempts for the tasks\n\nIn this commit, several scheduling parts have been changed which\nenforces the scheduler to respect maximum number of attempts for\nthe tasks.\n\nAlso, it has been observed that when a task being dropped and\nscheduled again, max number of attempts is not being respected.\nin this commit, further checks is added to not schedule the\ntasks again once we reach its maximum number of attempts.\n\nSeveral of the tests have been changed. Scpecially the once with\nstrict MaxNumberOfAttempts. These tests need to be changed\nbecause they are mostly related to the targetted jobs. Once\nwe start the participants and targetted partition bounces between\nthe participants, the tasks needs to be reassigned and number\nof attempts for the task will increase.", "committedDate": "2020-07-09T03:15:46Z", "type": "commit"}, {"oid": "be5497b0d1da02fe8fcbab12444947c91442e047", "url": "https://github.com/apache/helix/commit/be5497b0d1da02fe8fcbab12444947c91442e047", "message": "Respect Maximum Number Of Attempts for the tasks\n\nIn this commit, several scheduling parts have been changed which\nenforces the scheduler to respect maximum number of attempts for\nthe tasks.\n\nAlso, it has been observed that when a task being dropped and\nscheduled again, max number of attempts is not being respected.\nin this commit, further checks is added to not schedule the\ntasks again once we reach its maximum number of attempts.\n\nSeveral of the tests have been changed. Scpecially the once with\nstrict MaxNumberOfAttempts. These tests need to be changed\nbecause they are mostly related to the targetted jobs. Once\nwe start the participants and targetted partition bounces between\nthe participants, the tasks needs to be reassigned and number\nof attempts for the task will increase.", "committedDate": "2020-07-09T03:15:46Z", "type": "forcePushed"}, {"oid": "c607510303b86ef5060c160190dd08cede2f8dd1", "url": "https://github.com/apache/helix/commit/c607510303b86ef5060c160190dd08cede2f8dd1", "message": "Add message to existsLiveInstanceOrCurrentStateChange", "committedDate": "2020-07-09T22:43:10Z", "type": "commit"}, {"oid": "9a5ffdb6e578cd767f98076037bb60495305e3e4", "url": "https://github.com/apache/helix/commit/9a5ffdb6e578cd767f98076037bb60495305e3e4", "message": "change test to use customized rebalancer", "committedDate": "2020-07-09T22:50:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338543", "bodyText": "Would it be cleaner to define a set of enums (states) where we should mark the tasks aborted? Then we could do a simple exists() check.\nOr, it seems that if we check for != condition, we won't have to list out so many states here?", "author": "narendly", "createdAt": "2020-07-12T16:46:38Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -185,17 +185,13 @@ public void updatePreviousAssignedTasksStatus(\n         switch (currState) {\n         case RUNNING: {\n           TaskPartitionState nextState = TaskPartitionState.RUNNING;\n-          if (jobState == TaskState.TIMING_OUT) {\n+          if (jobState == TaskState.TIMING_OUT || jobState == TaskState.TIMED_OUT\n+              || jobState == TaskState.FAILING || jobState == TaskState.FAILED\n+              || jobState == TaskState.ABORTED) {", "originalCommit": "9a5ffdb6e578cd767f98076037bb60495305e3e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTU4MQ==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339581", "bodyText": "Also just to confirm, this is to make sure that task states that are stuck in RUNNING get updated to ABORTED?", "author": "narendly", "createdAt": "2020-07-12T16:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5NjkxMA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r455396910", "bodyText": "Done. I create a list for these states. Yes and also make sure task goes to ABORTED state and we release quota.", "author": "alirezazamani", "createdAt": "2020-07-15T22:17:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "7e153c6722c52f472447c3db922050f25e79b237", "chunk": "diff --git a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\nindex 75131743e..82ae87bb6 100644\n--- a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n+++ b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n\n@@ -185,9 +192,7 @@ public abstract class AbstractTaskDispatcher {\n         switch (currState) {\n         case RUNNING: {\n           TaskPartitionState nextState = TaskPartitionState.RUNNING;\n-          if (jobState == TaskState.TIMING_OUT || jobState == TaskState.TIMED_OUT\n-              || jobState == TaskState.FAILING || jobState == TaskState.FAILED\n-              || jobState == TaskState.ABORTED) {\n+          if (jobStatesForRunningTaskToAbortedState.contains(jobState)) {\n             nextState = TaskPartitionState.TASK_ABORTED;\n           } else if (jobTgtState == TargetState.STOP) {\n             nextState = TaskPartitionState.STOPPED;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU3OA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338578", "bodyText": "Thanks for renaming!", "author": "narendly", "createdAt": "2020-07-12T16:47:04Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -560,7 +556,7 @@ protected void handleAdditionalTaskAssignment(\n       excludeSet.addAll(assignedSet);\n     }\n     addCompletedTasks(excludeSet, jobCtx, allPartitions);\n-    addGiveupPartitions(excludeSet, jobCtx, allPartitions, jobCfg);\n+    addPartitionsReachedMaximumRetries(excludeSet, jobCtx, allPartitions, jobCfg);", "originalCommit": "9a5ffdb6e578cd767f98076037bb60495305e3e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwNzY3Ng==", "url": "https://github.com/apache/helix/pull/1142#discussion_r455407676", "bodyText": ";-)", "author": "alirezazamani", "createdAt": "2020-07-15T22:45:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODU3OA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODY1MA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453338650", "bodyText": "What is the distinction between \"given up\" and partitions that exceeded the # of maximum attempts? Do you think we could clarify that here?\nI have a feeling that \"given up\" might be too broad or abstract.", "author": "narendly", "createdAt": "2020-07-12T16:47:51Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,7 +743,7 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n+  // add all partitions that are given up", "originalCommit": "9a5ffdb6e578cd767f98076037bb60495305e3e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5OTE1Mg==", "url": "https://github.com/apache/helix/pull/1142#discussion_r455399152", "bodyText": "Thanks for the comment. I added comments for more explanation.\nAlso I noticed the given up tasks has been already added in job dispatcher. So we do not need to add the tasks again. However, we want to have a sanity check to filter all of the tasks, regarding of their states, if they reached their maximum number of attempts. Because it has been observed that sometimes, the state of the task might be INIT or DROPPED or other states (if we have pending messages) and we miss max number of attempts check for those tasks.", "author": "alirezazamani", "createdAt": "2020-07-15T22:22:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzODY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "7e153c6722c52f472447c3db922050f25e79b237", "chunk": "diff --git a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\nindex 75131743e..82ae87bb6 100644\n--- a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n+++ b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n\n@@ -743,8 +748,12 @@ public abstract class AbstractTaskDispatcher {\n     }\n   }\n \n-  // add all partitions that are given up\n-  protected static void addGiveupPartitions(Set<Integer> set, JobContext ctx,\n+  // Add all partitions/tasks that are cannot be retried. These tasks are:\n+  // 1- Task is in ABORTED or ERROR state.\n+  // 2- Task has just gone to TIMED_OUT, ERROR or DROPPED states and has reached to its\n+  // maxNumberAttempts\n+  // These tasks determine whether the job needs to FAILED or not.\n+  protected static void addGiveUpPartitions(Set<Integer> set, JobContext ctx,\n       Iterable<Integer> pIds, JobConfig cfg) {\n     for (Integer pId : pIds) {\n       if (isTaskGivenup(ctx, cfg, pId)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339180", "bodyText": "Could you explain further why this change is needed? If we add more nodes, the db partitions will get shifted. Are we adding nodes while the workflow is in progress and before its tasks complete? In that case, the shuffling of tasks (getting dropped and rescheduled onto another instance) makes sense and we should increment # of attempts.\nSince the focus of this test doesn't seem to be # of attempts, do you think we should add a new test that tests for this? We want to make sure that for targeted tasks, if we add more nodes and if that causes db partitions to be moved, then we should observe the tasks get dropped and retried with an increase in the # of attempts. One thing to watch out for here is \"rebalanceRunningTask\" - what role should that parameter play?", "author": "narendly", "createdAt": "2020-07-12T16:53:50Z", "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java", "diffHunk": "@@ -44,10 +44,10 @@ public void testForceDeleteJobFromJobQueue() throws InterruptedException {\n     // Create two jobs: job1 will complete fast, and job2 will be stuck in progress (taking a long\n     // time to finish). The idea is to force-delete a stuck job (job2).\n     JobConfig.Builder jobBuilder = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n+        .setWorkflow(jobQueueName)", "originalCommit": "9a5ffdb6e578cd767f98076037bb60495305e3e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTM0Mg==", "url": "https://github.com/apache/helix/pull/1142#discussion_r453339342", "bodyText": "I just realized that you did add a new test below, TestMaxNumberOfAttemptsMasterSwitch. Great job!\nNow, if we could reason through how the \"rebalanceRunningTask\" config should (or should not) apply here, we're set :)", "author": "narendly", "createdAt": "2020-07-12T16:55:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQwNjQxMA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r455406410", "bodyText": "I reverted this change for this test. However, we still need to increase number of attempts for some test that we stop the participant and start them again.\nFor example, in the TestForceDeleteWorkflow.testDeleteCompletedWorkflowForcefully:\nIn super.beforeClass() we start the participant, however, once the test started, we stop those participants and start them again. I have observed that if we start the workflow very close to the time that we start the participant one by one (kind of like rolling upgrade), Master might jump several times between the nodes. Hence if we set maxNumberOfAttempts to 1, then the test will fail. Since we just started to respect maxNumberOfAttempts in this PR.\nAbout your second comment: Yeah the newly added test checks maxNumberOfAttempts for targetJobChange. For \"rebalanceRunningTask\", I checked other tests and I saw we this logic is already covered in \"TestRebalanceRunningTask\" extensively for different scenarios. So if rebalance running task is set and we need to drop the task in one instance and run in other one (Send INIT->RUNNING), then this is new scheduling and we are respecting max number of attempts. So rebalanceRunningTask is happening until we reach maxNumberOfAttempts. In other word, once a task reaches maxNumberOfAttempts, it won't be scheduled again. So no new INIT->RUNNING once we reach maxNumberOfAttempts :)", "author": "alirezazamani", "createdAt": "2020-07-15T22:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzMzOTE4MA=="}], "type": "inlineReview", "revised_code": {"commit": "7e153c6722c52f472447c3db922050f25e79b237", "chunk": "diff --git a/helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java b/helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java\nindex eb43a7208..dfe6d4e38 100644\n--- a/helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java\n+++ b/helix-core/src/test/java/org/apache/helix/integration/task/TestDeleteJobFromJobQueue.java\n\n@@ -44,10 +44,10 @@ public class TestDeleteJobFromJobQueue extends TaskTestBase {\n     // Create two jobs: job1 will complete fast, and job2 will be stuck in progress (taking a long\n     // time to finish). The idea is to force-delete a stuck job (job2).\n     JobConfig.Builder jobBuilder = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setWorkflow(jobQueueName)\n+        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n         .setJobCommandConfigMap(ImmutableMap.of(MockTask.JOB_DELAY, \"10\"));\n     JobConfig.Builder jobBuilder2 = JobConfig.Builder.fromMap(WorkflowGenerator.DEFAULT_JOB_CONFIG)\n-        .setWorkflow(jobQueueName)\n+        .setMaxAttemptsPerTask(1).setWorkflow(jobQueueName)\n         .setJobCommandConfigMap(ImmutableMap.of(MockTask.JOB_DELAY, \"100000\")).setTimeout(100000);\n \n     JobQueue.Builder jobQueue = TaskTestUtil.buildJobQueue(jobQueueName);\n"}}, {"oid": "7e153c6722c52f472447c3db922050f25e79b237", "url": "https://github.com/apache/helix/commit/7e153c6722c52f472447c3db922050f25e79b237", "message": "Addressing the comments", "committedDate": "2020-07-15T22:16:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjQ0MQ==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457136441", "bodyText": "Nit:\nIf a job is in one of the following states and its tasks are in RUNNING states, the tasks will be aborted.", "author": "narendly", "createdAt": "2020-07-20T07:38:41Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.", "originalCommit": "7e153c6722c52f472447c3db922050f25e79b237", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTUxNg==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601516", "bodyText": "Fixed. Thanks", "author": "alirezazamani", "createdAt": "2020-07-20T18:15:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjQ0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "50df0e0bc1121d34f70b7b5dcef7145b706a4bf9", "chunk": "diff --git a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\nindex 82ae87bb6..3cdcb1e86 100644\n--- a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n+++ b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n\n@@ -75,9 +75,9 @@ public abstract class AbstractTaskDispatcher {\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n-    // If a job is in the following states and contains running tasks, all of the running tasks\n-    // should go to the ABORTED states.\n-    Set<TaskState> jobStatesForRunningTaskToAbortedState =\n+    // If a job is in one of the following states and its tasks are in RUNNING states, the tasks\n+    // will be aborted.\n+    Set<TaskState> jobStatesForAbortingTasks =\n         new HashSet<>(Arrays.asList(TaskState.TIMING_OUT, TaskState.TIMED_OUT, TaskState.FAILING,\n             TaskState.FAILED, TaskState.ABORTED));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjgyOQ==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457136829", "bodyText": "I think calling it jobStatesForAbortingTasks does the trick and is shorter. What do you think?", "author": "narendly", "createdAt": "2020-07-20T07:39:16Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -74,6 +75,12 @@ public void updatePreviousAssignedTasksStatus(\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n+    // If a job is in the following states and contains running tasks, all of the running tasks\n+    // should go to the ABORTED states.\n+    Set<TaskState> jobStatesForRunningTaskToAbortedState =", "originalCommit": "7e153c6722c52f472447c3db922050f25e79b237", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTg4MA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601880", "bodyText": "Yeah I wasn't happy with previous name as well. Change it as you suggested. Thanks.", "author": "alirezazamani", "createdAt": "2020-07-20T18:15:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzNjgyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "50df0e0bc1121d34f70b7b5dcef7145b706a4bf9", "chunk": "diff --git a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\nindex 82ae87bb6..3cdcb1e86 100644\n--- a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n+++ b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n\n@@ -75,9 +75,9 @@ public abstract class AbstractTaskDispatcher {\n       Set<Integer> skippedPartitions, WorkflowControllerDataProvider cache,\n       Map<String, Set<Integer>> tasksToDrop) {\n \n-    // If a job is in the following states and contains running tasks, all of the running tasks\n-    // should go to the ABORTED states.\n-    Set<TaskState> jobStatesForRunningTaskToAbortedState =\n+    // If a job is in one of the following states and its tasks are in RUNNING states, the tasks\n+    // will be aborted.\n+    Set<TaskState> jobStatesForAbortingTasks =\n         new HashSet<>(Arrays.asList(TaskState.TIMING_OUT, TaskState.TIMED_OUT, TaskState.FAILING,\n             TaskState.FAILED, TaskState.ABORTED));\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODQ2MQ==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457138461", "bodyText": "Typo: addGivenUpPartitions", "author": "narendly", "createdAt": "2020-07-20T07:41:41Z", "path": "helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java", "diffHunk": "@@ -745,8 +748,12 @@ protected void scheduleForNextTask(String job, JobContext jobCtx, long now) {\n     }\n   }\n \n-  // add all partitions that have been tried maxNumberAttempts\n-  protected static void addGiveupPartitions(Set<Integer> set, JobContext ctx,\n+  // Add all partitions/tasks that are cannot be retried. These tasks are:\n+  // 1- Task is in ABORTED or ERROR state.\n+  // 2- Task has just gone to TIMED_OUT, ERROR or DROPPED states and has reached to its\n+  // maxNumberAttempts\n+  // These tasks determine whether the job needs to FAILED or not.\n+  protected static void addGiveUpPartitions(Set<Integer> set, JobContext ctx,", "originalCommit": "7e153c6722c52f472447c3db922050f25e79b237", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzYwMTYxMA==", "url": "https://github.com/apache/helix/pull/1142#discussion_r457601610", "bodyText": "Done :-)", "author": "alirezazamani", "createdAt": "2020-07-20T18:15:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEzODQ2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "50df0e0bc1121d34f70b7b5dcef7145b706a4bf9", "chunk": "diff --git a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\nindex 82ae87bb6..3cdcb1e86 100644\n--- a/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n+++ b/helix-core/src/main/java/org/apache/helix/task/AbstractTaskDispatcher.java\n\n@@ -753,7 +753,7 @@ public abstract class AbstractTaskDispatcher {\n   // 2- Task has just gone to TIMED_OUT, ERROR or DROPPED states and has reached to its\n   // maxNumberAttempts\n   // These tasks determine whether the job needs to FAILED or not.\n-  protected static void addGiveUpPartitions(Set<Integer> set, JobContext ctx,\n+  protected static void addGivenUpPartitions(Set<Integer> set, JobContext ctx,\n       Iterable<Integer> pIds, JobConfig cfg) {\n     for (Integer pId : pIds) {\n       if (isTaskGivenup(ctx, cfg, pId)) {\n"}}, {"oid": "50df0e0bc1121d34f70b7b5dcef7145b706a4bf9", "url": "https://github.com/apache/helix/commit/50df0e0bc1121d34f70b7b5dcef7145b706a4bf9", "message": "Address new comments", "committedDate": "2020-07-20T18:17:00Z", "type": "commit"}]}