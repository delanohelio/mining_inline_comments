{"pr_number": 1346, "pr_title": "Stabilize TestEnqueueJobs ", "pr_createdAt": "2020-09-03T23:01:19Z", "pr_url": "https://github.com/apache/helix/pull/1346", "timeline": [{"oid": "7587e25744b4d5f8542d8818a17d7e52085a5e62", "url": "https://github.com/apache/helix/commit/7587e25744b4d5f8542d8818a17d7e52085a5e62", "message": "Stabilize TestEnqueueJobs by stop and resuming the queue\n\nIn this commit TestEnqueueJobs has been stabilized by stopping the\nqueue before addition and resuming it after adding the new jobs.\nAlso, instead of adding job one by one, batch enqueue job has been\nused.", "committedDate": "2020-09-03T19:56:15Z", "type": "commit"}, {"oid": "a5ab8b3d410b8fc6022e61276139a278c9188aeb", "url": "https://github.com/apache/helix/commit/a5ab8b3d410b8fc6022e61276139a278c9188aeb", "message": "New stabilizations", "committedDate": "2020-09-04T00:32:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNzE5Nw==", "url": "https://github.com/apache/helix/pull/1346#discussion_r485117197", "bodyText": "From my understanding, is this batching operation helped? It should impact the testing result, right? As long as the queue is STOPPED, the number of jobs added to queue one by one should not matter the final result before resume?", "author": "junkaixue", "createdAt": "2020-09-08T18:31:09Z", "path": "helix-core/src/test/java/org/apache/helix/integration/task/TestEnqueueJobs.java", "diffHunk": "@@ -135,10 +146,18 @@ public void testQueueParallelJobs() throws InterruptedException {\n             .setCommand(MockTask.TASK_COMMAND).setMaxAttemptsPerTask(2)\n             .setJobCommandConfigMap(Collections.singletonMap(MockTask.JOB_DELAY, \"10000\"));\n \n+    _driver.waitToStop(queueName, 5000L);\n+\n     // Add 4 jobs to the queue\n+    List<String> jobNames = new ArrayList<>();\n+    List<JobConfig.Builder> jobBuilders = new ArrayList<>();\n     for (int i = 0; i < numberOfJobsAddedBeforeControllerSwitch; i++) {\n-      _driver.enqueueJob(queueName, \"JOB\" + i, jobBuilder);\n+      jobNames.add(\"JOB\" + i);\n+      jobBuilders.add(jobBuilder);\n     }\n+    _driver.enqueueJobs(queueName, jobNames, jobBuilders);", "originalCommit": "a5ab8b3d410b8fc6022e61276139a278c9188aeb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTEyNjM3Mw==", "url": "https://github.com/apache/helix/pull/1346#discussion_r485126373", "bodyText": "Yeah. Batching would help. The issue seems to be in the nature of Helix/ZK if user adds jobs in a row. So if this is the behavior, it is better to do batch job addition. So let's say user is adding jobs one by one. Let's say user is job1, job2 and job3.\nAt T1: Job1 and job2 are added and jobDAG is changed.\nAt T2: We get children of config and know new configs of job1 and job2 and change in DAG.\nAt T3: job3 is added and is being added to DAG.\nAt T4: Refresh is started and and controller see config of job1, job2 and DAG will be Job1, job2 and job3.\nNow in the pipeline since we see job3 in the DAG and we do not see config, we purge job3 and remove config from ZK. Hence job3 will not be finished at all.\nTo answer your question, it depends. It depends on when you resume the queue, and what part of the logic we are. If we resume the but controller has not received the notification for new jobConfig, then it can skip scheduling for first pipeline. But the main issue is that if controller does not see the jobConfig (which is highly possible when we don't do add job in batch and in a loop we keep adding jobs one by one), then purge can delete that job (because controller realized the job is in the DAG but is missing from config) and then the job never get's finish.", "author": "alirezazamani", "createdAt": "2020-09-08T18:48:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNzE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTYxMjIyMg==", "url": "https://github.com/apache/helix/pull/1346#discussion_r495612222", "bodyText": "This fix only minimizes the chance, it does not really eliminate the chance.\nThe really issue is the race condition of workFlowControllerDataProvider updating sequence and job config and workflow config updating sequence. I will write a doc later to illustrate the cases here and potentially proposed solution.", "author": "kaisun2000", "createdAt": "2020-09-27T20:26:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTExNzE5Nw=="}], "type": "inlineReview", "revised_code": null}]}