{"pr_number": 5113, "pr_title": "Issue 5095: DR Integration tests included for readers stalling & watermarking", "pr_createdAt": "2020-08-26T06:30:17Z", "pr_url": "https://github.com/pravega/pravega/pull/5113", "timeline": [{"oid": "1470863fa656e0d97ccd8fd7a1c9c723c7decd7d", "url": "https://github.com/pravega/pravega/commit/1470863fa656e0d97ccd8fd7a1c9c723c7decd7d", "message": "Adding readers stall in DR integration test.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-08-26T06:13:07Z", "type": "commit"}, {"oid": "2cd75994ec37584d8c6cdf7a7d6c9de9b67e2e58", "url": "https://github.com/pravega/pravega/commit/2cd75994ec37584d8c6cdf7a7d6c9de9b67e2e58", "message": "Keeping only readers stall test.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-08-26T06:16:07Z", "type": "commit"}, {"oid": "a932b1dbda84af360e4e5d3fdcd26d860b81287c", "url": "https://github.com/pravega/pravega/commit/a932b1dbda84af360e4e5d3fdcd26d860b81287c", "message": "Adding DR integration test with watermarking.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-08-26T07:07:10Z", "type": "commit"}, {"oid": "0e56dedb83759a3395ec1e63ad240e9e005be193", "url": "https://github.com/pravega/pravega/commit/0e56dedb83759a3395ec1e63ad240e9e005be193", "message": "Updating tests.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-08-26T18:36:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzMzU4NA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485033584", "bodyText": "I don't see these two instances being closed anywhere.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:03:18Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -423,8 +463,8 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Segments have been recovered.\");\n \n         // Start a new segment store and controller\n-        this.segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, this.segmentStoreStarter.servicePort);\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0MzUxNA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486043514", "bodyText": "closing them in tearDown.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:38:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzMzU4NA=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDg0MQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485034841", "bodyText": "You really shouldn't be touching the class-level fields in a test. In unit tests, the only places where you can assign, un-assign or otherwise \"close\" such an object instance is in the setup/teardown methods.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:05:12Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -444,6 +484,495 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersStall() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        @Cleanup\n+        ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        log.info(\"Writing events on to stream: {}\", STREAM1);\n+        writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        log.info(\"Writing events on to stream: {}\", STREAM2);\n+        writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, NUM_EVENTS);\n+        Position p2 = readNEvents(reader2, NUM_EVENTS);\n+\n+        ReaderGroup readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0MzgwMQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486043801", "bodyText": "I need to reproduce the scenario of bookKeeper shut down, and then starting a new instance.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:39:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODk0ODg4Ng==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r488948886", "bodyText": "Then you need to create them locally and destroy them in the same scope where you created them. It is never a good practice to repurpose a higher-scoped variable (i.e., class-level field) in this case since your method will have unintended side effects.", "author": "andreipaduroiu", "createdAt": "2020-09-15T20:19:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDEzNTE3OQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r490135179", "bodyText": "Thanks. Made changes accordingly.", "author": "ManishKumarKeshri", "createdAt": "2020-09-17T10:22:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNDg0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNTI2MQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485035261", "bodyText": "Please make sure you close all object instances you create.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:05:50Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -444,6 +484,495 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersStall() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        @Cleanup\n+        ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        log.info(\"Writing events on to stream: {}\", STREAM1);\n+        writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        log.info(\"Writing events on to stream: {}\", STREAM2);\n+        writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, NUM_EVENTS);\n+        Position p2 = readNEvents(reader2, NUM_EVENTS);\n+\n+        ReaderGroup readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0Mzg1Ng==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486043856", "bodyText": "closing in tearDown.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:39:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNTI2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNTM5OQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485035399", "bodyText": "Same comments as in the previous tests.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:06:05Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -444,6 +484,495 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersStall() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        @Cleanup\n+        ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        log.info(\"Writing events on to stream: {}\", STREAM1);\n+        writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        log.info(\"Writing events on to stream: {}\", STREAM2);\n+        writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, NUM_EVENTS);\n+        Position p2 = readNEvents(reader2, NUM_EVENTS);\n+\n+        ReaderGroup readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+        Assert.assertNotNull(readerGroup1);\n+        Assert.assertNotNull(readerGroup2);\n+\n+        reader1 = clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+        reader2 = clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+\n+        // Read rest of the events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(5000).getEvent());\n+        assertNull(reader2.readNextEvent(5000).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String scope = \"scopeTx\";\n+        String stream = \"streamTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0NDE4Mw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486044183", "bodyText": "included a setUpPravega method which instantiates a bookeeper, segmentstore and controller with given bookie count, container count and instanceId.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:41:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNTM5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNzE3OA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485037178", "bodyText": "are writeEvent or commit async? If so, you need to wait on them.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:08:57Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -444,6 +484,495 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersStall() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        @Cleanup\n+        ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        log.info(\"Writing events on to stream: {}\", STREAM1);\n+        writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        log.info(\"Writing events on to stream: {}\", STREAM2);\n+        writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, NUM_EVENTS);\n+        Position p2 = readNEvents(reader2, NUM_EVENTS);\n+\n+        ReaderGroup readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+        Assert.assertNotNull(readerGroup1);\n+        Assert.assertNotNull(readerGroup2);\n+\n+        reader1 = clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+        reader2 = clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+\n+        // Read rest of the events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(5000).getEvent());\n+        assertNull(reader2.readNextEvent(5000).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String scope = \"scopeTx\";\n+        String stream = \"streamTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        Controller controller = controllerStarter.controller;\n+        StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(ScalingPolicy.fixed(5)).build();\n+\n+        URI controllerUri = URI.create(\"tcp://localhost:\" + controllerStarter.controllerPort);\n+        StreamManager streamManager = StreamManager.create(controllerUri);\n+        streamManager.createScope(scope);\n+        streamManager.createStream(scope, stream, config);\n+\n+        Stream streamObj = Stream.of(scope, stream);\n+\n+        // create 2 writers\n+        ClientConfig clientConfig = ClientConfig.builder().controllerURI(controllerUri).build();\n+        @Cleanup\n+        EventStreamClientFactory clientFactory = EventStreamClientFactory.withScope(scope, clientConfig);\n+        JavaSerializer<Long> javaSerializer = new JavaSerializer<>();\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer1 = clientFactory\n+                .createTransactionalEventWriter(\"writer1\", stream, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(10000).build());\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer2 = clientFactory\n+                .createTransactionalEventWriter(\"writer2\", stream, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(10000).build());\n+\n+        AtomicBoolean stopFlag = new AtomicBoolean(false);\n+        // write events\n+        CompletableFuture<Void> writer1Future = writeTxEvents(writer1, stopFlag);\n+        CompletableFuture<Void> writer2Future = writeTxEvents(writer2, stopFlag);\n+\n+        // scale the stream several times so that we get complex positions\n+        scale(controller, streamObj, config);\n+\n+        @Cleanup\n+        SynchronizerClientFactory syncClientFactory = SynchronizerClientFactory.withScope(scope, clientConfig);\n+\n+        String markStream = NameUtils.getMarkStreamForStream(stream);\n+        RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n+                new WatermarkSerializer(),\n+                SynchronizerConfig.builder().build());\n+\n+        LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n+        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+\n+        AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n+\n+        stopFlag.set(true);\n+\n+        writer1Future.join();\n+        writer2Future.join();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        controllerUri = URI.create(\"tcp://localhost:\" + controllerStarter.controllerPort);\n+        clientConfig = ClientConfig.builder().controllerURI(controllerUri).build();\n+        clientFactory = EventStreamClientFactory.withScope(scope, clientConfig);\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerUri)\n+                .build());\n+\n+        // read events from the stream\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(scope, clientConfig, connectionFactory);\n+\n+        Watermark watermark0 = watermarks.take();\n+        Watermark watermark1 = watermarks.take();\n+        assertTrue(watermark0.getLowerTimeBound() <= watermark0.getUpperTimeBound());\n+        assertTrue(watermark1.getLowerTimeBound() <= watermark1.getUpperTimeBound());\n+        assertTrue(watermark0.getLowerTimeBound() < watermark1.getLowerTimeBound());\n+\n+        Map<Segment, Long> positionMap0 = watermark0.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(scope, stream, x.getKey().getSegmentId()), Map.Entry::getValue));\n+        Map<Segment, Long> positionMap1 = watermark1.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(scope, stream, x.getKey().getSegmentId()), Map.Entry::getValue));\n+\n+        StreamCut streamCutFirst = new StreamCutImpl(streamObj, positionMap0);\n+        StreamCut streamCutSecond = new StreamCutImpl(streamObj, positionMap1);\n+        Map<Stream, StreamCut> firstMarkStreamCut = Collections.singletonMap(streamObj, streamCutFirst);\n+        Map<Stream, StreamCut> secondMarkStreamCut = Collections.singletonMap(streamObj, streamCutSecond);\n+\n+        // read from stream cut of first watermark\n+        String readerGroup = \"rgTx\";\n+        readerGroupManager.createReaderGroup(readerGroup, ReaderGroupConfig.builder().stream(streamObj)\n+                .startingStreamCuts(firstMarkStreamCut)\n+                .endingStreamCuts(secondMarkStreamCut)\n+                .build());\n+\n+        @Cleanup\n+        final EventStreamReader<Long> reader = clientFactory.createReader(\"myreaderTx\", readerGroup, javaSerializer,\n+                ReaderConfig.builder().build());\n+\n+        EventRead<Long> event = reader.readNextEvent(10000L);\n+        TimeWindow currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        while (event.getEvent() != null && currentTimeWindow.getLowerTimeBound() == null && currentTimeWindow.getUpperTimeBound() == null) {\n+            event = reader.readNextEvent(10000L);\n+            currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        }\n+\n+        assertNotNull(currentTimeWindow.getUpperTimeBound());\n+\n+        // read all events and verify that all events are below the bounds\n+        while (event.getEvent() != null) {\n+            Long time = event.getEvent();\n+            log.info(\"timewindow = {} event = {}\", currentTimeWindow, time);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || time >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || time <= currentTimeWindow.getUpperTimeBound());\n+\n+            TimeWindow nextTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || nextTimeWindow.getLowerTimeBound() >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || nextTimeWindow.getUpperTimeBound() >= currentTimeWindow.getUpperTimeBound());\n+            currentTimeWindow = nextTimeWindow;\n+\n+            event = reader.readNextEvent(10000L);\n+            if (event.isCheckpoint()) {\n+                event = reader.readNextEvent(10000L);\n+            }\n+        }\n+\n+        assertNotNull(currentTimeWindow.getLowerTimeBound());\n+    }\n+\n+    private void fetchWatermarks(RevisionedStreamClient<Watermark> watermarkReader, LinkedBlockingQueue<Watermark> watermarks,\n+                                 AtomicBoolean stop) {\n+        AtomicReference<Revision> revision = new AtomicReference<>(watermarkReader.fetchOldestRevision());\n+\n+        Futures.loop(() -> !stop.get(), () -> Futures.delayedTask(() -> {\n+            Iterator<Map.Entry<Revision, Watermark>> marks = watermarkReader.readFrom(revision.get());\n+            if (marks.hasNext()) {\n+                Map.Entry<Revision, Watermark> next = marks.next();\n+                log.info(\"watermark = {}\", next.getValue());\n+\n+                watermarks.add(next.getValue());\n+                revision.set(next.getKey());\n+            }\n+            return null;\n+        }, Duration.ofSeconds(10), executorService()), executorService());\n+    }\n+\n+    private CompletableFuture<Void> writeTxEvents(TransactionalEventStreamWriter<Long> writer, AtomicBoolean stopFlag) {\n+        AtomicInteger count = new AtomicInteger(0);\n+        return Futures.loop(() -> !stopFlag.get(), () -> Futures.delayedFuture(() -> {\n+            AtomicLong currentTime = new AtomicLong();\n+            Transaction<Long> txn = writer.beginTxn();\n+            return CompletableFuture.runAsync(() -> {\n+                try {\n+                    for (int i = 0; i < TOTAL_NUM_EVENTS; i++) {\n+                        currentTime.set(timer.incrementAndGet());\n+                        txn.writeEvent(count.toString(), currentTime.get());", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0NDI1Ng==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486044256", "bodyText": "It's sync.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzNzE3OA=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzODA0Nw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485038047", "bodyText": "Isn't the exception bubbling up going to have the same effect as your Assert.fail (i.e., stop the test)? You are suppressing this exception so you will never know the type of exception or its stack trace. My recommendation is to bubble it up.", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:10:17Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -444,6 +484,495 @@ public void testDurableDataLogFail() throws Exception {\n         log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersStall() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        @Cleanup\n+        ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        log.info(\"Writing events on to stream: {}\", STREAM1);\n+        writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        log.info(\"Writing events on to stream: {}\", STREAM2);\n+        writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientFactory, readerGroupManager, SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, NUM_EVENTS);\n+        Position p2 = readNEvents(reader2, NUM_EVENTS);\n+\n+        ReaderGroup readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerStarter.controllerURI).build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = readerGroupManager.getReaderGroup(testReaderGroup2);\n+        Assert.assertNotNull(readerGroup1);\n+        Assert.assertNotNull(readerGroup2);\n+\n+        reader1 = clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+        reader2 = clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(), ReaderConfig.builder().build());\n+\n+        // Read rest of the events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - NUM_EVENTS);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(5000).getEvent());\n+        assertNull(reader2.readNextEvent(5000).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int containerCount = 4;\n+        String scope = \"scopeTx\";\n+        String stream = \"streamTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        @Cleanup\n+        SegmentStoreStarter segmentStoreStarter = startSegmentStore(this.storageFactory, null, containerCount);\n+        @Cleanup\n+        ControllerStarter controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort,\n+                containerCount);\n+\n+        Controller controller = controllerStarter.controller;\n+        StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(ScalingPolicy.fixed(5)).build();\n+\n+        URI controllerUri = URI.create(\"tcp://localhost:\" + controllerStarter.controllerPort);\n+        StreamManager streamManager = StreamManager.create(controllerUri);\n+        streamManager.createScope(scope);\n+        streamManager.createStream(scope, stream, config);\n+\n+        Stream streamObj = Stream.of(scope, stream);\n+\n+        // create 2 writers\n+        ClientConfig clientConfig = ClientConfig.builder().controllerURI(controllerUri).build();\n+        @Cleanup\n+        EventStreamClientFactory clientFactory = EventStreamClientFactory.withScope(scope, clientConfig);\n+        JavaSerializer<Long> javaSerializer = new JavaSerializer<>();\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer1 = clientFactory\n+                .createTransactionalEventWriter(\"writer1\", stream, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(10000).build());\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer2 = clientFactory\n+                .createTransactionalEventWriter(\"writer2\", stream, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(10000).build());\n+\n+        AtomicBoolean stopFlag = new AtomicBoolean(false);\n+        // write events\n+        CompletableFuture<Void> writer1Future = writeTxEvents(writer1, stopFlag);\n+        CompletableFuture<Void> writer2Future = writeTxEvents(writer2, stopFlag);\n+\n+        // scale the stream several times so that we get complex positions\n+        scale(controller, streamObj, config);\n+\n+        @Cleanup\n+        SynchronizerClientFactory syncClientFactory = SynchronizerClientFactory.withScope(scope, clientConfig);\n+\n+        String markStream = NameUtils.getMarkStreamForStream(stream);\n+        RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n+                new WatermarkSerializer(),\n+                SynchronizerConfig.builder().build());\n+\n+        LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n+        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+\n+        AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n+\n+        stopFlag.set(true);\n+\n+        writer1Future.join();\n+        writer2Future.join();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = segmentStoreStarter.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), segmentStoreStarter.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        segmentStoreStarter.close(); // Shutdown SegmentStore\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bookKeeperStarter.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bookKeeperStarter = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bookKeeperStarter = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bookKeeperStarter.bkConfig.get(), this.bookKeeperStarter.zkClient.get(),\n+                executorService());\n+        this.dataLogFactory.initialize();\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+        // Use dataLogFactory from new BK instance.\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n+                executorService());\n+\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+\n+        // Create a debug segment container instances using a new dataLog and old storage.\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+\n+            // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            // Wait for metadata segment to be flushed to LTS\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+        // Start a new segment store and controller\n+        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        controllerUri = URI.create(\"tcp://localhost:\" + controllerStarter.controllerPort);\n+        clientConfig = ClientConfig.builder().controllerURI(controllerUri).build();\n+        clientFactory = EventStreamClientFactory.withScope(scope, clientConfig);\n+        @Cleanup\n+        ConnectionFactory connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n+                .controllerURI(controllerUri)\n+                .build());\n+\n+        // read events from the stream\n+        @Cleanup\n+        ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(scope, clientConfig, connectionFactory);\n+\n+        Watermark watermark0 = watermarks.take();\n+        Watermark watermark1 = watermarks.take();\n+        assertTrue(watermark0.getLowerTimeBound() <= watermark0.getUpperTimeBound());\n+        assertTrue(watermark1.getLowerTimeBound() <= watermark1.getUpperTimeBound());\n+        assertTrue(watermark0.getLowerTimeBound() < watermark1.getLowerTimeBound());\n+\n+        Map<Segment, Long> positionMap0 = watermark0.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(scope, stream, x.getKey().getSegmentId()), Map.Entry::getValue));\n+        Map<Segment, Long> positionMap1 = watermark1.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(scope, stream, x.getKey().getSegmentId()), Map.Entry::getValue));\n+\n+        StreamCut streamCutFirst = new StreamCutImpl(streamObj, positionMap0);\n+        StreamCut streamCutSecond = new StreamCutImpl(streamObj, positionMap1);\n+        Map<Stream, StreamCut> firstMarkStreamCut = Collections.singletonMap(streamObj, streamCutFirst);\n+        Map<Stream, StreamCut> secondMarkStreamCut = Collections.singletonMap(streamObj, streamCutSecond);\n+\n+        // read from stream cut of first watermark\n+        String readerGroup = \"rgTx\";\n+        readerGroupManager.createReaderGroup(readerGroup, ReaderGroupConfig.builder().stream(streamObj)\n+                .startingStreamCuts(firstMarkStreamCut)\n+                .endingStreamCuts(secondMarkStreamCut)\n+                .build());\n+\n+        @Cleanup\n+        final EventStreamReader<Long> reader = clientFactory.createReader(\"myreaderTx\", readerGroup, javaSerializer,\n+                ReaderConfig.builder().build());\n+\n+        EventRead<Long> event = reader.readNextEvent(10000L);\n+        TimeWindow currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        while (event.getEvent() != null && currentTimeWindow.getLowerTimeBound() == null && currentTimeWindow.getUpperTimeBound() == null) {\n+            event = reader.readNextEvent(10000L);\n+            currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        }\n+\n+        assertNotNull(currentTimeWindow.getUpperTimeBound());\n+\n+        // read all events and verify that all events are below the bounds\n+        while (event.getEvent() != null) {\n+            Long time = event.getEvent();\n+            log.info(\"timewindow = {} event = {}\", currentTimeWindow, time);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || time >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || time <= currentTimeWindow.getUpperTimeBound());\n+\n+            TimeWindow nextTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || nextTimeWindow.getLowerTimeBound() >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || nextTimeWindow.getUpperTimeBound() >= currentTimeWindow.getUpperTimeBound());\n+            currentTimeWindow = nextTimeWindow;\n+\n+            event = reader.readNextEvent(10000L);\n+            if (event.isCheckpoint()) {\n+                event = reader.readNextEvent(10000L);\n+            }\n+        }\n+\n+        assertNotNull(currentTimeWindow.getLowerTimeBound());\n+    }\n+\n+    private void fetchWatermarks(RevisionedStreamClient<Watermark> watermarkReader, LinkedBlockingQueue<Watermark> watermarks,\n+                                 AtomicBoolean stop) {\n+        AtomicReference<Revision> revision = new AtomicReference<>(watermarkReader.fetchOldestRevision());\n+\n+        Futures.loop(() -> !stop.get(), () -> Futures.delayedTask(() -> {\n+            Iterator<Map.Entry<Revision, Watermark>> marks = watermarkReader.readFrom(revision.get());\n+            if (marks.hasNext()) {\n+                Map.Entry<Revision, Watermark> next = marks.next();\n+                log.info(\"watermark = {}\", next.getValue());\n+\n+                watermarks.add(next.getValue());\n+                revision.set(next.getKey());\n+            }\n+            return null;\n+        }, Duration.ofSeconds(10), executorService()), executorService());\n+    }\n+\n+    private CompletableFuture<Void> writeTxEvents(TransactionalEventStreamWriter<Long> writer, AtomicBoolean stopFlag) {\n+        AtomicInteger count = new AtomicInteger(0);\n+        return Futures.loop(() -> !stopFlag.get(), () -> Futures.delayedFuture(() -> {\n+            AtomicLong currentTime = new AtomicLong();\n+            Transaction<Long> txn = writer.beginTxn();\n+            return CompletableFuture.runAsync(() -> {\n+                try {\n+                    for (int i = 0; i < TOTAL_NUM_EVENTS; i++) {\n+                        currentTime.set(timer.incrementAndGet());\n+                        txn.writeEvent(count.toString(), currentTime.get());\n+                    }\n+                    txn.commit(currentTime.get());\n+                } catch (TxnFailedException e) {\n+                    throw new CompletionException(e);\n+                }\n+            });\n+        }, 1000L, executorService()), executorService());\n+    }\n+\n+    private void scale(Controller controller, Stream streamObj, StreamConfiguration configuration) {\n+        // perform several scales\n+        int numOfSegments = configuration.getScalingPolicy().getMinNumSegments();\n+        double delta = 1.0 / numOfSegments;\n+        for (long segmentNumber = 0; segmentNumber < numOfSegments - 1; segmentNumber++) {\n+            double rangeLow = segmentNumber * delta;\n+            double rangeHigh = (segmentNumber + 1) * delta;\n+            double rangeMid = (rangeHigh + rangeLow) / 2;\n+\n+            Map<Double, Double> map = new HashMap<>();\n+            map.put(rangeLow, rangeMid);\n+            map.put(rangeMid, rangeHigh);\n+            controller.scaleStream(streamObj, Collections.singletonList(segmentNumber), map, executorService()).getFuture().join();\n+        }\n+    }\n+\n+    private EventStreamReader<String> createReader(ClientFactoryImpl clientFactory,\n+                                                   ReaderGroupManager readerGroupManager, String scope, String stream,\n+                                                   String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(scope, stream))\n+                        .automaticCheckpointIntervalMillis(2000)\n+                        .build());\n+\n+        return clientFactory.createReader(readerName,\n+                readerGroupName,\n+                new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+    }\n+\n+    private Position readNEvents(EventStreamReader<String> reader, int num) {\n+        Position position = null;\n+        for (int q = 0; q < num;) {\n+            try {\n+                EventRead<String> eventRead = reader.readNextEvent(SECONDS.toMillis(5000));\n+                Assert.assertEquals(\"Event written and read back don't match\", EVENT, eventRead.getEvent());\n+                q++;\n+                position = eventRead.getPosition();\n+            } catch (Exception e) {", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0NDMzMg==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486044332", "bodyText": "Letting them bubble up.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:41:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzODA0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -428,60 +470,102 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         // Create the environment for DebugSegmentContainer.\n         @Cleanup\n         DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n-        // Use dataLogFactory from new BK instance.\n-        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, this.dataLogFactory,\n-                executorService());\n \n-        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n-        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n-\n-        // Create a debug segment container instance using a new dataLog and old storage.\n-        DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n-                DebugStreamSegmentContainerTests.MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n-                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, this.storageFactory,\n-                context.getDefaultExtensions(), executorService());\n-\n-        Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.put(CONTAINER_ID, debugStreamSegmentContainer);\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n \n         // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n-        ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, CONTAINER_ID).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n \n         // List segments from storage and recover them using debug segment container instance.\n         ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n \n-        // Wait for metadata segment to be flushed to LTS\n-        String metadataSegmentName = NameUtils.getMetadataSegmentName(CONTAINER_ID);\n-        waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainer, storage)\n-                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        log.info(\"Long term storage has been update with a new container metadata segment.\");\n-\n-        // Stop the debug segment container\n-        this.dataLogFactory.close();\n-        Services.stopAsync(debugStreamSegmentContainerMap.get(CONTAINER_ID), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n-        debugStreamSegmentContainerMap.get(CONTAINER_ID).close();\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n         log.info(\"Segments have been recovered.\");\n \n+        this.dataLogFactory.close();\n         // Start a new segment store and controller\n-        segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory, containerCount);\n-        controllerStarter = startController(this.bookKeeperStarter.bkPort, segmentStoreStarter.servicePort, containerCount);\n+        segmentStoreRunner = new SegmentStoreRunner(this.storageFactory, this.dataLogFactory, containerCount);\n+        controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, segmentStoreRunner.servicePort, containerCount);\n         log.info(\"Started segment store and controller again.\");\n \n-        connectionFactory = new SocketConnectionFactoryImpl(ClientConfig.builder()\n-                .controllerURI(controllerStarter.controllerURI).build());\n-        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n-        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory);\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(controllerRunner);\n \n         // Try creating the same segments again with the new controller\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n-        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerRunner.controller, SCOPE, STREAM2);\n \n-        // Try reading all events again\n+        // Try reading all events again to verify that the recovery was successful.\n+        readEventsFromStreams(clientRunner.clientFactory, clientRunner.readerGroupManager);\n+        log.info(\"Read all events again to verify that segments were recovered.\");\n+    }\n+\n+    // Creates debug segment container instances, puts them in a map and returns it.\n+    private Map<Integer, DebugStreamSegmentContainer> startDebugSegmentContainers(DebugStreamSegmentContainerTests.TestContext\n+                                                                                          context, int containerCount,\n+                                                                                  BookKeeperLogFactory dataLogFactory,\n+                                                                                  StorageFactory storageFactory) throws Exception {\n+        // Start a debug segment container corresponding to the given container Id and put it in the Hashmap with the Id.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = new HashMap<>();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DURABLE_LOG_CONFIG, dataLogFactory, executorService());\n+\n+        // Create a debug segment container instances using a\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            DebugStreamSegmentContainerTests.MetadataCleanupContainer debugStreamSegmentContainer = new\n+                    DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, storageFactory,\n+                    context.getDefaultExtensions(), executorService());\n+\n+            Services.startAsync(debugStreamSegmentContainer, executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.put(containerId, debugStreamSegmentContainer);\n+        }\n+        return debugStreamSegmentContainerMap;\n+    }\n+\n+    // Closes the debug segment container instances in the given map after waiting for the metadata segment to be flushed to\n+    // the given storage.\n+    private void stopDebugSegmentContainersPostFlush(int containerCount, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap,\n+                                                     Storage storage) throws Exception {\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+            waitForSegmentsInStorage(Collections.singleton(metadataSegmentName), debugStreamSegmentContainerMap.get(containerId),\n+                    storage)\n+                    .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Long term storage has been update with a new container metadata segment.\");\n+\n+            // Stop the debug segment container\n+            Services.stopAsync(debugStreamSegmentContainerMap.get(containerId), executorService()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            debugStreamSegmentContainerMap.get(containerId).close();\n+        }\n+    }\n+\n+    // Writes events to the streams with/without transactions.\n+    private void writeEventsToStreams(ClientFactoryImpl clientFactory, boolean withTransaction)\n+            throws TxnFailedException {\n+        if (withTransaction) {\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM1);\n+            writeTransactionalEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing transactional events on to stream: {}\", STREAM2);\n+            writeTransactionalEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        } else {\n+            log.info(\"Writing events on to stream: {}\", STREAM1);\n+            writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+            log.info(\"Writing events on to stream: {}\", STREAM2);\n+            writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+        }\n+    }\n+\n+    // Reads all events from the streams.\n+    private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager) {\n         readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n         readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n-        log.info(\"Read all events again to verify that segments were recovered.\");\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzODI0Nw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r485038247", "bodyText": "are these async or sync calls?", "author": "andreipaduroiu", "createdAt": "2020-09-08T16:10:35Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -469,6 +998,21 @@ private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n         writer.close();\n     }\n \n+    private void writeTransactionalEvents(String streamName, ClientFactoryImpl clientFactory) throws TxnFailedException {\n+        EventWriterConfig writerConfig = EventWriterConfig.builder().transactionTimeoutTime(10000).build();\n+        @Cleanup\n+        TransactionalEventStreamWriter<String> txnWriter = clientFactory.createTransactionalEventWriter(streamName, new UTF8StringSerializer(),\n+                writerConfig);\n+\n+        Transaction<String> transaction = txnWriter.beginTxn();\n+        for (int i = 0; i < TOTAL_NUM_EVENTS; i++) {\n+            transaction.writeEvent(\"0\", EVENT);", "originalCommit": "0e56dedb83759a3395ec1e63ad240e9e005be193", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA0NDM1Ng==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r486044356", "bodyText": "sync.", "author": "ManishKumarKeshri", "createdAt": "2020-09-10T03:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAzODI0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "1182fbe37438095d5d01dcd4285c39d771cd5442", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex e9b1229fc6..6132d9a14e 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -998,8 +1084,9 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         writer.close();\n     }\n \n+    // Writes the required number of events to the given stream with using transactions.\n     private void writeTransactionalEvents(String streamName, ClientFactoryImpl clientFactory) throws TxnFailedException {\n-        EventWriterConfig writerConfig = EventWriterConfig.builder().transactionTimeoutTime(10000).build();\n+        EventWriterConfig writerConfig = EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build();\n         @Cleanup\n         TransactionalEventStreamWriter<String> txnWriter = clientFactory.createTransactionalEventWriter(streamName, new UTF8StringSerializer(),\n                 writerConfig);\n"}}, {"oid": "1182fbe37438095d5d01dcd4285c39d771cd5442", "url": "https://github.com/pravega/pravega/commit/1182fbe37438095d5d01dcd4285c39d771cd5442", "message": "Merge branch 'feature-4938-dr-tools-base-case' into issue-5095-Additional-integration-tests-for-Data-Recovery-Readers-stall", "committedDate": "2020-09-09T02:12:24Z", "type": "commit"}, {"oid": "15b26d6d1506e7c2426e77d8070da9dea6a8f23f", "url": "https://github.com/pravega/pravega/commit/15b26d6d1506e7c2426e77d8070da9dea6a8f23f", "message": "Taking the changes from last PR.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-09T04:22:37Z", "type": "commit"}, {"oid": "2853db0b4a468ddf7be23d56bba65c4867bb46a6", "url": "https://github.com/pravega/pravega/commit/2853db0b4a468ddf7be23d56bba65c4867bb46a6", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-10T03:32:55Z", "type": "commit"}, {"oid": "cbfdf6fd94cf50d72f98f12d2bbb78e815800bfc", "url": "https://github.com/pravega/pravega/commit/cbfdf6fd94cf50d72f98f12d2bbb78e815800bfc", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-11T06:16:31Z", "type": "commit"}, {"oid": "e59000d1121d8b6bbbac13e3e1f146988265b4fe", "url": "https://github.com/pravega/pravega/commit/e59000d1121d8b6bbbac13e3e1f146988265b4fe", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-14T20:12:25Z", "type": "commit"}, {"oid": "a33742f28b38197f53bed9b0c01133cf937d1089", "url": "https://github.com/pravega/pravega/commit/a33742f28b38197f53bed9b0c01133cf937d1089", "message": "Ignore.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-15T07:23:56Z", "type": "commit"}, {"oid": "f08bd52ff625239541f68aa1bbb7f409b451ef8f", "url": "https://github.com/pravega/pravega/commit/f08bd52ff625239541f68aa1bbb7f409b451ef8f", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-17T10:23:17Z", "type": "commit"}, {"oid": "db677f6cb8d8e4dd012bf4e3a6216ffe2eaacf26", "url": "https://github.com/pravega/pravega/commit/db677f6cb8d8e4dd012bf4e3a6216ffe2eaacf26", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-17T10:26:30Z", "type": "commit"}, {"oid": "0d9e01b34ba5a69289b56e65e407919817ca5afc", "url": "https://github.com/pravega/pravega/commit/0d9e01b34ba5a69289b56e65e407919817ca5afc", "message": "Refactoring.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-18T10:34:46Z", "type": "commit"}, {"oid": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "url": "https://github.com/pravega/pravega/commit/78aa8dd649428fb2c91b46361a93d253e52af0d6", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-18T11:01:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NDk1Nw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495184957", "bodyText": "Remove this line", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:18:01Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -298,6 +323,7 @@ public void close() throws Exception {\n     }\n \n     /**\n+", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTQzMQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495695431", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NDk1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -316,6 +318,7 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n             this.controller = controllerWrapper.getController();\n         }\n \n+        @Override\n         public void close() throws Exception {\n             this.controller.close();\n             this.controllerWrapper.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTUxMQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495185511", "bodyText": "implements AutoCloseable", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:19:15Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -319,6 +345,37 @@ public void close() {\n         }\n     }\n \n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner {", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTQ4OQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495695489", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:25:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTUxMQ=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -338,6 +340,7 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n             this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n         }\n \n+        @Override\n         public void close() {\n             this.readerGroupManager.close();\n             this.clientFactory.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTYzMA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495185630", "bodyText": "This one is only set in the constructor. Make it final.", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:19:37Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -319,6 +345,37 @@ public void close() {\n         }\n     }\n \n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner {\n+        private final int containerCount;\n+        private BookKeeperRunner bookKeeperRunner;", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTc5Mg==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495695792", "bodyText": "It is set in one more place within the test to restart bk/zk.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:26:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTYzMA=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -338,6 +340,7 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n             this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n         }\n \n+        @Override\n         public void close() {\n             this.readerGroupManager.close();\n             this.clientFactory.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc5Mw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495185793", "bodyText": "These last 2 lines are the same as in restart... below. Replace them with an invocation of that method.", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:19:59Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -319,6 +345,37 @@ public void close() {\n         }\n     }\n \n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner {\n+        private final int containerCount;\n+        private BookKeeperRunner bookKeeperRunner;\n+        private SegmentStoreRunner segmentStoreRunner;\n+        private ControllerRunner controllerRunner;\n+\n+        PravegaRunner(int instanceId, int bookieCount, int containerCount, StorageFactory storageFactory) throws Exception {\n+            this.containerCount = containerCount;\n+            this.bookKeeperRunner = new BookKeeperRunner(instanceId, bookieCount);\n+            this.segmentStoreRunner = new SegmentStoreRunner(storageFactory, null, containerCount);", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTg0MA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495695840", "bodyText": "Done.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:26:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -338,6 +340,7 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n             this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n         }\n \n+        @Override\n         public void close() {\n             this.readerGroupManager.close();\n             this.clientFactory.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjA0MQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495186041", "bodyText": "You'll need to add an @Override tag once you implement AutoCloseable", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:20:30Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -319,6 +345,37 @@ public void close() {\n         }\n     }\n \n+\n+    /**\n+     * Creates a Pravega instance.\n+     */\n+    private static class PravegaRunner {\n+        private final int containerCount;\n+        private BookKeeperRunner bookKeeperRunner;\n+        private SegmentStoreRunner segmentStoreRunner;\n+        private ControllerRunner controllerRunner;\n+\n+        PravegaRunner(int instanceId, int bookieCount, int containerCount, StorageFactory storageFactory) throws Exception {\n+            this.containerCount = containerCount;\n+            this.bookKeeperRunner = new BookKeeperRunner(instanceId, bookieCount);\n+            this.segmentStoreRunner = new SegmentStoreRunner(storageFactory, null, containerCount);\n+            this.controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, this.segmentStoreRunner.servicePort,\n+                    containerCount);\n+        }\n+\n+        public void restartControllerAndSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory)\n+                throws DurableDataLogException, InterruptedException {\n+            this.segmentStoreRunner = new SegmentStoreRunner(storageFactory, dataLogFactory, this.containerCount);\n+            this.controllerRunner = new ControllerRunner(this.bookKeeperRunner.bkPort, this.segmentStoreRunner.servicePort, containerCount);\n+        }\n+\n+        public void close() throws Exception {", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NjI0MQ==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495696241", "bodyText": "Fixed.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:28:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjA0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -338,6 +340,7 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n             this.readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerRunner.controller, clientFactory);\n         }\n \n+        @Override\n         public void close() {\n             this.readerGroupManager.close();\n             this.clientFactory.close();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjY3Mg==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495186672", "bodyText": "Why do you re-wrap a list as a new list?", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:21:56Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -546,6 +582,428 @@ private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupM\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersPaused() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        int eventsReadCount = RANDOM.nextInt(TOTAL_NUM_EVENTS);\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Write events to the streams.\n+        writeEventsToStreams(clientRunner.clientFactory, true);\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, eventsReadCount);\n+        Position p2 = readNEvents(reader2, eventsReadCount);\n+\n+        ReaderGroup readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        clientRunner.close();\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+        assertNotNull(readerGroup1);\n+        assertNotNull(readerGroup2);\n+\n+        reader1 = clientRunner.clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+        reader2 = clientRunner.clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        // Read the remaining number of events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - eventsReadCount);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - eventsReadCount);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        assertNull(reader2.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    private void runRecovery(int containerCount, Storage storage) throws Exception {\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to a segment with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        String readerGroup = \"rgTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create a scope and a stream\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Create two writers\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer1 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer1\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer2 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer2\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+\n+        AtomicBoolean stopFlag = new AtomicBoolean(false);\n+        // write events\n+        CompletableFuture<Void> writer1Future = writeTxEvents(writer1, stopFlag);\n+        CompletableFuture<Void> writer2Future = writeTxEvents(writer2, stopFlag);\n+\n+        // scale the stream several times so that we get complex positions\n+        StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(ScalingPolicy.fixed(5)).build();\n+        Stream streamObj = Stream.of(SCOPE, STREAM1);\n+        scale(pravegaRunner.controllerRunner.controller, streamObj, config);\n+\n+        // get watermarks\n+        LinkedBlockingQueue<Watermark> watermarks = getWatermarks(pravegaRunner, stopFlag, writer1Future, writer2Future);\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // read events and verify\n+        readVerifyEventsWithWatermarks(readerGroup, clientRunner, streamObj, watermarks);\n+    }\n+\n+    /**\n+     * Creates reader and verifies watermarking by verifying the time bounds for events.\n+     */\n+    private void readVerifyEventsWithWatermarks(String readerGroup, ClientRunner clientRunner, Stream streamObj,\n+                                                LinkedBlockingQueue<Watermark> watermarks) throws InterruptedException {\n+        List<Map<Stream, StreamCut>> streamCuts = getStreamCutsFromWaterMarks(streamObj, watermarks);\n+        // read from stream cut of first watermark\n+        clientRunner.readerGroupManager.createReaderGroup(readerGroup, ReaderGroupConfig.builder().stream(streamObj)\n+                .startingStreamCuts(streamCuts.get(0))\n+                .endingStreamCuts(streamCuts.get(1))\n+                .build());\n+        @Cleanup\n+        final EventStreamReader<Long> reader = clientRunner.clientFactory.createReader(\"myreaderTx\", readerGroup,\n+                new JavaSerializer<>(), ReaderConfig.builder().build());\n+\n+        EventRead<Long> event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+        TimeWindow currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        while (event.getEvent() != null && currentTimeWindow.getLowerTimeBound() == null && currentTimeWindow.getUpperTimeBound() == null) {\n+            event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        }\n+\n+        assertNotNull(currentTimeWindow.getUpperTimeBound());\n+\n+        currentTimeWindow = verifyEventsWithTimeBounds(streamObj, reader, event, currentTimeWindow);\n+\n+        assertNotNull(currentTimeWindow.getLowerTimeBound());\n+    }\n+\n+    /**\n+     * Gets watermarks used while writing the events\n+     */\n+    private LinkedBlockingQueue<Watermark> getWatermarks(PravegaRunner pravegaRunner, AtomicBoolean stopFlag,\n+                                                         CompletableFuture<Void> writer1Future, CompletableFuture<Void>\n+                                                                 writer2Future) throws Exception {\n+        @Cleanup\n+        SynchronizerClientFactory syncClientFactory = SynchronizerClientFactory.withScope(SCOPE,\n+                ClientConfig.builder().controllerURI(pravegaRunner.controllerRunner.controllerURI).build());\n+\n+        String markStream = NameUtils.getMarkStreamForStream(STREAM1);\n+        RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n+                new WatermarkSerializer(), SynchronizerConfig.builder().build());\n+        LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n+        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n+        stopFlag.set(true);\n+        writer1Future.join();\n+        writer2Future.join();\n+        return watermarks;\n+    }\n+\n+    private List<Map<Stream, StreamCut>> getStreamCutsFromWaterMarks(Stream streamObj, LinkedBlockingQueue<Watermark> watermarks)\n+            throws InterruptedException {\n+        Watermark watermark0 = watermarks.take();\n+        Watermark watermark1 = watermarks.take();\n+        assertTrue(watermark0.getLowerTimeBound() <= watermark0.getUpperTimeBound());\n+        assertTrue(watermark1.getLowerTimeBound() <= watermark1.getUpperTimeBound());\n+        assertTrue(watermark0.getLowerTimeBound() < watermark1.getLowerTimeBound());\n+\n+        Map<Segment, Long> positionMap0 = watermark0.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+        Map<Segment, Long> positionMap1 = watermark1.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+\n+        StreamCut streamCutFirst = new StreamCutImpl(streamObj, positionMap0);\n+        StreamCut streamCutSecond = new StreamCutImpl(streamObj, positionMap1);\n+        Map<Stream, StreamCut> firstMarkStreamCut = Collections.singletonMap(streamObj, streamCutFirst);\n+        Map<Stream, StreamCut> secondMarkStreamCut = Collections.singletonMap(streamObj, streamCutSecond);\n+\n+        return new ArrayList<Map<Stream, StreamCut>>(Arrays.asList(firstMarkStreamCut, secondMarkStreamCut));", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcxNTg3Ng==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495715876", "bodyText": "Fixed.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T06:33:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjY3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -872,9 +871,10 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n                 new WatermarkSerializer(), SynchronizerConfig.builder().build());\n         LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n-        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        CompletableFuture<Void> fetchWaterMarksFuture = fetchWatermarks(watermarkReader, watermarks, stopFlag);\n         AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n         stopFlag.set(true);\n+        fetchWaterMarksFuture.join();\n         writer1Future.join();\n         writer2Future.join();\n         return watermarks;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTcwNA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495189704", "bodyText": "One comment here. I see you stop this loop by setting stop too true and then you immediately move on to the next thing in your test.\nYou never wait on this Future  loop to complete. You may set stop to true, but if it's currently waiting (for 10s), it will still execute the subsequent code eventually which could result in your watermarks collection to be modified. That will most likely cause issues.\nConsider rewriting this so that this method:\n\nReturns the Futures.loop\nThe callsite holds it in a variable and waits for it to end.\nReducing the delay between iterations to 1s. That will prevent this from executing long after your test is done.\nIn the callback (line 933), bail out if stop is false.", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:28:41Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -546,6 +582,428 @@ private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupM\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersPaused() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        int eventsReadCount = RANDOM.nextInt(TOTAL_NUM_EVENTS);\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Write events to the streams.\n+        writeEventsToStreams(clientRunner.clientFactory, true);\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, eventsReadCount);\n+        Position p2 = readNEvents(reader2, eventsReadCount);\n+\n+        ReaderGroup readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        clientRunner.close();\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+        assertNotNull(readerGroup1);\n+        assertNotNull(readerGroup2);\n+\n+        reader1 = clientRunner.clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+        reader2 = clientRunner.clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        // Read the remaining number of events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - eventsReadCount);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - eventsReadCount);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        assertNull(reader2.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    private void runRecovery(int containerCount, Storage storage) throws Exception {\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to a segment with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        String readerGroup = \"rgTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create a scope and a stream\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Create two writers\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer1 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer1\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer2 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer2\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+\n+        AtomicBoolean stopFlag = new AtomicBoolean(false);\n+        // write events\n+        CompletableFuture<Void> writer1Future = writeTxEvents(writer1, stopFlag);\n+        CompletableFuture<Void> writer2Future = writeTxEvents(writer2, stopFlag);\n+\n+        // scale the stream several times so that we get complex positions\n+        StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(ScalingPolicy.fixed(5)).build();\n+        Stream streamObj = Stream.of(SCOPE, STREAM1);\n+        scale(pravegaRunner.controllerRunner.controller, streamObj, config);\n+\n+        // get watermarks\n+        LinkedBlockingQueue<Watermark> watermarks = getWatermarks(pravegaRunner, stopFlag, writer1Future, writer2Future);\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // read events and verify\n+        readVerifyEventsWithWatermarks(readerGroup, clientRunner, streamObj, watermarks);\n+    }\n+\n+    /**\n+     * Creates reader and verifies watermarking by verifying the time bounds for events.\n+     */\n+    private void readVerifyEventsWithWatermarks(String readerGroup, ClientRunner clientRunner, Stream streamObj,\n+                                                LinkedBlockingQueue<Watermark> watermarks) throws InterruptedException {\n+        List<Map<Stream, StreamCut>> streamCuts = getStreamCutsFromWaterMarks(streamObj, watermarks);\n+        // read from stream cut of first watermark\n+        clientRunner.readerGroupManager.createReaderGroup(readerGroup, ReaderGroupConfig.builder().stream(streamObj)\n+                .startingStreamCuts(streamCuts.get(0))\n+                .endingStreamCuts(streamCuts.get(1))\n+                .build());\n+        @Cleanup\n+        final EventStreamReader<Long> reader = clientRunner.clientFactory.createReader(\"myreaderTx\", readerGroup,\n+                new JavaSerializer<>(), ReaderConfig.builder().build());\n+\n+        EventRead<Long> event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+        TimeWindow currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        while (event.getEvent() != null && currentTimeWindow.getLowerTimeBound() == null && currentTimeWindow.getUpperTimeBound() == null) {\n+            event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        }\n+\n+        assertNotNull(currentTimeWindow.getUpperTimeBound());\n+\n+        currentTimeWindow = verifyEventsWithTimeBounds(streamObj, reader, event, currentTimeWindow);\n+\n+        assertNotNull(currentTimeWindow.getLowerTimeBound());\n+    }\n+\n+    /**\n+     * Gets watermarks used while writing the events\n+     */\n+    private LinkedBlockingQueue<Watermark> getWatermarks(PravegaRunner pravegaRunner, AtomicBoolean stopFlag,\n+                                                         CompletableFuture<Void> writer1Future, CompletableFuture<Void>\n+                                                                 writer2Future) throws Exception {\n+        @Cleanup\n+        SynchronizerClientFactory syncClientFactory = SynchronizerClientFactory.withScope(SCOPE,\n+                ClientConfig.builder().controllerURI(pravegaRunner.controllerRunner.controllerURI).build());\n+\n+        String markStream = NameUtils.getMarkStreamForStream(STREAM1);\n+        RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n+                new WatermarkSerializer(), SynchronizerConfig.builder().build());\n+        LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n+        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n+        stopFlag.set(true);\n+        writer1Future.join();\n+        writer2Future.join();\n+        return watermarks;\n+    }\n+\n+    private List<Map<Stream, StreamCut>> getStreamCutsFromWaterMarks(Stream streamObj, LinkedBlockingQueue<Watermark> watermarks)\n+            throws InterruptedException {\n+        Watermark watermark0 = watermarks.take();\n+        Watermark watermark1 = watermarks.take();\n+        assertTrue(watermark0.getLowerTimeBound() <= watermark0.getUpperTimeBound());\n+        assertTrue(watermark1.getLowerTimeBound() <= watermark1.getUpperTimeBound());\n+        assertTrue(watermark0.getLowerTimeBound() < watermark1.getLowerTimeBound());\n+\n+        Map<Segment, Long> positionMap0 = watermark0.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+        Map<Segment, Long> positionMap1 = watermark1.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+\n+        StreamCut streamCutFirst = new StreamCutImpl(streamObj, positionMap0);\n+        StreamCut streamCutSecond = new StreamCutImpl(streamObj, positionMap1);\n+        Map<Stream, StreamCut> firstMarkStreamCut = Collections.singletonMap(streamObj, streamCutFirst);\n+        Map<Stream, StreamCut> secondMarkStreamCut = Collections.singletonMap(streamObj, streamCutSecond);\n+\n+        return new ArrayList<Map<Stream, StreamCut>>(Arrays.asList(firstMarkStreamCut, secondMarkStreamCut));\n+    }\n+\n+    private TimeWindow verifyEventsWithTimeBounds(Stream streamObj, EventStreamReader<Long> reader, EventRead<Long> event, TimeWindow currentTimeWindow) {\n+        // read all events and verify that all events are below the bounds\n+        while (event.getEvent() != null) {\n+            Long time = event.getEvent();\n+            log.info(\"timewindow = {} event = {}\", currentTimeWindow, time);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || time >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || time <= currentTimeWindow.getUpperTimeBound());\n+\n+            TimeWindow nextTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || nextTimeWindow.getLowerTimeBound() >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || nextTimeWindow.getUpperTimeBound() >= currentTimeWindow.getUpperTimeBound());\n+            currentTimeWindow = nextTimeWindow;\n+\n+            event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            if (event.isCheckpoint()) {\n+                event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            }\n+        }\n+        return currentTimeWindow;\n+    }\n+\n+    /**\n+     * Adds water marks to the watermarks queue.\n+     */\n+    private void fetchWatermarks(RevisionedStreamClient<Watermark> watermarkReader, LinkedBlockingQueue<Watermark> watermarks,\n+                                 AtomicBoolean stop) {\n+        AtomicReference<Revision> revision = new AtomicReference<>(watermarkReader.fetchOldestRevision());\n+\n+        Futures.loop(() -> !stop.get(), () -> Futures.delayedTask(() -> {", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NzY0Nw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495697647", "bodyText": "Returned Futures.loop\nThe callsite holds it in a variable and waits for it to end.\nReduced the delay to 1s.\nI didn't get this one.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTcwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5NTI3Mg==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r496095272", "bodyText": "For #4, if you set stop to true, but are still waiting for that delayed future, the callback will still execute, even though stop is true. Ideally it shouldn't.", "author": "andreipaduroiu", "createdAt": "2020-09-28T16:52:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTcwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE3MzQxMA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r496173410", "bodyText": "Exiting from delayed future if stop is set to true.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T19:10:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTcwNA=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -872,9 +871,10 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n                 new WatermarkSerializer(), SynchronizerConfig.builder().build());\n         LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n-        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        CompletableFuture<Void> fetchWaterMarksFuture = fetchWatermarks(watermarkReader, watermarks, stopFlag);\n         AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n         stopFlag.set(true);\n+        fetchWaterMarksFuture.join();\n         writer1Future.join();\n         writer2Future.join();\n         return watermarks;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTgzNA==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495189834", "bodyText": "Same comments here as above.", "author": "andreipaduroiu", "createdAt": "2020-09-25T19:28:59Z", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -546,6 +582,428 @@ private void readEventsFromStreams(ClientFactoryImpl clientFactory, ReaderGroupM\n                 \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n     }\n \n+    /**\n+     * Tests the data recovery scenario with readers stalling while reading. Readers read some events and then they are\n+     * stopped. Durable data log is erased and restored. It's validated that readers are able to read rest of the unread\n+     * events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to two different segments.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Let readers read N number of events.\n+     *  5. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  6. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  7. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  8. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  9. Starts segment store and controller.\n+     *  10. Let readers read rest of the 300-N number of events.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryReadersPaused() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        int eventsReadCount = RANDOM.nextInt(TOTAL_NUM_EVENTS);\n+        String testReader1 = \"readerDRIntegrationTest1\";\n+        String testReader2 = \"readerDRIntegrationTest2\";\n+        String testReaderGroup1 = \"readerGroupDRIntegrationTest1\";\n+        String testReaderGroup2 = \"readerGroupDRIntegrationTest2\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+        log.info(\"Created two streams.\");\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Write events to the streams.\n+        writeEventsToStreams(clientRunner.clientFactory, true);\n+\n+        // Create two readers for reading both the streams.\n+        EventStreamReader<String> reader1 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM1, testReaderGroup1, testReader1);\n+        EventStreamReader<String> reader2 = createReader(clientRunner.clientFactory, clientRunner.readerGroupManager,\n+                SCOPE, STREAM2, testReaderGroup2, testReader2);\n+\n+        // Let readers read N number of events and mark their positions.\n+        Position p1 = readNEvents(reader1, eventsReadCount);\n+        Position p2 = readNEvents(reader2, eventsReadCount);\n+\n+        ReaderGroup readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        ReaderGroup readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+\n+        readerGroup1.readerOffline(testReader1, p1);\n+        readerGroup2.readerOffline(testReader2, p2);\n+\n+        clientRunner.close();\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM2);\n+\n+        // Get reader group.\n+        readerGroup1 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup1);\n+        readerGroup2 = clientRunner.readerGroupManager.getReaderGroup(testReaderGroup2);\n+        assertNotNull(readerGroup1);\n+        assertNotNull(readerGroup2);\n+\n+        reader1 = clientRunner.clientFactory.createReader(testReader1, testReaderGroup1, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+        reader2 = clientRunner.clientFactory.createReader(testReader2, testReaderGroup2, new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        // Read the remaining number of events.\n+        readNEvents(reader1, TOTAL_NUM_EVENTS - eventsReadCount);\n+        readNEvents(reader2, TOTAL_NUM_EVENTS - eventsReadCount);\n+\n+        // Reading next event should return null.\n+        assertNull(reader1.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        assertNull(reader2.readNextEvent(READ_TIMEOUT.toMillis()).getEvent());\n+        reader1.close();\n+        reader2.close();\n+    }\n+\n+    private void runRecovery(int containerCount, Storage storage) throws Exception {\n+        // Create the environment for DebugSegmentContainer.\n+        @Cleanup\n+        DebugStreamSegmentContainerTests.TestContext context = DebugStreamSegmentContainerTests.createContext(executorService());\n+\n+        // create debug segment container instances using new new dataLog and old storage.\n+        Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerMap = startDebugSegmentContainers(context,\n+                containerCount, this.dataLogFactory, this.storageFactory);\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            ContainerRecoveryUtils.deleteMetadataAndAttributeSegments(storage, containerId).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+        }\n+\n+        // List segments from storage and recover them using debug segment container instance.\n+        ContainerRecoveryUtils.recoverAllSegments(storage, debugStreamSegmentContainerMap, executorService());\n+\n+        // Waits for metadata segments to be flushed to LTS and then stops the debug segment containers\n+        stopDebugSegmentContainersPostFlush(containerCount, debugStreamSegmentContainerMap, storage);\n+        log.info(\"Segments have been recovered.\");\n+\n+        this.dataLogFactory.close();\n+    }\n+\n+    /**\n+     * Tests the data recovery scenario with watermarking events.\n+     *  What test does, step by step:\n+     *  1. Starts Pravega locally with just 4 segment containers.\n+     *  2. Writes 300 events to a segment with watermarks.\n+     *  3. Waits for all segments created to be flushed to the long term storage.\n+     *  4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+     *  5. Deletes container metadata segment and its attribute segment from the old LTS.\n+     *  6. Starts 4 debug segment containers using a new bookeeper/zookeeper and the old LTS.\n+     *  7. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+     *  8. Starts segment store and controller.\n+     *  9. Read all events and verify that all events are below the bounds.\n+     * @throws Exception    In case of an exception occurred while execution.\n+     */\n+    @Test(timeout = 180000)\n+    public void testDurableDataLogFailRecoveryWatermarking() throws Exception {\n+        int instanceId = 0;\n+        int bookieCount = 1;\n+        int containerCount = 4;\n+        String readerGroup = \"rgTx\";\n+\n+        // Creating a long term storage only once here.\n+        this.storageFactory = new InMemoryStorageFactory(executorService());\n+        log.info(\"Created a long term storage.\");\n+\n+        // Start a new BK & ZK, segment store and controller\n+        @Cleanup\n+        PravegaRunner pravegaRunner = new PravegaRunner(instanceId++, bookieCount, containerCount, this.storageFactory);\n+\n+        // Create a scope and a stream\n+        createScopeStream(pravegaRunner.controllerRunner.controller, SCOPE, STREAM1);\n+\n+        // Create a client to read and write events.\n+        @Cleanup\n+        ClientRunner clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // Create two writers\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer1 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer1\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+        @Cleanup\n+        TransactionalEventStreamWriter<Long> writer2 = clientRunner.clientFactory\n+                .createTransactionalEventWriter(\"writer2\", STREAM1, new JavaSerializer<>(),\n+                        EventWriterConfig.builder().transactionTimeoutTime(TRANSACTION_TIMEOUT.toMillis()).build());\n+\n+        AtomicBoolean stopFlag = new AtomicBoolean(false);\n+        // write events\n+        CompletableFuture<Void> writer1Future = writeTxEvents(writer1, stopFlag);\n+        CompletableFuture<Void> writer2Future = writeTxEvents(writer2, stopFlag);\n+\n+        // scale the stream several times so that we get complex positions\n+        StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(ScalingPolicy.fixed(5)).build();\n+        Stream streamObj = Stream.of(SCOPE, STREAM1);\n+        scale(pravegaRunner.controllerRunner.controller, streamObj, config);\n+\n+        // get watermarks\n+        LinkedBlockingQueue<Watermark> watermarks = getWatermarks(pravegaRunner, stopFlag, writer1Future, writer2Future);\n+\n+        pravegaRunner.controllerRunner.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        ConcurrentHashMap<String, Boolean> allSegments = pravegaRunner.segmentStoreRunner.segmentsTracker.getSegments();\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup\n+        Storage storage = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), executorService());\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments.keySet(), pravegaRunner.segmentStoreRunner.segmentsTracker, storage)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        pravegaRunner.segmentStoreRunner.close(); // Shutdown SegmentStore\n+        pravegaRunner.bookKeeperRunner.close(); // Shutdown BookKeeper & ZooKeeper\n+        log.info(\"SegmentStore, BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        pravegaRunner.bookKeeperRunner = new BookKeeperRunner(instanceId++, bookieCount);\n+        createBookKeeperLogFactory(pravegaRunner);\n+        log.info(\"Started a new BookKeeper and ZooKeeper.\");\n+\n+        // Recover segments\n+        runRecovery(containerCount, storage);\n+\n+        // Start a new segment store and controller\n+        pravegaRunner.restartControllerAndSegmentStore(this.storageFactory, this.dataLogFactory);\n+        log.info(\"Started segment store and controller again.\");\n+\n+        // Create the client with new controller.\n+        clientRunner = new ClientRunner(pravegaRunner.controllerRunner);\n+\n+        // read events and verify\n+        readVerifyEventsWithWatermarks(readerGroup, clientRunner, streamObj, watermarks);\n+    }\n+\n+    /**\n+     * Creates reader and verifies watermarking by verifying the time bounds for events.\n+     */\n+    private void readVerifyEventsWithWatermarks(String readerGroup, ClientRunner clientRunner, Stream streamObj,\n+                                                LinkedBlockingQueue<Watermark> watermarks) throws InterruptedException {\n+        List<Map<Stream, StreamCut>> streamCuts = getStreamCutsFromWaterMarks(streamObj, watermarks);\n+        // read from stream cut of first watermark\n+        clientRunner.readerGroupManager.createReaderGroup(readerGroup, ReaderGroupConfig.builder().stream(streamObj)\n+                .startingStreamCuts(streamCuts.get(0))\n+                .endingStreamCuts(streamCuts.get(1))\n+                .build());\n+        @Cleanup\n+        final EventStreamReader<Long> reader = clientRunner.clientFactory.createReader(\"myreaderTx\", readerGroup,\n+                new JavaSerializer<>(), ReaderConfig.builder().build());\n+\n+        EventRead<Long> event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+        TimeWindow currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        while (event.getEvent() != null && currentTimeWindow.getLowerTimeBound() == null && currentTimeWindow.getUpperTimeBound() == null) {\n+            event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            currentTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+        }\n+\n+        assertNotNull(currentTimeWindow.getUpperTimeBound());\n+\n+        currentTimeWindow = verifyEventsWithTimeBounds(streamObj, reader, event, currentTimeWindow);\n+\n+        assertNotNull(currentTimeWindow.getLowerTimeBound());\n+    }\n+\n+    /**\n+     * Gets watermarks used while writing the events\n+     */\n+    private LinkedBlockingQueue<Watermark> getWatermarks(PravegaRunner pravegaRunner, AtomicBoolean stopFlag,\n+                                                         CompletableFuture<Void> writer1Future, CompletableFuture<Void>\n+                                                                 writer2Future) throws Exception {\n+        @Cleanup\n+        SynchronizerClientFactory syncClientFactory = SynchronizerClientFactory.withScope(SCOPE,\n+                ClientConfig.builder().controllerURI(pravegaRunner.controllerRunner.controllerURI).build());\n+\n+        String markStream = NameUtils.getMarkStreamForStream(STREAM1);\n+        RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n+                new WatermarkSerializer(), SynchronizerConfig.builder().build());\n+        LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n+        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n+        stopFlag.set(true);\n+        writer1Future.join();\n+        writer2Future.join();\n+        return watermarks;\n+    }\n+\n+    private List<Map<Stream, StreamCut>> getStreamCutsFromWaterMarks(Stream streamObj, LinkedBlockingQueue<Watermark> watermarks)\n+            throws InterruptedException {\n+        Watermark watermark0 = watermarks.take();\n+        Watermark watermark1 = watermarks.take();\n+        assertTrue(watermark0.getLowerTimeBound() <= watermark0.getUpperTimeBound());\n+        assertTrue(watermark1.getLowerTimeBound() <= watermark1.getUpperTimeBound());\n+        assertTrue(watermark0.getLowerTimeBound() < watermark1.getLowerTimeBound());\n+\n+        Map<Segment, Long> positionMap0 = watermark0.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+        Map<Segment, Long> positionMap1 = watermark1.getStreamCut().entrySet().stream().collect(\n+                Collectors.toMap(x -> new Segment(SCOPE, STREAM1, x.getKey().getSegmentId()), Map.Entry::getValue));\n+\n+        StreamCut streamCutFirst = new StreamCutImpl(streamObj, positionMap0);\n+        StreamCut streamCutSecond = new StreamCutImpl(streamObj, positionMap1);\n+        Map<Stream, StreamCut> firstMarkStreamCut = Collections.singletonMap(streamObj, streamCutFirst);\n+        Map<Stream, StreamCut> secondMarkStreamCut = Collections.singletonMap(streamObj, streamCutSecond);\n+\n+        return new ArrayList<Map<Stream, StreamCut>>(Arrays.asList(firstMarkStreamCut, secondMarkStreamCut));\n+    }\n+\n+    private TimeWindow verifyEventsWithTimeBounds(Stream streamObj, EventStreamReader<Long> reader, EventRead<Long> event, TimeWindow currentTimeWindow) {\n+        // read all events and verify that all events are below the bounds\n+        while (event.getEvent() != null) {\n+            Long time = event.getEvent();\n+            log.info(\"timewindow = {} event = {}\", currentTimeWindow, time);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || time >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || time <= currentTimeWindow.getUpperTimeBound());\n+\n+            TimeWindow nextTimeWindow = reader.getCurrentTimeWindow(streamObj);\n+            assertTrue(currentTimeWindow.getLowerTimeBound() == null || nextTimeWindow.getLowerTimeBound() >= currentTimeWindow.getLowerTimeBound());\n+            assertTrue(currentTimeWindow.getUpperTimeBound() == null || nextTimeWindow.getUpperTimeBound() >= currentTimeWindow.getUpperTimeBound());\n+            currentTimeWindow = nextTimeWindow;\n+\n+            event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            if (event.isCheckpoint()) {\n+                event = reader.readNextEvent(READ_TIMEOUT.toMillis());\n+            }\n+        }\n+        return currentTimeWindow;\n+    }\n+\n+    /**\n+     * Adds water marks to the watermarks queue.\n+     */\n+    private void fetchWatermarks(RevisionedStreamClient<Watermark> watermarkReader, LinkedBlockingQueue<Watermark> watermarks,\n+                                 AtomicBoolean stop) {\n+        AtomicReference<Revision> revision = new AtomicReference<>(watermarkReader.fetchOldestRevision());\n+\n+        Futures.loop(() -> !stop.get(), () -> Futures.delayedTask(() -> {\n+            Iterator<Map.Entry<Revision, Watermark>> marks = watermarkReader.readFrom(revision.get());\n+            if (marks.hasNext()) {\n+                Map.Entry<Revision, Watermark> next = marks.next();\n+                log.info(\"watermark = {}\", next.getValue());\n+\n+                watermarks.add(next.getValue());\n+                revision.set(next.getKey());\n+            }\n+            return null;\n+        }, Duration.ofSeconds(10), executorService()), executorService());\n+    }\n+\n+    private CompletableFuture<Void> writeTxEvents(TransactionalEventStreamWriter<Long> writer, AtomicBoolean stopFlag) {\n+        AtomicInteger count = new AtomicInteger(0);\n+        return Futures.loop(() -> !stopFlag.get(), () -> Futures.delayedFuture(() -> {", "originalCommit": "78aa8dd649428fb2c91b46361a93d253e52af0d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5ODAzNg==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r495698036", "bodyText": "It already returned Futures.loop and callsite waits for it be finished.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T05:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE3MzUxMw==", "url": "https://github.com/pravega/pravega/pull/5113#discussion_r496173513", "bodyText": "Exiting from delayed future if stop is set to true.", "author": "ManishKumarKeshri", "createdAt": "2020-09-28T19:11:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4OTgzNA=="}], "type": "inlineReview", "revised_code": {"commit": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "chunk": "diff --git a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\nindex 4e87ed485e..a22b90b059 100644\n--- a/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n+++ b/test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java\n\n@@ -872,9 +871,10 @@ public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n         RevisionedStreamClient<Watermark> watermarkReader = syncClientFactory.createRevisionedStreamClient(markStream,\n                 new WatermarkSerializer(), SynchronizerConfig.builder().build());\n         LinkedBlockingQueue<Watermark> watermarks = new LinkedBlockingQueue<>();\n-        fetchWatermarks(watermarkReader, watermarks, stopFlag);\n+        CompletableFuture<Void> fetchWaterMarksFuture = fetchWatermarks(watermarkReader, watermarks, stopFlag);\n         AssertExtensions.assertEventuallyEquals(true, () -> watermarks.size() >= 2, 100000);\n         stopFlag.set(true);\n+        fetchWaterMarksFuture.join();\n         writer1Future.join();\n         writer2Future.join();\n         return watermarks;\n"}}, {"oid": "937b148a3a4726ae53ff1ba304a396ec0c80e68b", "url": "https://github.com/pravega/pravega/commit/937b148a3a4726ae53ff1ba304a396ec0c80e68b", "message": "Fixing comments.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-28T05:36:01Z", "type": "commit"}, {"oid": "745f8e30521fa755d40cbf55187c40a833b26f9e", "url": "https://github.com/pravega/pravega/commit/745f8e30521fa755d40cbf55187c40a833b26f9e", "message": "Updating.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-28T06:56:46Z", "type": "commit"}, {"oid": "0a2f2fddfcf38506a7328b034c0eae408da25ee6", "url": "https://github.com/pravega/pravega/commit/0a2f2fddfcf38506a7328b034c0eae408da25ee6", "message": "Fixing comment.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-28T18:50:30Z", "type": "commit"}, {"oid": "316e7ed2af4095d8812fe46d0a767baaaacd3a5d", "url": "https://github.com/pravega/pravega/commit/316e7ed2af4095d8812fe46d0a767baaaacd3a5d", "message": "Minor change.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-28T18:59:57Z", "type": "commit"}, {"oid": "5cf2751d4f8a2c5cd75a0d4f3af791b90d3d490a", "url": "https://github.com/pravega/pravega/commit/5cf2751d4f8a2c5cd75a0d4f3af791b90d3d490a", "message": "Minor changes.\n\nSigned-off-by: ManishKumarKeshri <manish.keshri562@gmail.com>", "committedDate": "2020-09-28T19:06:09Z", "type": "commit"}]}