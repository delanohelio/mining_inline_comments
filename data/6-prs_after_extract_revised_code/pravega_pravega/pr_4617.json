{"pr_number": 4617, "pr_title": "Issue 4609: ExtendedS3Storage.concat using appends when source segment is small", "pr_createdAt": "2020-03-11T22:37:20Z", "pr_url": "https://github.com/pravega/pravega/pull/4617", "timeline": [{"oid": "ec66213c74dd865f0bbae3059843160310e6c776", "url": "https://github.com/pravega/pravega/commit/ec66213c74dd865f0bbae3059843160310e6c776", "message": "Issue 4609 - (ExtendedS3Storage): For smaller source segments, concat should read and append instead of using multi-part upload\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-03-11T22:33:47Z", "type": "commit"}, {"oid": "72502422c719cfee0ee90c16eacfa30e8919dc03", "url": "https://github.com/pravega/pravega/commit/72502422c719cfee0ee90c16eacfa30e8919dc03", "message": "Issue 4609 - (ExtendedS3Storage): Additional concat unit tests for forcing multipart upload.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-03-11T22:33:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTI2Nw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391315267", "bodyText": "If the threshold is configurable, then please add it into config.properties as commented line with description.", "author": "kevinhan88", "createdAt": "2020-03-11T22:53:07Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java", "diffHunk": "@@ -93,6 +97,7 @@ private ExtendedS3StorageConfig(TypedProperties properties) throws Configuration\n         String givenPrefix = Preconditions.checkNotNull(properties.get(PREFIX), \"prefix\");\n         this.prefix = givenPrefix.endsWith(PATH_SEPARATOR) ? givenPrefix : givenPrefix + PATH_SEPARATOR;\n         this.useNoneMatch = properties.getBoolean(USENONEMATCH);\n+        this.smallObjectThreshold = properties.getInt(SMALL_OBJECT_THRESHOLD);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODY0Nw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391318647", "bodyText": "+1. In config.properties, please add recommended values and explain what could happen if setting it too small or too large.", "author": "andreipaduroiu", "createdAt": "2020-03-11T23:02:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTI2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMyNjM1Mg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391326352", "bodyText": "Do we use any kind of prefix to highlight which component this config property from? Something like extendeds3.", "author": "medvedevigorek", "createdAt": "2020-03-11T23:27:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTI2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMyNzY2OA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391327668", "bodyText": "yes, there are namespaces for configs. so in config.properties it's going to be something like extendeds3.smallObjectThreshold, which will be only picked up by ExtendedS3Config", "author": "kevinhan88", "createdAt": "2020-03-11T23:32:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTI2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NTc5NQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391345795", "bodyText": "Good catch @kevinhan88 .Added to config value.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:39:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNTI2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\nindex 3b27b5c747..e13b1c6768 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n\n@@ -97,7 +102,7 @@ public class ExtendedS3StorageConfig {\n         String givenPrefix = Preconditions.checkNotNull(properties.get(PREFIX), \"prefix\");\n         this.prefix = givenPrefix.endsWith(PATH_SEPARATOR) ? givenPrefix : givenPrefix + PATH_SEPARATOR;\n         this.useNoneMatch = properties.getBoolean(USENONEMATCH);\n-        this.smallObjectThreshold = properties.getInt(SMALL_OBJECT_THRESHOLD);\n+        this.smallObjectSizeLimitForConcat = properties.getInt(SMALL_OBJECT_THRESHOLD);\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNjU4Ng==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391316586", "bodyText": "Can you break these into 2 different methods please? It will make it so much easier to read.", "author": "andreipaduroiu", "createdAt": "2020-03-11T22:56:51Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -403,39 +405,53 @@ private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegm\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        //Copy the first part\n-        CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                targetPath,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                1).withSourceRange(Range.fromOffsetLength(0, offset));\n-        CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Copy the second part\n-        S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                config.getPrefix() + sourceSegment);\n-        long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-        copyRequest = new CopyPartRequest(config.getBucket(),\n-                config.getPrefix() + sourceSegment,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-        copyResult = client.copyPart(copyRequest);\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Close the upload\n-        client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                targetPath, uploadId).withParts(partEtags));\n-\n-        client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-        Duration elapsed = timer.getElapsed();\n+        if (config.getSmallObjectThreshold() < si.getLength()) {\n+\n+            //Copy the first part", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMyNzA1Mg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391327052", "bodyText": "Shall the size of a segment to merge be strictly less than the threshold?", "author": "medvedevigorek", "createdAt": "2020-03-11T23:29:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNjU4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NDg0Mw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391344843", "bodyText": "broke those into two methods.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:35:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNjU4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NTY4Mw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391345683", "bodyText": "check is such that large object is greater than threshold", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:38:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNjU4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\nindex 0ef1591c5d..e84fc0c3fd 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n\n@@ -388,69 +388,31 @@ public class ExtendedS3Storage implements SyncStorage {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        if (config.getSmallObjectThreshold() < si.getLength()) {\n-\n-            //Copy the first part\n-            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                    targetPath,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n-            CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Copy the second part\n-            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                    config.getPrefix() + sourceSegment);\n-            long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-            copyRequest = new CopyPartRequest(config.getBucket(),\n-                    config.getPrefix() + sourceSegment,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-            copyResult = client.copyPart(copyRequest);\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Close the upload\n-            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                    targetPath, uploadId).withParts(partEtags));\n-\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-\n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n         } else {\n-            try (InputStream reader = client.readObjectStream(config.getBucket(),\n-                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n-                client.putObject(this.config.getBucket(),\n-                        targetPath,\n-                        Range.fromOffsetLength(offset, si.getLength()),\n-                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n-            } catch (Exception e) {\n-                throw Exceptions.sneakyThrow(e);\n-            }\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n         }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n         Duration elapsed = timer.getElapsed();\n         log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNzIxNg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391317216", "bodyText": "What exceptions are you catching here? Blindly catching Exception is a bad thing.\nCan this partially copy the source segment and then throw an exception? IF true, then we have a problem and we might have to read the object into a byte array and then write it back fully.", "author": "andreipaduroiu", "createdAt": "2020-03-11T22:58:43Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -403,39 +405,53 @@ private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegm\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        //Copy the first part\n-        CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                targetPath,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                1).withSourceRange(Range.fromOffsetLength(0, offset));\n-        CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Copy the second part\n-        S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                config.getPrefix() + sourceSegment);\n-        long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-        copyRequest = new CopyPartRequest(config.getBucket(),\n-                config.getPrefix() + sourceSegment,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-        copyResult = client.copyPart(copyRequest);\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Close the upload\n-        client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                targetPath, uploadId).withParts(partEtags));\n-\n-        client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-        Duration elapsed = timer.getElapsed();\n+        if (config.getSmallObjectThreshold() < si.getLength()) {\n+\n+            //Copy the first part\n+            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                    targetPath,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n+            CopyPartResult copyResult = client.copyPart(copyRequest);\n+\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+\n+            //Copy the second part\n+            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + sourceSegment);\n+            long objectSize = metadataResult.getContentLength(); // in bytes\n+\n+            copyRequest = new CopyPartRequest(config.getBucket(),\n+                    config.getPrefix() + sourceSegment,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n+\n+            copyResult = client.copyPart(copyRequest);\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n \n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+\n+        } else {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n+                client.putObject(this.config.getBucket(),\n+                        targetPath,\n+                        Range.fromOffsetLength(offset, si.getLength()),\n+                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n+            } catch (Exception e) {\n+                throw Exceptions.sneakyThrow(e);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NTQwNA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391345404", "bodyText": "Actually we don't need to handle exception here. It will bubble up to execute method in this class and then will be handled in throwException", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:37:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNzIxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NTQ3Mg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391345472", "bodyText": "remove catch block, we don't need it.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:37:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNzIxNg=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\nindex 0ef1591c5d..e84fc0c3fd 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n\n@@ -388,69 +388,31 @@ public class ExtendedS3Storage implements SyncStorage {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        if (config.getSmallObjectThreshold() < si.getLength()) {\n-\n-            //Copy the first part\n-            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                    targetPath,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n-            CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Copy the second part\n-            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                    config.getPrefix() + sourceSegment);\n-            long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-            copyRequest = new CopyPartRequest(config.getBucket(),\n-                    config.getPrefix() + sourceSegment,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-            copyResult = client.copyPart(copyRequest);\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Close the upload\n-            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                    targetPath, uploadId).withParts(partEtags));\n-\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-\n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n         } else {\n-            try (InputStream reader = client.readObjectStream(config.getBucket(),\n-                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n-                client.putObject(this.config.getBucket(),\n-                        targetPath,\n-                        Range.fromOffsetLength(offset, si.getLength()),\n-                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n-            } catch (Exception e) {\n-                throw Exceptions.sneakyThrow(e);\n-            }\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n         }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n         Duration elapsed = timer.getElapsed();\n         log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNzI5NA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391317294", "bodyText": "This line is the same as in the block above. Move it out.", "author": "andreipaduroiu", "createdAt": "2020-03-11T22:58:59Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -403,39 +405,53 @@ private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegm\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        //Copy the first part\n-        CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                targetPath,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                1).withSourceRange(Range.fromOffsetLength(0, offset));\n-        CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Copy the second part\n-        S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                config.getPrefix() + sourceSegment);\n-        long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-        copyRequest = new CopyPartRequest(config.getBucket(),\n-                config.getPrefix() + sourceSegment,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-        copyResult = client.copyPart(copyRequest);\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Close the upload\n-        client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                targetPath, uploadId).withParts(partEtags));\n-\n-        client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-        Duration elapsed = timer.getElapsed();\n+        if (config.getSmallObjectThreshold() < si.getLength()) {\n+\n+            //Copy the first part\n+            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                    targetPath,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n+            CopyPartResult copyResult = client.copyPart(copyRequest);\n+\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+\n+            //Copy the second part\n+            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + sourceSegment);\n+            long objectSize = metadataResult.getContentLength(); // in bytes\n+\n+            copyRequest = new CopyPartRequest(config.getBucket(),\n+                    config.getPrefix() + sourceSegment,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n+\n+            copyResult = client.copyPart(copyRequest);\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n \n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+\n+        } else {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n+                client.putObject(this.config.getBucket(),\n+                        targetPath,\n+                        Range.fromOffsetLength(offset, si.getLength()),\n+                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n+            } catch (Exception e) {\n+                throw Exceptions.sneakyThrow(e);\n+            }\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NTg2Ng==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391345866", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T00:39:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxNzI5NA=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\nindex 0ef1591c5d..e84fc0c3fd 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n\n@@ -388,69 +388,31 @@ public class ExtendedS3Storage implements SyncStorage {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        if (config.getSmallObjectThreshold() < si.getLength()) {\n-\n-            //Copy the first part\n-            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                    targetPath,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n-            CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Copy the second part\n-            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                    config.getPrefix() + sourceSegment);\n-            long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-            copyRequest = new CopyPartRequest(config.getBucket(),\n-                    config.getPrefix() + sourceSegment,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-            copyResult = client.copyPart(copyRequest);\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Close the upload\n-            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                    targetPath, uploadId).withParts(partEtags));\n-\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-\n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n         } else {\n-            try (InputStream reader = client.readObjectStream(config.getBucket(),\n-                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n-                client.putObject(this.config.getBucket(),\n-                        targetPath,\n-                        Range.fromOffsetLength(offset, si.getLength()),\n-                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n-            } catch (Exception e) {\n-                throw Exceptions.sneakyThrow(e);\n-            }\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n         }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n         Duration elapsed = timer.getElapsed();\n         log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODMwNQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391318305", "bodyText": "What does smallObjectThreshold mean? Someone who has no idea what this means will not be able to use this properly.\nSecond you must always include the unit of measure in your property names. Is this bytes, KB, MB, ..., TB?\nPlease name this something more appropriate, such as nativeConcatThresholdBytes or multiPartUploadConcatThresholdBytes .", "author": "andreipaduroiu", "createdAt": "2020-03-11T23:01:58Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java", "diffHunk": "@@ -30,6 +30,7 @@\n     public static final Property<String> BUCKET = Property.named(\"bucket\", \"\");\n     public static final Property<String> PREFIX = Property.named(\"prefix\", \"/\");\n     public static final Property<Boolean> USENONEMATCH = Property.named(\"useNoneMatch\", false);\n+    public static final Property<Integer> SMALL_OBJECT_THRESHOLD = Property.named(\"smallObjectThreshold\", 1024 * 1024);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM1NzExMA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391357110", "bodyText": "renamed", "author": "sachin-j-joshi", "createdAt": "2020-03-12T01:27:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODMwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\nindex 3b27b5c747..e13b1c6768 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n\n@@ -30,7 +30,7 @@ public class ExtendedS3StorageConfig {\n     public static final Property<String> BUCKET = Property.named(\"bucket\", \"\");\n     public static final Property<String> PREFIX = Property.named(\"prefix\", \"/\");\n     public static final Property<Boolean> USENONEMATCH = Property.named(\"useNoneMatch\", false);\n-    public static final Property<Integer> SMALL_OBJECT_THRESHOLD = Property.named(\"smallObjectThreshold\", 1024 * 1024);\n+    public static final Property<Integer> SMALL_OBJECT_THRESHOLD = Property.named(\"smallObjectSizeLimitForConcat\", 1024 * 1024);\n \n     private static final String COMPONENT_CODE = \"extendeds3\";\n     private static final String PATH_SEPARATOR = \"/\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODQ0Mw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391318443", "bodyText": "Add a comment to the field explaining exactly what the behavior is.", "author": "andreipaduroiu", "createdAt": "2020-03-11T23:02:18Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java", "diffHunk": "@@ -75,6 +76,9 @@\n     @Getter\n     private final boolean useNoneMatch;\n \n+    @Getter\n+    private final int smallObjectThreshold;", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\nindex 3b27b5c747..e13b1c6768 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3StorageConfig.java\n\n@@ -76,8 +76,13 @@ public class ExtendedS3StorageConfig {\n     @Getter\n     private final boolean useNoneMatch;\n \n+    /**\n+     * Size of ECS objects in bytes above which it is no longer considered a small object.\n+     * For small source objects, to implement concat ExtendedS3Storage reads complete objects and appends it to target\n+     * instead of using multi part upload.\n+     */\n     @Getter\n-    private final int smallObjectThreshold;\n+    private final int smallObjectSizeLimitForConcat;\n \n     //endregion\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODc4Ng==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391318786", "bodyText": "How do you know MPU was used?", "author": "andreipaduroiu", "createdAt": "2020-03-11T23:03:22Z", "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3StorageTest.java", "diffHunk": "@@ -227,6 +227,26 @@ public void testExistsWithPrefix() throws Exception {\n         }\n     }\n \n+    /**\n+     * Tests the concat() method forcing to use multipart upload.\n+     *\n+     * @throws Exception if an unexpected error occurred.\n+     */\n+    @Test\n+    public void testConcatWithMultipartUpload() throws Exception {\n+        val adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, setup.configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, setup.adapterConfig.getBucket())\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .with(ExtendedS3StorageConfig.USENONEMATCH, true)\n+                .with(ExtendedS3StorageConfig.SMALL_OBJECT_THRESHOLD, 1)\n+                .build();\n+        final String context = createSegmentName(\"Concat\");\n+        try (Storage s = createStorage(setup.client, adapterConfig, executorService())) {\n+            testConcat(context, s);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM1NzE4NA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391357184", "bodyText": "added check", "author": "sachin-j-joshi", "createdAt": "2020-03-12T01:28:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxODc4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3StorageTest.java b/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3StorageTest.java\nindex 6a50c597a7..9caccc03d6 100644\n--- a/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3StorageTest.java\n+++ b/bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3StorageTest.java\n\n@@ -242,8 +239,11 @@ public class ExtendedS3StorageTest extends IdempotentStorageTestBase {\n                 .with(ExtendedS3StorageConfig.SMALL_OBJECT_THRESHOLD, 1)\n                 .build();\n         final String context = createSegmentName(\"Concat\");\n+        assertEquals(0, ExtendedS3Metrics.LARGE_CONCAT_COUNT.get());\n         try (Storage s = createStorage(setup.client, adapterConfig, executorService())) {\n             testConcat(context, s);\n+            assertTrue(ExtendedS3Metrics.LARGE_CONCAT_COUNT.get() > 0);\n+            assertEquals(ExtendedS3Metrics.CONCAT_COUNT.get(), ExtendedS3Metrics.LARGE_CONCAT_COUNT.get());\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391319350", "bodyText": "Only delete original object in case of success, right? move this line into try block.", "author": "kevinhan88", "createdAt": "2020-03-11T23:05:05Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -403,39 +405,53 @@ private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegm\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        //Copy the first part\n-        CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                targetPath,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                1).withSourceRange(Range.fromOffsetLength(0, offset));\n-        CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Copy the second part\n-        S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                config.getPrefix() + sourceSegment);\n-        long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-        copyRequest = new CopyPartRequest(config.getBucket(),\n-                config.getPrefix() + sourceSegment,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-        copyResult = client.copyPart(copyRequest);\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Close the upload\n-        client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                targetPath, uploadId).withParts(partEtags));\n-\n-        client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-        Duration elapsed = timer.getElapsed();\n+        if (config.getSmallObjectThreshold() < si.getLength()) {\n+\n+            //Copy the first part\n+            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                    targetPath,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n+            CopyPartResult copyResult = client.copyPart(copyRequest);\n+\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+\n+            //Copy the second part\n+            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + sourceSegment);\n+            long objectSize = metadataResult.getContentLength(); // in bytes\n+\n+            copyRequest = new CopyPartRequest(config.getBucket(),\n+                    config.getPrefix() + sourceSegment,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n+\n+            copyResult = client.copyPart(copyRequest);\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n \n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+\n+        } else {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n+                client.putObject(this.config.getBucket(),\n+                        targetPath,\n+                        Range.fromOffsetLength(offset, si.getLength()),\n+                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n+            } catch (Exception e) {\n+                throw Exceptions.sneakyThrow(e);\n+            }\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMyODg4Mw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391328883", "bodyText": "Delete operation may fail too.  Need some error handling here and of course sufficient logging.", "author": "medvedevigorek", "createdAt": "2020-03-11T23:36:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM2MzIzMA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391363230", "bodyText": "Yes. actually this could be a nasty bug. If concat succeeds but delete fails then we are leaving target segment in a state where it is bigger than before. But for the caller the operation has failed so it will not update size. If now operation is retried it will be done with older offset which will fail again and again. Solution is to either move delete as a separate step to upper layers or not fail the concat call when delete operation fails.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T01:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTcwNzMzNg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391707336", "bodyText": "This is not as bad as you think. If the concat succeeds but the delete fails, the storage writer will reattempt. You will need to throw a BadOffsetException because your precondition will not hold. The StorageWriter will them enter a reconciliation state and figure out that the source segment has actually been merged in.\nThe only downside is that the old segment will be left behind, occupying space but not being used.", "author": "andreipaduroiu", "createdAt": "2020-03-12T15:37:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc0MDE0Nw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391740147", "bodyText": "Okay, it makes sense then.", "author": "medvedevigorek", "createdAt": "2020-03-12T16:27:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc0NjM4MQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391746381", "bodyText": "I just realized that in case of delete failures, the concat operation will be retried with same offset and we'll simply overwrite the same byte range with same contents. There is a test case that tests this idempotent behavior (concat at same offset). If the delete failure was intermittent then eventually it will succeed. Otherwise its likely that root cause will manifest in other operations too.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T16:36:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTc0NjY3NA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391746674", "bodyText": "The other observation is S3Client being used here doesn't declare any exceptions for its methods but actually it does throw S3Exception if it receives a response with status code other than 2xx.\nSince S3Exception is defined as a runtime exception we need to ensure the call stack handles it correctly and doesn't lead to a serious disaster as it may not assume to handle a runtime exception at all.", "author": "medvedevigorek", "createdAt": "2020-03-12T16:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgxNzE0Ng==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391817146", "bodyText": "Updated to throw BadOffsetException on failure during delete. This will trigger reconciliation.\nIn normal course we continue to allow concat to be idempotent.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T18:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTM1MA=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\nindex 0ef1591c5d..e84fc0c3fd 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n\n@@ -388,69 +388,31 @@ public class ExtendedS3Storage implements SyncStorage {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        if (config.getSmallObjectThreshold() < si.getLength()) {\n-\n-            //Copy the first part\n-            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                    targetPath,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n-            CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Copy the second part\n-            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                    config.getPrefix() + sourceSegment);\n-            long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-            copyRequest = new CopyPartRequest(config.getBucket(),\n-                    config.getPrefix() + sourceSegment,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-            copyResult = client.copyPart(copyRequest);\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Close the upload\n-            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                    targetPath, uploadId).withParts(partEtags));\n-\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-\n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n         } else {\n-            try (InputStream reader = client.readObjectStream(config.getBucket(),\n-                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n-                client.putObject(this.config.getBucket(),\n-                        targetPath,\n-                        Range.fromOffsetLength(offset, si.getLength()),\n-                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n-            } catch (Exception e) {\n-                throw Exceptions.sneakyThrow(e);\n-            }\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n         }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n         Duration elapsed = timer.getElapsed();\n         log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTY1MA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391319650", "bodyText": "Please give more information here with the exception, particularly this is something new we're trying", "author": "kevinhan88", "createdAt": "2020-03-11T23:05:48Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -403,39 +405,53 @@ private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegm\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        //Copy the first part\n-        CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                targetPath,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                1).withSourceRange(Range.fromOffsetLength(0, offset));\n-        CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Copy the second part\n-        S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                config.getPrefix() + sourceSegment);\n-        long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-        copyRequest = new CopyPartRequest(config.getBucket(),\n-                config.getPrefix() + sourceSegment,\n-                config.getBucket(),\n-                targetPath,\n-                uploadId,\n-                2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-        copyResult = client.copyPart(copyRequest);\n-        partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-        //Close the upload\n-        client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                targetPath, uploadId).withParts(partEtags));\n-\n-        client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-        Duration elapsed = timer.getElapsed();\n+        if (config.getSmallObjectThreshold() < si.getLength()) {\n+\n+            //Copy the first part\n+            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                    targetPath,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n+            CopyPartResult copyResult = client.copyPart(copyRequest);\n+\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+\n+            //Copy the second part\n+            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + sourceSegment);\n+            long objectSize = metadataResult.getContentLength(); // in bytes\n+\n+            copyRequest = new CopyPartRequest(config.getBucket(),\n+                    config.getPrefix() + sourceSegment,\n+                    config.getBucket(),\n+                    targetPath,\n+                    uploadId,\n+                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n+\n+            copyResult = client.copyPart(copyRequest);\n+            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n \n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+\n+            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+\n+        } else {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n+                client.putObject(this.config.getBucket(),\n+                        targetPath,\n+                        Range.fromOffsetLength(offset, si.getLength()),\n+                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n+            } catch (Exception e) {\n+                throw Exceptions.sneakyThrow(e);", "originalCommit": "72502422c719cfee0ee90c16eacfa30e8919dc03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM1NzM4MA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391357380", "bodyText": "actually we don't need to catch exception here.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T01:28:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTMxOTY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "chunk": "diff --git a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\nindex 0ef1591c5d..e84fc0c3fd 100644\n--- a/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n+++ b/bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java\n\n@@ -388,69 +388,31 @@ public class ExtendedS3Storage implements SyncStorage {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n-        if (config.getSmallObjectThreshold() < si.getLength()) {\n-\n-            //Copy the first part\n-            CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n-                    targetPath,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    1).withSourceRange(Range.fromOffsetLength(0, offset));\n-            CopyPartResult copyResult = client.copyPart(copyRequest);\n-\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Copy the second part\n-            S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n-                    config.getPrefix() + sourceSegment);\n-            long objectSize = metadataResult.getContentLength(); // in bytes\n-\n-            copyRequest = new CopyPartRequest(config.getBucket(),\n-                    config.getPrefix() + sourceSegment,\n-                    config.getBucket(),\n-                    targetPath,\n-                    uploadId,\n-                    2).withSourceRange(Range.fromOffsetLength(0, objectSize));\n-\n-            copyResult = client.copyPart(copyRequest);\n-            partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n-\n-            //Close the upload\n-            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n-                    targetPath, uploadId).withParts(partEtags));\n-\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n-\n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n         } else {\n-            try (InputStream reader = client.readObjectStream(config.getBucket(),\n-                    config.getPrefix() + sourceSegment, Range.fromOffsetLength(0, si.getLength()))) {\n-                client.putObject(this.config.getBucket(),\n-                        targetPath,\n-                        Range.fromOffsetLength(offset, si.getLength()),\n-                        new BufferedInputStream(reader, Math.toIntExact(si.getLength())));\n-            } catch (Exception e) {\n-                throw Exceptions.sneakyThrow(e);\n-            }\n-            client.deleteObject(config.getBucket(), config.getPrefix() + sourceSegment);\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n         }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n         Duration elapsed = timer.getElapsed();\n         log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n \n"}}, {"oid": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "url": "https://github.com/pravega/pravega/commit/bece4e0c48531a64a2919c3f91b8469d70248ac2", "message": "Issue 4609 - (ExtendedS3Storage): Metric for large concat count. Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-03-12T01:39:14Z", "type": "commit"}, {"oid": "bece4e0c48531a64a2919c3f91b8469d70248ac2", "url": "https://github.com/pravega/pravega/commit/bece4e0c48531a64a2919c3f91b8469d70248ac2", "message": "Issue 4609 - (ExtendedS3Storage): Metric for large concat count. Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-03-12T01:39:14Z", "type": "forcePushed"}, {"oid": "6aaaacaf966632a09552a4033b6d4c829cebcf28", "url": "https://github.com/pravega/pravega/commit/6aaaacaf966632a09552a4033b6d4c829cebcf28", "message": "Issue 4609 - (ExtendedS3Storage): Update config file to include defualt value.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-03-12T01:48:10Z", "type": "commit"}, {"oid": "9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "url": "https://github.com/pravega/pravega/commit/9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "message": "Merge branch 'master' into issue-4609-concat-using-appends-for-small-source-master", "committedDate": "2020-03-12T15:38:02Z", "type": "commit"}, {"oid": "9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "url": "https://github.com/pravega/pravega/commit/9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "message": "Merge branch 'master' into issue-4609-concat-using-appends-for-small-source-master", "committedDate": "2020-03-12T15:38:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyODI0NQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391928245", "bodyText": "concat_count should be updated before calling deleteObject() otherwise it will not be consistent with large_concat_count in case of deleteObject() fails.", "author": "medvedevigorek", "createdAt": "2020-03-12T21:57:56Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -386,23 +388,57 @@ private void setPermission(SegmentHandle handle, Permission permission) {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n+        } else {\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n+        }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n+\n+        ExtendedS3Metrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ExtendedS3Metrics.CONCAT_BYTES.add(si.getLength());\n+        ExtendedS3Metrics.CONCAT_COUNT.inc();", "originalCommit": "9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzMTIyMQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391931221", "bodyText": "In other words we should update them around the same time. I prefer after, so that we may only record successful runs.\nIf we record it before, then every retry will be counted, which is not what we want.", "author": "andreipaduroiu", "createdAt": "2020-03-12T22:05:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyODI0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NjAzMA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391946030", "bodyText": "I think I would agree with @andreipaduroiu , for 0.7 let's record metrics only for successful events. Let's track this as a separate issue to be resolved  in 0.8 along with BadOffsetException change.", "author": "sachin-j-joshi", "createdAt": "2020-03-12T22:49:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyODI0NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391929326", "bodyText": "latency and bytes are recorded only in case of success, in case of a failure in deleteObject() those will not be recorded. Don't they need to be recorded anyway?", "author": "medvedevigorek", "createdAt": "2020-03-12T22:00:01Z", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -386,23 +388,57 @@ private void setPermission(SegmentHandle handle, Permission permission) {\n      * completeMultiPartUpload call. Specifically, to concatenate, we are copying the target segment T and the\n      * source segment S to T, so essentially we are doing T <- T + S.\n      */\n-    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws StreamSegmentNotExistsException {\n+    private Void doConcat(SegmentHandle targetHandle, long offset, String sourceSegment) throws Exception {\n         Preconditions.checkArgument(!targetHandle.isReadOnly(), \"target handle must not be read-only.\");\n         long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle.getSegmentName(), offset, sourceSegment);\n         Timer timer = new Timer();\n-        SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n-        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n-        String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n \n+        String targetPath = config.getPrefix() + targetHandle.getSegmentName();\n         // check whether the target exists\n         if (!doExists(targetHandle.getSegmentName())) {\n             throw new StreamSegmentNotExistsException(targetHandle.getSegmentName());\n         }\n         // check whether the source is sealed\n         SegmentProperties si = doGetStreamSegmentInfo(sourceSegment);\n+        String sourcePath = config.getPrefix() + sourceSegment;\n         Preconditions.checkState(si.isSealed(), \"Cannot concat segment '%s' into '%s' because it is not sealed.\",\n                 sourceSegment, targetHandle.getSegmentName());\n \n+        if (config.getSmallObjectSizeLimitForConcat() < si.getLength()) {\n+            doConcatWithMultipartUpload(targetPath, sourceSegment, offset);\n+            ExtendedS3Metrics.LARGE_CONCAT_COUNT.inc();\n+        } else {\n+            doConcatWithAppend(targetPath, sourcePath, offset, si.getLength());\n+        }\n+        // Now delete the source object.\n+        client.deleteObject(config.getBucket(), sourcePath);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"Concat target={} source={} offset={} bytesWritten={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, si.getLength(), elapsed.toMillis());\n+\n+        ExtendedS3Metrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);", "originalCommit": "9ef13dd50bd0a6e14ca8345a8fffa82e28ecb8cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzMDgwOQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391930809", "bodyText": "We never record these for failed requests.", "author": "andreipaduroiu", "createdAt": "2020-03-12T22:04:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzNTkxOQ==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391935919", "bodyText": "We can start reportFailEvent in case concat (or/and delete) failed.\nWe may report success only after both concat and delete succeeded, otherwise report failure.", "author": "kevinhan88", "createdAt": "2020-03-12T22:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzNjc0Nw==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391936747", "bodyText": "Yes, we could do that. But this would be the only place in the code we currently do that. We can tackle this as a widespread change in another PR.", "author": "andreipaduroiu", "createdAt": "2020-03-12T22:20:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzNzE0Mg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391937142", "bodyText": "Shall we delete the source segment through ExtendedS3Storage.delete(), instead of calling client.delete() directly?\nGoing through that path, we have Tier2 DELETE metrics triggered.", "author": "kevinhan88", "createdAt": "2020-03-12T22:21:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkzODIzNA==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391938234", "bodyText": "Yes, just overload the doDelete(SegmentHandle handle) with doDelete(String bucket, String prefix, String segmentName).", "author": "kevinhan88", "createdAt": "2020-03-12T22:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk0NjU0Mg==", "url": "https://github.com/pravega/pravega/pull/4617#discussion_r391946542", "bodyText": "For 0.7 let's update metrics for only successful requests.\nLet's track the issue as separate issue for 0.8", "author": "sachin-j-joshi", "createdAt": "2020-03-12T22:51:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTkyOTMyNg=="}], "type": "inlineReview", "revised_code": null}, {"oid": "9914a4f9c3407767bd78a0443ac6dac122b02b87", "url": "https://github.com/pravega/pravega/commit/9914a4f9c3407767bd78a0443ac6dac122b02b87", "message": "Merge branch 'master' into issue-4609-concat-using-appends-for-small-source-master", "committedDate": "2020-03-12T23:25:13Z", "type": "commit"}]}