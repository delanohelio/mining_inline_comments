{"pr_number": 10605, "pr_title": "Add DiskDisruptionIT to integration tests", "pr_createdAt": "2020-09-28T13:06:04Z", "pr_url": "https://github.com/crate/crate/pull/10605", "timeline": [{"oid": "6d6a38a84b89fcf6048a038554adb12b1aa677c1", "url": "https://github.com/crate/crate/commit/6d6a38a84b89fcf6048a038554adb12b1aa677c1", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-09-29T16:53:22Z", "type": "forcePushed"}, {"oid": "3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "url": "https://github.com/crate/crate/commit/3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "message": "cleanup", "committedDate": "2020-09-30T11:50:12Z", "type": "forcePushed"}, {"oid": "44072398444cabca810730fcb4d0bc8be21c2b34", "url": "https://github.com/crate/crate/commit/44072398444cabca810730fcb4d0bc8be21c2b34", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T09:07:59Z", "type": "forcePushed"}, {"oid": "5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "url": "https://github.com/crate/crate/commit/5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:25:32Z", "type": "forcePushed"}, {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390", "url": "https://github.com/crate/crate/commit/83965e3e129b6ad89036b0b115fa6919c8375390", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:28:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw==", "url": "https://github.com/crate/crate/pull/10605#discussion_r498824147", "bodyText": "I removed useAutoGeneratedIDs because afaik primary keys can not be autogenerated using sql.", "author": "mkleen", "createdAt": "2020-10-02T13:33:42Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {", "originalCommit": "83965e3e129b6ad89036b0b115fa6919c8375390", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzODA3MQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499438071", "bodyText": "If no primary key is defined on the table, the internal id _id will be generated, otherwise the values of the PK cols are used to compute a _id value.\nAs the _id column can be selected, I see no issue on keeping the useAutogeneratedIDs feature. Do I miss anything?", "author": "seut", "createdAt": "2020-10-05T08:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ2MDg5OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499460898", "bodyText": "This requires to initialise the table with or without primary key depending if the id is autogenerated or not. If the primary key is defined it cannot be null, so there is a dependency between the table definition and the indexing routine. However, i can do that.", "author": "mkleen", "createdAt": "2020-10-05T09:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3MjA0OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499472048", "bodyText": "Ah right, so I missed something indeed ;-)\nI've looked into this again and it seems that using autogenerated id's is not required for any current scenario.\nI'd then prefer to not use it like you did.", "author": "seut", "createdAt": "2020-10-05T09:43:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "803846af4c06be3f5b09c4f5c7a3b54877999888", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\nindex e6d8ae8f57..25b89d160a 100644\n--- a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n+++ b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n\n@@ -136,12 +136,12 @@ public class BackgroundIndexer implements AutoCloseable {\n                                         continue;\n                                     }\n                                 }\n-                                var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n                                 var insertData = new Object[batchSize];\n                                 for (int i = 0; i < batchSize; i++) {\n                                     insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n                                 }\n                                 try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n                                     var response = sqlExecutor.execute(insert, insertData).actionGet();\n                                     for (var generatedId : response.rows()[0]) {\n                                         ids.add((String) generatedId);\n"}}, {"oid": "803846af4c06be3f5b09c4f5c7a3b54877999888", "url": "https://github.com/crate/crate/commit/803846af4c06be3f5b09c4f5c7a3b54877999888", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:41:40Z", "type": "forcePushed"}, {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "url": "https://github.com/crate/crate/commit/92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-02T13:48:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM1NDQ3OA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499354478", "bodyText": "This makes sure file disruption ends after all nodes are stopped.", "author": "mkleen", "createdAt": "2020-10-05T05:38:20Z", "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "originalCommit": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\nindex 1606ceaf38..90976d9423 100644\n--- a/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\n+++ b/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\n\n@@ -1692,8 +1692,8 @@ public final class InternalTestCluster extends TestCluster {\n             nodesByRoles.computeIfAbsent(discoveryNode.getRoles(), k -> new ArrayList<>()).add(nodeAndClient);\n         }\n \n-        assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n         callback.onAllNodesStopped();\n+        assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n \n         // randomize start up order, but making sure that:\n         // 1) A data folder that was assigned to a data node will stay so\n"}}, {"oid": "c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "url": "https://github.com/crate/crate/commit/c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T05:40:08Z", "type": "forcePushed"}, {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5", "url": "https://github.com/crate/crate/commit/00b983c496ae824e23e9a4a9b9c67733289123c5", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T05:47:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQyODg5NA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499428894", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\", null).actionGet();\n          \n          \n            \n                                var response = execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\");", "author": "seut", "createdAt": "2020-10-05T08:33:03Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\nindex f1e23a182e..3cd0c89c1b 100644\n--- a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n+++ b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n\n@@ -120,8 +120,8 @@ public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n         final Thread globalCheckpointSampler = new Thread(() -> {\n             while (stopGlobalCheckpointFetcher.get() == false) {\n                 try {\n-                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n-                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\");\n                     for (var row : response.rows()) {\n                         final int shardId = (int) row[0];\n                         final long globalCheckpoint = (long) row[1];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMDI5Nw==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499430297", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n          \n          \n            \n                    execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "author": "seut", "createdAt": "2020-10-05T08:35:31Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\nindex f1e23a182e..3cd0c89c1b 100644\n--- a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n+++ b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n\n@@ -120,8 +120,8 @@ public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n         final Thread globalCheckpointSampler = new Thread(() -> {\n             while (stopGlobalCheckpointFetcher.get() == false) {\n                 try {\n-                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n-                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\");\n                     for (var row : response.rows()) {\n                         final int shardId = (int) row[0];\n                         final long globalCheckpoint = (long) row[1];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMjg5NQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499432895", "bodyText": "Any reason why this method is not added to the ESIntegTestCase like in the ES upstream?", "author": "seut", "createdAt": "2020-10-05T08:40:20Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n+               \"routing_state in ('STARTED', 'RELOCATING')\");\n+\n+        assertThat(response.rowCount(), is((long) numberOfShards));\n+\n+        for (var row : response.rows()) {\n+            final int shardId = (int) row[0];\n+            final long maxSeqNo = (long) row[1];\n+            assertThat(maxSeqNo, greaterThanOrEqualTo(shardToGcp.get(shardId)));\n+        }\n+    }\n+\n+    /**\n+     * Waits until at least a give number of document is visible for searchers\n+     *\n+     * @param numDocs number of documents to wait for\n+     * @param indexer a {@link org.elasticsearch.test.BackgroundIndexer}. It will be first checked for documents indexed.\n+     *                This saves on unneeded searches.\n+     */\n+    public void waitForDocs(final long numDocs, final BackgroundIndexer indexer, String table) throws Exception {", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\nindex f1e23a182e..3cd0c89c1b 100644\n--- a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n+++ b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n\n@@ -120,8 +120,8 @@ public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n         final Thread globalCheckpointSampler = new Thread(() -> {\n             while (stopGlobalCheckpointFetcher.get() == false) {\n                 try {\n-                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n-                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\");\n                     for (var row : response.rows()) {\n                         final int shardId = (int) row[0];\n                         final long globalCheckpoint = (long) row[1];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDAzOA==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440038", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, insertData).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, insertData);\n          \n      \n    \n    \n  \n\nOtherwise, please also keep in mind to always add proper timeouts to any blocking calls in general to let if fail early.", "author": "seut", "createdAt": "2020-10-05T08:51:57Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\nindex 25b89d160a..a979d1225f 100644\n--- a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n+++ b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n\n@@ -142,7 +142,7 @@ public class BackgroundIndexer implements AutoCloseable {\n                                 }\n                                 try {\n                                     var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n-                                    var response = sqlExecutor.execute(insert, insertData).actionGet();\n+                                    var response = sqlExecutor.exec(insert, insertData);\n                                     for (var generatedId : response.rows()[0]) {\n                                         ids.add((String) generatedId);\n                                     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDI1OQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440259", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, generateValues(idGenerator.incrementAndGet(), threadRandom));", "author": "seut", "createdAt": "2020-10-05T08:52:18Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();\n+                                    for (var generatedId : response.rows()[0]) {\n+                                        ids.add((String) generatedId);\n+                                    }\n+                                } catch (Exception e) {\n+                                    if (ignoreIndexingFailures == false) {\n+                                        throw e;\n+                                    }\n+                                }\n+                            } else {\n+                                if (hasBudget.get() && !availableBudget.tryAcquire(250, TimeUnit.MILLISECONDS)) {\n+                                    // time out -> check if we have to stop.\n+                                    continue;\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) values(?, ?) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\nindex 25b89d160a..a979d1225f 100644\n--- a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n+++ b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n\n@@ -142,7 +142,7 @@ public class BackgroundIndexer implements AutoCloseable {\n                                 }\n                                 try {\n                                     var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n-                                    var response = sqlExecutor.execute(insert, insertData).actionGet();\n+                                    var response = sqlExecutor.exec(insert, insertData);\n                                     for (var generatedId : response.rows()[0]) {\n                                         ids.add((String) generatedId);\n                                     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTY2Ng==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499441666", "bodyText": "In the upstream code, this line is above the assertion line. Any reason for this change?", "author": "seut", "createdAt": "2020-10-05T08:54:38Z", "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "originalCommit": "00b983c496ae824e23e9a4a9b9c67733289123c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ2Nzc0Mg==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499467742", "bodyText": "\ud83d\udc4d  My fault, seems like i was on the wrong state of elasticsearch, there was no assert at all when i looked at it.", "author": "mkleen", "createdAt": "2020-10-05T09:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTY2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java b/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\nindex 1606ceaf38..90976d9423 100644\n--- a/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\n+++ b/server/src/test/java/org/elasticsearch/test/InternalTestCluster.java\n\n@@ -1692,8 +1692,8 @@ public final class InternalTestCluster extends TestCluster {\n             nodesByRoles.computeIfAbsent(discoveryNode.getRoles(), k -> new ArrayList<>()).add(nodeAndClient);\n         }\n \n-        assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n         callback.onAllNodesStopped();\n+        assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n \n         // randomize start up order, but making sure that:\n         // 1) A data folder that was assigned to a data node will stay so\n"}}, {"oid": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "url": "https://github.com/crate/crate/commit/bd4045719c3e8fc23f35a833ace4263e92fc212c", "message": "Fix SqlExecutor execution", "committedDate": "2020-10-05T11:14:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MTc5MQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499551791", "bodyText": "could use execute(..) instead. sorry my fault, wrong suggestion by me ;)", "author": "seut", "createdAt": "2020-10-05T12:12:33Z", "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +", "originalCommit": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "chunk": "diff --git a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\nindex 97116e31f1..d2add649ea 100644\n--- a/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n+++ b/server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java\n\n@@ -116,7 +116,7 @@ public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n         final Thread globalCheckpointSampler = new Thread(() -> {\n             while (stopGlobalCheckpointFetcher.get() == false) {\n                 try {\n-                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                    var response = execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n                                                        \"from sys.shards where table_name='test'\");\n                     for (var row : response.rows()) {\n                         final int shardId = (int) row[0];\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MzAwNQ==", "url": "https://github.com/crate/crate/pull/10605#discussion_r499553005", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n          \n          \n            \n                                                var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "author": "seut", "createdAt": "2020-10-05T12:14:40Z", "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,323 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+    final String table;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.table = table;\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "originalCommit": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "chunk": "diff --git a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\nindex 64c5068280..6f0a2ddf0b 100644\n--- a/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n+++ b/server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java\n\n@@ -143,7 +143,7 @@ public class BackgroundIndexer implements AutoCloseable {\n                                     insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n                                 }\n                                 try {\n-                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n                                     var response = sqlExecutor.exec(insert, insertData);\n                                     for (var generatedId : response.rows()[0]) {\n                                         ids.add((String) generatedId);\n"}}, {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T12:26:46Z", "type": "commit"}, {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "message": "Add DiskDisruptionIT to integration tests", "committedDate": "2020-10-05T12:26:46Z", "type": "forcePushed"}]}