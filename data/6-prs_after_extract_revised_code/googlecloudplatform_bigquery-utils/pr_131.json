{"pr_number": 131, "pr_title": "Template Based Query Generation: Query generation (Merge Fix)", "pr_createdAt": "2020-08-03T18:17:20Z", "pr_url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131", "timeline": [{"oid": "64db68e63e84f7a8de8f07f344d2acf2961819da", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/64db68e63e84f7a8de8f07f344d2acf2961819da", "message": "brought skeleton up to date", "committedDate": "2020-08-03T16:50:22Z", "type": "commit"}, {"oid": "bb726fb2aa2df0852723d50a881d7361025d403b", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bb726fb2aa2df0852723d50a881d7361025d403b", "message": "brought up to date with Victor's progress---it works", "committedDate": "2020-08-03T17:30:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzA2Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r465297063", "bodyText": "Please fix the indentation of this file to use 2 space of tab", "author": "Bei-z", "createdAt": "2020-08-04T20:00:30Z", "path": "tools/template_based_query_generation/src/main/java/QueryGenerator.java", "diffHunk": "@@ -1,31 +1,146 @@\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.gson.Gson;\n+import graph.MarkovChain;\n+import graph.Node;\n+import parser.*;\n+import token.Tokenizer;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+import static java.nio.charset.StandardCharsets.UTF_8;\n \n /**\n  * Class that parses config file and creates queries from markov chain\n  */\n public class QueryGenerator {\n \n+\tprivate final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";", "originalCommit": "bb726fb2aa2df0852723d50a881d7361025d403b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA0MTk3MA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r466041970", "bodyText": "Resolved!", "author": "AllenWang314", "createdAt": "2020-08-05T22:36:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzA2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "00122b104c00185e78cc9f723bc6357b289da33a", "chunk": "diff --git a/tools/template_based_query_generation/src/main/java/QueryGenerator.java b/tools/template_based_query_generation/src/main/java/QueryGenerator.java\nindex 97f104c..266adc7 100644\n--- a/tools/template_based_query_generation/src/main/java/QueryGenerator.java\n+++ b/tools/template_based_query_generation/src/main/java/QueryGenerator.java\n\n@@ -20,127 +20,127 @@ import static java.nio.charset.StandardCharsets.UTF_8;\n  */\n public class QueryGenerator {\n \n-\tprivate final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";\n-\tprivate final String filePathConfigDML = \"./src/main/resources/user_config/dml.json\";\n-\tprivate final String filePathConfigDQL = \"./src/main/resources/user_config/dql.json\";\n-\tprivate final String filePathDependenciesRoot = \"./src/main/resources/dialect_config/root_dependencies.json\";\n-\tprivate final String filePathDependenciesDDL = \"./src/main/resources/dialect_config/ddl_dependencies.json\";\n-\tprivate final String filePathDependenciesDML = \"./src/main/resources/dialect_config/dml_dependencies.json\";\n-\tprivate final String filePathDependenciesDQL = \"./src/main/resources/dialect_config/dql_dependencies.json\";\n-\n-\tprivate final MarkovChain<Query> markovChain;\n-\tprivate Random r = new Random();\n-\tprivate Node<Query> source = new Node<>(new Query(FeatureType.FEATURE_ROOT), r);\n-\n-\t/**\n-\t *\n-\t * @throws Exception\n-\t */\n-\tpublic QueryGenerator() throws Exception {\n-\t\t// TODO (Victor):\n-\t\t//  1. Use parser.Utils to parse user json and create graph.MarkovChain and nodes\n-\t\t//  2. Generate number of queries given in config\n-\t\t//  3. pass to them to Keyword or Skeleton\n-\n-\t\t// create nodes\n-\t\tMap<String, Node<Query>> nodeMap = new HashMap<>();\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDDL), r);\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDML), r);\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDQL), r);\n-\n-\t\t// TODO (Victor): Parse these two helper nodes from user config\n-\t\tnodeMap.put(\"FEATURE_ROOT\", source);\n-\t\tnodeMap.put(\"FEATURE_SINK\", new Node<>(new Query(FeatureType.FEATURE_SINK), r));\n-\n-\t\tMap<String, List<String>> neighborMap = new HashMap<>();\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDDL));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDML));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDQL));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesRoot));\n-\n-\t\tfor (String nodeKey : nodeMap.keySet()) {\n-\t\t\tHashSet<Node<Query>> nodeNeighbors = new HashSet<>();\n-\t\t\tfor (String neighbor : neighborMap.get(nodeKey)) {\n-\t\t\t\tif (nodeMap.keySet().contains(neighbor)) {\n-\t\t\t\t\tnodeNeighbors.add(nodeMap.get(neighbor));\n-\t\t\t\t}\n-\t\t\t\tnodeMap.get(nodeKey).setNeighbors(nodeNeighbors);\n-\t\t\t}\n-\t\t}\n-\n-\t\tmarkovChain = new MarkovChain(new HashSet<Node<Query>>(nodeMap.values()));\n-\t}\n-\n-\t/**\n-\t * generates queries from markov chain starting from root\n-\t */\n-\tpublic void generateQueries(int numberQueries) {\n-\t\tImmutableList.Builder<String> postgreBuilder = ImmutableList.builder();\n-\t\tImmutableList.Builder<String> bigQueryBuilder = ImmutableList.builder();\n-\t\tTokenizer tokenizer = new Tokenizer(r);\n-\n-\t\tint i = 0;\n-\t\twhile (i < numberQueries) {\n-\t\t\tList<Query> rawQueries = markovChain.randomWalk(source);\n-\n-\t\t\tif (rawQueries.get(rawQueries.size()-1).getType() == FeatureType.FEATURE_SINK) {\n-\t\t\t\tList<Query> actualQueries = rawQueries.subList(2, rawQueries.size()-1);\n-\t\t\t\tSkeleton skeleton = new Skeleton(actualQueries, tokenizer);\n-\t\t\t\tpostgreBuilder.add(String.join(\" \", skeleton.getPostgreSkeleton()));\n-\t\t\t\tbigQueryBuilder.add(String.join(\" \", skeleton.getBigQuerySkeleton()));\n-\t\t\t\ti++;\n-\t\t\t}\n-\t\t}\n-\n-\t\tImmutableList<String> postgreSyntax = postgreBuilder.build();\n-\t\tImmutableList<String> bigQuerySyntax = bigQueryBuilder.build();\n-\n-\t\tImmutableMap.Builder<String, ImmutableList<String>> builder = ImmutableMap.builder();\n-\t\tbuilder.put(\"PostgreSQL\", postgreSyntax);\n-\t\tbuilder.put(\"BigQuery\", bigQuerySyntax);\n-\t\tImmutableMap<String, ImmutableList<String>> outputs = builder.build();\n-\n-\t\ttry {\n-\t\t\tUtils.writeDirectory(outputs);\n-\t\t} catch (IOException exception){\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\t}\n-\n-\tprivate Map<String, Node<Query>> addNodeMap(Map<String, Node<Query>> nodeMap, Path input, Random r) {\n-\t\ttry {\n-\t\t\tBufferedReader reader = Files.newBufferedReader(input, UTF_8);\n-\t\t\tGson gson = new Gson();\n-\t\t\tFeatureIndicators featureIndicators = gson.fromJson(reader, FeatureIndicators.class);\n-\n-\t\t\tfor (FeatureIndicator featureIndicator : featureIndicators.getFeatureIndicators()) {\n-\t\t\t\tif (featureIndicator.getIsIncluded()) {\n-\t\t\t\t\tnodeMap.put(featureIndicator.getFeature().name(), new Node<>(new Query(featureIndicator.getFeature()), r));\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} catch (IOException exception) {\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\n-\t\treturn nodeMap;\n-\t}\n-\n-\tprivate Map<String, List<String>> addNeighborMap(Map<String, List<String>> neighborMap, Set<String> nodes, Path input) {\n-\t\ttry {\n-\t\t\tBufferedReader reader = Files.newBufferedReader(input, UTF_8);\n-\t\t\tGson gson = new Gson();\n-\t\t\tDependencies dependencies = gson.fromJson(reader, Dependencies.class);\n-\n-\t\t\tfor (Dependency dependency : dependencies.getDependencies()) {\n-\t\t\t\tif (nodes.contains(dependency.getNode())) {\n-\t\t\t\t\tneighborMap.put(dependency.getNode(), dependency.getNeighbors());\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} catch (IOException exception) {\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\n-\t\treturn neighborMap;\n-\t}\n+  private final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";\n+  private final String filePathConfigDML = \"./src/main/resources/user_config/dml.json\";\n+  private final String filePathConfigDQL = \"./src/main/resources/user_config/dql.json\";\n+  private final String filePathDependenciesRoot = \"./src/main/resources/dialect_config/root_dependencies.json\";\n+  private final String filePathDependenciesDDL = \"./src/main/resources/dialect_config/ddl_dependencies.json\";\n+  private final String filePathDependenciesDML = \"./src/main/resources/dialect_config/dml_dependencies.json\";\n+  private final String filePathDependenciesDQL = \"./src/main/resources/dialect_config/dql_dependencies.json\";\n+\n+  private final MarkovChain<Query> markovChain;\n+  private Random r = new Random();\n+  private Node<Query> source = new Node<>(new Query(FeatureType.FEATURE_ROOT), r);\n+\n+  /**\n+   *\n+   * @throws Exception\n+   */\n+  public QueryGenerator() throws Exception {\n+    // TODO (Victor):\n+    //  1. Use parser.Utils to parse user json and create graph.MarkovChain and nodes\n+    //  2. Generate number of queries given in config\n+    //  3. pass to them to Keyword or Skeleton\n+\n+    // create nodes\n+    Map<String, Node<Query>> nodeMap = new HashMap<>();\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDDL), r);\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDML), r);\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDQL), r);\n+\n+    // TODO (Victor): Parse these two helper nodes from user config\n+    nodeMap.put(\"FEATURE_ROOT\", source);\n+    nodeMap.put(\"FEATURE_SINK\", new Node<>(new Query(FeatureType.FEATURE_SINK), r));\n+\n+    Map<String, List<String>> neighborMap = new HashMap<>();\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDDL));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDML));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDQL));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesRoot));\n+\n+    for (String nodeKey : nodeMap.keySet()) {\n+      HashSet<Node<Query>> nodeNeighbors = new HashSet<>();\n+      for (String neighbor : neighborMap.get(nodeKey)) {\n+        if (nodeMap.keySet().contains(neighbor)) {\n+          nodeNeighbors.add(nodeMap.get(neighbor));\n+        }\n+        nodeMap.get(nodeKey).setNeighbors(nodeNeighbors);\n+      }\n+    }\n+\n+    markovChain = new MarkovChain(new HashSet<Node<Query>>(nodeMap.values()));\n+  }\n+\n+  /**\n+   * generates queries from markov chain starting from root\n+   */\n+  public void generateQueries(int numberQueries) {\n+    ImmutableList.Builder<String> postgreBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<String> bigQueryBuilder = ImmutableList.builder();\n+    Tokenizer tokenizer = new Tokenizer(r);\n+\n+    int i = 0;\n+    while (i < numberQueries) {\n+      List<Query> rawQueries = markovChain.randomWalk(source);\n+\n+      if (rawQueries.get(rawQueries.size()-1).getType() == FeatureType.FEATURE_SINK) {\n+        List<Query> actualQueries = rawQueries.subList(2, rawQueries.size()-1);\n+        Skeleton skeleton = new Skeleton(actualQueries, tokenizer);\n+        postgreBuilder.add(String.join(\" \", skeleton.getPostgreSkeleton()));\n+        bigQueryBuilder.add(String.join(\" \", skeleton.getBigQuerySkeleton()));\n+        i++;\n+      }\n+    }\n+\n+    ImmutableList<String> postgreSyntax = postgreBuilder.build();\n+    ImmutableList<String> bigQuerySyntax = bigQueryBuilder.build();\n+\n+    ImmutableMap.Builder<String, ImmutableList<String>> builder = ImmutableMap.builder();\n+    builder.put(\"PostgreSQL\", postgreSyntax);\n+    builder.put(\"BigQuery\", bigQuerySyntax);\n+    ImmutableMap<String, ImmutableList<String>> outputs = builder.build();\n+\n+    try {\n+      Utils.writeDirectory(outputs);\n+    } catch (IOException exception){\n+      exception.printStackTrace();\n+    }\n+  }\n+\n+  private Map<String, Node<Query>> addNodeMap(Map<String, Node<Query>> nodeMap, Path input, Random r) {\n+    try {\n+      BufferedReader reader = Files.newBufferedReader(input, UTF_8);\n+      Gson gson = new Gson();\n+      FeatureIndicators featureIndicators = gson.fromJson(reader, FeatureIndicators.class);\n+\n+      for (FeatureIndicator featureIndicator : featureIndicators.getFeatureIndicators()) {\n+        if (featureIndicator.getIsIncluded()) {\n+          nodeMap.put(featureIndicator.getFeature().name(), new Node<>(new Query(featureIndicator.getFeature()), r));\n+        }\n+      }\n+    } catch (IOException exception) {\n+      exception.printStackTrace();\n+    }\n+\n+    return nodeMap;\n+  }\n+\n+  private Map<String, List<String>> addNeighborMap(Map<String, List<String>> neighborMap, Set<String> nodes, Path input) {\n+    try {\n+      BufferedReader reader = Files.newBufferedReader(input, UTF_8);\n+      Gson gson = new Gson();\n+      Dependencies dependencies = gson.fromJson(reader, Dependencies.class);\n+\n+      for (Dependency dependency : dependencies.getDependencies()) {\n+        if (nodes.contains(dependency.getNode())) {\n+          neighborMap.put(dependency.getNode(), dependency.getNeighbors());\n+        }\n+      }\n+    } catch (IOException exception) {\n+      exception.printStackTrace();\n+    }\n+\n+    return neighborMap;\n+  }\n \n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA1Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r465410056", "bodyText": "As discussed offline, please consider generalize this file to easier adding configs for other dialects.", "author": "Bei-z", "createdAt": "2020-08-05T00:55:19Z", "path": "tools/template_based_query_generation/src/main/java/QueryGenerator.java", "diffHunk": "@@ -1,31 +1,146 @@\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.gson.Gson;\n+import graph.MarkovChain;\n+import graph.Node;\n+import parser.*;\n+import token.Tokenizer;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.*;\n+\n+import static java.nio.charset.StandardCharsets.UTF_8;\n \n /**\n  * Class that parses config file and creates queries from markov chain\n  */\n public class QueryGenerator {\n \n+\tprivate final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";\n+\tprivate final String filePathConfigDML = \"./src/main/resources/user_config/dml.json\";\n+\tprivate final String filePathConfigDQL = \"./src/main/resources/user_config/dql.json\";\n+\tprivate final String filePathDependenciesRoot = \"./src/main/resources/dialect_config/root_dependencies.json\";\n+\tprivate final String filePathDependenciesDDL = \"./src/main/resources/dialect_config/ddl_dependencies.json\";", "originalCommit": "bb726fb2aa2df0852723d50a881d7361025d403b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "00122b104c00185e78cc9f723bc6357b289da33a", "chunk": "diff --git a/tools/template_based_query_generation/src/main/java/QueryGenerator.java b/tools/template_based_query_generation/src/main/java/QueryGenerator.java\nindex 97f104c..266adc7 100644\n--- a/tools/template_based_query_generation/src/main/java/QueryGenerator.java\n+++ b/tools/template_based_query_generation/src/main/java/QueryGenerator.java\n\n@@ -20,127 +20,127 @@ import static java.nio.charset.StandardCharsets.UTF_8;\n  */\n public class QueryGenerator {\n \n-\tprivate final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";\n-\tprivate final String filePathConfigDML = \"./src/main/resources/user_config/dml.json\";\n-\tprivate final String filePathConfigDQL = \"./src/main/resources/user_config/dql.json\";\n-\tprivate final String filePathDependenciesRoot = \"./src/main/resources/dialect_config/root_dependencies.json\";\n-\tprivate final String filePathDependenciesDDL = \"./src/main/resources/dialect_config/ddl_dependencies.json\";\n-\tprivate final String filePathDependenciesDML = \"./src/main/resources/dialect_config/dml_dependencies.json\";\n-\tprivate final String filePathDependenciesDQL = \"./src/main/resources/dialect_config/dql_dependencies.json\";\n-\n-\tprivate final MarkovChain<Query> markovChain;\n-\tprivate Random r = new Random();\n-\tprivate Node<Query> source = new Node<>(new Query(FeatureType.FEATURE_ROOT), r);\n-\n-\t/**\n-\t *\n-\t * @throws Exception\n-\t */\n-\tpublic QueryGenerator() throws Exception {\n-\t\t// TODO (Victor):\n-\t\t//  1. Use parser.Utils to parse user json and create graph.MarkovChain and nodes\n-\t\t//  2. Generate number of queries given in config\n-\t\t//  3. pass to them to Keyword or Skeleton\n-\n-\t\t// create nodes\n-\t\tMap<String, Node<Query>> nodeMap = new HashMap<>();\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDDL), r);\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDML), r);\n-\t\taddNodeMap(nodeMap, Paths.get(filePathConfigDQL), r);\n-\n-\t\t// TODO (Victor): Parse these two helper nodes from user config\n-\t\tnodeMap.put(\"FEATURE_ROOT\", source);\n-\t\tnodeMap.put(\"FEATURE_SINK\", new Node<>(new Query(FeatureType.FEATURE_SINK), r));\n-\n-\t\tMap<String, List<String>> neighborMap = new HashMap<>();\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDDL));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDML));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDQL));\n-\t\taddNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesRoot));\n-\n-\t\tfor (String nodeKey : nodeMap.keySet()) {\n-\t\t\tHashSet<Node<Query>> nodeNeighbors = new HashSet<>();\n-\t\t\tfor (String neighbor : neighborMap.get(nodeKey)) {\n-\t\t\t\tif (nodeMap.keySet().contains(neighbor)) {\n-\t\t\t\t\tnodeNeighbors.add(nodeMap.get(neighbor));\n-\t\t\t\t}\n-\t\t\t\tnodeMap.get(nodeKey).setNeighbors(nodeNeighbors);\n-\t\t\t}\n-\t\t}\n-\n-\t\tmarkovChain = new MarkovChain(new HashSet<Node<Query>>(nodeMap.values()));\n-\t}\n-\n-\t/**\n-\t * generates queries from markov chain starting from root\n-\t */\n-\tpublic void generateQueries(int numberQueries) {\n-\t\tImmutableList.Builder<String> postgreBuilder = ImmutableList.builder();\n-\t\tImmutableList.Builder<String> bigQueryBuilder = ImmutableList.builder();\n-\t\tTokenizer tokenizer = new Tokenizer(r);\n-\n-\t\tint i = 0;\n-\t\twhile (i < numberQueries) {\n-\t\t\tList<Query> rawQueries = markovChain.randomWalk(source);\n-\n-\t\t\tif (rawQueries.get(rawQueries.size()-1).getType() == FeatureType.FEATURE_SINK) {\n-\t\t\t\tList<Query> actualQueries = rawQueries.subList(2, rawQueries.size()-1);\n-\t\t\t\tSkeleton skeleton = new Skeleton(actualQueries, tokenizer);\n-\t\t\t\tpostgreBuilder.add(String.join(\" \", skeleton.getPostgreSkeleton()));\n-\t\t\t\tbigQueryBuilder.add(String.join(\" \", skeleton.getBigQuerySkeleton()));\n-\t\t\t\ti++;\n-\t\t\t}\n-\t\t}\n-\n-\t\tImmutableList<String> postgreSyntax = postgreBuilder.build();\n-\t\tImmutableList<String> bigQuerySyntax = bigQueryBuilder.build();\n-\n-\t\tImmutableMap.Builder<String, ImmutableList<String>> builder = ImmutableMap.builder();\n-\t\tbuilder.put(\"PostgreSQL\", postgreSyntax);\n-\t\tbuilder.put(\"BigQuery\", bigQuerySyntax);\n-\t\tImmutableMap<String, ImmutableList<String>> outputs = builder.build();\n-\n-\t\ttry {\n-\t\t\tUtils.writeDirectory(outputs);\n-\t\t} catch (IOException exception){\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\t}\n-\n-\tprivate Map<String, Node<Query>> addNodeMap(Map<String, Node<Query>> nodeMap, Path input, Random r) {\n-\t\ttry {\n-\t\t\tBufferedReader reader = Files.newBufferedReader(input, UTF_8);\n-\t\t\tGson gson = new Gson();\n-\t\t\tFeatureIndicators featureIndicators = gson.fromJson(reader, FeatureIndicators.class);\n-\n-\t\t\tfor (FeatureIndicator featureIndicator : featureIndicators.getFeatureIndicators()) {\n-\t\t\t\tif (featureIndicator.getIsIncluded()) {\n-\t\t\t\t\tnodeMap.put(featureIndicator.getFeature().name(), new Node<>(new Query(featureIndicator.getFeature()), r));\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} catch (IOException exception) {\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\n-\t\treturn nodeMap;\n-\t}\n-\n-\tprivate Map<String, List<String>> addNeighborMap(Map<String, List<String>> neighborMap, Set<String> nodes, Path input) {\n-\t\ttry {\n-\t\t\tBufferedReader reader = Files.newBufferedReader(input, UTF_8);\n-\t\t\tGson gson = new Gson();\n-\t\t\tDependencies dependencies = gson.fromJson(reader, Dependencies.class);\n-\n-\t\t\tfor (Dependency dependency : dependencies.getDependencies()) {\n-\t\t\t\tif (nodes.contains(dependency.getNode())) {\n-\t\t\t\t\tneighborMap.put(dependency.getNode(), dependency.getNeighbors());\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} catch (IOException exception) {\n-\t\t\texception.printStackTrace();\n-\t\t}\n-\n-\t\treturn neighborMap;\n-\t}\n+  private final String filePathConfigDDL = \"./src/main/resources/user_config/ddl.json\";\n+  private final String filePathConfigDML = \"./src/main/resources/user_config/dml.json\";\n+  private final String filePathConfigDQL = \"./src/main/resources/user_config/dql.json\";\n+  private final String filePathDependenciesRoot = \"./src/main/resources/dialect_config/root_dependencies.json\";\n+  private final String filePathDependenciesDDL = \"./src/main/resources/dialect_config/ddl_dependencies.json\";\n+  private final String filePathDependenciesDML = \"./src/main/resources/dialect_config/dml_dependencies.json\";\n+  private final String filePathDependenciesDQL = \"./src/main/resources/dialect_config/dql_dependencies.json\";\n+\n+  private final MarkovChain<Query> markovChain;\n+  private Random r = new Random();\n+  private Node<Query> source = new Node<>(new Query(FeatureType.FEATURE_ROOT), r);\n+\n+  /**\n+   *\n+   * @throws Exception\n+   */\n+  public QueryGenerator() throws Exception {\n+    // TODO (Victor):\n+    //  1. Use parser.Utils to parse user json and create graph.MarkovChain and nodes\n+    //  2. Generate number of queries given in config\n+    //  3. pass to them to Keyword or Skeleton\n+\n+    // create nodes\n+    Map<String, Node<Query>> nodeMap = new HashMap<>();\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDDL), r);\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDML), r);\n+    addNodeMap(nodeMap, Paths.get(filePathConfigDQL), r);\n+\n+    // TODO (Victor): Parse these two helper nodes from user config\n+    nodeMap.put(\"FEATURE_ROOT\", source);\n+    nodeMap.put(\"FEATURE_SINK\", new Node<>(new Query(FeatureType.FEATURE_SINK), r));\n+\n+    Map<String, List<String>> neighborMap = new HashMap<>();\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDDL));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDML));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesDQL));\n+    addNeighborMap(neighborMap, nodeMap.keySet(), Paths.get(filePathDependenciesRoot));\n+\n+    for (String nodeKey : nodeMap.keySet()) {\n+      HashSet<Node<Query>> nodeNeighbors = new HashSet<>();\n+      for (String neighbor : neighborMap.get(nodeKey)) {\n+        if (nodeMap.keySet().contains(neighbor)) {\n+          nodeNeighbors.add(nodeMap.get(neighbor));\n+        }\n+        nodeMap.get(nodeKey).setNeighbors(nodeNeighbors);\n+      }\n+    }\n+\n+    markovChain = new MarkovChain(new HashSet<Node<Query>>(nodeMap.values()));\n+  }\n+\n+  /**\n+   * generates queries from markov chain starting from root\n+   */\n+  public void generateQueries(int numberQueries) {\n+    ImmutableList.Builder<String> postgreBuilder = ImmutableList.builder();\n+    ImmutableList.Builder<String> bigQueryBuilder = ImmutableList.builder();\n+    Tokenizer tokenizer = new Tokenizer(r);\n+\n+    int i = 0;\n+    while (i < numberQueries) {\n+      List<Query> rawQueries = markovChain.randomWalk(source);\n+\n+      if (rawQueries.get(rawQueries.size()-1).getType() == FeatureType.FEATURE_SINK) {\n+        List<Query> actualQueries = rawQueries.subList(2, rawQueries.size()-1);\n+        Skeleton skeleton = new Skeleton(actualQueries, tokenizer);\n+        postgreBuilder.add(String.join(\" \", skeleton.getPostgreSkeleton()));\n+        bigQueryBuilder.add(String.join(\" \", skeleton.getBigQuerySkeleton()));\n+        i++;\n+      }\n+    }\n+\n+    ImmutableList<String> postgreSyntax = postgreBuilder.build();\n+    ImmutableList<String> bigQuerySyntax = bigQueryBuilder.build();\n+\n+    ImmutableMap.Builder<String, ImmutableList<String>> builder = ImmutableMap.builder();\n+    builder.put(\"PostgreSQL\", postgreSyntax);\n+    builder.put(\"BigQuery\", bigQuerySyntax);\n+    ImmutableMap<String, ImmutableList<String>> outputs = builder.build();\n+\n+    try {\n+      Utils.writeDirectory(outputs);\n+    } catch (IOException exception){\n+      exception.printStackTrace();\n+    }\n+  }\n+\n+  private Map<String, Node<Query>> addNodeMap(Map<String, Node<Query>> nodeMap, Path input, Random r) {\n+    try {\n+      BufferedReader reader = Files.newBufferedReader(input, UTF_8);\n+      Gson gson = new Gson();\n+      FeatureIndicators featureIndicators = gson.fromJson(reader, FeatureIndicators.class);\n+\n+      for (FeatureIndicator featureIndicator : featureIndicators.getFeatureIndicators()) {\n+        if (featureIndicator.getIsIncluded()) {\n+          nodeMap.put(featureIndicator.getFeature().name(), new Node<>(new Query(featureIndicator.getFeature()), r));\n+        }\n+      }\n+    } catch (IOException exception) {\n+      exception.printStackTrace();\n+    }\n+\n+    return nodeMap;\n+  }\n+\n+  private Map<String, List<String>> addNeighborMap(Map<String, List<String>> neighborMap, Set<String> nodes, Path input) {\n+    try {\n+      BufferedReader reader = Files.newBufferedReader(input, UTF_8);\n+      Gson gson = new Gson();\n+      Dependencies dependencies = gson.fromJson(reader, Dependencies.class);\n+\n+      for (Dependency dependency : dependencies.getDependencies()) {\n+        if (nodes.contains(dependency.getNode())) {\n+          neighborMap.put(dependency.getNode(), dependency.getNeighbors());\n+        }\n+      }\n+    } catch (IOException exception) {\n+      exception.printStackTrace();\n+    }\n+\n+    return neighborMap;\n+  }\n \n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDM4Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r465410382", "bodyText": "Java has it's own pair class. Any reason to create another pair class here?\nhttps://docs.oracle.com/javase/8/javafx/api/javafx/util/Pair.html", "author": "Bei-z", "createdAt": "2020-08-05T00:56:24Z", "path": "tools/template_based_query_generation/src/main/java/parser/Pair.java", "diffHunk": "@@ -0,0 +1,16 @@\n+package parser;\n+\n+public class Pair<F, S> {", "originalCommit": "bb726fb2aa2df0852723d50a881d7361025d403b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA0NjQ0MA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r466046440", "bodyText": "For some reason, Intellij + maven is giving me difficulties with javafx (this was why I decided to do it myself originally). I have deleted the class and used import internal.net.http.common.Pair instead", "author": "AllenWang314", "createdAt": "2020-08-05T22:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDM4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE5ODY0MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/131#discussion_r467198641", "bodyText": "So turns out that package isn't that great to work with either. This one was the one that worked in the end: org.apache.commons.lang3.tuple.MutablePair", "author": "AllenWang314", "createdAt": "2020-08-07T18:19:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDM4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "00122b104c00185e78cc9f723bc6357b289da33a", "chunk": "diff --git a/tools/template_based_query_generation/src/main/java/parser/Pair.java b/tools/template_based_query_generation/src/main/java/parser/Pair.java\ndeleted file mode 100644\nindex a4afb08..0000000\n--- a/tools/template_based_query_generation/src/main/java/parser/Pair.java\n+++ /dev/null\n\n@@ -1,16 +0,0 @@\n-package parser;\n-\n-public class Pair<F, S> {\n-  public F first; //first member of pair\n-  public S second; //second member of pair\n-\n-  public Pair(F first, S second) {\n-    this.first = first;\n-    this.second = second;\n-  }\n-\n-  public String toString() {\n-    return \"(\" + first + \", \" + second + \")\";\n-  }\n-\n-}\n"}}, {"oid": "00122b104c00185e78cc9f723bc6357b289da33a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/00122b104c00185e78cc9f723bc6357b289da33a", "message": "refactored Pairs and fixed indentation", "committedDate": "2020-08-05T22:50:48Z", "type": "commit"}, {"oid": "ff8955b0765d36443a7c254dcefda2f1901caa57", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ff8955b0765d36443a7c254dcefda2f1901caa57", "message": "restructured json for generalizability", "committedDate": "2020-08-06T21:25:13Z", "type": "commit"}, {"oid": "533dfe7df4f16ffad8dff0f42492cd8444dca4b9", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/533dfe7df4f16ffad8dff0f42492cd8444dca4b9", "message": "made edits to correct queries, added pair class correctly", "committedDate": "2020-08-07T21:06:17Z", "type": "commit"}, {"oid": "1c11d2b03d813bb64cbfe3f6bf8c4a8d8728ea03", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/1c11d2b03d813bb64cbfe3f6bf8c4a8d8728ea03", "message": "Merge branch 'query-generation' of https://github.com/AllenWang314/bigquery-utils into query-generation", "committedDate": "2020-08-07T21:06:51Z", "type": "commit"}, {"oid": "c7acd85719e3efb53ed0b7e59549306feb7287da", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/c7acd85719e3efb53ed0b7e59549306feb7287da", "message": "added random date generation", "committedDate": "2020-08-07T21:14:39Z", "type": "commit"}, {"oid": "330b6e33d35d3829d4dd82258145190a37621274", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/330b6e33d35d3829d4dd82258145190a37621274", "message": "refactored DataTypeMap parser for generalizability", "committedDate": "2020-08-07T23:44:30Z", "type": "commit"}, {"oid": "a9f1f902e8ef6e4c1c8f1a19e9a1d3416aebbcf6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a9f1f902e8ef6e4c1c8f1a19e9a1d3416aebbcf6", "message": "fixed merge conflicts", "committedDate": "2020-08-07T23:47:30Z", "type": "commit"}, {"oid": "95c1e0edf9d06b04878fa4f82f33f8a2ee0a60ee", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/95c1e0edf9d06b04878fa4f82f33f8a2ee0a60ee", "message": "generalization refactoring complete", "committedDate": "2020-08-09T22:24:53Z", "type": "commit"}, {"oid": "de9689a6ce7a088e6ff4926e675700e148ddf3d7", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/de9689a6ce7a088e6ff4926e675700e148ddf3d7", "message": "Merge branch 'master' into query-generation", "committedDate": "2020-08-09T22:25:40Z", "type": "commit"}]}