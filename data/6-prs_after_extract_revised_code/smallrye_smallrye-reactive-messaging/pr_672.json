{"pr_number": 672, "pr_title": "Set fetch size to before pause value when resuming kafka source", "pr_createdAt": "2020-07-20T07:10:50Z", "pr_url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/672", "timeline": [{"oid": "8752c5943fb561d653d9bb9f187895781d2f3c7d", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/8752c5943fb561d653d9bb9f187895781d2f3c7d", "message": "Set fetch size to before pause value when resuming kafka source", "committedDate": "2020-07-20T07:10:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyMDg0Nw==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/672#discussion_r457120847", "bodyText": "Why 128?", "author": "cescoffier", "createdAt": "2020-07-20T07:14:42Z", "path": "smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java", "diffHunk": "@@ -129,6 +129,10 @@ public KafkaSource(Vertx vertx,\n                                                     \"10000\")))\n                             + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n \n+                    // When resuming the consumer we must set the fetch amount to what it was before pausing\n+                    // When partitions is greater than 1 this value will be 128 otherwise it will be 1.\n+                    final int resumeFetchAmount = config.getPartitions() > 1 ? 128 : 1;", "originalCommit": "8752c5943fb561d653d9bb9f187895781d2f3c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE0MzgxNg==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/672#discussion_r457143816", "bodyText": "When partitions is greater than 1 the initial fetch is the concurrency value from MultiMerge which by default is 128.", "author": "pcasaes", "createdAt": "2020-07-20T07:49:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyMDg0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "chunk": "diff --git a/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java b/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\nindex 362911434..1a10afdaf 100644\n--- a/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\n+++ b/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\n\n@@ -111,56 +129,60 @@ public class KafkaSource<K, V> {\n                         return Optional.of(rebalanceFromGroupListeners.get());\n                     }\n                     return Optional.empty();\n-                })\n-                .ifPresent(listener -> {\n-                    // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n-                    // re-balance. To do so we must wait until after the poll interval time or\n-                    // poll interval time + session timeout if group instance id is not null.\n-                    // We will retry the re-balance consumer listener on failure using an exponential backoff until\n-                    // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n-                    // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n-                    // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n-                    // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n-                    final long consumerReEnableWaitTime = Long.parseLong(\n-                            kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n-                            + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n-                                    : Long.parseLong(\n-                                            kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n-                                                    \"10000\")))\n-                            + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n-\n-                    // When resuming the consumer we must set the fetch amount to what it was before pausing\n-                    // When partitions is greater than 1 this value will be 128 otherwise it will be 1.\n-                    final int resumeFetchAmount = config.getPartitions() > 1 ? 128 : 1;\n-\n-                    kafkaConsumer.partitionsAssignedHandler(set -> {\n-                        kafkaConsumer.pause();\n-                        log.executingConsumerAssignedRebalanceListener(group);\n-                        listener.onPartitionsAssigned(kafkaConsumer, set)\n-                                .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n-                                .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n-                                .expireIn(consumerReEnableWaitTime)\n-                                .subscribe()\n-                                .with(\n-                                        a -> {\n-                                            log.executedConsumerAssignedRebalanceListener(group);\n-                                            kafkaConsumer.fetch(resumeFetchAmount);\n-                                        },\n-                                        t -> {\n-                                            log.reEnablingConsumerforGroup(group);\n-                                            kafkaConsumer.fetch(resumeFetchAmount);\n-                                        });\n-                    });\n-\n-                    kafkaConsumer.partitionsRevokedHandler(set -> {\n-                        log.executingConsumerRevokedRebalanceListener(group);\n-                        listener.onPartitionsRevoked(kafkaConsumer, set)\n-                                .subscribe()\n-                                .with(\n-                                        a -> log.executedConsumerRevokedRebalanceListener(group),\n-                                        t -> log.unableToExecuteConsumerRevokedRebalanceListener(group, t));\n-                    });\n                 });\n+\n+        if (rebalanceListener.isPresent()) {\n+            KafkaConsumerRebalanceListener listener = rebalanceListener.get();\n+            // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n+            // re-balance. To do so we must wait until after the poll interval time or\n+            // poll interval time + session timeout if group instance id is not null.\n+            // We will retry the re-balance consumer listener on failure using an exponential backoff until\n+            // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n+            // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n+            // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n+            // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n+            final long consumerReEnableWaitTime = Long.parseLong(\n+                    kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n+                    + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n+                            : Long.parseLong(\n+                                    kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n+                                            \"10000\")))\n+                    + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n+\n+            kafkaConsumer.partitionsAssignedHandler(set -> {\n+                final long currentDemand = kafkaConsumer.demand();\n+                kafkaConsumer.pause();\n+                log.executingConsumerAssignedRebalanceListener(group);\n+                listener.onPartitionsAssigned(kafkaConsumer, set)\n+                        .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n+                        .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n+                        .expireIn(consumerReEnableWaitTime)\n+                        .subscribe()\n+                        .with(\n+                                a -> {\n+                                    log.executedConsumerAssignedRebalanceListener(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);\n+                                    kafkaConsumer.fetch(currentDemand);\n+                                },\n+                                t -> {\n+                                    log.reEnablingConsumerforGroup(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);\n+                                    kafkaConsumer.fetch(currentDemand);\n+                                });\n+            });\n+\n+            kafkaConsumer.partitionsRevokedHandler(set -> {\n+                log.executingConsumerRevokedRebalanceListener(group);\n+                listener.onPartitionsRevoked(kafkaConsumer, set)\n+                        .subscribe()\n+                        .with(\n+                                a -> log.executedConsumerRevokedRebalanceListener(group),\n+                                t -> log.unableToExecuteConsumerRevokedRebalanceListener(group, t));\n+            });\n+        } else {\n+            kafkaConsumer.partitionsAssignedHandler(set -> commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set));\n+        }\n+\n         this.consumer = kafkaConsumer;\n \n         failureHandler = createFailureHandler(config, vertx, kafkaConfiguration);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyMTMxOQ==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/672#discussion_r457121319", "bodyText": "What happens when the amount is consumed? Would it get more?", "author": "cescoffier", "createdAt": "2020-07-20T07:15:23Z", "path": "smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java", "diffHunk": "@@ -140,11 +144,11 @@ public KafkaSource(Vertx vertx,\n                                 .with(\n                                         a -> {\n                                             log.executedConsumerAssignedRebalanceListener(group);\n-                                            kafkaConsumer.resume();\n+                                            kafkaConsumer.fetch(resumeFetchAmount);", "originalCommit": "8752c5943fb561d653d9bb9f187895781d2f3c7d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzE0Njg4Nw==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/672#discussion_r457146887", "bodyText": "It will get more. That's the current behavior without the consumer re-balance.", "author": "pcasaes", "createdAt": "2020-07-20T07:53:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzEyMTMxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "chunk": "diff --git a/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java b/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\nindex 362911434..1a10afdaf 100644\n--- a/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\n+++ b/smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java\n\n@@ -111,56 +129,60 @@ public class KafkaSource<K, V> {\n                         return Optional.of(rebalanceFromGroupListeners.get());\n                     }\n                     return Optional.empty();\n-                })\n-                .ifPresent(listener -> {\n-                    // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n-                    // re-balance. To do so we must wait until after the poll interval time or\n-                    // poll interval time + session timeout if group instance id is not null.\n-                    // We will retry the re-balance consumer listener on failure using an exponential backoff until\n-                    // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n-                    // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n-                    // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n-                    // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n-                    final long consumerReEnableWaitTime = Long.parseLong(\n-                            kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n-                            + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n-                                    : Long.parseLong(\n-                                            kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n-                                                    \"10000\")))\n-                            + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n-\n-                    // When resuming the consumer we must set the fetch amount to what it was before pausing\n-                    // When partitions is greater than 1 this value will be 128 otherwise it will be 1.\n-                    final int resumeFetchAmount = config.getPartitions() > 1 ? 128 : 1;\n-\n-                    kafkaConsumer.partitionsAssignedHandler(set -> {\n-                        kafkaConsumer.pause();\n-                        log.executingConsumerAssignedRebalanceListener(group);\n-                        listener.onPartitionsAssigned(kafkaConsumer, set)\n-                                .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n-                                .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n-                                .expireIn(consumerReEnableWaitTime)\n-                                .subscribe()\n-                                .with(\n-                                        a -> {\n-                                            log.executedConsumerAssignedRebalanceListener(group);\n-                                            kafkaConsumer.fetch(resumeFetchAmount);\n-                                        },\n-                                        t -> {\n-                                            log.reEnablingConsumerforGroup(group);\n-                                            kafkaConsumer.fetch(resumeFetchAmount);\n-                                        });\n-                    });\n-\n-                    kafkaConsumer.partitionsRevokedHandler(set -> {\n-                        log.executingConsumerRevokedRebalanceListener(group);\n-                        listener.onPartitionsRevoked(kafkaConsumer, set)\n-                                .subscribe()\n-                                .with(\n-                                        a -> log.executedConsumerRevokedRebalanceListener(group),\n-                                        t -> log.unableToExecuteConsumerRevokedRebalanceListener(group, t));\n-                    });\n                 });\n+\n+        if (rebalanceListener.isPresent()) {\n+            KafkaConsumerRebalanceListener listener = rebalanceListener.get();\n+            // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n+            // re-balance. To do so we must wait until after the poll interval time or\n+            // poll interval time + session timeout if group instance id is not null.\n+            // We will retry the re-balance consumer listener on failure using an exponential backoff until\n+            // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n+            // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n+            // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n+            // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n+            final long consumerReEnableWaitTime = Long.parseLong(\n+                    kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n+                    + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n+                            : Long.parseLong(\n+                                    kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n+                                            \"10000\")))\n+                    + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n+\n+            kafkaConsumer.partitionsAssignedHandler(set -> {\n+                final long currentDemand = kafkaConsumer.demand();\n+                kafkaConsumer.pause();\n+                log.executingConsumerAssignedRebalanceListener(group);\n+                listener.onPartitionsAssigned(kafkaConsumer, set)\n+                        .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n+                        .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n+                        .expireIn(consumerReEnableWaitTime)\n+                        .subscribe()\n+                        .with(\n+                                a -> {\n+                                    log.executedConsumerAssignedRebalanceListener(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);\n+                                    kafkaConsumer.fetch(currentDemand);\n+                                },\n+                                t -> {\n+                                    log.reEnablingConsumerforGroup(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);\n+                                    kafkaConsumer.fetch(currentDemand);\n+                                });\n+            });\n+\n+            kafkaConsumer.partitionsRevokedHandler(set -> {\n+                log.executingConsumerRevokedRebalanceListener(group);\n+                listener.onPartitionsRevoked(kafkaConsumer, set)\n+                        .subscribe()\n+                        .with(\n+                                a -> log.executedConsumerRevokedRebalanceListener(group),\n+                                t -> log.unableToExecuteConsumerRevokedRebalanceListener(group, t));\n+            });\n+        } else {\n+            kafkaConsumer.partitionsAssignedHandler(set -> commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set));\n+        }\n+\n         this.consumer = kafkaConsumer;\n \n         failureHandler = createFailureHandler(config, vertx, kafkaConfiguration);\n"}}, {"oid": "194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "message": "Set fetch size to before pause value when resuming kafka source", "committedDate": "2020-09-19T04:54:21Z", "type": "commit"}, {"oid": "194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/194fc2c961cacfb5f0395f1ad1a6fb1d6e47c8bc", "message": "Set fetch size to before pause value when resuming kafka source", "committedDate": "2020-09-19T04:54:21Z", "type": "forcePushed"}]}