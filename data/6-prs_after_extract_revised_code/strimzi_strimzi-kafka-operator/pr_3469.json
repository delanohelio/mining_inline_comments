{"pr_number": 3469, "pr_title": "Dynamcially changeable logging levels in Kafka", "pr_createdAt": "2020-08-06T14:10:45Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY1ODkzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r466658938", "bodyText": "I'm not sure I understand the naming and the Javadoc here. If you add something to the map, it is not updated yet so the parameter name is wrong? I'm also not sure why a separate method is needed here. It looks like any call for this method can be done in single line.", "author": "scholzj", "createdAt": "2020-08-06T20:10:53Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -111,10 +112,11 @@ private boolean isEntryReadOnly(ConfigEntry entry) {\n     }\n \n     /**\n-     * @return A map which can be used for dynamic configuration of kafka broker\n+     * Adds an entry to a map which can be used for dynamic configuration of kafka broker\n+     * @param updatedConfig map to add an entry\n      */\n-    public Map<ConfigResource, Collection<AlterConfigOp>> getConfigDiff() {\n-        return Collections.singletonMap(Util.getBrokersConfig(brokerId), diff);\n+    public void addConfigDiff(Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig) {", "originalCommit": "88e9fdb021e0332369ca3b84a2b8a87329b90792", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7e36673afedf918dcdb9dd5db0df189ce352b4e", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java\nindex 4b1d7054d4..638e5f63da 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java\n\n@@ -112,11 +110,11 @@ public class KafkaBrokerConfigurationDiff extends AbstractResourceDiff {\n     }\n \n     /**\n-     * Adds an entry to a map which can be used for dynamic configuration of kafka broker\n-     * @param updatedConfig map to add an entry\n+     * Returns configuration difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired configuration\n      */\n-    public void addConfigDiff(Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig) {\n-        updatedConfig.put(Util.getBrokersConfig(brokerId), diff);\n+    public Collection<AlterConfigOp> getConfigDiff() {\n+        return diff;\n     }\n \n     /**\n"}}, {"oid": "b7e36673afedf918dcdb9dd5db0df189ce352b4e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/b7e36673afedf918dcdb9dd5db0df189ce352b4e", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T08:49:44Z", "type": "forcePushed"}, {"oid": "703cf1db7e2ab15d31ea5bd8a5f40bec90c572e0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/703cf1db7e2ab15d31ea5bd8a5f40bec90c572e0", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T09:07:18Z", "type": "forcePushed"}, {"oid": "97dfcf4427fa1032d0ea0f0a456a603f17c42585", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/97dfcf4427fa1032d0ea0f0a456a603f17c42585", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T10:09:37Z", "type": "forcePushed"}, {"oid": "5dde48c57ebd145860e333bcd33b20c40869e1d8", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5dde48c57ebd145860e333bcd33b20c40869e1d8", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-10T10:33:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r467919163", "bodyText": "Can you please move these tests before the @BeforeAll (after the bridge logging test)?", "author": "im-konge", "createdAt": "2020-08-10T13:53:13Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java", "diffHunk": "@@ -472,4 +472,212 @@ protected void tearDownEnvironmentAfterAll() {\n     protected void assertNoCoErrorsLogged(long sinceSeconds) {\n         LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n     }\n+\n+    @Test\n+    void testDynamicallySetKafkaLoggingLevels() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging ilOff = new InlineLogging();\n+        ilOff.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"INFO\"));\n+\n+        LOGGER.info(\"Changing rootLogger level to DEBUG with inline logging\");\n+        InlineLogging ilDebug = new InlineLogging();\n+        ilDebug.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"DEBUG\"));\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(ilDebug);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                .contains(\"root=DEBUG\"));\n+\n+        LOGGER.info(\"Setting external logging INFO\");\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-configmap\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+\n+        ExternalLogging elKafka = new ExternalLoggingBuilder()\n+                .withName(\"external-configmap\")\n+                .build();\n+\n+        LOGGER.info(\"Setting log level of kafka INFO\");\n+        // change to external logging\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(elKafka);\n+        });\n+\n+        LOGGER.info(\"Waiting for dynamic change in the kafka pod\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                    \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                    .contains(\"root=INFO\"));\n+\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLogger() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.logger.paprika\", \"INFO\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"paprika=INFO\"));\n+    }\n+\n+    @Test\n+    void testDynamicallySetUnknownKafkaLoggerValue() {\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1).done();\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        InlineLogging il = new InlineLogging();\n+        il.setLoggers(Collections.singletonMap(\"log4j.rootLogger\", \"PAPRIKA\"));\n+\n+        KafkaResource.replaceKafkaResource(CLUSTER_NAME, k -> {\n+            k.getSpec().getKafka().setLogging(il);\n+        });\n+\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+    }\n+\n+    @Test\n+    void testDynamicallySetKafkaExternalLogging() {\n+        ConfigMap configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=INFO\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=INFO\\n\" +\n+                        \"log4j.logger.kafka=INFO\\n\" +\n+                        \"log4j.logger.org.apache.kafka=INFO\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=WARN\\n\" +\n+                        \"log4j.logger.kafka.controller=TRACE\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=INFO\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=INFO\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        ExternalLogging el = new ExternalLogging();\n+        el.setName(\"external-cm\");\n+\n+        KafkaResource.kafkaPersistent(CLUSTER_NAME, 3, 1)\n+                .editOrNewSpec()\n+                    .editKafka()\n+                        .withExternalLogging(el)\n+                    .endKafka()\n+                .endSpec()\n+                .done();\n+\n+        String kafkaName = KafkaResources.kafkaStatefulSetName(CLUSTER_NAME);\n+        Map<String, String> kafkaPods = StatefulSetUtils.ssSnapshot(kafkaName);\n+\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]%n\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=ERROR\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitForNoRollingUpdate(kafkaName, kafkaPods);\n+        assertThat(\"Kafka pod should not roll\", StatefulSetUtils.ssHasRolled(kafkaName, kafkaPods), is(false));\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=ERROR\"));\n+\n+        // log4j.appender.CONSOLE.layout.ConversionPattern is changed and thus we need RU\n+        configMap = new ConfigMapBuilder()\n+                .withNewMetadata()\n+                .withName(\"external-cm\")\n+                .withNamespace(NAMESPACE)\n+                .endMetadata()\n+                .withData(Collections.singletonMap(\"log4j.properties\", \"log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender\\n\" +\n+                        \"log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout\\n\" +\n+                        \"log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} %p %m (%c) [%t]\\n\" +\n+                        \"log4j.rootLogger=INFO, CONSOLE\\n\" +\n+                        \"log4j.logger.org.I0Itec.zkclient.ZkClient=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.zookeeper=ERROR\\n\" +\n+                        \"log4j.logger.kafka=ERROR\\n\" +\n+                        \"log4j.logger.org.apache.kafka=ERROR\\n\" +\n+                        \"log4j.logger.kafka.request.logger=WARN\\n\" +\n+                        \"log4j.logger.kafka.network.Processor=ERROR\\n\" +\n+                        \"log4j.logger.kafka.server.KafkaApis=ERROR\\n\" +\n+                        \"log4j.logger.kafka.network.RequestChannel$=ERROR\\n\" +\n+                        \"log4j.logger.kafka.controller=ERROR\\n\" +\n+                        \"log4j.logger.kafka.log.LogCleaner=ERROR\\n\" +\n+                        \"log4j.logger.state.change.logger=TRACE\\n\" +\n+                        \"log4j.logger.kafka.authorizer.logger=DEBUG\"))\n+                .build();\n+\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(configMap);\n+        StatefulSetUtils.waitTillSsHasRolled(kafkaName, kafkaPods);\n+\n+        TestUtils.waitFor(\"Verify logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPodContainer(KafkaResources.kafkaPodName(CLUSTER_NAME, 0),\n+                        \"kafka\", \"/bin/bash\", \"-c\", \"bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type broker-loggers --entity-name 0\").out()\n+                        .contains(\"kafka.authorizer.logger=DEBUG\"));\n+    }", "originalCommit": "5dde48c57ebd145860e333bcd33b20c40869e1d8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3OTA3OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468179078", "bodyText": "Do the STs have BeforeAll methods at the end of the class?", "author": "scholzj", "createdAt": "2020-08-10T20:52:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1ODg2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468458863", "bodyText": "They should have \ud83d\ude04 at least for me it's more readable. I think that some STs are little bit mixed, but I'm gonna fix this in some future PRs :)", "author": "im-konge", "createdAt": "2020-08-11T09:46:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODQ1OTIwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r468459200", "bodyText": "Or do you think that BeforeAll should be on the beginning of the class?", "author": "im-konge", "createdAt": "2020-08-11T09:47:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzkxOTE2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e19cef5d56736102267da6bdea22231921ce6499", "chunk": "diff --git a/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java b/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java\nindex f1dc25cc12..25294a9567 100644\n--- a/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java\n+++ b/systemtest/src/test/java/io/strimzi/systemtest/log/LoggingChangeST.java\n\n@@ -456,21 +457,97 @@ class LoggingChangeST extends AbstractST {\n         assertThat(\"Bridge pod should not roll\", DeploymentUtils.depSnapshot(KafkaBridgeResources.deploymentName(CLUSTER_NAME)), equalTo(bridgeSnapshot));\n     }\n \n-    @BeforeAll\n-    void setup() throws Exception {\n-        ResourceManager.setClassResources();\n-        installClusterOperator(NAMESPACE);\n-    }\n+    @Test\n+    void testDynamicallySetClusterOperatorLoggingLevels() throws InterruptedException {\n+        Map<String, String> coPod = DeploymentUtils.depSnapshot(STRIMZI_DEPLOYMENT_NAME);\n+        String coPodName = kubeClient().listPodsByPrefixInName(STRIMZI_DEPLOYMENT_NAME).get(0).getMetadata().getName();\n+        String command = \"cat /opt/strimzi/custom-config/log4j2.properties\";\n+\n+        String log4jConfig =\n+            \"name = COConfig\\n\" +\n+            \"monitorInterval = 30\\n\" +\n+            \"\\n\" +\n+            \"    appender.console.type = Console\\n\" +\n+            \"    appender.console.name = STDOUT\\n\" +\n+            \"    appender.console.layout.type = PatternLayout\\n\" +\n+            \"    appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n\\n\" +\n+            \"\\n\" +\n+            \"    rootLogger.level = OFF\\n\" +\n+            \"    rootLogger.appenderRefs = stdout\\n\" +\n+            \"    rootLogger.appenderRef.console.ref = STDOUT\\n\" +\n+            \"    rootLogger.additivity = false\\n\" +\n+            \"\\n\" +\n+            \"    # Kafka AdminClient logging is a bit noisy at INFO level\\n\" +\n+            \"    logger.kafka.name = org.apache.kafka\\n\" +\n+            \"    logger.kafka.level = OFF\\n\" +\n+            \"    logger.kafka.additivity = false\\n\" +\n+            \"\\n\" +\n+            \"    # Zookeeper is very verbose even on INFO level -> We set it to WARN by default\\n\" +\n+            \"    logger.zookeepertrustmanager.name = org.apache.zookeeper\\n\" +\n+            \"    logger.zookeepertrustmanager.level = OFF\\n\" +\n+            \"    logger.zookeepertrustmanager.additivity = false\";\n+\n+        ConfigMap coMap = new ConfigMapBuilder()\n+            .withNewMetadata()\n+                .addToLabels(\"app\", \"strimzi\")\n+                .withName(STRIMZI_DEPLOYMENT_NAME)\n+                .withNamespace(NAMESPACE)\n+            .endMetadata()\n+            .withData(Collections.singletonMap(\"log4j2.properties\", log4jConfig))\n+            .build();\n \n-    @Override\n-    protected void tearDownEnvironmentAfterAll() {\n-        teardownEnvForOperator();\n-    }\n+        LOGGER.info(\"Checking that original logging config is different from the new one\");\n+        assertThat(log4jConfig, not(equalTo(cmdKubeClient().execInPod(coPodName, \"/bin/bash\", \"-c\", command).out().trim())));\n \n+        LOGGER.info(\"Changing logging for cluster-operator\");\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(coMap);\n \n-    @Override\n-    protected void assertNoCoErrorsLogged(long sinceSeconds) {\n-        LOGGER.info(\"Skipping assertion if CO has some unexpected errors\");\n+        LOGGER.info(\"Waiting for log4j2.properties will contain desired settings\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPod(coPodName, \"/bin/bash\", \"-c\", command).out().contains(\"rootLogger.level = OFF\")\n+        );\n+\n+        LOGGER.info(\"Checking log4j2.properties in CO pod\");\n+        String podLogConfig = cmdKubeClient().execInPod(coPodName, \"/bin/bash\", \"-c\", command).out().trim();\n+        assertThat(podLogConfig, equalTo(log4jConfig));\n+\n+        LOGGER.info(\"Checking if CO rolled its pod\");\n+        assertThat(coPod, equalTo(DeploymentUtils.depSnapshot(STRIMZI_DEPLOYMENT_NAME)));\n+\n+        LOGGER.info(\"Waiting {} ms log to be empty\", LOGGING_RELOADING_INTERVAL);\n+        // wait some time and check whether logs after this time are empty\n+        Thread.sleep(LOGGING_RELOADING_INTERVAL);\n+\n+        LOGGER.info(\"Asserting if log will contain no records\");\n+        assertThat(StUtils.getLogFromPodByTime(coPodName, STRIMZI_DEPLOYMENT_NAME, \"30s\"), is(emptyString()));\n+\n+        LOGGER.info(\"Changing all levels from OFF to INFO/WARN\");\n+        log4jConfig = log4jConfig.replaceAll(\"OFF\", \"INFO\");\n+        coMap.setData(Collections.singletonMap(\"log4j2.properties\", log4jConfig));\n+\n+        LOGGER.info(\"Changing logging for cluster-operator\");\n+        kubeClient().getClient().configMaps().inNamespace(NAMESPACE).createOrReplace(coMap);\n+\n+        LOGGER.info(\"Waiting for log4j2.properties will contain desired settings\");\n+        TestUtils.waitFor(\"Logger change\", Constants.GLOBAL_POLL_INTERVAL, Constants.GLOBAL_TIMEOUT,\n+            () -> cmdKubeClient().execInPod(coPodName, \"/bin/bash\", \"-c\", command).out().contains(\"rootLogger.level = INFO\")\n+        );\n+\n+        LOGGER.info(\"Checking log4j2.properties in CO pod\");\n+        podLogConfig = cmdKubeClient().execInPod(coPodName, \"/bin/bash\", \"-c\", command).out().trim();\n+        assertThat(podLogConfig, equalTo(log4jConfig));\n+\n+        LOGGER.info(\"Checking if CO rolled its pod\");\n+        assertThat(coPod, equalTo(DeploymentUtils.depSnapshot(STRIMZI_DEPLOYMENT_NAME)));\n+\n+        LOGGER.info(\"Waiting {} ms log not to be empty\", LOGGING_RELOADING_INTERVAL);\n+        // wait some time and check whether logs after this time are not empty\n+        Thread.sleep(LOGGING_RELOADING_INTERVAL);\n+\n+        LOGGER.info(\"Asserting if log will contain no records\");\n+        String coLog = StUtils.getLogFromPodByTime(coPodName, STRIMZI_DEPLOYMENT_NAME, \"30s\");\n+        assertThat(coLog, is(not(emptyString())));\n+        assertThat(coLog.contains(\"INFO\"), is(true));\n     }\n \n     @Test\n"}}, {"oid": "e19cef5d56736102267da6bdea22231921ce6499", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/e19cef5d56736102267da6bdea22231921ce6499", "message": "comment\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-12T13:26:54Z", "type": "forcePushed"}, {"oid": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/c7022f109100a8ee2e2b96a3574380f89c20eeee", "message": "simpification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-17T07:56:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwNzQzMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471307430", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender\")) {\n          \n          \n            \n                        if (entry.getKey().startsWith(\"log4j.appender.\")) {", "author": "tombentley", "createdAt": "2020-08-17T08:00:22Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {\n+        OrderedProperties ops = new OrderedProperties();\n+        ops.addStringPairs(loggingConfiguration);\n+        StringBuilder result = new StringBuilder();\n+        for (Map.Entry<String, String> entry: ops.asMap().entrySet()) {\n+            if (entry.getKey().startsWith(\"log4j.appender\")) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\nindex 53b85c7b76..14b0ed21bc 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\n\n@@ -3474,39 +3473,8 @@ public class KafkaAssemblyOperator extends AbstractAssemblyOperator<KubernetesCl\n \n     }\n \n-    private String getLoggingAppenders(String loggingConfiguration) {\n-        OrderedProperties ops = new OrderedProperties();\n-        ops.addStringPairs(loggingConfiguration);\n-        StringBuilder result = new StringBuilder();\n-        for (Map.Entry<String, String> entry: ops.asMap().entrySet()) {\n-            if (entry.getKey().startsWith(\"log4j.appender\")) {\n-                result.append(entry.getKey()).append(\"=\").append(entry.getValue());\n-            }\n-        }\n-        return result.toString();\n-    }\n-\n     /* test */ Date dateSupplier() {\n         return new Date();\n     }\n \n-    private String getStringHash(String toBeHashed)  {\n-        try {\n-            MessageDigest hashFunc = MessageDigest.getInstance(\"SHA-512\");\n-\n-            byte[] hash = hashFunc.digest(toBeHashed.getBytes(StandardCharsets.UTF_8));\n-\n-            StringBuffer stringHash = new StringBuffer();\n-\n-            for (int i = 0; i < hash.length; i++) {\n-                String hex = Integer.toHexString(0xff & hash[i]);\n-                if (hex.length() == 1) stringHash.append('0');\n-                stringHash.append(hex);\n-            }\n-\n-            return stringHash.toString();\n-        } catch (NoSuchAlgorithmException e)    {\n-            throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n-        }\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODI5Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308297", "bodyText": "Factor out patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true) into a constant.", "author": "tombentley", "createdAt": "2020-08-17T08:02:02Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerConfigurationDiff.java", "diffHunk": "@@ -159,8 +159,8 @@ private static boolean isIgnorableProperty(String key) {\n \n         fillPlaceholderValue(desiredMap, \"STRIMZI_BROKER_ID\", Integer.toString(brokerId));\n \n-        JsonNode source = patchMapper().valueToTree(currentMap);\n-        JsonNode target = patchMapper().valueToTree(desiredMap);\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMwODc3Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471308776", "bodyText": "And use the constant here.", "author": "tombentley", "createdAt": "2020-08-17T08:03:04Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex c11957dfb4..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -9,18 +9,18 @@ import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMTUxNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471311515", "bodyText": "Any reason why you can't change this in the desired map like you do for the ones starting log4j.logger?", "author": "tombentley", "createdAt": "2020-08-17T08:08:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex c11957dfb4..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -9,18 +9,18 @@ import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMjgyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471312823", "bodyText": "More efficient to use indexOf(\",\") + trim(). Both split and replaceAll will have to use Pattern under the hood.", "author": "tombentley", "createdAt": "2020-08-17T08:11:17Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex c11957dfb4..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -9,18 +9,18 @@ import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzYyMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313623", "bodyText": "What do you think I'm going to tell you about these two methods?", "author": "tombentley", "createdAt": "2020-08-17T08:12:42Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(currentMap);\n+        JsonNode target = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                        String level = parseLogLevelFromAppenderCouple(desiredMap.get(pathValueWithoutSlash));\n+                        if (!level.equals(currentMap.get(\"root\"))) {\n+                            updateOrAddRoot(level, updatedCE);\n+                        }\n+                    } else {\n+                        // entry is not in the current, it is added\n+                        updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                    }\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String lev) {\n+        String[] arr = lev.split(\",\");\n+        String level = arr[0].replaceAll(\"\\\\s\", \"\");\n+        return level;\n+    }\n+\n+    private static void updateOrAddRoot(String level, Collection<AlterConfigOp> updatedCE) {\n+        level = parseLogLevelFromAppenderCouple(level);\n+        if (isValidLoggerLevel(level)) {\n+            updatedCE.add(new AlterConfigOp(new ConfigEntry(\"root\", level), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in current or has deprecated value. Setting to {}\", \"root\", level);\n+        } else {\n+            log.warn(\"Level {} is not valid logging level\", level);\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex c11957dfb4..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -9,18 +9,18 @@ import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxMzk1OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471313959", "bodyText": "It doesn't return a Future.", "author": "tombentley", "createdAt": "2020-08-17T08:13:18Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -433,22 +444,44 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns a Future which completes with the logging of the given broker.", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\nindex a84d3ceb5d..c4e955a3b1 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\n\n@@ -468,8 +468,8 @@ public class KafkaRoller {\n     protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff, KafkaBrokerLoggingConfigurationDiff logDiff)\n             throws ForceableProblem, InterruptedException {\n         Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig = new HashMap<>(2);\n-        updatedConfig.put(Util.getBrokersConfig(podId), configurationDiff.getConfigDiff());\n-        updatedConfig.put(Util.getBrokersLogging(podId), logDiff.getLoggingDiff());\n+        configurationDiff.addConfigDiff(updatedConfig);\n+        logDiff.addLoggingDiff(updatedConfig);\n \n         log.info(\"{}: Altering broker {} with {}\", reconciliation, podId, updatedConfig);\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzEwMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317103", "bodyText": "TBH, I'm not completely convinced it's worth adding the dynamic logging changes to the KafkaRoller. KafkaRoller's job is to safely orchestrate the restart or reconfiguration of some or all brokers in the cluster. Changing a broker's logging doesn't seem to me to be a very risky thing to do. It's not going to need any special pre- or post-conditions before/after happening, for example. I can see that from a certain PoV it makes sense to do it here, but the question is whether adding extra complexity to KafkaRoller to do it here really makes sense, compared with some other phase of the reconciliation which updates the logging after brokers have been restarted/reconfigured.", "author": "tombentley", "createdAt": "2020-08-17T08:18:47Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -271,13 +274,15 @@ public String toString() {\n         private final boolean needsRestart;\n         private final boolean needsReconfig;\n         private final KafkaBrokerConfigurationDiff diff;\n+        private final KafkaBrokerLoggingConfigurationDiff logDiff;\n         private final Admin adminClient;\n \n-        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff) {\n+        public RestartPlan(Admin adminClient, boolean needsRestart, boolean needsReconfig, KafkaBrokerConfigurationDiff diff, KafkaBrokerLoggingConfigurationDiff logDiff) {", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471317847", "bodyText": "Do we need this? It seems the annotation is added, but never used? If we do need it then I think a little Javadoc about how it's used wouldn't go amiss.", "author": "tombentley", "createdAt": "2020-08-17T08:20:10Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -110,6 +110,7 @@\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "originalCommit": "c7022f109100a8ee2e2b96a3574380f89c20eeee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM1NjYxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471356614", "bodyText": "It is used in KAO", "author": "sknot-rh", "createdAt": "2020-08-17T09:31:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Mzg5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471363894", "bodyText": "Yeah, it's added there, (https://github.com/strimzi/strimzi-kafka-operator/pull/3469/files#diff-f19c7c3bdf294affc86939228666be84R2458), but never read. Why is it needed if it's never read? Is it simply so that we can tell that the logging was changed? If so then add a comment explaining how that works.", "author": "tombentley", "createdAt": "2020-08-17T09:44:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2Njk5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471366994", "bodyText": "It changes the annotation an thus kafka pods are rolled (appenders are not changeable dynamically). I will add a doc.", "author": "sknot-rh", "createdAt": "2020-08-17T09:50:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTM2OTUyOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r471369528", "bodyText": "Thanks, I know it makes sense to us, here and now, but I for one will forget this within about 5 minutes and stuff like this makes it harder for people who are not familiar with it to understand what the purpose of this is. Can you also comment the other hash which is used in the same way?", "author": "tombentley", "createdAt": "2020-08-17T09:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTMxNzg0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\nindex c496050f54..df8e112585 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n\n@@ -110,7 +111,6 @@ public abstract class AbstractModel {\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n-    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";\n \n     @Deprecated\n     public static final String ANNO_CO_STRIMZI_IO_DELETE_CLAIM = ClusterOperator.STRIMZI_CLUSTER_OPERATOR_DOMAIN + \"/delete-claim\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472486889", "bodyText": "What is the ANNO_STRIMZI_LOGGING_HASH used for now? Is it still used somewhere? I saw it is not used anymore on line 2458 in Kafka assembly operator.", "author": "scholzj", "createdAt": "2020-08-18T20:53:51Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";", "originalCommit": "ef83c883c47bf680286850675c6bca21245ecdc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0NTA4OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472745088", "bodyText": "It is still used for rolling updates of ZK pods when ZK logging is changed.", "author": "sknot-rh", "createdAt": "2020-08-19T06:13:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc0OTg2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472749863", "bodyText": "Ok, makes sense. Thanks.", "author": "scholzj", "createdAt": "2020-08-19T06:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQ4Njg4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\nindex 0e552fcaaa..df8e112585 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n\n@@ -109,12 +110,7 @@ public abstract class AbstractModel {\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n-\n-    /**\n-     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n-     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n-    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";\n \n     @Deprecated\n     public static final String ANNO_CO_STRIMZI_IO_DELETE_CLAIM = ClusterOperator.STRIMZI_CLUSTER_OPERATOR_DOMAIN + \"/delete-claim\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjUxMzI1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472513258", "bodyText": "Should these be in Annotations.java?", "author": "scholzj", "createdAt": "2020-08-18T21:49:57Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java", "diffHunk": "@@ -109,7 +109,12 @@\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n+\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n+     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n+    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";", "originalCommit": "ef83c883c47bf680286850675c6bca21245ecdc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\nindex 0e552fcaaa..df8e112585 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/model/AbstractModel.java\n\n@@ -109,12 +110,7 @@ public abstract class AbstractModel {\n      */\n     public static final String ANNO_STRIMZI_IO_STORAGE = Annotations.STRIMZI_DOMAIN + \"storage\";\n     public static final String ANNO_STRIMZI_IO_DELETE_CLAIM = Annotations.STRIMZI_DOMAIN + \"delete-claim\";\n-\n-    /**\n-     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n-     */\n     public static final String ANNO_STRIMZI_LOGGING_HASH = Annotations.STRIMZI_DOMAIN + \"logging-hash\";\n-    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = Annotations.STRIMZI_DOMAIN + \"logging-appenders-hash\";\n \n     @Deprecated\n     public static final String ANNO_CO_STRIMZI_IO_DELETE_CLAIM = ClusterOperator.STRIMZI_CLUSTER_OPERATOR_DOMAIN + \"/delete-claim\";\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472928000", "bodyText": "Can be static?", "author": "tombentley", "createdAt": "2020-08-19T10:30:31Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java", "diffHunk": "@@ -3471,6 +3474,18 @@ String getInternalServiceHostname(String serviceName)    {\n \n     }\n \n+    private String getLoggingAppenders(String loggingConfiguration) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzY3NzkxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r473677914", "bodyText": "In the other PR it is under Utils class so it will be refactored later.", "author": "sknot-rh", "createdAt": "2020-08-20T07:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyODAwMA=="}], "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\nindex 10e2f89e73..14b0ed21bc 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/assembly/KafkaAssemblyOperator.java\n\n@@ -3474,39 +3473,8 @@ public class KafkaAssemblyOperator extends AbstractAssemblyOperator<KubernetesCl\n \n     }\n \n-    private String getLoggingAppenders(String loggingConfiguration) {\n-        OrderedProperties ops = new OrderedProperties();\n-        ops.addStringPairs(loggingConfiguration);\n-        StringBuilder result = new StringBuilder();\n-        for (Map.Entry<String, String> entry: ops.asMap().entrySet()) {\n-            if (entry.getKey().startsWith(\"log4j.appender.\")) {\n-                result.append(entry.getKey()).append(\"=\").append(entry.getValue());\n-            }\n-        }\n-        return result.toString();\n-    }\n-\n     /* test */ Date dateSupplier() {\n         return new Date();\n     }\n \n-    private String getStringHash(String toBeHashed)  {\n-        try {\n-            MessageDigest hashFunc = MessageDigest.getInstance(\"SHA-512\");\n-\n-            byte[] hash = hashFunc.digest(toBeHashed.getBytes(StandardCharsets.UTF_8));\n-\n-            StringBuffer stringHash = new StringBuffer();\n-\n-            for (int i = 0; i < hash.length; i++) {\n-                String hex = Integer.toHexString(0xff & hash[i]);\n-                if (hex.length() == 1) stringHash.append('0');\n-                stringHash.append(hex);\n-            }\n-\n-            return stringHash.toString();\n-        } catch (NoSuchAlgorithmException e)    {\n-            throw new RuntimeException(\"Failed to create SHA-512 MessageDigest instance\");\n-        }\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTM3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472929374", "bodyText": "I think we probably don't want to force the rest of Fabric 8's patch code to use ordered maps when computing diffs. So we should probably instantiate our own ObjectMapper here since it's the only place where we do case about order (right?)", "author": "tombentley", "createdAt": "2020-08-19T10:33:04Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex 1339fb7d67..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -6,22 +6,21 @@\n package io.strimzi.operator.cluster.operator.resource;\n \n import com.fasterxml.jackson.databind.JsonNode;\n-import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDQ2Mg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472930462", "bodyText": "Why did use use a List for VALID_LOGGER_LEVELS if you're only needing contains()? A HashSet will be more efficient since it likely needs only 1 hash() and 1 or maybe 2 equals().", "author": "tombentley", "createdAt": "2020-08-19T10:34:58Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import io.fabric8.zjsonpatch.JsonDiff;\n+import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n+import org.apache.kafka.clients.admin.AlterConfigOp;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static io.fabric8.kubernetes.client.internal.PatchUtils.patchMapper;\n+\n+public class KafkaBrokerLoggingConfigurationDiff extends AbstractResourceDiff {\n+\n+    private static final Logger log = LogManager.getLogger(KafkaBrokerLoggingConfigurationDiff.class);\n+    private static final ObjectMapper MAPPER = patchMapper().configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true);\n+    private final Collection<AlterConfigOp> diff;\n+    private int brokerId;\n+\n+    private static final List VALID_LOGGER_LEVELS = Arrays.asList(\"INFO\", \"ERROR\", \"WARN\", \"TRACE\", \"DEBUG\", \"FATAL\", \"OFF\");\n+\n+    public KafkaBrokerLoggingConfigurationDiff(Config brokerConfigs, String desired, int brokerId) {\n+        this.brokerId = brokerId;\n+        this.diff = diff(brokerId, desired, brokerConfigs);\n+    }\n+\n+    /**\n+     * Returns logging difference\n+     * @return Collection of AlterConfigOp containing difference between current and desired logging configuration\n+     */\n+    public Collection<AlterConfigOp> getLoggingDiff() {\n+        return diff;\n+    }\n+\n+    /**\n+     * @return The number of broker configs which are different.\n+     */\n+    public int getDiffSize() {\n+        return diff.size();\n+    }\n+\n+    /**\n+     * Computes logging diff\n+     * @param brokerId id of compared broker\n+     * @param desired desired logging configuration, may be null if the related ConfigMap does not exist yet or no changes are required\n+     * @param brokerConfigs current configuration\n+     * @return Collection of AlterConfigOp containing all entries which were changed from current in desired configuration\n+     */\n+    private static Collection<AlterConfigOp> diff(int brokerId, String desired,\n+                                                  Config brokerConfigs) {\n+        if (brokerConfigs == null || desired == null) {\n+            return Collections.emptyList();\n+        }\n+        Map<String, String> currentMap;\n+        Collection<AlterConfigOp> updatedCE = new ArrayList<>();\n+        currentMap = brokerConfigs.entries().stream().collect(\n+            Collectors.toMap(\n+                ConfigEntry::name,\n+                configEntry -> configEntry.value() == null ? \"null\" : configEntry.value()));\n+\n+        OrderedProperties orderedProperties = new OrderedProperties();\n+        desired = desired.replaceAll(\"log4j\\\\.logger\\\\.\", \"\");\n+        desired = desired.replaceAll(\"log4j\\\\.rootLogger\", \"root\");\n+        orderedProperties.addStringPairs(desired);\n+        Map<String, String> desiredMap = orderedProperties.asMap();\n+\n+        JsonNode source = MAPPER.valueToTree(currentMap);\n+        JsonNode target = MAPPER.valueToTree(desiredMap);\n+        JsonNode jsonDiff = JsonDiff.asJson(source, target);\n+\n+        for (JsonNode d : jsonDiff) {\n+            String pathValue = d.get(\"path\").asText();\n+            String pathValueWithoutSlash = pathValue.substring(1);\n+\n+            Optional<ConfigEntry> optEntry = brokerConfigs.entries().stream()\n+                    .filter(configEntry -> configEntry.name().equals(pathValueWithoutSlash))\n+                    .findFirst();\n+\n+            if (pathValueWithoutSlash.equals(\"log4j.rootLogger\")) {\n+                if (!desiredMap.get(pathValueWithoutSlash).matches(\".+,.+\")) {\n+                    log.warn(\"Broker {} logging: Logger log4j.rootLogger should contain level and appender, e.g. \\'log4j.rootLogger = INFO, CONSOLE\\'\", brokerId);\n+                }\n+            }\n+            String op = d.get(\"op\").asText();\n+            if (optEntry.isPresent()) {\n+                ConfigEntry entry = optEntry.get();\n+                if (\"remove\".equals(op)) {\n+                    removeProperty(updatedCE, pathValueWithoutSlash, entry);\n+                } else if (\"replace\".equals(op)) {\n+                    // entry is in the current, desired is updated value\n+                    if (!entry.value().equals(parseLogLevelFromAppenderCouple(desiredMap.get(entry.name())))) {\n+                        updateOrAdd(entry.name(), desiredMap, updatedCE);\n+                    }\n+                }\n+            } else {\n+                if (\"add\".equals(op)) {\n+                    // entry is not in the current, it is added\n+                    updateOrAdd(pathValueWithoutSlash, desiredMap, updatedCE);\n+                }\n+            }\n+\n+            log.debug(\"Kafka Broker {} Logging Config Differs : {}\", brokerId, d);\n+            log.debug(\"Current Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(source, pathValue));\n+            log.debug(\"Desired Kafka Broker Logging Config path {} has value {}\", pathValueWithoutSlash, lookupPath(target, pathValue));\n+        }\n+        return updatedCE;\n+    }\n+\n+    private static String parseLogLevelFromAppenderCouple(String level) {\n+        int index = level.indexOf(\",\");\n+        if (index > 0) {\n+            return level.substring(0, index).trim();\n+        } else {\n+            return level.trim();\n+        }\n+    }\n+\n+    private static void updateOrAdd(String propertyName, Map<String, String> desiredMap, Collection<AlterConfigOp> updatedCE) {\n+        if (!propertyName.contains(\"log4j.appender\") && !propertyName.equals(\"monitorInterval\")) {\n+            String level = parseLogLevelFromAppenderCouple(desiredMap.get(propertyName));\n+            if (isValidLoggerLevel(level)) {\n+                updatedCE.add(new AlterConfigOp(new ConfigEntry(propertyName, level), AlterConfigOp.OpType.SET));\n+                log.trace(\"{} not set in current or has deprecated value. Setting to {}\", propertyName, level);\n+            } else {\n+                log.warn(\"Level {} is not valid logging level\", level);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * All loggers can be set dynamically. If the logger is not set in desire, set it to ERROR. Loggers with already set to ERROR should be skipped.\n+     * ERROR is set as inactive because log4j does not support OFF logger value.\n+     * We want to skip \"root\" logger as well to avoid duplicated key in alterConfigOps collection.\n+     * @param alterConfigOps collection of AlterConfigOp\n+     * @param pathValueWithoutSlash name of \"removed\" logger\n+     * @param entry entry to be removed (set to ERROR)\n+     */\n+    private static void removeProperty(Collection<AlterConfigOp> alterConfigOps, String pathValueWithoutSlash, ConfigEntry entry) {\n+        if (!pathValueWithoutSlash.contains(\"log4j.appender\") && !pathValueWithoutSlash.equals(\"root\") && !\"ERROR\".equals(entry.value())) {\n+            alterConfigOps.add(new AlterConfigOp(new ConfigEntry(pathValueWithoutSlash, \"ERROR\"), AlterConfigOp.OpType.SET));\n+            log.trace(\"{} not set in desired, setting to ERROR\", entry.name());\n+        }\n+    }\n+\n+    /**\n+     * @return whether the current config and the desired config are identical (thus, no update is necessary).\n+     */\n+    @Override\n+    public boolean isEmpty() {\n+        return  diff.size() == 0;\n+    }\n+\n+    private static boolean isValidLoggerLevel(String level) {\n+        return VALID_LOGGER_LEVELS.contains(level);", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\nindex 1339fb7d67..9ec2de2722 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiff.java\n\n@@ -6,22 +6,21 @@\n package io.strimzi.operator.cluster.operator.resource;\n \n import com.fasterxml.jackson.databind.JsonNode;\n-import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.SerializationFeature;\n import io.fabric8.zjsonpatch.JsonDiff;\n import io.strimzi.operator.cluster.model.OrderedProperties;\n+import io.strimzi.operator.common.Util;\n import io.strimzi.operator.common.operator.resource.AbstractResourceDiff;\n import org.apache.kafka.clients.admin.AlterConfigOp;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.apache.kafka.common.config.ConfigResource;\n import org.apache.logging.log4j.LogManager;\n import org.apache.logging.log4j.Logger;\n \n import java.util.ArrayList;\n-import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n-import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n import java.util.stream.Collectors;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMjE3NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472932174", "bodyText": "You just wrote a Util method for new ConfigResource(ConfigResource.Type.BROKER_LOGGER, so use it!", "author": "tombentley", "createdAt": "2020-08-19T10:38:03Z", "path": "cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java", "diffHunk": "@@ -420,35 +427,61 @@ private RestartPlan restartPlan(int podId, Pod pod) throws ForceableProblem, Int\n                     needsRestart = true;\n                 }\n             }\n+            if (loggingDiff.getDiffSize() > 0) {\n+                log.info(\"{}: Pod {} logging needs to be reconfigured.\", reconciliation, podId);\n+                needsReconfig = true;\n+            }\n         } else {\n             log.info(\"{}: Pod {} needs to be restarted. Reason: {}\", reconciliation, podId, reasonToRestartPod);\n         }\n-        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff);\n+        return new RestartPlan(adminClient, needsRestart, needsReconfig, diff, loggingDiff);\n     }\n \n     /**\n-     * Returns a Future which completes with the config of the given broker.\n+     * Returns a config of the given broker.\n      * @param ac The admin client\n      * @param brokerId The id of the broker.\n      * @return a Future which completes with the config of the given broker.\n      */\n     protected Config brokerConfig(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n-        ConfigResource resource = new ConfigResource(ConfigResource.Type.BROKER, String.valueOf(brokerId));\n+        ConfigResource resource = Util.getBrokersConfig(brokerId);\n         return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n             30, TimeUnit.SECONDS,\n             error -> new ForceableProblem(\"Error getting broker config\", error)\n         );\n     }\n \n-    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff)\n+    /**\n+     * Returns logging of the given broker.\n+     * @param ac The admin client\n+     * @param brokerId The id of the broker.\n+     * @return a Future which completes with the logging of the given broker.\n+     */\n+    protected Config brokerLogging(Admin ac, int brokerId) throws ForceableProblem, InterruptedException {\n+        ConfigResource resource = Util.getBrokersLogging(brokerId);\n+        return await(Util.kafkaFutureToVertxFuture(vertx, ac.describeConfigs(singletonList(resource)).values().get(resource)),\n+                30, TimeUnit.SECONDS,\n+            error -> new ForceableProblem(\"Error getting broker logging\", error)\n+        );\n+    }\n+\n+    protected void dynamicUpdateBrokerConfig(int podId, Admin ac, KafkaBrokerConfigurationDiff configurationDiff, KafkaBrokerLoggingConfigurationDiff logDiff)\n             throws ForceableProblem, InterruptedException {\n-        Map<ConfigResource, Collection<AlterConfigOp>> configDiff = configurationDiff.getConfigDiff();\n-        log.debug(\"{}: Altering broker {} with {}\", reconciliation, podId, configDiff);\n-        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(configDiff);\n+        Map<ConfigResource, Collection<AlterConfigOp>> updatedConfig = new HashMap<>(2);\n+        updatedConfig.put(Util.getBrokersConfig(podId), configurationDiff.getConfigDiff());\n+        updatedConfig.put(Util.getBrokersLogging(podId), logDiff.getLoggingDiff());\n+\n+        log.info(\"{}: Altering broker {} with {}\", reconciliation, podId, updatedConfig);\n+\n+        AlterConfigsResult alterConfigResult = ac.incrementalAlterConfigs(updatedConfig);\n         KafkaFuture<Void> brokerConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER, Integer.toString(podId)));\n+        KafkaFuture<Void> brokerLoggingConfigFuture = alterConfigResult.values().get(new ConfigResource(ConfigResource.Type.BROKER_LOGGER, Integer.toString(podId)));", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\nindex 145ddd6784..c4e955a3b1 100644\n--- a/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\n+++ b/cluster-operator/src/main/java/io/strimzi/operator/cluster/operator/resource/KafkaRoller.java\n\n@@ -438,7 +438,7 @@ public class KafkaRoller {\n     }\n \n     /**\n-     * Returns a config of the given broker.\n+     * Returns a Future which completes with the config of the given broker.\n      * @param ac The admin client\n      * @param brokerId The id of the broker.\n      * @return a Future which completes with the config of the given broker.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMzMzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472933337", "bodyText": "Is it even worth putting a two line file in a resource like that? Just use a String literal.\nIf you must use a file, the name desired-kafka-broker-logging.conf is a bit weird. It's a properties file, so call it with .properties suffix at least.", "author": "tombentley", "createdAt": "2020-08-19T10:40:20Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de057864d5b91a9bef3e3da7f7cae629c964ccaa", "chunk": "diff --git a/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java b/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\nindex 11e0e99d4c..acf2303f24 100644\n--- a/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\n+++ b/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\n\n@@ -5,13 +5,10 @@\n \n package io.strimzi.operator.cluster.operator.resource;\n \n-import io.strimzi.test.TestUtils;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n import org.junit.jupiter.api.Test;\n \n-import java.io.IOException;\n-import java.io.InputStream;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDE2NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472934164", "bodyText": "Same comment.", "author": "tombentley", "createdAt": "2020-08-19T10:41:49Z", "path": "cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+\n+package io.strimzi.operator.cluster.operator.resource;\n+\n+import io.strimzi.test.TestUtils;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.ConfigEntry;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static java.util.Collections.emptyList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaBrokerLoggingConfigurationDiffTest {\n+\n+    private final int brokerId = 0;\n+\n+    private String getDesiredConfiguration(List<ConfigEntry> additional) {\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"desired-kafka-broker-logging.conf\")) {\n+            StringBuilder desiredConfigString = new StringBuilder(TestUtils.readResource(is));\n+\n+            for (ConfigEntry ce : additional) {\n+                desiredConfigString.append(\"\\n\").append(ce.name()).append(\"=\").append(ce.value());\n+            }\n+\n+            return desiredConfigString.toString();\n+        } catch (IOException e) {\n+            fail(e);\n+        }\n+        return \"\";\n+    }\n+\n+    private Config getCurrentConfiguration(List<ConfigEntry> additional) {\n+        List<ConfigEntry> entryList = new ArrayList<>();\n+\n+        try (InputStream is = getClass().getClassLoader().getResourceAsStream(\"current-kafka-broker-logging.conf\")) {", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de057864d5b91a9bef3e3da7f7cae629c964ccaa", "chunk": "diff --git a/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java b/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\nindex 11e0e99d4c..acf2303f24 100644\n--- a/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\n+++ b/cluster-operator/src/test/java/io/strimzi/operator/cluster/operator/resource/KafkaBrokerLoggingConfigurationDiffTest.java\n\n@@ -5,13 +5,10 @@\n \n package io.strimzi.operator.cluster.operator.resource;\n \n-import io.strimzi.test.TestUtils;\n import org.apache.kafka.clients.admin.Config;\n import org.apache.kafka.clients.admin.ConfigEntry;\n import org.junit.jupiter.api.Test;\n \n-import java.io.IOException;\n-import java.io.InputStream;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzOTQxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/3469#discussion_r472939414", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n          \n          \n            \n                 * Annotations for rolling a cluster whenever the logging (or it's part) has changed.\n          \n          \n            \n                   By changing the annotation we force a restart since the pod will be out of date compared to the statefulset.", "author": "tombentley", "createdAt": "2020-08-19T10:51:53Z", "path": "operator-common/src/main/java/io/strimzi/operator/common/Annotations.java", "diffHunk": "@@ -18,6 +18,12 @@\n \n     public static final String STRIMZI_DOMAIN = \"strimzi.io/\";\n     public static final String STRIMZI_LOGGING_ANNOTATION = STRIMZI_DOMAIN + \"logging\";\n+    /**\n+     * Annotations for rolling a cluster whenever the logging (or it's part) has changed", "originalCommit": "d6d4c2bd82e5518fe73a8f3fbf77f5f489c2e36b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "chunk": "diff --git a/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java b/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java\nindex c5d280a188..44a472d2a5 100644\n--- a/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java\n+++ b/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java\n\n@@ -18,12 +18,7 @@ public class Annotations {\n \n     public static final String STRIMZI_DOMAIN = \"strimzi.io/\";\n     public static final String STRIMZI_LOGGING_ANNOTATION = STRIMZI_DOMAIN + \"logging\";\n-    /**\n-     * Annotations for rolling a cluster whenever the logging (or it's part) has changed\n-     */\n-    public static final String ANNO_STRIMZI_LOGGING_HASH = STRIMZI_DOMAIN + \"logging-hash\";\n-    public static final String ANNO_STRIMZI_LOGGING_APPENDERS_HASH = STRIMZI_DOMAIN + \"logging-appenders-hash\";\n-\n+    public static final String ANNO_STRIMZI_LOGGING_DYNAMICALLY_UNCHANGEABLE_HASH = STRIMZI_DOMAIN + \"logging-appenders-hash\";\n     public static final String STRIMZI_IO_USE_CONNECTOR_RESOURCES = STRIMZI_DOMAIN + \"use-connector-resources\";\n     public static final String ANNO_STRIMZI_IO_MANUAL_ROLLING_UPDATE = STRIMZI_DOMAIN + \"manual-rolling-update\";\n     // this annotation with related possible values (approve, stop, refresh) is set by the user for interacting\n"}}, {"oid": "fdffb2d5504562a8f01ee94786bf2880744ed62a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fdffb2d5504562a8f01ee94786bf2880744ed62a", "message": "dyn log changes in kafka\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:33:01Z", "type": "commit"}, {"oid": "141ae67259d20f20245d6017ee7c24163063df6e", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/141ae67259d20f20245d6017ee7c24163063df6e", "message": "checkstyle\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:37:46Z", "type": "commit"}, {"oid": "0f4230eb5270eea8c920bd61f7688885ad9eb342", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0f4230eb5270eea8c920bd61f7688885ad9eb342", "message": "rolling\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:37:46Z", "type": "commit"}, {"oid": "3144a85554461710a0bf49802184c806695ddb81", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3144a85554461710a0bf49802184c806695ddb81", "message": "fixes\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "eadc54662e112d812d703aa260ea3549c57732a9", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/eadc54662e112d812d703aa260ea3549c57732a9", "message": "commit\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "10f99e3bfc8c3172d7550090dc1327435d82f840", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/10f99e3bfc8c3172d7550090dc1327435d82f840", "message": "st\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:39:58Z", "type": "commit"}, {"oid": "f8d99bd3ba20db97d82d178cddd4549377e46cf1", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f8d99bd3ba20db97d82d178cddd4549377e46cf1", "message": "comment\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "5d0a413c910a3dd080c5bb8fdc292e3d634546dd", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5d0a413c910a3dd080c5bb8fdc292e3d634546dd", "message": "simpification\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "36737e1c06ea5f96fe918ccc2a78626ab1818a4c", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/36737e1c06ea5f96fe918ccc2a78626ab1818a4c", "message": "review comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:41:52Z", "type": "commit"}, {"oid": "ceeddb22adb88d42c146f3d4742a6dccca29d4d4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ceeddb22adb88d42c146f3d4742a6dccca29d4d4", "message": "Jakub's rc\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:43:27Z", "type": "commit"}, {"oid": "de057864d5b91a9bef3e3da7f7cae629c964ccaa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/de057864d5b91a9bef3e3da7f7cae629c964ccaa", "message": "Tom's comments\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T06:43:27Z", "type": "commit"}, {"oid": "bccfb86bf20212ed311c5d511850dd93b5143f48", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bccfb86bf20212ed311c5d511850dd93b5143f48", "message": "rebase to master\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T08:01:13Z", "type": "commit"}, {"oid": "bccfb86bf20212ed311c5d511850dd93b5143f48", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/bccfb86bf20212ed311c5d511850dd93b5143f48", "message": "rebase to master\n\nSigned-off-by: Stanislav Knot <sknot@redhat.com>", "committedDate": "2020-08-25T08:01:13Z", "type": "forcePushed"}]}