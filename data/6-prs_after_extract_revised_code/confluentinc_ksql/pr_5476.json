{"pr_number": 5476, "pr_title": "feat: Add consumer offsets to DESCRIBE EXTENDED", "pr_createdAt": "2020-05-25T22:52:33Z", "pr_url": "https://github.com/confluentinc/ksql/pull/5476", "timeline": [{"oid": "257c4d061db59030377aec97793cc6389efa935c", "url": "https://github.com/confluentinc/ksql/commit/257c4d061db59030377aec97793cc6389efa935c", "message": "add unit tests", "committedDate": "2020-05-26T21:00:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430717147", "bodyText": "This is the only naive way I've found so far to get the consumer group id. Would love some feedback on how to get this in a proper/better way.", "author": "jeqo", "createdAt": "2020-05-26T21:26:06Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTMxNw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432725317", "bodyText": "I think this is the same as StreamsConfig.APPLICATION_ID_CONFIG in the QueryMetadata.getStreamsProperties, but I'd need to double check that", "author": "agavra", "createdAt": "2020-05-29T20:42:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNzU5Mg==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432727592", "bodyText": "You can see how we build it in QueryExecutor#getApplicationId", "author": "agavra", "createdAt": "2020-05-29T20:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzE0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 74c11f5aa4..13ab3962a0 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -200,9 +206,9 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n         q -> q.getSourceNames().contains(dataSource.getName()));\n-    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430717518", "bodyText": "Would be possible to get more than one source query?", "author": "jeqo", "createdAt": "2020-05-26T21:26:58Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMzg3MA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432723870", "bodyText": "yes, this is in the case of a JOIN you could possibly have more than one source query - it would be good to test a JOIN scenario end to end", "author": "agavra", "createdAt": "2020-05-29T20:38:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxOTI4NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r434919285", "bodyText": "I'm wondering how it will have more than one source when there is only one topic:\n        final TopicDescription topicDescription = serviceContext.getTopicClient()\n            .describeTopic(dataSource.getKafkaTopicName());\n\nThinking out loud: When a query is joining 2 streams, is acting on top of the streams, not a topic itself, therefore no consumer group.", "author": "jeqo", "createdAt": "2020-06-03T23:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNzUxOA=="}], "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 74c11f5aa4..13ab3962a0 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -200,9 +206,9 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n         q -> q.getSourceNames().contains(dataSource.getName()));\n-    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODU3Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430718577", "bodyText": "Currently using blocking get(). Probably should use a timeout, just not sure what's the best way to pass through the value from config or somewhere else", "author": "jeqo", "createdAt": "2020-05-26T21:29:13Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 74c11f5aa4..13ab3962a0 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -200,9 +206,9 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n         q -> q.getSourceNames().contains(dataSource.getName()));\n-    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODg0NA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r430718844", "bodyText": "Not sure how to get this from config.", "author": "jeqo", "createdAt": "2020-05-26T21:29:50Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMzU1Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432723557", "bodyText": "I think you're looking for KsqlConfig.KSQL_SERVICE_ID_CONFIG the config is availabile in the ConfigureStatement class", "author": "agavra", "createdAt": "2020-05-29T20:38:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxODg0NA=="}], "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 74c11f5aa4..13ab3962a0 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -200,9 +206,9 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n         q -> q.getSourceNames().contains(dataSource.getName()));\n-    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcxOTc2Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432719767", "bodyText": "nit: our checkstyle should enforce some things like local variables being final whenever possible, you can run mvn checkstyle:checkstyle to make sure that your code passes all checkstyle checks", "author": "agavra", "createdAt": "2020-05-29T20:28:54Z", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -618,6 +619,27 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    Optional<SourceConsumerOffsets> consumerGroupOffsetsOptional = source.getConsumerGroupOffsets();", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\nindex e6e53587eb..05b5290d2f 100644\n--- a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n+++ b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n\n@@ -619,15 +620,19 @@ public class Console implements Closeable {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n-    Optional<SourceConsumerOffsets> consumerGroupOffsetsOptional = source.getConsumerGroupOffsets();\n+    final Optional<SourceConsumerOffsets> consumerGroupOffsetsOptional = source\n+        .getConsumerGroupOffsets();\n     if (consumerGroupOffsetsOptional.isPresent()) {\n       writer().println();\n-      SourceConsumerOffsets sourceConsumerOffsets = consumerGroupOffsetsOptional.get();\n-      writer().println(String.format(\"%-20s : %s\", \"Consumer Group\", sourceConsumerOffsets.getGroupId()));\n-      writer().println(String.format(\"%-20s : %s\", \"Kafka topic\", sourceConsumerOffsets.getKafkaTopic()));\n+      final SourceConsumerOffsets sourceConsumerOffsets = consumerGroupOffsetsOptional.get();\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Consumer Group\", sourceConsumerOffsets.getGroupId()));\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Kafka topic\", sourceConsumerOffsets.getKafkaTopic()));\n       writer().println(\"\");\n       final Table taskTable = new Table.Builder()\n-          .withColumnHeaders(ImmutableList.of(\"Partition\", \"Start Offset\", \"End Offset\", \"Offset\", \"Lag\"))\n+          .withColumnHeaders(\n+              ImmutableList.of(\"Partition\", \"Start Offset\", \"End Offset\", \"Offset\", \"Lag\"))\n           .withRows(sourceConsumerOffsets.getOffsets()\n               .stream()\n               .map(offset -> ImmutableList.of(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMTQ1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432721456", "bodyText": "we should not be passing the real admin client to the sandbox - otherwise it becomes possible for the sandbox to make real changes to the kafka cluster. I recommend instead following the pattern of the above Sandboxes and create a proxy that proxies the admin client for read-only commands.\nBetter yet, I think it might make sense to bake this into the KafkaTopicClient, which already has an api for describeTopic", "author": "agavra", "createdAt": "2020-05-29T20:32:57Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java", "diffHunk": "@@ -46,29 +48,33 @@ public static SandboxedServiceContext create(final ServiceContext serviceContext\n     final SchemaRegistryClient schemaRegistryClient =\n         SandboxedSchemaRegistryClient.createProxy(serviceContext.getSchemaRegistryClient());\n     final ConnectClient connectClient = SandboxConnectClient.createProxy();\n+    final Admin adminClient = serviceContext.getAdminClient();", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4MzkxMw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r435483913", "bodyText": "fixed.", "author": "jeqo", "createdAt": "2020-06-04T18:58:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMTQ1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f628d074e28aaedf5b6eea83f4f6a22192382db2", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java\nindex 01c62b3fb4..2bd815a79e 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/SandboxedServiceContext.java\n\n@@ -48,14 +47,15 @@ public final class SandboxedServiceContext implements ServiceContext {\n     final SchemaRegistryClient schemaRegistryClient =\n         SandboxedSchemaRegistryClient.createProxy(serviceContext.getSchemaRegistryClient());\n     final ConnectClient connectClient = SandboxConnectClient.createProxy();\n-    final Admin adminClient = serviceContext.getAdminClient();\n+    final KafkaConsumerGroupClient kafkaConsumerGroupClient = SandboxedKafkaConsumerGroupClient\n+        .createProxy(serviceContext.getConsumerGroupClient());\n \n     return new SandboxedServiceContext(\n         kafkaClientSupplier,\n         kafkaTopicClient,\n         schemaRegistryClient,\n         connectClient,\n-        adminClient);\n+        kafkaConsumerGroupClient);\n   }\n \n   private SandboxedServiceContext(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyMjI4NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432722285", "bodyText": "I think it would make sense to encapsulate all of this into a single class and have one Map<TopicPartition, SourceConsumerOffsets>", "author": "agavra", "createdAt": "2020-05-29T20:35:01Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java", "diffHunk": "@@ -34,7 +41,11 @@ public static SourceDescription create(\n       final boolean extended,\n       final List<RunningQuery> readQueries,\n       final List<RunningQuery> writeQueries,\n-      final Optional<TopicDescription> topicDescription\n+      final Optional<TopicDescription> topicDescription,\n+      final Optional<ConsumerGroupDescription> consumerGroupDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f628d074e28aaedf5b6eea83f4f6a22192382db2", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java\nindex c0d61c7912..7957ac009f 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/entity/SourceDescriptionFactory.java\n\n@@ -42,10 +35,7 @@ public final class SourceDescriptionFactory {\n       final List<RunningQuery> readQueries,\n       final List<RunningQuery> writeQueries,\n       final Optional<TopicDescription> topicDescription,\n-      final Optional<ConsumerGroupDescription> consumerGroupDescription,\n-      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n-      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n-      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+      final Optional<SourceConsumerGroupOffsets> sourceConsumerOffsets\n   ) {\n     return new SourceDescription(\n         dataSource.getName().toString(FormatOptions.noEscape()),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTc4Mg==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r432725782", "bodyText": "as mentioned above, it would be nice to encapsulate this into the KafkaTopicClient (I think, I'm not 100% sure anymore - cc @big-andy-coates for his thoughts on this one)", "author": "agavra", "createdAt": "2020-05-29T20:43:23Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,14 +200,42 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n+    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-      } catch (final KafkaException | KafkaResponseGetFailedException e) {\n+        String serviceId = \"default\"; //FIXME not sure how to get this\n+        if (sourceQueries.isEmpty()){\n+          consumerGroupDescription = Optional.empty();\n+        } else {\n+          String queryId = sourceQueries.get(0).getId().toString();\n+          String consumerGroupId = \"_confluent-ksql-\" + serviceId + \"_\" + KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_DEFAULT + queryId; //FIXME there should be a better way to build this\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();", "originalCommit": "257c4d061db59030377aec97793cc6389efa935c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ4NTQ0Mw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r435485443", "bodyText": "@agavra I went by adding methods to KafkaTopicClient, and elevate KafkaConsumerGroupClient\u2014that was hidden in utils\u2014 to handle consumer group offsets query and add it to ServiceContext. Hope this looks better now.", "author": "jeqo", "createdAt": "2020-06-04T19:01:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcyNTc4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 74c11f5aa4..13ab3962a0 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -200,9 +206,9 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n         q -> q.getSourceNames().contains(dataSource.getName()));\n-    List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n"}}, {"oid": "457a5f6146ba64aebfb1865a9a936736b633f29d", "url": "https://github.com/confluentinc/ksql/commit/457a5f6146ba64aebfb1865a9a936736b633f29d", "message": "add unit tests", "committedDate": "2020-06-01T21:55:20Z", "type": "forcePushed"}, {"oid": "14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "url": "https://github.com/confluentinc/ksql/commit/14eb7a77ae6b46159c8f8516ed14c923e4a5f38d", "message": "fix: checkstyle", "committedDate": "2020-06-03T22:03:32Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436059159", "bodyText": "what was the motivation behind passing in a supplier? It looks like we open ourselves to accidentally creating a new admin client each time we make any describe/list request, which could potentially be expensive (I'm not sure how it manages handshakes/connection management with the Kafka broker)\nin most places, it looks like we're just passing in () -> adminClient anyway - but there are a few places we aren't. would be good to audit and understand if this is necessary", "author": "agavra", "createdAt": "2020-06-05T17:24:10Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -52,7 +56,10 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n     try {\n       final Map<String, ConsumerGroupDescription> groups = ExecutorUtil\n           .executeWithRetries(\n-              () -> adminClient.describeConsumerGroups(Collections.singleton(group)).all().get(),\n+              () -> adminClient.get()", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NTg3MA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441175870", "bodyText": "In this case I followed KafkaTopicClient design as it is the most similar in terms of behavior and instance creatin.\nAFAIK, as well as Consumer and Producer, Admin client does not do any handshake when instantiated; though it make sense to reduce unnecessary recreations.\nFollowing creation path, KafkaConsumerGroupClient and KafkaTopicClient both receive admin client supplier from ServiceContext which has all instances memoized to avoid recreation, e.g.:\nprivate final MemoizedSupplier<Admin> adminClientSupplier;\nWith this design I don't see a need to change the current behavior. wdyt?", "author": "jeqo", "createdAt": "2020-06-16T22:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4MDcwMQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445480701", "bodyText": "Yeah, this follows the same patterns as KafkaTopicClient.  The Memorized supplier pattern is used to avoid Admin Clients being instantiated, potentially per request due to different user credentials, during service context creation.  Admin client instantiation is expensive and should be done lazily, as is the case here.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:09:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA1OTE1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\nindex c10a647868..aa8841f613 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n\n@@ -52,9 +55,8 @@ public class KafkaConsumerGroupClientImpl implements KafkaConsumerGroupClient {\n   }\n \n   public ConsumerGroupSummary describeConsumerGroup(final String group) {\n-\n     try {\n-      final Map<String, ConsumerGroupDescription> groups = ExecutorUtil\n+      final Map<String, ConsumerGroupDescription> groupDescriptions = ExecutorUtil\n           .executeWithRetries(\n               () -> adminClient.get()\n                   .describeConsumerGroups(Collections.singleton(group))\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA2MDIzNA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436060234", "bodyText": "nit: any reason we need a LinkedHashMap? is ordering important?", "author": "agavra", "createdAt": "2020-06-05T17:26:23Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "diffHunk": "@@ -311,6 +316,27 @@ public void deleteInternalTopics(final String applicationId) {\n     }\n   }\n \n+  @Override\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n+      final String topicName,\n+      final OffsetSpec offsetSpec\n+  ) {\n+    final TopicDescription topicDescription = describeTopic(topicName);\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new LinkedHashMap<>();", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE3NzA3NA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441177074", "bodyText": "Not anymore. Initially I was using this map to iterate and show results in the right order. This has changed. Will update it.", "author": "jeqo", "createdAt": "2020-06-16T22:27:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA2MDIzNA=="}], "type": "inlineReview", "revised_code": {"commit": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\nindex 972d3f552e..52ae1c5bf6 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\n\n@@ -322,7 +322,7 @@ public class KafkaTopicClientImpl implements KafkaTopicClient {\n       final OffsetSpec offsetSpec\n   ) {\n     final TopicDescription topicDescription = describeTopic(topicName);\n-    final Map<TopicPartition, OffsetSpec> offsetsRequest = new LinkedHashMap<>();\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new HashMap<>();\n     for (TopicPartitionInfo tpInfo : topicDescription.partitions()) {\n       final TopicPartition tp = new TopicPartition(topicName, tpInfo.partition());\n       offsetsRequest.put(tp, offsetSpec);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5OTAwNA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r436099004", "bodyText": "A single source can have multiple read-from and write-to queries:\nQueries that read from this STREAM\n-----------------------------------\nCSAS_BAZ_7 (RUNNING) : CREATE STREAM BAZ WITH (KAFKA_TOPIC='BAZ', PARTITIONS=1, REPLICAS=1) AS SELECT * FROM FOO FOO INNER JOIN BAR BAR WITHIN 10 SECONDS ON ((FOO.ID = BAR.ID)) EMIT CHANGES;\nINSERTQUERY_0 (RUNNING) : INSERT INTO foo SELECT * FROM bar;\n\nFor query topology and execution plan please run: EXPLAIN <QueryId>\n\nQueries that write from this STREAM\n-----------------------------------\nINSERTQUERY_9 (RUNNING) : INSERT INTO bar SELECT * FROM foo;\nINSERTQUERY_13 (RUNNING) : INSERT INTO bar SELECT id from bob;\nBasically you can have insert into statements running left and right. In this case, there will be different consumer IDs fro each of the queries.", "author": "agavra", "createdAt": "2020-06-05T18:37:54Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +210,49 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    Optional<SourceConsumerGroupOffsets> sourceConsumerOffsets = Optional.empty();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+            .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        if (!sourceQueries.isEmpty()) {\n+          final QueryId queryId = sourceQueries.get(0).getId();", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM4MTg3Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441381877", "bodyText": "Got it. I have changed impl from Optional to List depending on the input sourceQueries.", "author": "jeqo", "createdAt": "2020-06-17T08:42:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA5OTAwNA=="}], "type": "inlineReview", "revised_code": {"commit": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 756854bb7e..bd040b7305 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -217,33 +215,30 @@ public final class ListSourceExecutor {\n \n     Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n         .empty();\n-    Optional<SourceConsumerGroupOffsets> sourceConsumerOffsets = Optional.empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         final String kafkaTopicName = dataSource.getKafkaTopicName();\n         final TopicDescription topicDescription = serviceContext.getTopicClient()\n-            .describeTopic(kafkaTopicName);\n+                .describeTopic(kafkaTopicName);\n         topicDescriptionOptional = Optional.of(topicDescription);\n-        if (!sourceQueries.isEmpty()) {\n-          final QueryId queryId = sourceQueries.get(0).getId();\n+        for (RunningQuery sourceQuery : sourceQueries) {\n+          final QueryId queryId = sourceQuery.getId();\n           final String persistenceQueryPrefix =\n               ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n           final String applicationId = getQueryApplicationId(\n-              getServiceId(ksqlConfig),\n+              KsqlConfig.getServiceId(ksqlConfig),\n               persistenceQueryPrefix,\n               queryId\n           );\n-          final Optional<ConsumerGroupSummary> consumerGroupDescription = Optional.of(\n-              serviceContext.getConsumerGroupClient().describeConsumerGroup(applicationId)\n-          );\n           final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n               serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n           final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n               serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n           final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n               serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n-          sourceConsumerOffsets = consumerGroupDescription.map(cg ->\n+          sourceConsumerOffsets.add(\n               new SourceConsumerGroupOffsets(\n                   applicationId,\n                   topicDescription.name(),\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NDQ4Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439554487", "bodyText": "when will these be null and is 0 a good default?", "author": "agavra", "createdAt": "2020-06-12T17:34:07Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE5NDM1NA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441194354", "bodyText": "It will be null when no offsets has been stored yet. e.g. stream created a new topic but topic does not have any data yet.\n0 seemed like a good default, though it might not be the right value: it gives the impression that offset 0 has been committed.\nWould -1 or just a hyphen - be better representation for this scenario?\nAlso, instead of dealing with this here, I can wrap this as an optional an let the client decide how to print. wdyt?", "author": "jeqo", "createdAt": "2020-06-16T23:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NDQ4Nw=="}], "type": "inlineReview", "revised_code": {"commit": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 756854bb7e..bd040b7305 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -295,11 +290,6 @@ public final class ListSourceExecutor {\n     return sourceConsumerGroupOffsets;\n   }\n \n-  private static String getServiceId(final KsqlConfig ksqlConfig) {\n-    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n-        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n-  }\n-\n   private static String getQueryApplicationId(\n       final String serviceId,\n       final String queryPrefix,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NTAwNA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439555004", "bodyText": "we should extract this to a utility class and reuse it in QueryExecutor (and ditto below)", "author": "agavra", "createdAt": "2020-06-12T17:35:14Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +263,50 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescriptionOptional,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<SourceConsumerGroupOffset> consumerOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets,\n+      final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<SourceConsumerGroupOffset> sourceConsumerGroupOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final ListOffsetsResultInfo startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final ListOffsetsResultInfo endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      sourceConsumerGroupOffsets.add(\n+          new SourceConsumerGroupOffset(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo != null ? startOffsetResultInfo.offset() : 0,\n+              endOffsetResultInfo != null ? endOffsetResultInfo.offset() : 0,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0\n+          ));\n+    }\n+    return sourceConsumerGroupOffsets;\n+  }\n+\n+  private static String getServiceId(final KsqlConfig ksqlConfig) {\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+  }", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTM5MTEyMg==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r441391122", "bodyText": "I've move it into KsqlConfig itself. Let me know if this is a better place.", "author": "jeqo", "createdAt": "2020-06-17T08:56:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NTAwNA=="}], "type": "inlineReview", "revised_code": {"commit": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 756854bb7e..bd040b7305 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -295,11 +290,6 @@ public final class ListSourceExecutor {\n     return sourceConsumerGroupOffsets;\n   }\n \n-  private static String getServiceId(final KsqlConfig ksqlConfig) {\n-    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n-        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n-  }\n-\n   private static String getQueryApplicationId(\n       final String serviceId,\n       final String queryPrefix,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NjQ3MA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r439556470", "bodyText": "we should add at least one test where this isn't empty to make sure equals/hashcode properly include it", "author": "agavra", "createdAt": "2020-06-12T17:38:28Z", "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "diffHunk": "@@ -53,117 +54,117 @@ public void shouldImplementHashCodeAndEqualsProperty() {\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING),\n+                    SOME_STRING, Optional.empty()),", "originalCommit": "46263e21f6feb7f38dc7e745de1fc9c9c7079e3a", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "chunk": "diff --git a/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java b/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\nindex d5d7ab18b3..9b96fcb475 100644\n--- a/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\n+++ b/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\n\n@@ -54,117 +53,117 @@ public class SourceDescriptionTest {\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty()),\n+                    SOME_STRING, Collections.emptyList()),\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     \"diff\", Optional.of(WindowType.SESSION), readQueries, writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), ImmutableList.of(), writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, ImmutableList.of(), fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, ImmutableList.of(),\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, \"diff\",\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                      \"diff\", SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, \"diff\", SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, \"diff\",\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, \"diff\", SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     !SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, \"diff\", SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, \"diff\", SOME_INT, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT + 1, SOME_INT,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT + 1,\n-                    SOME_STRING, Optional.empty())\n+                    SOME_STRING, Collections.emptyList())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    \"diff\", Optional.empty())\n+                    \"diff\", Collections.emptyList())\n             )\n             .testEquals();\n     }\n"}}, {"oid": "f628d074e28aaedf5b6eea83f4f6a22192382db2", "url": "https://github.com/confluentinc/ksql/commit/f628d074e28aaedf5b6eea83f4f6a22192382db2", "message": "fix: moar checkstyling", "committedDate": "2020-06-16T22:08:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ3ODU1NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445478555", "bodyText": "This method is not returning what the name suggests.  It's not returning the service id, (which would be just ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG).\nInstead, its returning the service id prefixed with KSQL_INTERNAL_TOPIC_PREFIX.\nI see you've moved this from another class, but I still think this isn't what we want.  KsqlConfig doesn't need to know about ReservedInternalTopics.\nThis is only needed to construct the query application id. Would you mind moving code into a QueryApplicationId util class? e.g.\n/**\n * Util for creating query application ids.\n */\npublic final class QueryApplicationId {\n\n  private QueryApplicationId() {\n  }\n\n  public static String getQueryApplicationId(\n      final KsqlConfig config,\n      final boolean persistent,\n      final QueryId queryId\n  ) {\n    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n\n    final String configName = persistent\n        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n    \n    final String queryPrefix = config.getString(configName);\n    \n    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n        + serviceId\n        + queryPrefix\n        + queryId;\n  }\n}\nThereby decoupling KqlConfig from any notion of internal topics, and also streamlining the existing code.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:04:44Z", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java", "diffHunk": "@@ -949,4 +949,9 @@ public KsqlConfig overrideBreakingConfigsWithOriginalValues(final Map<String, ?>\n     SslConfigs.addClientSslSupport(sslConfig);\n     return sslConfig.names();\n   }\n+\n+  public static String getServiceId(KsqlConfig ksqlConfig) {\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+  }", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java b/ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java\nindex d1558be93c..146c50c6d5 100644\n--- a/ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java\n+++ b/ksqldb-common/src/main/java/io/confluent/ksql/util/KsqlConfig.java\n\n@@ -949,9 +962,4 @@ public class KsqlConfig extends AbstractConfig {\n     SslConfigs.addClientSslSupport(sslConfig);\n     return sslConfig.names();\n   }\n-\n-  public static String getServiceId(KsqlConfig ksqlConfig) {\n-    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n-        + ksqlConfig.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n-  }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4MjM3Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445482376", "bodyText": "I think you need something move like to give a better error message to the user on failure:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                } catch (final Exception e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get offsets for Kafka Topic \" + topicName, e);\n          \n          \n            \n                }\n          \n          \n            \n               } catch (final TopicAuthorizationException e) {\n          \n          \n            \n                  final Set<String> topics = partitions.stream()\n          \n          \n            \n                      .map(TopicPartition::topic)\n          \n          \n            \n                      .collect(Collectors.toSet());\n          \n          \n            \n            \n          \n          \n            \n                  throw new KsqlTopicAuthorizationException(\n          \n          \n            \n                      AclOperation.DESCRIBE, topics);\n          \n          \n            \n                } catch (final ExecutionException e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get topic offsets. partitions: \" + partitions, e.getCause());\n          \n          \n            \n                } catch (final Exception e) {\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\n          \n          \n            \n                      \"Failed to get topic offsets. partitions: \" + partitions, e);\n          \n          \n            \n                }", "author": "big-andy-coates", "createdAt": "2020-06-25T11:12:50Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java", "diffHunk": "@@ -311,6 +316,27 @@ public void deleteInternalTopics(final String applicationId) {\n     }\n   }\n \n+  @Override\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n+      final String topicName,\n+      final OffsetSpec offsetSpec\n+  ) {\n+    final TopicDescription topicDescription = describeTopic(topicName);\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest = new HashMap<>();\n+    for (TopicPartitionInfo tpInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicName, tpInfo.partition());\n+      offsetsRequest.put(tp, offsetSpec);\n+    }\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get().listOffsets(offsetsRequest).all().get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to get offsets for Kafka Topic \" + topicName, e);\n+    }", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\nindex 52ae1c5bf6..94bc947954 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaTopicClientImpl.java\n\n@@ -317,23 +315,29 @@ public class KafkaTopicClientImpl implements KafkaTopicClient {\n   }\n \n   @Override\n-  public Map<TopicPartition, ListOffsetsResultInfo> listTopicOffsets(\n-      final String topicName,\n+  public Map<TopicPartition, ListOffsetsResultInfo> listTopicsOffsets(\n+      final Collection<String> topicNames,\n       final OffsetSpec offsetSpec\n   ) {\n-    final TopicDescription topicDescription = describeTopic(topicName);\n-    final Map<TopicPartition, OffsetSpec> offsetsRequest = new HashMap<>();\n-    for (TopicPartitionInfo tpInfo : topicDescription.partitions()) {\n-      final TopicPartition tp = new TopicPartition(topicName, tpInfo.partition());\n-      offsetsRequest.put(tp, offsetSpec);\n-    }\n+    final Map<TopicPartition, OffsetSpec> offsetsRequest =\n+        describeTopics(topicNames).entrySet().stream()\n+            .flatMap(entry ->\n+                entry.getValue().partitions()\n+                    .stream()\n+                    .map(tpInfo -> new TopicPartition(entry.getKey(), tpInfo.partition())))\n+            .collect(Collectors.toMap(tp -> tp, tp -> offsetSpec));\n     try {\n       return ExecutorUtil.executeWithRetries(\n           () -> adminClient.get().listOffsets(offsetsRequest).all().get(),\n           RetryBehaviour.ON_RETRYABLE);\n+    } catch (final TopicAuthorizationException e) {\n+      throw new KsqlTopicAuthorizationException(AclOperation.DESCRIBE, e.unauthorizedTopics());\n+    } catch (final ExecutionException e) {\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to get topic offsets. partitions: \" + offsetsRequest.keySet(), e.getCause());\n     } catch (final Exception e) {\n       throw new KafkaResponseGetFailedException(\n-          \"Failed to get offsets for Kafka Topic \" + topicName, e);\n+          \"Failed to get topic offsets. partitions: \" + offsetsRequest.keySet(), e);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NDU3Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445484576", "bodyText": "don't return null.  The contract is to throw an exception, e.g. KafkaResponseGetFailedException, on an unknown group.  (Not that the contract is documented!).", "author": "big-andy-coates", "createdAt": "2020-06-25T11:17:39Z", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package io.confluent.ksql.services;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class FakeKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(String group) {\n+    if (groups.contains(group)) {\n+      Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java b/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\nindex 71a0862db8..ef8693f12c 100644\n--- a/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\n+++ b/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\n\n@@ -2,7 +2,7 @@ package io.confluent.ksql.services;\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n-import java.util.Collections;\n+import io.confluent.ksql.exception.KafkaResponseGetFailedException;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NDgwOQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445484809", "bodyText": "Likewise.... throw on unknown group.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:18:06Z", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package io.confluent.ksql.services;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class FakeKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(String group) {\n+    if (groups.contains(group)) {\n+      Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(String group) {\n+    if (groups.contains(group)) {\n+      Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+      offsets.put(new TopicPartition(\"topic1\", 0), new OffsetAndMetadata(10));\n+      offsets.put(new TopicPartition(\"topic1\", 1), new OffsetAndMetadata(11));\n+      return offsets;\n+    } else {\n+      return Collections.emptyMap();", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java b/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\nindex 71a0862db8..ef8693f12c 100644\n--- a/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\n+++ b/ksqldb-engine/src/test/java/io/confluent/ksql/services/FakeKafkaConsumerGroupClient.java\n\n@@ -2,7 +2,7 @@ package io.confluent.ksql.services;\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n-import java.util.Collections;\n+import io.confluent.ksql.exception.KafkaResponseGetFailedException;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTA2NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485065", "bodyText": "As before - throw on unknown group", "author": "big-andy-coates", "createdAt": "2020-06-25T11:18:38Z", "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.test.tools.stubs;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.services.KafkaConsumerGroupClient;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class StubKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(final String group) {\n+    if (groups.contains(group)) {\n+      final Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java b/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\nindex 10939f14c7..31598c6075 100644\n--- a/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\n+++ b/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\n\n@@ -17,8 +17,8 @@ package io.confluent.ksql.test.tools.stubs;\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.exception.KafkaResponseGetFailedException;\n import io.confluent.ksql.services.KafkaConsumerGroupClient;\n-import java.util.Collections;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTA4OA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485088", "bodyText": "As before - throw on unknown group", "author": "big-andy-coates", "createdAt": "2020-06-25T11:18:42Z", "path": "ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.test.tools.stubs;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.services.KafkaConsumerGroupClient;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.apache.kafka.common.TopicPartition;\n+\n+public class StubKafkaConsumerGroupClient implements KafkaConsumerGroupClient {\n+\n+  private static final List<String> groups = ImmutableList.of(\"cg1\", \"cg2\");\n+\n+  @Override\n+  public List<String> listGroups() {\n+    return groups;\n+  }\n+\n+  @Override\n+  public ConsumerGroupSummary describeConsumerGroup(final String group) {\n+    if (groups.contains(group)) {\n+      final Set<ConsumerSummary> instances = ImmutableSet.of(\n+          new ConsumerSummary(group + \"-1\"),\n+          new ConsumerSummary(group + \"-2\")\n+      );\n+      return new ConsumerGroupSummary(instances);\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    if (groups.contains(group)) {\n+      final Map<TopicPartition, OffsetAndMetadata> offsets = new LinkedHashMap<>();\n+      offsets.put(new TopicPartition(\"topic1\", 0), new OffsetAndMetadata(10));\n+      offsets.put(new TopicPartition(\"topic1\", 1), new OffsetAndMetadata(11));\n+      return offsets;\n+    } else {\n+      return Collections.emptyMap();", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java b/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\nindex 10939f14c7..31598c6075 100644\n--- a/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\n+++ b/ksqldb-functional-tests/src/main/java/io/confluent/ksql/test/tools/stubs/StubKafkaConsumerGroupClient.java\n\n@@ -17,8 +17,8 @@ package io.confluent.ksql.test.tools.stubs;\n \n import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n+import io.confluent.ksql.exception.KafkaResponseGetFailedException;\n import io.confluent.ksql.services.KafkaConsumerGroupClient;\n-import java.util.Collections;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4NTY3MQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445485671", "bodyText": "Can you extend this error handling, as suggestted above for another function, to return more helpful error messages to the user please?\n\nadd catch block for auth errors\nadd catch block to unwrap execution errors.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:19:56Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -73,4 +80,18 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n       throw new KafkaResponseGetFailedException(\"Failed to describe Kafka consumer groups\", e);\n     }\n   }\n+\n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get()\n+              .listConsumerGroupOffsets(group)\n+              .partitionsToOffsetAndMetadata()\n+              .get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);\n+    }", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\nindex c10a647868..aa8841f613 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n\n@@ -75,9 +77,11 @@ public class KafkaConsumerGroupClientImpl implements KafkaConsumerGroupClient {\n                   })).collect(Collectors.toSet());\n \n       return new ConsumerGroupSummary(results);\n-\n+    } catch (final GroupAuthorizationException e) {\n+      throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n     } catch (final Exception e) {\n-      throw new KafkaResponseGetFailedException(\"Failed to describe Kafka consumer groups\", e);\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to describe Kafka consumer groups: \" + group, e);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4Njk2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445486966", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. Objects.requireNonNull", "author": "big-andy-coates", "createdAt": "2020-06-25T11:22:43Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffsets {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<SourceConsumerGroupOffset> offsets;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffsets(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;\n+    this.offsets = offsets;", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\nsimilarity index 79%\nrename from ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java\nrename to ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\nindex 56aad68fa5..b2157a2766 100644\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java\n+++ b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\n\n@@ -18,24 +18,26 @@ package io.confluent.ksql.rest.entity;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n import java.util.List;\n import java.util.Objects;\n \n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class SourceConsumerGroupOffsets {\n+public class QueryOffsetSummary {\n   private final String groupId;\n   private final String kafkaTopic;\n-  private final List<SourceConsumerGroupOffset> offsets;\n+  private final List<ConsumerPartitionOffsets> offsets;\n \n   @JsonCreator\n-  public SourceConsumerGroupOffsets(\n+  public QueryOffsetSummary(\n       @JsonProperty(\"groupId\") final String groupId,\n       @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n-      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n   ) {\n     this.groupId = groupId;\n     this.kafkaTopic = kafkaTopic;\n-    this.offsets = offsets;\n+    this.offsets =\n+        ImmutableList.copyOf(Objects.requireNonNull(offsets, \"offsets\"));\n   }\n \n   public String getGroupId() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4Nzg2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445487866", "bodyText": "Feels like a good candidate to move into its own function.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:24:44Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        for (RunningQuery sourceQuery : sourceQueries) {\n+          final QueryId queryId = sourceQuery.getId();\n+          final String persistenceQueryPrefix =\n+              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n+          final String applicationId = getQueryApplicationId(\n+              KsqlConfig.getServiceId(ksqlConfig),\n+              persistenceQueryPrefix,\n+              queryId\n+          );\n+          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n+              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n+              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n+          sourceConsumerOffsets.add(\n+              new SourceConsumerGroupOffsets(\n+                  applicationId,\n+                  topicDescription.name(),\n+                  consumerOffsets(\n+                      topicDescription,\n+                      topicAndStartOffsets,\n+                      topicAndEndOffsets,\n+                      topicAndConsumerOffsets)));\n+        }", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex bd040b7305..9eb9dba9a4 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -213,41 +218,15 @@ public final class ListSourceExecutor {\n     final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n-        .empty();\n-    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional =\n+        Optional.empty();\n+    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        final String kafkaTopicName = dataSource.getKafkaTopicName();\n-        final TopicDescription topicDescription = serviceContext.getTopicClient()\n-                .describeTopic(kafkaTopicName);\n-        topicDescriptionOptional = Optional.of(topicDescription);\n-        for (RunningQuery sourceQuery : sourceQueries) {\n-          final QueryId queryId = sourceQuery.getId();\n-          final String persistenceQueryPrefix =\n-              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n-          final String applicationId = getQueryApplicationId(\n-              KsqlConfig.getServiceId(ksqlConfig),\n-              persistenceQueryPrefix,\n-              queryId\n-          );\n-          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n-              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n-              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n-              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n-          sourceConsumerOffsets.add(\n-              new SourceConsumerGroupOffsets(\n-                  applicationId,\n-                  topicDescription.name(),\n-                  consumerOffsets(\n-                      topicDescription,\n-                      topicAndStartOffsets,\n-                      topicAndEndOffsets,\n-                      topicAndConsumerOffsets)));\n-        }\n+        topicDescriptionOptional = Optional.of(\n+            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName()));\n+        sourceConsumerOffsets = offsetsSummary(ksqlConfig, serviceContext, sinkQueries);\n       } catch (final KafkaException | KafkaResponseGetFailedException e) {\n         warnings.add(new KsqlWarning(\"Error from Kafka: \" + e.getMessage()));\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ4OTgzNw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445489837", "bodyText": "I think this is a bug: this is getting the offsets of the data source's sink topic, i.e. kafkaTopicName and comparing this to the consumer group offsets of queries writing into the sink topic.  Those queries won't be consuming kafkaTopicName, they'll be producing to it.\nOr am I missing somethinng?", "author": "big-andy-coates", "createdAt": "2020-06-25T11:28:52Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);\n+        for (RunningQuery sourceQuery : sourceQueries) {\n+          final QueryId queryId = sourceQuery.getId();\n+          final String persistenceQueryPrefix =\n+              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n+          final String applicationId = getQueryApplicationId(\n+              KsqlConfig.getServiceId(ksqlConfig),\n+              persistenceQueryPrefix,\n+              queryId\n+          );\n+          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n+              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n+          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n+              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex bd040b7305..9eb9dba9a4 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -213,41 +218,15 @@ public final class ListSourceExecutor {\n     final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n-        .empty();\n-    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional =\n+        Optional.empty();\n+    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        final String kafkaTopicName = dataSource.getKafkaTopicName();\n-        final TopicDescription topicDescription = serviceContext.getTopicClient()\n-                .describeTopic(kafkaTopicName);\n-        topicDescriptionOptional = Optional.of(topicDescription);\n-        for (RunningQuery sourceQuery : sourceQueries) {\n-          final QueryId queryId = sourceQuery.getId();\n-          final String persistenceQueryPrefix =\n-              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n-          final String applicationId = getQueryApplicationId(\n-              KsqlConfig.getServiceId(ksqlConfig),\n-              persistenceQueryPrefix,\n-              queryId\n-          );\n-          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n-              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n-              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n-              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n-          sourceConsumerOffsets.add(\n-              new SourceConsumerGroupOffsets(\n-                  applicationId,\n-                  topicDescription.name(),\n-                  consumerOffsets(\n-                      topicDescription,\n-                      topicAndStartOffsets,\n-                      topicAndEndOffsets,\n-                      topicAndConsumerOffsets)));\n-        }\n+        topicDescriptionOptional = Optional.of(\n+            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName()));\n+        sourceConsumerOffsets = offsetsSummary(ksqlConfig, serviceContext, sinkQueries);\n       } catch (final KafkaException | KafkaResponseGetFailedException e) {\n         warnings.add(new KsqlWarning(\"Error from Kafka: \" + e.getMessage()));\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MDQzOQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445490439", "bodyText": "nit: inline this.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:30:05Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +208,46 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n+        .empty();\n+    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        topicDescription = Optional.of(\n-            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n-        );\n+        final String kafkaTopicName = dataSource.getKafkaTopicName();\n+        final TopicDescription topicDescription = serviceContext.getTopicClient()\n+                .describeTopic(kafkaTopicName);\n+        topicDescriptionOptional = Optional.of(topicDescription);", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex bd040b7305..9eb9dba9a4 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -213,41 +218,15 @@ public final class ListSourceExecutor {\n     final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n         q -> q.getSinkName().equals(dataSource.getName()));\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional = Optional\n-        .empty();\n-    final List<SourceConsumerGroupOffsets> sourceConsumerOffsets = new ArrayList<>();\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescriptionOptional =\n+        Optional.empty();\n+    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n-        final String kafkaTopicName = dataSource.getKafkaTopicName();\n-        final TopicDescription topicDescription = serviceContext.getTopicClient()\n-                .describeTopic(kafkaTopicName);\n-        topicDescriptionOptional = Optional.of(topicDescription);\n-        for (RunningQuery sourceQuery : sourceQueries) {\n-          final QueryId queryId = sourceQuery.getId();\n-          final String persistenceQueryPrefix =\n-              ksqlConfig.getString(KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG);\n-          final String applicationId = getQueryApplicationId(\n-              KsqlConfig.getServiceId(ksqlConfig),\n-              persistenceQueryPrefix,\n-              queryId\n-          );\n-          final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n-              serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndStartOffsets =\n-              serviceContext.getTopicClient().listTopicStartOffsets(kafkaTopicName);\n-          final Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets =\n-              serviceContext.getTopicClient().listTopicEndOffsets(kafkaTopicName);\n-          sourceConsumerOffsets.add(\n-              new SourceConsumerGroupOffsets(\n-                  applicationId,\n-                  topicDescription.name(),\n-                  consumerOffsets(\n-                      topicDescription,\n-                      topicAndStartOffsets,\n-                      topicAndEndOffsets,\n-                      topicAndConsumerOffsets)));\n-        }\n+        topicDescriptionOptional = Optional.of(\n+            serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName()));\n+        sourceConsumerOffsets = offsetsSummary(ksqlConfig, serviceContext, sinkQueries);\n       } catch (final KafkaException | KafkaResponseGetFailedException e) {\n         warnings.add(new KsqlWarning(\"Error from Kafka: \" + e.getMessage()));\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MjU3NA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445492574", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. ensure none of these are negative:\nPreconditions.checkArgument(partition <= 0, \"invalid partition: \" +. partition);\n // etc", "author": "big-andy-coates", "createdAt": "2020-06-25T11:34:43Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffset {\n+\n+  private final int partition;\n+  private final long logStartOffset;\n+  private final long logEndOffset;\n+  private final long consumerOffset;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffset(\n+      @JsonProperty(\"partition\") final int partition,\n+      @JsonProperty(\"logStartOffset\") final long logStartOffset,\n+      @JsonProperty(\"logEndOffset\") final long logEndOffset,\n+      @JsonProperty(\"consumerOffset\") final long consumerOffset\n+  ) {\n+    this.partition = partition;\n+    this.logStartOffset = logStartOffset;\n+    this.logEndOffset = logEndOffset;\n+    this.consumerOffset = consumerOffset;", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/ConsumerPartitionOffsets.java\nsimilarity index 93%\nrename from ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java\nrename to ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/ConsumerPartitionOffsets.java\nindex c9430d6ed0..b75eef8171 100644\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffset.java\n+++ b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/ConsumerPartitionOffsets.java\n\n@@ -21,7 +21,7 @@ import com.fasterxml.jackson.annotation.JsonProperty;\n import java.util.Objects;\n \n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class SourceConsumerGroupOffset {\n+public class ConsumerPartitionOffsets {\n \n   private final int partition;\n   private final long logStartOffset;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MzQyOQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445493429", "bodyText": "nit: Prefer ImmutableList.copyOf to Collections.unmodifiableList.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:36:28Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "diffHunk": "@@ -88,6 +90,8 @@ public SourceDescription(\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n+    this.consumerGroupsOffsets = Collections.unmodifiableList(", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\nindex a41cd7f51b..b1bf2123b4 100644\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\n+++ b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\n\n@@ -90,8 +90,8 @@ public class SourceDescription {\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n-    this.consumerGroupsOffsets = Collections.unmodifiableList(\n-            Objects.requireNonNull(consumerGroupsOffsets, \"consumerOffsets\"));\n+    this.queryOffsetSummary = Collections.unmodifiableList(\n+            Objects.requireNonNull(queryOffsetSummary, \"queryOffsetSummary\"));\n   }\n \n   public String getStatement() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTQ5MzcwNA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r445493704", "bodyText": "This exposes mutable state of the object, which breaks encapsulation.  Please use ImmutableList.copyOf in the constructor.", "author": "big-andy-coates", "createdAt": "2020-06-25T11:37:06Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class SourceConsumerGroupOffsets {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<SourceConsumerGroupOffset> offsets;\n+\n+  @JsonCreator\n+  public SourceConsumerGroupOffsets(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;\n+    this.offsets = offsets;\n+  }\n+\n+  public String getGroupId() {\n+    return groupId;\n+  }\n+\n+  public String getKafkaTopic() {\n+    return kafkaTopic;\n+  }\n+\n+  public List<SourceConsumerGroupOffset> getOffsets() {\n+    return offsets;", "originalCommit": "d7de0def3b0a72ffff8a6916978e3d588facf0cf", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c68155154523f7e8fcdad0096f47d5148c112a85", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\nsimilarity index 79%\nrename from ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java\nrename to ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\nindex 56aad68fa5..b2157a2766 100644\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceConsumerGroupOffsets.java\n+++ b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\n\n@@ -18,24 +18,26 @@ package io.confluent.ksql.rest.entity;\n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n import java.util.List;\n import java.util.Objects;\n \n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class SourceConsumerGroupOffsets {\n+public class QueryOffsetSummary {\n   private final String groupId;\n   private final String kafkaTopic;\n-  private final List<SourceConsumerGroupOffset> offsets;\n+  private final List<ConsumerPartitionOffsets> offsets;\n \n   @JsonCreator\n-  public SourceConsumerGroupOffsets(\n+  public QueryOffsetSummary(\n       @JsonProperty(\"groupId\") final String groupId,\n       @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n-      @JsonProperty(\"offsets\") final List<SourceConsumerGroupOffset> offsets\n+      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n   ) {\n     this.groupId = groupId;\n     this.kafkaTopic = kafkaTopic;\n-    this.offsets = offsets;\n+    this.offsets =\n+        ImmutableList.copyOf(Objects.requireNonNull(offsets, \"offsets\"));\n   }\n \n   public String getGroupId() {\n"}}, {"oid": "8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "url": "https://github.com/confluentinc/ksql/commit/8c33496cafc3e689efbe6c45d1bd0e46f8c7fc5f", "message": "turn to static", "committedDate": "2020-07-06T20:57:25Z", "type": "forcePushed"}, {"oid": "c68155154523f7e8fcdad0096f47d5148c112a85", "url": "https://github.com/confluentinc/ksql/commit/c68155154523f7e8fcdad0096f47d5148c112a85", "message": "fix sandboxed topic client mapping test", "committedDate": "2020-07-11T16:16:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDg0MQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r453204841", "bodyText": "@big-andy-coates should we also add time suffix_ as in:\n  private static String addTimeSuffix(final String original) {\n    return String.format(\"%s_%d\", original, System.currentTimeMillis());\n  }", "author": "jeqo", "createdAt": "2020-07-11T15:19:47Z", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {\n+\n+  private QueryApplicationId() {\n+  }\n+\n+  public static String build(\n+      final KsqlConfig config,\n+      final boolean persistent,\n+      final QueryId queryId\n+  ) {\n+    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n+\n+    final String configName = persistent\n+        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n+        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n+\n+    final String queryPrefix = config.getString(configName);\n+\n+    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n+        + serviceId\n+        + queryPrefix\n+        + queryId;", "originalCommit": "d795becb3948f8388cb74d75cb7985b694ef606c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDM1OQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463044359", "bodyText": "Good call. Makes sense to have this method append the time suffix for transient queries, i.e. when !persistent.", "author": "big-andy-coates", "createdAt": "2020-07-30T14:37:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDg0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java b/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java\ndeleted file mode 100644\nindex a9f8cfc9e9..0000000000\n--- a/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java\n+++ /dev/null\n\n@@ -1,46 +0,0 @@\n-/*\n- * Copyright 2019 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.util;\n-\n-import io.confluent.ksql.query.QueryId;\n-\n-/**\n- * Util to build query application ids.\n- */\n-public final class QueryApplicationId {\n-\n-  private QueryApplicationId() {\n-  }\n-\n-  public static String build(\n-      final KsqlConfig config,\n-      final boolean persistent,\n-      final QueryId queryId\n-  ) {\n-    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n-\n-    final String configName = persistent\n-        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n-        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n-\n-    final String queryPrefix = config.getString(configName);\n-\n-    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n-        + serviceId\n-        + queryPrefix\n-        + queryId;\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNTA2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r453205066", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);\n          \n          \n            \n                  throw new KafkaResponseGetFailedException(\"Failed to list Kafka consumer groups offsets\", e);", "author": "jeqo", "createdAt": "2020-07-11T15:21:50Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java", "diffHunk": "@@ -68,9 +77,27 @@ public ConsumerGroupSummary describeConsumerGroup(final String group) {\n                   })).collect(Collectors.toSet());\n \n       return new ConsumerGroupSummary(results);\n+    } catch (final GroupAuthorizationException e) {\n+      throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n+    } catch (final Exception e) {\n+      throw new KafkaResponseGetFailedException(\n+          \"Failed to describe Kafka consumer groups: \" + group, e);\n+    }\n+  }\n \n+  @Override\n+  public Map<TopicPartition, OffsetAndMetadata> listConsumerGroupOffsets(final String group) {\n+    try {\n+      return ExecutorUtil.executeWithRetries(\n+          () -> adminClient.get()\n+              .listConsumerGroupOffsets(group)\n+              .partitionsToOffsetAndMetadata()\n+              .get(),\n+          RetryBehaviour.ON_RETRYABLE);\n+    } catch (final GroupAuthorizationException e) {\n+      throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n     } catch (final Exception e) {\n-      throw new KafkaResponseGetFailedException(\"Failed to describe Kafka consumer groups\", e);\n+      throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);", "originalCommit": "d795becb3948f8388cb74d75cb7985b694ef606c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4b3964d4c21dbe30996a0efc747d74801b73d559", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\nindex aa8841f613..76df012d3c 100644\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n+++ b/ksqldb-engine/src/main/java/io/confluent/ksql/services/KafkaConsumerGroupClientImpl.java\n\n@@ -97,7 +97,7 @@ public class KafkaConsumerGroupClientImpl implements KafkaConsumerGroupClient {\n     } catch (final GroupAuthorizationException e) {\n       throw new KsqlGroupAuthorizationException(AclOperation.DESCRIBE, group);\n     } catch (final Exception e) {\n-      throw new KafkaResponseGetFailedException(\"Failed to retrieve Kafka consumer groups\", e);\n+      throw new KafkaResponseGetFailedException(\"Failed to list Kafka consumer groups offsets\", e);\n     }\n   }\n }\n"}}, {"oid": "4b3964d4c21dbe30996a0efc747d74801b73d559", "url": "https://github.com/confluentinc/ksql/commit/4b3964d4c21dbe30996a0efc747d74801b73d559", "message": "fix preconditions", "committedDate": "2020-07-15T09:27:43Z", "type": "forcePushed"}, {"oid": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "url": "https://github.com/confluentinc/ksql/commit/086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "message": "fix call to sourcedescription", "committedDate": "2020-07-22T15:53:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDExNQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463044115", "bodyText": "Class would benefit from a couple of tests!  Maybe one test for persistent and one for transient.", "author": "big-andy-coates", "createdAt": "2020-07-30T14:36:59Z", "path": "ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.util;\n+\n+import io.confluent.ksql.query.QueryId;\n+\n+/**\n+ * Util to build query application ids.\n+ */\n+public final class QueryApplicationId {", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NzU2MQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465747561", "bodyText": "ack", "author": "jeqo", "createdAt": "2020-08-05T13:59:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0NDExNQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java b/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java\ndeleted file mode 100644\nindex a9f8cfc9e9..0000000000\n--- a/ksqldb-common/src/main/java/io/confluent/ksql/util/QueryApplicationId.java\n+++ /dev/null\n\n@@ -1,46 +0,0 @@\n-/*\n- * Copyright 2019 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.util;\n-\n-import io.confluent.ksql.query.QueryId;\n-\n-/**\n- * Util to build query application ids.\n- */\n-public final class QueryApplicationId {\n-\n-  private QueryApplicationId() {\n-  }\n-\n-  public static String build(\n-      final KsqlConfig config,\n-      final boolean persistent,\n-      final QueryId queryId\n-  ) {\n-    final String serviceId = config.getString(KsqlConfig.KSQL_SERVICE_ID_CONFIG);\n-\n-    final String configName = persistent\n-        ? KsqlConfig.KSQL_PERSISTENT_QUERY_NAME_PREFIX_CONFIG\n-        : KsqlConfig.KSQL_TRANSIENT_QUERY_NAME_PREFIX_CONFIG;\n-\n-    final String queryPrefix = config.getString(configName);\n-\n-    return ReservedInternalTopics.KSQL_INTERNAL_TOPIC_PREFIX\n-        + serviceId\n-        + queryPrefix\n-        + queryId;\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA0OTI0OQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463049249", "bodyText": "nit: commented out code.", "author": "big-andy-coates", "createdAt": "2020-07-30T14:43:59Z", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java", "diffHunk": "@@ -747,6 +817,29 @@ private void givenTopicConfigs(\n     };\n   }\n \n+  private Answer<ListOffsetsResult> listTopicOffsets() {\n+    return inv -> {\n+      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n+      when(result.all()).thenReturn(KafkaFuture.completedFuture(ImmutableMap.of(\n+          new TopicPartition(\"topicA\", 0),\n+          new ListOffsetsResultInfo(100L, 0L, Optional.empty()))));\n+      return result;\n+    };\n+  }\n+\n+  private Answer<ListOffsetsResult> listTopicOffsets(final Exception e) {\n+    return inv -> {\n+      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n+      final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> f = failedFuture(e);\n+      when(result.all()).thenReturn(f);\n+//      return new ListOffsetsResult(\n+//          ImmutableMap.of(\n+//              new TopicPartition(\"topicA\", 0),\n+//              new ListOffsetsResultInfo(100L, 0L, Optional.empty())));", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java b/ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java\nindex 56d9cbb5ba..4330eba52a 100644\n--- a/ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java\n+++ b/ksqldb-engine/src/test/java/io/confluent/ksql/services/KafkaTopicClientImplTest.java\n\n@@ -817,29 +787,6 @@ public class KafkaTopicClientImplTest {\n     };\n   }\n \n-  private Answer<ListOffsetsResult> listTopicOffsets() {\n-    return inv -> {\n-      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n-      when(result.all()).thenReturn(KafkaFuture.completedFuture(ImmutableMap.of(\n-          new TopicPartition(\"topicA\", 0),\n-          new ListOffsetsResultInfo(100L, 0L, Optional.empty()))));\n-      return result;\n-    };\n-  }\n-\n-  private Answer<ListOffsetsResult> listTopicOffsets(final Exception e) {\n-    return inv -> {\n-      final ListOffsetsResult result = mock(ListOffsetsResult.class);\n-      final KafkaFuture<Map<TopicPartition, ListOffsetsResultInfo>> f = failedFuture(e);\n-      when(result.all()).thenReturn(f);\n-//      return new ListOffsetsResult(\n-//          ImmutableMap.of(\n-//              new TopicPartition(\"topicA\", 0),\n-//              new ListOffsetsResultInfo(100L, 0L, Optional.empty())));\n-      return result;\n-    };\n-  }\n-\n   private static Answer<ListTopicsResult> listTopicResult(final Exception e) {\n     return inv -> {\n       final ListTopicsResult listTopicsResult = mock(ListTopicsResult.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MDUwMw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463050503", "bodyText": "Can you add tests to SupportedMethods for these new methods?", "author": "big-andy-coates", "createdAt": "2020-07-30T14:45:33Z", "path": "ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java", "diffHunk": "@@ -78,6 +78,8 @@ private SandboxedKafkaTopicClientTest() {\n           .ignore(\"describeTopic\", String.class)\n           .ignore(\"describeTopics\", Collection.class)\n           .ignore(\"deleteTopics\", Collection.class)\n+          .ignore(\"listTopicsStartOffsets\", Collection.class)\n+          .ignore(\"listTopicsEndOffsets\", Collection.class)", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1NDQyNw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465754427", "bodyText": "ack", "author": "jeqo", "createdAt": "2020-08-05T14:09:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MDUwMw=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java b/ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java\nindex b5d626c159..2e3e121264 100644\n--- a/ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java\n+++ b/ksqldb-engine/src/test/java/io/confluent/ksql/services/SandboxedKafkaTopicClientTest.java\n\n@@ -78,8 +78,6 @@ public class SandboxedKafkaTopicClientTest {\n           .ignore(\"describeTopic\", String.class)\n           .ignore(\"describeTopics\", Collection.class)\n           .ignore(\"deleteTopics\", Collection.class)\n-          .ignore(\"listTopicsStartOffsets\", Collection.class)\n-          .ignore(\"listTopicsEndOffsets\", Collection.class)\n           .build();\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MzQwMQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463053401", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state.\ni.e. Objects.requireNonNull", "author": "big-andy-coates", "createdAt": "2020-07-30T14:49:28Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.entity;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.google.common.collect.ImmutableList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class QueryOffsetSummary {\n+  private final String groupId;\n+  private final String kafkaTopic;\n+  private final List<ConsumerPartitionOffsets> offsets;\n+\n+  @JsonCreator\n+  public QueryOffsetSummary(\n+      @JsonProperty(\"groupId\") final String groupId,\n+      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n+      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n+  ) {\n+    this.groupId = groupId;\n+    this.kafkaTopic = kafkaTopic;", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1NDYwMA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465754600", "bodyText": "ack/", "author": "jeqo", "createdAt": "2020-08-05T14:09:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1MzQwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\ndeleted file mode 100644\nindex b2157a2766..0000000000\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/QueryOffsetSummary.java\n+++ /dev/null\n\n@@ -1,73 +0,0 @@\n-/*\n- * Copyright 2018 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.rest.entity;\n-\n-import com.fasterxml.jackson.annotation.JsonCreator;\n-import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n-import com.fasterxml.jackson.annotation.JsonProperty;\n-import com.google.common.collect.ImmutableList;\n-import java.util.List;\n-import java.util.Objects;\n-\n-@JsonIgnoreProperties(ignoreUnknown = true)\n-public class QueryOffsetSummary {\n-  private final String groupId;\n-  private final String kafkaTopic;\n-  private final List<ConsumerPartitionOffsets> offsets;\n-\n-  @JsonCreator\n-  public QueryOffsetSummary(\n-      @JsonProperty(\"groupId\") final String groupId,\n-      @JsonProperty(\"kafkaTopic\") final String kafkaTopic,\n-      @JsonProperty(\"offsets\") final List<ConsumerPartitionOffsets> offsets\n-  ) {\n-    this.groupId = groupId;\n-    this.kafkaTopic = kafkaTopic;\n-    this.offsets =\n-        ImmutableList.copyOf(Objects.requireNonNull(offsets, \"offsets\"));\n-  }\n-\n-  public String getGroupId() {\n-    return groupId;\n-  }\n-\n-  public String getKafkaTopic() {\n-    return kafkaTopic;\n-  }\n-\n-  public List<ConsumerPartitionOffsets> getOffsets() {\n-    return offsets;\n-  }\n-\n-  @Override\n-  public boolean equals(final Object o) {\n-    if (this == o) {\n-      return true;\n-    }\n-    if (o == null || getClass() != o.getClass()) {\n-      return false;\n-    }\n-    final QueryOffsetSummary that = (QueryOffsetSummary) o;\n-    return Objects.equals(groupId, that.groupId)\n-        && Objects.equals(kafkaTopic, that.kafkaTopic)\n-        && Objects.equals(offsets, that.offsets);\n-  }\n-\n-  @Override\n-  public int hashCode() {\n-    return Objects.hash(groupId, kafkaTopic, offsets);\n-  }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NDY5MA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463054690", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                this.queryOffsetSummaries = Collections.unmodifiableList(\n          \n          \n            \n                this.queryOffsetSummaries = ImmutableList.copyOf(", "author": "big-andy-coates", "createdAt": "2020-07-30T14:51:13Z", "path": "ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java", "diffHunk": "@@ -88,6 +90,8 @@ public SourceDescription(\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n+    this.queryOffsetSummaries = Collections.unmodifiableList(", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\nindex 65bb82825b..b194ad8223 100644\n--- a/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\n+++ b/ksqldb-rest-model/src/main/java/io/confluent/ksql/rest/entity/SourceDescription.java\n\n@@ -90,8 +89,7 @@ public class SourceDescription {\n     this.partitions = partitions;\n     this.replication = replication;\n     this.statement = Objects.requireNonNull(statement, \"statement\");\n-    this.queryOffsetSummaries = Collections.unmodifiableList(\n-            Objects.requireNonNull(queryOffsetSummaries, \"queryOffsetSummaries\"));\n+    this.sourceConsumerGroupOffsets = sourceConsumerGroupOffsets;\n   }\n \n   public String getStatement() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NTc0NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463055745", "bodyText": "nit: Can you add a new equality group where the summaries are different please?", "author": "big-andy-coates", "createdAt": "2020-07-30T14:52:41Z", "path": "ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java", "diffHunk": "@@ -39,131 +39,141 @@\n     private RunningQuery query2;\n     @Mock\n     private FieldInfo fieldInfo;\n+    @Mock\n+    private QueryOffsetSummary summary;\n \n     @SuppressWarnings(\"UnstableApiUsage\")\n     @Test\n     public void shouldImplementHashCodeAndEqualsProperty() {\n         final List<RunningQuery> readQueries = Collections.singletonList(query1);\n         final List<RunningQuery> writeQueries = Collections.singletonList(query2);\n         final List<FieldInfo> fields = Collections.singletonList(fieldInfo);\n+        final List<QueryOffsetSummary> summaries = Collections.singletonList(summary);\n \n         new EqualsTester()\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING),\n+                    SOME_STRING, summaries),\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     \"diff\", Optional.of(WindowType.SESSION), readQueries, writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), ImmutableList.of(), writeQueries, fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, ImmutableList.of(), fields,\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, ImmutableList.of(),\n                     SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n+            )\n+            .addEqualityGroup(\n+                new SourceDescription(\n+                    SOME_STRING, Optional.empty(), readQueries, writeQueries, fields,\n+                    SOME_STRING, SOME_STRING, SOME_STRING, SOME_STRING,\n+                    SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n+                    SOME_STRING, ImmutableList.of())\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, \"diff\",\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                      \"diff\", SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, \"diff\", SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, \"diff\",\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, \"diff\", SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     !SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, \"diff\", SOME_STRING, SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, \"diff\", SOME_INT, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT + 1, SOME_INT,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT + 1,\n-                    SOME_STRING)\n+                    SOME_STRING, summaries)\n             )\n             .addEqualityGroup(\n                 new SourceDescription(\n                     SOME_STRING, Optional.empty(), readQueries, writeQueries, fields, SOME_STRING,\n                     SOME_STRING, SOME_STRING, SOME_STRING,\n                     SOME_BOOL, SOME_STRING, SOME_STRING, SOME_STRING, SOME_INT, SOME_INT,\n-                    \"diff\")\n+                    \"diff\", summaries)", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyODcyOA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465828728", "bodyText": "ack.", "author": "jeqo", "createdAt": "2020-08-05T15:52:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA1NTc0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java b/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\nindex 61af6aff61..d44abbc60a 100644\n--- a/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\n+++ b/ksqldb-rest-model/src/test/java/io/confluent/ksql/rest/entity/SourceDescriptionTest.java\n\n@@ -32,6 +33,8 @@ public class SourceDescriptionTest {\n     private static final String SOME_STRING = \"some string\";\n     private static final int SOME_INT = 3;\n     private static final boolean SOME_BOOL = true;\n+    private static final SourceConsumerGroupOffsets SOME_CG_OFFSETS =\n+        new SourceConsumerGroupOffsets(\"group\", \"topic\", new ArrayList<>());\n \n     @Mock\n     private RunningQuery query1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2MTEwOA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463061108", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                          .orElse(-1)));\n          \n          \n            \n                          .orElse(0)));\n          \n      \n    \n    \n  \n\n??", "author": "big-andy-coates", "createdAt": "2020-07-30T14:59:59Z", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -614,6 +616,33 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n+      writer().println();\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Consumer Group\", queryOffsetSummary.getGroupId()));\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Kafka topic\", queryOffsetSummary.getKafkaTopic()));\n+      writer().println(String.format(\"%-20s : %s\",\n+          \"Max lag\", queryOffsetSummary.getOffsets().stream()\n+              .mapToLong(s -> s.getLogEndOffset() - s.getConsumerOffset())\n+              .max()\n+              .orElse(-1)));", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\nindex dbd59bd3e0..83353c03d1 100644\n--- a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n+++ b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n\n@@ -616,33 +624,6 @@ public class Console implements Closeable {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n-    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n-      writer().println();\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Consumer Group\", queryOffsetSummary.getGroupId()));\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Kafka topic\", queryOffsetSummary.getKafkaTopic()));\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Max lag\", queryOffsetSummary.getOffsets().stream()\n-              .mapToLong(s -> s.getLogEndOffset() - s.getConsumerOffset())\n-              .max()\n-              .orElse(-1)));\n-      writer().println(\"\");\n-      final Table taskTable = new Table.Builder()\n-          .withColumnHeaders(\n-              ImmutableList.of(\"Partition\", \"Start Offset\", \"End Offset\", \"Offset\", \"Lag\"))\n-          .withRows(queryOffsetSummary.getOffsets()\n-              .stream()\n-              .map(offset -> ImmutableList.of(\n-                  String.valueOf(offset.getPartition()),\n-                  String.valueOf(offset.getLogStartOffset()),\n-                  String.valueOf(offset.getLogEndOffset()),\n-                  String.valueOf(offset.getConsumerOffset()),\n-                  String.valueOf(offset.getLogEndOffset() - offset.getConsumerOffset())\n-              )))\n-          .build();\n-      taskTable.print(this);\n-    }\n   }\n \n   private void printSourceDescriptionList(final SourceDescriptionList sourceDescriptionList) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NDA0MQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463064041", "bodyText": "Rather than adding a whole new test, just enhance shouldPrintTopicDescribeExtended a non-empty list in as the last parameter of the SourceDescriptionEntity constructor.", "author": "big-andy-coates", "createdAt": "2020-07-30T15:03:54Z", "path": "ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java", "diffHunk": "@@ -1135,6 +1139,175 @@ public void testPrintExecuptionPlan() {\n     }\n   }\n \n+  @Test\n+  public void shouldPrintTopicDescribeExtendedWithConsumerOffsets() {\n+    // Given:\n+    final List<RunningQuery> readQueries = ImmutableList.of(\n+        new RunningQuery(\"read query\", ImmutableSet.of(\"sink1\"), ImmutableSet.of(\"sink1 topic\"), new QueryId(\"readId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+    final List<RunningQuery> writeQueries = ImmutableList.of(\n+        new RunningQuery(\"write query\", ImmutableSet.of(\"sink2\"), ImmutableSet.of(\"sink2 topic\"), new QueryId(\"writeId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n+    );\n+\n+    final KsqlEntityList entityList = new KsqlEntityList(ImmutableList.of(\n+        new SourceDescriptionEntity(\n+            \"e\",\n+            new SourceDescription(\n+                \"TestSource\",\n+                Optional.empty(),\n+                readQueries,\n+                writeQueries,\n+                buildTestSchema(SqlTypes.STRING),\n+                DataSourceType.KTABLE.getKsqlType(),\n+                \"2000-01-01\",\n+                \"stats\",\n+                \"errors\",\n+                true,\n+                \"kafka\",\n+                \"avro\",\n+                \"kadka-topic\",\n+                2, 1,\n+                \"sql statement text\",\n+                ImmutableList.of(\n+                    new QueryOffsetSummary(\n+                        \"consumer1\",\n+                        \"kadka-topic\",\n+                        ImmutableList.of(\n+                            new ConsumerPartitionOffsets(0, 100, 900, 800),\n+                            new ConsumerPartitionOffsets(1, 50, 900, 900)\n+                        ))\n+                )),\n+            Collections.emptyList()\n+        ))\n+    );\n+\n+    // When:\n+    console.printKsqlEntityList(entityList);\n+\n+    // Then:\n+    final String output = terminal.getOutputString();\n+    if (console.getOutputFormat() == OutputFormat.JSON) {\n+      assertThat(output, is(\"[ {\" + NEWLINE\n+          + \"  \\\"@type\\\" : \\\"sourceDescription\\\",\" + NEWLINE\n+          + \"  \\\"statementText\\\" : \\\"e\\\",\" + NEWLINE\n+          + \"  \\\"sourceDescription\\\" : {\" + NEWLINE\n+          + \"    \\\"name\\\" : \\\"TestSource\\\",\" + NEWLINE\n+          + \"    \\\"windowType\\\" : null,\" + NEWLINE\n+          + \"    \\\"readQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"read query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink1\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink1 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"readId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"writeQueries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"queryString\\\" : \\\"write query\\\",\" + NEWLINE\n+          + \"      \\\"sinks\\\" : [ \\\"sink2\\\" ],\" + NEWLINE\n+          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink2 topic\\\" ],\" + NEWLINE\n+          + \"      \\\"id\\\" : \\\"writeId\\\",\" + NEWLINE\n+          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n+          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n+          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n+          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"fields\\\" : [ {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"ROWKEY\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      },\" + NEWLINE\n+          + \"      \\\"type\\\" : \\\"KEY\\\"\" + NEWLINE\n+          + \"    }, {\" + NEWLINE\n+          + \"      \\\"name\\\" : \\\"f_0\\\",\" + NEWLINE\n+          + \"      \\\"schema\\\" : {\" + NEWLINE\n+          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n+          + \"        \\\"fields\\\" : null,\" + NEWLINE\n+          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n+          + \"      }\" + NEWLINE\n+          + \"    } ],\" + NEWLINE\n+          + \"    \\\"type\\\" : \\\"TABLE\\\",\" + NEWLINE\n+          + \"    \\\"timestamp\\\" : \\\"2000-01-01\\\",\" + NEWLINE\n+          + \"    \\\"statistics\\\" : \\\"stats\\\",\" + NEWLINE\n+          + \"    \\\"errorStats\\\" : \\\"errors\\\",\" + NEWLINE\n+          + \"    \\\"extended\\\" : true,\" + NEWLINE\n+          + \"    \\\"keyFormat\\\" : \\\"kafka\\\",\" + NEWLINE\n+          + \"    \\\"valueFormat\\\" : \\\"avro\\\",\" + NEWLINE\n+          + \"    \\\"topic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"    \\\"partitions\\\" : 2,\" + NEWLINE\n+          + \"    \\\"replication\\\" : 1,\" + NEWLINE\n+          + \"    \\\"statement\\\" : \\\"sql statement text\\\",\" + NEWLINE\n+          + \"    \\\"queryOffsetSummaries\\\" : [ {\" + NEWLINE\n+          + \"      \\\"groupId\\\" : \\\"consumer1\\\",\" + NEWLINE\n+          + \"      \\\"kafkaTopic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n+          + \"      \\\"offsets\\\" : [ {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 0,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 100,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 800\" + NEWLINE\n+          + \"      }, {\" + NEWLINE\n+          + \"        \\\"partition\\\" : 1,\" + NEWLINE\n+          + \"        \\\"logStartOffset\\\" : 50,\" + NEWLINE\n+          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n+          + \"        \\\"consumerOffset\\\" : 900\" + NEWLINE\n+          + \"      } ]\" + NEWLINE\n+          + \"    } ]\" + NEWLINE\n+          + \"  },\" + NEWLINE\n+          + \"  \\\"warnings\\\" : [ ]\" + NEWLINE\n+          + \"} ]\" + NEWLINE));\n+    } else {\n+      assertThat(output, is(\"\" + NEWLINE\n+          + \"Name                 : TestSource\" + NEWLINE\n+          + \"Type                 : TABLE\" + NEWLINE\n+          + \"Timestamp field      : 2000-01-01\" + NEWLINE\n+          + \"Key format           : kafka\" + NEWLINE\n+          + \"Value format         : avro\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic (partitions: 2, replication: 1)\" + NEWLINE\n+          + \"Statement            : sql statement text\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \" Field  | Type                           \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \" ROWKEY | VARCHAR(STRING)  (primary key) \" + NEWLINE\n+          + \" f_0    | VARCHAR(STRING)                \" + NEWLINE\n+          + \"-----------------------------------------\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that read from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"readId (\" + AGGREGATE_STATUS +\") : read query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Queries that write from this TABLE\" + NEWLINE\n+          + \"-----------------------------------\" + NEWLINE\n+          + \"writeId (\" + AGGREGATE_STATUS + \") : write query\" + NEWLINE\n+          + \"\\n\"\n+          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n+          + \"\" + NEWLINE\n+          + \"Local runtime statistics\" + NEWLINE\n+          + \"------------------------\" + NEWLINE\n+          + \"stats\" + NEWLINE\n+          + \"errors\" + NEWLINE\n+          + \"(Statistics of the local KSQL server interaction with the Kafka topic kadka-topic)\" + NEWLINE\n+          + NEWLINE\n+          + \"Consumer Group       : consumer1\" + NEWLINE\n+          + \"Kafka topic          : kadka-topic\" + NEWLINE\n+          + \"Max lag              : 100\" + NEWLINE\n+          + NEWLINE\n+          + \" Partition | Start Offset | End Offset | Offset | Lag \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE\n+          + \" 0         | 100          | 900        | 800    | 100 \" + NEWLINE\n+          + \" 1         | 50           | 900        | 900    | 0   \" + NEWLINE\n+          + \"------------------------------------------------------\" + NEWLINE));\n+    }", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyODAyOQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465828029", "bodyText": "ack.", "author": "jeqo", "createdAt": "2020-08-05T15:51:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NDA0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java b/ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java\nindex 7ca9d58fde..b067f26fe1 100644\n--- a/ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java\n+++ b/ksqldb-cli/src/test/java/io/confluent/ksql/cli/console/ConsoleTest.java\n\n@@ -1139,175 +1147,6 @@ public class ConsoleTest {\n     }\n   }\n \n-  @Test\n-  public void shouldPrintTopicDescribeExtendedWithConsumerOffsets() {\n-    // Given:\n-    final List<RunningQuery> readQueries = ImmutableList.of(\n-        new RunningQuery(\"read query\", ImmutableSet.of(\"sink1\"), ImmutableSet.of(\"sink1 topic\"), new QueryId(\"readId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n-    );\n-    final List<RunningQuery> writeQueries = ImmutableList.of(\n-        new RunningQuery(\"write query\", ImmutableSet.of(\"sink2\"), ImmutableSet.of(\"sink2 topic\"), new QueryId(\"writeId\"), queryStatusCount, KsqlConstants.KsqlQueryType.PERSISTENT)\n-    );\n-\n-    final KsqlEntityList entityList = new KsqlEntityList(ImmutableList.of(\n-        new SourceDescriptionEntity(\n-            \"e\",\n-            new SourceDescription(\n-                \"TestSource\",\n-                Optional.empty(),\n-                readQueries,\n-                writeQueries,\n-                buildTestSchema(SqlTypes.STRING),\n-                DataSourceType.KTABLE.getKsqlType(),\n-                \"2000-01-01\",\n-                \"stats\",\n-                \"errors\",\n-                true,\n-                \"kafka\",\n-                \"avro\",\n-                \"kadka-topic\",\n-                2, 1,\n-                \"sql statement text\",\n-                ImmutableList.of(\n-                    new QueryOffsetSummary(\n-                        \"consumer1\",\n-                        \"kadka-topic\",\n-                        ImmutableList.of(\n-                            new ConsumerPartitionOffsets(0, 100, 900, 800),\n-                            new ConsumerPartitionOffsets(1, 50, 900, 900)\n-                        ))\n-                )),\n-            Collections.emptyList()\n-        ))\n-    );\n-\n-    // When:\n-    console.printKsqlEntityList(entityList);\n-\n-    // Then:\n-    final String output = terminal.getOutputString();\n-    if (console.getOutputFormat() == OutputFormat.JSON) {\n-      assertThat(output, is(\"[ {\" + NEWLINE\n-          + \"  \\\"@type\\\" : \\\"sourceDescription\\\",\" + NEWLINE\n-          + \"  \\\"statementText\\\" : \\\"e\\\",\" + NEWLINE\n-          + \"  \\\"sourceDescription\\\" : {\" + NEWLINE\n-          + \"    \\\"name\\\" : \\\"TestSource\\\",\" + NEWLINE\n-          + \"    \\\"windowType\\\" : null,\" + NEWLINE\n-          + \"    \\\"readQueries\\\" : [ {\" + NEWLINE\n-          + \"      \\\"queryString\\\" : \\\"read query\\\",\" + NEWLINE\n-          + \"      \\\"sinks\\\" : [ \\\"sink1\\\" ],\" + NEWLINE\n-          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink1 topic\\\" ],\" + NEWLINE\n-          + \"      \\\"id\\\" : \\\"readId\\\",\" + NEWLINE\n-          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n-          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n-          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n-          + \"      },\" + NEWLINE\n-          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n-          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n-          + \"    } ],\" + NEWLINE\n-          + \"    \\\"writeQueries\\\" : [ {\" + NEWLINE\n-          + \"      \\\"queryString\\\" : \\\"write query\\\",\" + NEWLINE\n-          + \"      \\\"sinks\\\" : [ \\\"sink2\\\" ],\" + NEWLINE\n-          + \"      \\\"sinkKafkaTopics\\\" : [ \\\"sink2 topic\\\" ],\" + NEWLINE\n-          + \"      \\\"id\\\" : \\\"writeId\\\",\" + NEWLINE\n-          + \"      \\\"statusCount\\\" : {\" + NEWLINE\n-          + \"        \\\"RUNNING\\\" : 1,\" + NEWLINE\n-          + \"        \\\"ERROR\\\" : 2\" + NEWLINE\n-          + \"      },\" + NEWLINE\n-          + \"      \\\"queryType\\\" : \\\"PERSISTENT\\\",\" + NEWLINE\n-          + \"      \\\"state\\\" : \\\"\" + AGGREGATE_STATUS +\"\\\"\" + NEWLINE\n-          + \"    } ],\" + NEWLINE\n-          + \"    \\\"fields\\\" : [ {\" + NEWLINE\n-          + \"      \\\"name\\\" : \\\"ROWKEY\\\",\" + NEWLINE\n-          + \"      \\\"schema\\\" : {\" + NEWLINE\n-          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n-          + \"        \\\"fields\\\" : null,\" + NEWLINE\n-          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n-          + \"      },\" + NEWLINE\n-          + \"      \\\"type\\\" : \\\"KEY\\\"\" + NEWLINE\n-          + \"    }, {\" + NEWLINE\n-          + \"      \\\"name\\\" : \\\"f_0\\\",\" + NEWLINE\n-          + \"      \\\"schema\\\" : {\" + NEWLINE\n-          + \"        \\\"type\\\" : \\\"STRING\\\",\" + NEWLINE\n-          + \"        \\\"fields\\\" : null,\" + NEWLINE\n-          + \"        \\\"memberSchema\\\" : null\" + NEWLINE\n-          + \"      }\" + NEWLINE\n-          + \"    } ],\" + NEWLINE\n-          + \"    \\\"type\\\" : \\\"TABLE\\\",\" + NEWLINE\n-          + \"    \\\"timestamp\\\" : \\\"2000-01-01\\\",\" + NEWLINE\n-          + \"    \\\"statistics\\\" : \\\"stats\\\",\" + NEWLINE\n-          + \"    \\\"errorStats\\\" : \\\"errors\\\",\" + NEWLINE\n-          + \"    \\\"extended\\\" : true,\" + NEWLINE\n-          + \"    \\\"keyFormat\\\" : \\\"kafka\\\",\" + NEWLINE\n-          + \"    \\\"valueFormat\\\" : \\\"avro\\\",\" + NEWLINE\n-          + \"    \\\"topic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n-          + \"    \\\"partitions\\\" : 2,\" + NEWLINE\n-          + \"    \\\"replication\\\" : 1,\" + NEWLINE\n-          + \"    \\\"statement\\\" : \\\"sql statement text\\\",\" + NEWLINE\n-          + \"    \\\"queryOffsetSummaries\\\" : [ {\" + NEWLINE\n-          + \"      \\\"groupId\\\" : \\\"consumer1\\\",\" + NEWLINE\n-          + \"      \\\"kafkaTopic\\\" : \\\"kadka-topic\\\",\" + NEWLINE\n-          + \"      \\\"offsets\\\" : [ {\" + NEWLINE\n-          + \"        \\\"partition\\\" : 0,\" + NEWLINE\n-          + \"        \\\"logStartOffset\\\" : 100,\" + NEWLINE\n-          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n-          + \"        \\\"consumerOffset\\\" : 800\" + NEWLINE\n-          + \"      }, {\" + NEWLINE\n-          + \"        \\\"partition\\\" : 1,\" + NEWLINE\n-          + \"        \\\"logStartOffset\\\" : 50,\" + NEWLINE\n-          + \"        \\\"logEndOffset\\\" : 900,\" + NEWLINE\n-          + \"        \\\"consumerOffset\\\" : 900\" + NEWLINE\n-          + \"      } ]\" + NEWLINE\n-          + \"    } ]\" + NEWLINE\n-          + \"  },\" + NEWLINE\n-          + \"  \\\"warnings\\\" : [ ]\" + NEWLINE\n-          + \"} ]\" + NEWLINE));\n-    } else {\n-      assertThat(output, is(\"\" + NEWLINE\n-          + \"Name                 : TestSource\" + NEWLINE\n-          + \"Type                 : TABLE\" + NEWLINE\n-          + \"Timestamp field      : 2000-01-01\" + NEWLINE\n-          + \"Key format           : kafka\" + NEWLINE\n-          + \"Value format         : avro\" + NEWLINE\n-          + \"Kafka topic          : kadka-topic (partitions: 2, replication: 1)\" + NEWLINE\n-          + \"Statement            : sql statement text\" + NEWLINE\n-          + \"\" + NEWLINE\n-          + \" Field  | Type                           \" + NEWLINE\n-          + \"-----------------------------------------\" + NEWLINE\n-          + \" ROWKEY | VARCHAR(STRING)  (primary key) \" + NEWLINE\n-          + \" f_0    | VARCHAR(STRING)                \" + NEWLINE\n-          + \"-----------------------------------------\" + NEWLINE\n-          + \"\" + NEWLINE\n-          + \"Queries that read from this TABLE\" + NEWLINE\n-          + \"-----------------------------------\" + NEWLINE\n-          + \"readId (\" + AGGREGATE_STATUS +\") : read query\" + NEWLINE\n-          + \"\\n\"\n-          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n-          + \"\" + NEWLINE\n-          + \"Queries that write from this TABLE\" + NEWLINE\n-          + \"-----------------------------------\" + NEWLINE\n-          + \"writeId (\" + AGGREGATE_STATUS + \") : write query\" + NEWLINE\n-          + \"\\n\"\n-          + \"For query topology and execution plan please run: EXPLAIN <QueryId>\" + NEWLINE\n-          + \"\" + NEWLINE\n-          + \"Local runtime statistics\" + NEWLINE\n-          + \"------------------------\" + NEWLINE\n-          + \"stats\" + NEWLINE\n-          + \"errors\" + NEWLINE\n-          + \"(Statistics of the local KSQL server interaction with the Kafka topic kadka-topic)\" + NEWLINE\n-          + NEWLINE\n-          + \"Consumer Group       : consumer1\" + NEWLINE\n-          + \"Kafka topic          : kadka-topic\" + NEWLINE\n-          + \"Max lag              : 100\" + NEWLINE\n-          + NEWLINE\n-          + \" Partition | Start Offset | End Offset | Offset | Lag \" + NEWLINE\n-          + \"------------------------------------------------------\" + NEWLINE\n-          + \" 0         | 100          | 900        | 800    | 100 \" + NEWLINE\n-          + \" 1         | 50           | 900        | 900    | 0   \" + NEWLINE\n-          + \"------------------------------------------------------\" + NEWLINE));\n-    }\n-  }\n-\n   @Test\n   public void shouldPrintTopicDescribeExtended() {\n     // Given:\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NzE1NQ==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463067155", "bodyText": "This inheritance smells!  A group is not a topic. So why inherit from TopicAuthorizationException?  Maybe consider inheriting from GroupAuthorizationException?", "author": "big-andy-coates", "createdAt": "2020-07-30T15:08:24Z", "path": "ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright 2019 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.exception;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.kafka.common.acl.AclOperation;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n+\n+/**\n+ * Used to return custom error messages when TopicAuthorizationException returned from Kafka\n+ */\n+public class KsqlGroupAuthorizationException extends TopicAuthorizationException {", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODU0NA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708544", "bodyText": "ack", "author": "jeqo", "createdAt": "2020-08-05T13:00:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2NzE1NQ=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java b/ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java\ndeleted file mode 100644\nindex 952e6e6771..0000000000\n--- a/ksqldb-engine/src/main/java/io/confluent/ksql/exception/KsqlGroupAuthorizationException.java\n+++ /dev/null\n\n@@ -1,36 +0,0 @@\n-/*\n- * Copyright 2019 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.exception;\n-\n-import org.apache.commons.lang3.StringUtils;\n-import org.apache.kafka.common.acl.AclOperation;\n-import org.apache.kafka.common.errors.TopicAuthorizationException;\n-\n-/**\n- * Used to return custom error messages when TopicAuthorizationException returned from Kafka\n- */\n-public class KsqlGroupAuthorizationException extends TopicAuthorizationException {\n-\n-  public KsqlGroupAuthorizationException(\n-      final AclOperation operation,\n-      final String group) {\n-    super(String.format(\"Authorization denied to %s on group: [%s]\",\n-        StringUtils.capitalize(\n-            operation.toString().toLowerCase()),\n-        group));\n-  }\n-\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2OTIwOA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463069208", "bodyText": "sourceQueries and sinkQueries may be a little counterintuitive.  Can you rename sourceQueries -> readQueries, i.e. queries that read from this source and sinkQueries -> writeQueries i.e. queries that write to this source.", "author": "big-andy-coates", "createdAt": "2020-07-30T15:11:22Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +213,21 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODUxNw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708517", "bodyText": "ack.", "author": "jeqo", "createdAt": "2020-08-05T13:00:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA2OTIwOA=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 4453615668..84bd2d3bf2 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -213,21 +199,30 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n-        q -> q.getSourceNames().contains(dataSource.getName()));\n-    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n-        q -> q.getSinkName().equals(dataSource.getName()));\n-\n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription =\n-        Optional.empty();\n-    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-        sourceConsumerOffsets = offsetSummaries(ksqlConfig, serviceContext, sinkQueries);\n+        String consumerGroupId = \"\";\n+        try {\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();\n+          Map<TopicPartition, OffsetSpec> request = new LinkedHashMap<>();\n+          for (Map.Entry<TopicPartition, OffsetAndMetadata> entry: topicAndConsumerOffsets.entrySet()) {\n+            request.put(entry.getKey(), OffsetSpec.earliest());\n+          }\n+          topicAndEndOffsets = serviceContext.getAdminClient().listOffsets(request).all().get();\n+        } catch (InterruptedException | ExecutionException e) {\n+          e.printStackTrace();\n+        }\n       } catch (final KafkaException | KafkaResponseGetFailedException e) {\n         warnings.add(new KsqlWarning(\"Error from Kafka: \" + e.getMessage()));\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3ODk5Mw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463078993", "bodyText": "Under what situations would this be null?", "author": "big-andy-coates", "createdAt": "2020-07-30T15:24:54Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +238,86 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescription,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<QueryOffsetSummary> offsetSummaries(\n+      final KsqlConfig ksqlConfig,\n+      final ServiceContext serviceContext,\n+      final List<RunningQuery> sinkQueries\n+  ) {\n+    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n+    final Map<String, Map<TopicPartition, OffsetAndMetadata>> offsetsPerQuery =\n+        new HashMap<>(sinkQueries.size());\n+    final Map<String, Set<String>> topicsPerQuery = new HashMap<>();\n+    final Set<String> allTopics = new HashSet<>();\n+    // Get topics and offsets per running query\n+    for (RunningQuery query : sinkQueries) {\n+      final QueryId queryId = query.getId();\n+      final String applicationId =\n+          QueryApplicationId.build(ksqlConfig, true, queryId);\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n+          serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n+      offsetsPerQuery.put(applicationId, topicAndConsumerOffsets);\n+      final Set<String> topics = topicAndConsumerOffsets.keySet().stream()\n+          .map(TopicPartition::topic)\n+          .collect(Collectors.toSet());\n+      topicsPerQuery.put(applicationId, topics);\n+      allTopics.addAll(topics);\n+    }\n+    // Get topics descriptions and start/end offsets\n+    final Map<String, TopicDescription> sourceTopicDescriptions =\n+        serviceContext.getTopicClient().describeTopics(allTopics);\n+    final Map<TopicPartition, Long> topicAndStartOffsets =\n+        serviceContext.getTopicClient().listTopicsStartOffsets(allTopics);\n+    final Map<TopicPartition, Long> topicAndEndOffsets =\n+        serviceContext.getTopicClient().listTopicsEndOffsets(allTopics);\n+    // Build consumer offsets summary\n+    for (Entry<String, Set<String>> entry : topicsPerQuery.entrySet()) {\n+      for (String topic : entry.getValue()) {\n+        sourceConsumerOffsets.add(\n+            new QueryOffsetSummary(\n+                entry.getKey(),\n+                topic,\n+                consumerPartitionOffsets(\n+                    sourceTopicDescriptions.get(topic),\n+                    topicAndStartOffsets,\n+                    topicAndEndOffsets,\n+                    offsetsPerQuery.get(entry.getKey()))));\n+      }\n+    }\n+    return sourceConsumerOffsets;\n+  }\n+\n+  private static List<ConsumerPartitionOffsets> consumerPartitionOffsets(\n+      final TopicDescription topicDescription,\n+      final Map<TopicPartition, Long> topicAndStartOffsets,\n+      final Map<TopicPartition, Long> topicAndEndOffsets,\n+      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n+  ) {\n+    final List<ConsumerPartitionOffsets> consumerPartitionOffsets = new ArrayList<>();\n+    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n+      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n+          topicPartitionInfo.partition());\n+      final Long startOffsetResultInfo = topicAndStartOffsets.get(tp);\n+      final Long endOffsetResultInfo = topicAndEndOffsets.get(tp);\n+      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n+      consumerPartitionOffsets.add(\n+          new ConsumerPartitionOffsets(\n+              topicPartitionInfo.partition(),\n+              startOffsetResultInfo,\n+              endOffsetResultInfo,\n+              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODQ3Nw==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r465708477", "bodyText": "I've seen this when consumers haven't poll from a topic-partition yet, the list of offset and metadata only returns the ones that exist at the moment. Adding a short comment to clarify this.", "author": "jeqo", "createdAt": "2020-08-05T13:00:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3ODk5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 4453615668..84bd2d3bf2 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -238,86 +233,16 @@ public final class ListSourceExecutor {\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            sourceQueries,\n-            sinkQueries,\n+            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n+            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n             topicDescription,\n-            sourceConsumerOffsets\n+            consumerGroupDescription,\n+            topicAndEndOffsets,\n+            topicAndConsumerOffsets\n         )\n     );\n   }\n \n-  private static List<QueryOffsetSummary> offsetSummaries(\n-      final KsqlConfig ksqlConfig,\n-      final ServiceContext serviceContext,\n-      final List<RunningQuery> sinkQueries\n-  ) {\n-    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n-    final Map<String, Map<TopicPartition, OffsetAndMetadata>> offsetsPerQuery =\n-        new HashMap<>(sinkQueries.size());\n-    final Map<String, Set<String>> topicsPerQuery = new HashMap<>();\n-    final Set<String> allTopics = new HashSet<>();\n-    // Get topics and offsets per running query\n-    for (RunningQuery query : sinkQueries) {\n-      final QueryId queryId = query.getId();\n-      final String applicationId =\n-          QueryApplicationId.build(ksqlConfig, true, queryId);\n-      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n-          serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n-      offsetsPerQuery.put(applicationId, topicAndConsumerOffsets);\n-      final Set<String> topics = topicAndConsumerOffsets.keySet().stream()\n-          .map(TopicPartition::topic)\n-          .collect(Collectors.toSet());\n-      topicsPerQuery.put(applicationId, topics);\n-      allTopics.addAll(topics);\n-    }\n-    // Get topics descriptions and start/end offsets\n-    final Map<String, TopicDescription> sourceTopicDescriptions =\n-        serviceContext.getTopicClient().describeTopics(allTopics);\n-    final Map<TopicPartition, Long> topicAndStartOffsets =\n-        serviceContext.getTopicClient().listTopicsStartOffsets(allTopics);\n-    final Map<TopicPartition, Long> topicAndEndOffsets =\n-        serviceContext.getTopicClient().listTopicsEndOffsets(allTopics);\n-    // Build consumer offsets summary\n-    for (Entry<String, Set<String>> entry : topicsPerQuery.entrySet()) {\n-      for (String topic : entry.getValue()) {\n-        sourceConsumerOffsets.add(\n-            new QueryOffsetSummary(\n-                entry.getKey(),\n-                topic,\n-                consumerPartitionOffsets(\n-                    sourceTopicDescriptions.get(topic),\n-                    topicAndStartOffsets,\n-                    topicAndEndOffsets,\n-                    offsetsPerQuery.get(entry.getKey()))));\n-      }\n-    }\n-    return sourceConsumerOffsets;\n-  }\n-\n-  private static List<ConsumerPartitionOffsets> consumerPartitionOffsets(\n-      final TopicDescription topicDescription,\n-      final Map<TopicPartition, Long> topicAndStartOffsets,\n-      final Map<TopicPartition, Long> topicAndEndOffsets,\n-      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n-  ) {\n-    final List<ConsumerPartitionOffsets> consumerPartitionOffsets = new ArrayList<>();\n-    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n-      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n-          topicPartitionInfo.partition());\n-      final Long startOffsetResultInfo = topicAndStartOffsets.get(tp);\n-      final Long endOffsetResultInfo = topicAndEndOffsets.get(tp);\n-      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n-      consumerPartitionOffsets.add(\n-          new ConsumerPartitionOffsets(\n-              topicPartitionInfo.partition(),\n-              startOffsetResultInfo,\n-              endOffsetResultInfo,\n-              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0\n-          ));\n-    }\n-    return consumerPartitionOffsets;\n-  }\n-\n   private static List<RunningQuery> getQueries(\n       final KsqlExecutionContext ksqlEngine,\n       final Predicate<PersistentQueryMetadata> predicate\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3OTU3MA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463079570", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n          \n          \n            \n                List<QueryOffsetSummary> queryOffsetSummaries = new ArrayList<>();", "author": "big-andy-coates", "createdAt": "2020-07-30T15:25:43Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -191,13 +213,21 @@ private static SourceDescriptionWithWarnings describeSource(\n       ), statementText);\n     }\n \n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n+        q -> q.getSourceNames().contains(dataSource.getName()));\n+    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n+        q -> q.getSinkName().equals(dataSource.getName()));\n+\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription =\n+        Optional.empty();\n+    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 4453615668..84bd2d3bf2 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -213,21 +199,30 @@ public final class ListSourceExecutor {\n       ), statementText);\n     }\n \n-    final List<RunningQuery> sourceQueries = getQueries(ksqlEngine,\n-        q -> q.getSourceNames().contains(dataSource.getName()));\n-    final List<RunningQuery> sinkQueries = getQueries(ksqlEngine,\n-        q -> q.getSinkName().equals(dataSource.getName()));\n-\n-    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription =\n-        Optional.empty();\n-    List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n+    Optional<org.apache.kafka.clients.admin.TopicDescription> topicDescription = Optional.empty();\n+    Optional<ConsumerGroupDescription> consumerGroupDescription = Optional.empty();\n+    Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets = new LinkedHashMap<>();\n+    Map<TopicPartition, ListOffsetsResultInfo> topicAndEndOffsets = new LinkedHashMap<>();\n     final List<KsqlWarning> warnings = new LinkedList<>();\n     if (extended) {\n       try {\n         topicDescription = Optional.of(\n             serviceContext.getTopicClient().describeTopic(dataSource.getKafkaTopicName())\n         );\n-        sourceConsumerOffsets = offsetSummaries(ksqlConfig, serviceContext, sinkQueries);\n+        String consumerGroupId = \"\";\n+        try {\n+          consumerGroupDescription = Optional.of(\n+              serviceContext.getAdminClient().describeConsumerGroups(Collections.singletonList(consumerGroupId)).describedGroups().get(consumerGroupId).get()\n+          );\n+          topicAndConsumerOffsets = serviceContext.getAdminClient().listConsumerGroupOffsets(consumerGroupId).partitionsToOffsetAndMetadata().get();\n+          Map<TopicPartition, OffsetSpec> request = new LinkedHashMap<>();\n+          for (Map.Entry<TopicPartition, OffsetAndMetadata> entry: topicAndConsumerOffsets.entrySet()) {\n+            request.put(entry.getKey(), OffsetSpec.earliest());\n+          }\n+          topicAndEndOffsets = serviceContext.getAdminClient().listOffsets(request).all().get();\n+        } catch (InterruptedException | ExecutionException e) {\n+          e.printStackTrace();\n+        }\n       } catch (final KafkaException | KafkaResponseGetFailedException e) {\n         warnings.add(new KsqlWarning(\"Error from Kafka: \" + e.getMessage()));\n       }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzA3OTY1OA==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r463079658", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n          \n          \n            \n                final List<QueryOffsetSummary> queryOffsetSummaries = new ArrayList<>();", "author": "big-andy-coates", "createdAt": "2020-07-30T15:25:51Z", "path": "ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java", "diffHunk": "@@ -208,13 +238,86 @@ private static SourceDescriptionWithWarnings describeSource(\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n-            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n-            topicDescription\n+            sourceQueries,\n+            sinkQueries,\n+            topicDescription,\n+            sourceConsumerOffsets\n         )\n     );\n   }\n \n+  private static List<QueryOffsetSummary> offsetSummaries(\n+      final KsqlConfig ksqlConfig,\n+      final ServiceContext serviceContext,\n+      final List<RunningQuery> sinkQueries\n+  ) {\n+    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();", "originalCommit": "086c4b0be3b8a22ac7e23d2d1a6e1e19d22dd795", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\nindex 4453615668..84bd2d3bf2 100644\n--- a/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n+++ b/ksqldb-rest-app/src/main/java/io/confluent/ksql/rest/server/execution/ListSourceExecutor.java\n\n@@ -238,86 +233,16 @@ public final class ListSourceExecutor {\n         SourceDescriptionFactory.create(\n             dataSource,\n             extended,\n-            sourceQueries,\n-            sinkQueries,\n+            getQueries(ksqlEngine, q -> q.getSourceNames().contains(dataSource.getName())),\n+            getQueries(ksqlEngine, q -> q.getSinkName().equals(dataSource.getName())),\n             topicDescription,\n-            sourceConsumerOffsets\n+            consumerGroupDescription,\n+            topicAndEndOffsets,\n+            topicAndConsumerOffsets\n         )\n     );\n   }\n \n-  private static List<QueryOffsetSummary> offsetSummaries(\n-      final KsqlConfig ksqlConfig,\n-      final ServiceContext serviceContext,\n-      final List<RunningQuery> sinkQueries\n-  ) {\n-    final List<QueryOffsetSummary> sourceConsumerOffsets = new ArrayList<>();\n-    final Map<String, Map<TopicPartition, OffsetAndMetadata>> offsetsPerQuery =\n-        new HashMap<>(sinkQueries.size());\n-    final Map<String, Set<String>> topicsPerQuery = new HashMap<>();\n-    final Set<String> allTopics = new HashSet<>();\n-    // Get topics and offsets per running query\n-    for (RunningQuery query : sinkQueries) {\n-      final QueryId queryId = query.getId();\n-      final String applicationId =\n-          QueryApplicationId.build(ksqlConfig, true, queryId);\n-      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets =\n-          serviceContext.getConsumerGroupClient().listConsumerGroupOffsets(applicationId);\n-      offsetsPerQuery.put(applicationId, topicAndConsumerOffsets);\n-      final Set<String> topics = topicAndConsumerOffsets.keySet().stream()\n-          .map(TopicPartition::topic)\n-          .collect(Collectors.toSet());\n-      topicsPerQuery.put(applicationId, topics);\n-      allTopics.addAll(topics);\n-    }\n-    // Get topics descriptions and start/end offsets\n-    final Map<String, TopicDescription> sourceTopicDescriptions =\n-        serviceContext.getTopicClient().describeTopics(allTopics);\n-    final Map<TopicPartition, Long> topicAndStartOffsets =\n-        serviceContext.getTopicClient().listTopicsStartOffsets(allTopics);\n-    final Map<TopicPartition, Long> topicAndEndOffsets =\n-        serviceContext.getTopicClient().listTopicsEndOffsets(allTopics);\n-    // Build consumer offsets summary\n-    for (Entry<String, Set<String>> entry : topicsPerQuery.entrySet()) {\n-      for (String topic : entry.getValue()) {\n-        sourceConsumerOffsets.add(\n-            new QueryOffsetSummary(\n-                entry.getKey(),\n-                topic,\n-                consumerPartitionOffsets(\n-                    sourceTopicDescriptions.get(topic),\n-                    topicAndStartOffsets,\n-                    topicAndEndOffsets,\n-                    offsetsPerQuery.get(entry.getKey()))));\n-      }\n-    }\n-    return sourceConsumerOffsets;\n-  }\n-\n-  private static List<ConsumerPartitionOffsets> consumerPartitionOffsets(\n-      final TopicDescription topicDescription,\n-      final Map<TopicPartition, Long> topicAndStartOffsets,\n-      final Map<TopicPartition, Long> topicAndEndOffsets,\n-      final Map<TopicPartition, OffsetAndMetadata> topicAndConsumerOffsets\n-  ) {\n-    final List<ConsumerPartitionOffsets> consumerPartitionOffsets = new ArrayList<>();\n-    for (TopicPartitionInfo topicPartitionInfo : topicDescription.partitions()) {\n-      final TopicPartition tp = new TopicPartition(topicDescription.name(),\n-          topicPartitionInfo.partition());\n-      final Long startOffsetResultInfo = topicAndStartOffsets.get(tp);\n-      final Long endOffsetResultInfo = topicAndEndOffsets.get(tp);\n-      final OffsetAndMetadata offsetAndMetadata = topicAndConsumerOffsets.get(tp);\n-      consumerPartitionOffsets.add(\n-          new ConsumerPartitionOffsets(\n-              topicPartitionInfo.partition(),\n-              startOffsetResultInfo,\n-              endOffsetResultInfo,\n-              offsetAndMetadata != null ? offsetAndMetadata.offset() : 0\n-          ));\n-    }\n-    return consumerPartitionOffsets;\n-  }\n-\n   private static List<RunningQuery> getQueries(\n       final KsqlExecutionContext ksqlEngine,\n       final Predicate<PersistentQueryMetadata> predicate\n"}}, {"oid": "f63b2c07befd90df020b6cac091124b56bc2b958", "url": "https://github.com/confluentinc/ksql/commit/f63b2c07befd90df020b6cac091124b56bc2b958", "message": "fix call to sourcedescription", "committedDate": "2020-08-05T12:40:32Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjM4NDY5Mg==", "url": "https://github.com/confluentinc/ksql/pull/5476#discussion_r466384692", "bodyText": "Would be good to have some kind of heading here, e.g.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n          \n          \n            \n                writer().println(\"Consumers:\");\n          \n          \n            \n                writer().println(\"\");\n          \n          \n            \n                \n          \n          \n            \n                for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {", "author": "big-andy-coates", "createdAt": "2020-08-06T12:44:41Z", "path": "ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java", "diffHunk": "@@ -614,6 +616,33 @@ private void printSourceDescription(final SourceDescription source) {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n+    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {", "originalCommit": "cafb125bb984eb42c39002b01d1251dbbe4e0191", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "chunk": "diff --git a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\nindex 17d4783990..83353c03d1 100644\n--- a/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n+++ b/ksqldb-cli/src/main/java/io/confluent/ksql/cli/console/Console.java\n\n@@ -616,33 +624,6 @@ public class Console implements Closeable {\n         \"Statistics of the local KSQL server interaction with the Kafka topic \"\n             + source.getTopic()\n     ));\n-    for (QueryOffsetSummary queryOffsetSummary : source.getQueryOffsetSummaries()) {\n-      writer().println();\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Consumer Group\", queryOffsetSummary.getGroupId()));\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Kafka topic\", queryOffsetSummary.getKafkaTopic()));\n-      writer().println(String.format(\"%-20s : %s\",\n-          \"Max lag\", queryOffsetSummary.getOffsets().stream()\n-              .mapToLong(s -> s.getLogEndOffset() - s.getConsumerOffset())\n-              .max()\n-              .orElse(0)));\n-      writer().println(\"\");\n-      final Table taskTable = new Table.Builder()\n-          .withColumnHeaders(\n-              ImmutableList.of(\"Partition\", \"Start Offset\", \"End Offset\", \"Offset\", \"Lag\"))\n-          .withRows(queryOffsetSummary.getOffsets()\n-              .stream()\n-              .map(offset -> ImmutableList.of(\n-                  String.valueOf(offset.getPartition()),\n-                  String.valueOf(offset.getLogStartOffset()),\n-                  String.valueOf(offset.getLogEndOffset()),\n-                  String.valueOf(offset.getConsumerOffset()),\n-                  String.valueOf(offset.getLogEndOffset() - offset.getConsumerOffset())\n-              )))\n-          .build();\n-      taskTable.print(this);\n-    }\n   }\n \n   private void printSourceDescriptionList(final SourceDescriptionList sourceDescriptionList) {\n"}}, {"oid": "27a04e02910eb07c59c98451804a4f39f59b2b4b", "url": "https://github.com/confluentinc/ksql/commit/27a04e02910eb07c59c98451804a4f39f59b2b4b", "message": "introduce source consumer offsets", "committedDate": "2020-08-07T09:30:37Z", "type": "commit"}, {"oid": "b02f1f93f79992989e435e2caec6ea6024c1dddb", "url": "https://github.com/confluentinc/ksql/commit/b02f1f93f79992989e435e2caec6ea6024c1dddb", "message": "successfully build a response", "committedDate": "2020-08-07T09:30:37Z", "type": "commit"}, {"oid": "fba70db82dfaf34fe4dff1756d5336328f265958", "url": "https://github.com/confluentinc/ksql/commit/fba70db82dfaf34fe4dff1756d5336328f265958", "message": "feature working", "committedDate": "2020-08-07T09:30:38Z", "type": "commit"}, {"oid": "9eefb4a20978e17a71e10c332f81d66e392e72b7", "url": "https://github.com/confluentinc/ksql/commit/9eefb4a20978e17a71e10c332f81d66e392e72b7", "message": "enforce offsets only for sources", "committedDate": "2020-08-07T09:30:38Z", "type": "commit"}, {"oid": "9f41e5d2a79fc803c350ff378ec43d2bf28a72bf", "url": "https://github.com/confluentinc/ksql/commit/9f41e5d2a79fc803c350ff378ec43d2bf28a72bf", "message": "add unit tests", "committedDate": "2020-08-07T09:30:38Z", "type": "commit"}, {"oid": "26678084932bbbc8fb438735f93c6e0254d5927a", "url": "https://github.com/confluentinc/ksql/commit/26678084932bbbc8fb438735f93c6e0254d5927a", "message": "fix: apply checkstyle feedback", "committedDate": "2020-08-07T09:30:39Z", "type": "commit"}, {"oid": "67923114cabb2dc37fce00fce1814e4f04e743d3", "url": "https://github.com/confluentinc/ksql/commit/67923114cabb2dc37fce00fce1814e4f04e743d3", "message": "fix: remove unused import", "committedDate": "2020-08-07T09:30:39Z", "type": "commit"}, {"oid": "14a30b7216e3b6ec380e96315a3855c34e304679", "url": "https://github.com/confluentinc/ksql/commit/14a30b7216e3b6ec380e96315a3855c34e304679", "message": "fix: replace appId mapping based on QueryExecutor", "committedDate": "2020-08-07T09:30:39Z", "type": "commit"}, {"oid": "b11860838678b54da7181fd1f44cadbb67fd5acd", "url": "https://github.com/confluentinc/ksql/commit/b11860838678b54da7181fd1f44cadbb67fd5acd", "message": "fix: checkstyle", "committedDate": "2020-08-07T09:30:39Z", "type": "commit"}, {"oid": "fd1d86a2ba641d9359e79e8761882c3d1a4bb0d9", "url": "https://github.com/confluentinc/ksql/commit/fd1d86a2ba641d9359e79e8761882c3d1a4bb0d9", "message": "refactor", "committedDate": "2020-08-07T09:30:40Z", "type": "commit"}, {"oid": "a4386209baca380208340e9d2cf0596d832f18ad", "url": "https://github.com/confluentinc/ksql/commit/a4386209baca380208340e9d2cf0596d832f18ad", "message": "chore: add final", "committedDate": "2020-08-07T09:30:40Z", "type": "commit"}, {"oid": "0a1ce04abe0d090a3d5fda2447f0b6758e6bcac0", "url": "https://github.com/confluentinc/ksql/commit/0a1ce04abe0d090a3d5fda2447f0b6758e6bcac0", "message": "elevate consumer group client to service\n\nreuse existing client to support list cg offsets", "committedDate": "2020-08-07T09:30:40Z", "type": "commit"}, {"oid": "074849bfec34e4dd666a51d45cd6a60cc8131247", "url": "https://github.com/confluentinc/ksql/commit/074849bfec34e4dd666a51d45cd6a60cc8131247", "message": "fix: checkstyle", "committedDate": "2020-08-07T09:30:40Z", "type": "commit"}, {"oid": "309e3dc7aa25b5983060acce6e80cf7007f97336", "url": "https://github.com/confluentinc/ksql/commit/309e3dc7aa25b5983060acce6e80cf7007f97336", "message": "fix: checkstyle", "committedDate": "2020-08-07T09:30:41Z", "type": "commit"}, {"oid": "708da6ea057ed963f39385f21287f237d28e0f41", "url": "https://github.com/confluentinc/ksql/commit/708da6ea057ed963f39385f21287f237d28e0f41", "message": "fix: failing sandboxed tests", "committedDate": "2020-08-07T09:30:41Z", "type": "commit"}, {"oid": "51c3beff0c8245fa19502899d1234a6ea2452378", "url": "https://github.com/confluentinc/ksql/commit/51c3beff0c8245fa19502899d1234a6ea2452378", "message": "fix: more failing sandboxed tests", "committedDate": "2020-08-07T09:30:41Z", "type": "commit"}, {"oid": "a9d3a4f9e3ca2cabf3d3c7823bf503125e86abcc", "url": "https://github.com/confluentinc/ksql/commit/a9d3a4f9e3ca2cabf3d3c7823bf503125e86abcc", "message": "fix: renaming to cg", "committedDate": "2020-08-07T09:30:41Z", "type": "commit"}, {"oid": "96915c369eced97b26670e959e4bd9a08bf9429c", "url": "https://github.com/confluentinc/ksql/commit/96915c369eced97b26670e959e4bd9a08bf9429c", "message": "fix: moar checkstyling", "committedDate": "2020-08-07T09:30:42Z", "type": "commit"}, {"oid": "cf2f9898472152e407a96ca2ba1a5f4f7fa76ed9", "url": "https://github.com/confluentinc/ksql/commit/cf2f9898472152e407a96ca2ba1a5f4f7fa76ed9", "message": "fix: moar checkstyling", "committedDate": "2020-08-07T09:30:42Z", "type": "commit"}, {"oid": "9d90b2474082e1da6467730bd6518d1b98445780", "url": "https://github.com/confluentinc/ksql/commit/9d90b2474082e1da6467730bd6518d1b98445780", "message": "replace linked for hashmap", "committedDate": "2020-08-07T09:30:42Z", "type": "commit"}, {"oid": "8ce48c55db5bb8ecf0ac9a814dcf3be36cacd258", "url": "https://github.com/confluentinc/ksql/commit/8ce48c55db5bb8ecf0ac9a814dcf3be36cacd258", "message": "from optional to list of offsets", "committedDate": "2020-08-07T09:30:42Z", "type": "commit"}, {"oid": "72e1de8935ed1fe9cd94569dce875782cdc2090a", "url": "https://github.com/confluentinc/ksql/commit/72e1de8935ed1fe9cd94569dce875782cdc2090a", "message": "fix topic description empty when no sources", "committedDate": "2020-08-07T09:30:43Z", "type": "commit"}, {"oid": "84b17606bf5634b962e2349e97f9f0d194b72fc3", "url": "https://github.com/confluentinc/ksql/commit/84b17606bf5634b962e2349e97f9f0d194b72fc3", "message": "abstract getServiceId for reusage", "committedDate": "2020-08-07T09:30:43Z", "type": "commit"}, {"oid": "d6b8c0ddd63b800bfbbc72ffc30bfc819b3d99c4", "url": "https://github.com/confluentinc/ksql/commit/d6b8c0ddd63b800bfbbc72ffc30bfc819b3d99c4", "message": "fix checkstyle", "committedDate": "2020-08-07T09:30:43Z", "type": "commit"}, {"oid": "8409f2c3178d1ccdd4141ef1abdf9f5af5cef0ee", "url": "https://github.com/confluentinc/ksql/commit/8409f2c3178d1ccdd4141ef1abdf9f5af5cef0ee", "message": "turn to static", "committedDate": "2020-08-07T09:30:44Z", "type": "commit"}, {"oid": "8eaa8e1362825d9fd1d30ac617943adb0fff9d7d", "url": "https://github.com/confluentinc/ksql/commit/8eaa8e1362825d9fd1d30ac617943adb0fff9d7d", "message": "improve authz exception handling\n\nCo-authored-by: Andy Coates <8012398+big-andy-coates@users.noreply.github.com>", "committedDate": "2020-08-07T09:30:44Z", "type": "commit"}, {"oid": "fb0450ec54c6a5fd46e72fdf99bb1084c4e22949", "url": "https://github.com/confluentinc/ksql/commit/fb0450ec54c6a5fd46e72fdf99bb1084c4e22949", "message": "refactor describe based on sink queries;", "committedDate": "2020-08-07T09:30:44Z", "type": "commit"}, {"oid": "ff6515ac66d1f6e34612a8c32f9602d1fffe3924", "url": "https://github.com/confluentinc/ksql/commit/ff6515ac66d1f6e34612a8c32f9602d1fffe3924", "message": "apply andys feedback", "committedDate": "2020-08-07T09:30:45Z", "type": "commit"}, {"oid": "43ba2f5218f49f4b55bf0e1ad9c933297b49da95", "url": "https://github.com/confluentinc/ksql/commit/43ba2f5218f49f4b55bf0e1ad9c933297b49da95", "message": "fix checkstyle", "committedDate": "2020-08-07T09:30:45Z", "type": "commit"}, {"oid": "2eba55b1d575f5a915cd815c8174631b05a0a809", "url": "https://github.com/confluentinc/ksql/commit/2eba55b1d575f5a915cd815c8174631b05a0a809", "message": "rename to represent offsets better", "committedDate": "2020-08-07T09:30:45Z", "type": "commit"}, {"oid": "567af3dcb8eea3151a0cead3fc48d2afed2e9fb7", "url": "https://github.com/confluentinc/ksql/commit/567af3dcb8eea3151a0cead3fc48d2afed2e9fb7", "message": "fix: checkstyle", "committedDate": "2020-08-07T09:30:46Z", "type": "commit"}, {"oid": "3824c1c636e304f64c02c4ba3c263af2601483c5", "url": "https://github.com/confluentinc/ksql/commit/3824c1c636e304f64c02c4ba3c263af2601483c5", "message": "fix: checkstyle", "committedDate": "2020-08-07T09:30:46Z", "type": "commit"}, {"oid": "f914c1cb31f3eef6067047ce13d6efe9bf62112e", "url": "https://github.com/confluentinc/ksql/commit/f914c1cb31f3eef6067047ce13d6efe9bf62112e", "message": "fix sandboxed topic client mapping test", "committedDate": "2020-08-07T09:30:46Z", "type": "commit"}, {"oid": "9cfb380900843ae3557d25d9d2087d218e3bc1d7", "url": "https://github.com/confluentinc/ksql/commit/9cfb380900843ae3557d25d9d2087d218e3bc1d7", "message": "nit: rename back var", "committedDate": "2020-08-07T09:30:46Z", "type": "commit"}, {"oid": "adccb696823a8356b17589132e8c38fa95d91588", "url": "https://github.com/confluentinc/ksql/commit/adccb696823a8356b17589132e8c38fa95d91588", "message": "more renaming", "committedDate": "2020-08-07T09:30:46Z", "type": "commit"}, {"oid": "7c6f6f935d56778fc37fb81e6d0195feb910f255", "url": "https://github.com/confluentinc/ksql/commit/7c6f6f935d56778fc37fb81e6d0195feb910f255", "message": "test sourceDescription", "committedDate": "2020-08-07T09:30:47Z", "type": "commit"}, {"oid": "65fc9c7140c7f9657ca00b58652a24606718578e", "url": "https://github.com/confluentinc/ksql/commit/65fc9c7140c7f9657ca00b58652a24606718578e", "message": "test kafka consumer group client", "committedDate": "2020-08-07T09:30:47Z", "type": "commit"}, {"oid": "5aad5c9d23cf72edd3357a81b9d38b9f0554f9c8", "url": "https://github.com/confluentinc/ksql/commit/5aad5c9d23cf72edd3357a81b9d38b9f0554f9c8", "message": "test kafka topic client list offsets", "committedDate": "2020-08-07T09:30:47Z", "type": "commit"}, {"oid": "4fb003ca9174b9b1ddd61ae80abb68607a1781f8", "url": "https://github.com/confluentinc/ksql/commit/4fb003ca9174b9b1ddd61ae80abb68607a1781f8", "message": "add preconditions for offsets", "committedDate": "2020-08-07T09:30:48Z", "type": "commit"}, {"oid": "7155b67f142fe217eb50c80a2d8666088427959a", "url": "https://github.com/confluentinc/ksql/commit/7155b67f142fe217eb50c80a2d8666088427959a", "message": "improve exception msg", "committedDate": "2020-08-07T09:30:48Z", "type": "commit"}, {"oid": "ae505f884a269809fb8cfea600af5f45468a0cae", "url": "https://github.com/confluentinc/ksql/commit/ae505f884a269809fb8cfea600af5f45468a0cae", "message": "fix preconditions", "committedDate": "2020-08-07T09:30:48Z", "type": "commit"}, {"oid": "c2657ff187a25e7785fe79a7bcf80bd61b28690c", "url": "https://github.com/confluentinc/ksql/commit/c2657ff187a25e7785fe79a7bcf80bd61b28690c", "message": "fix call to sourcedescription", "committedDate": "2020-08-07T09:30:48Z", "type": "commit"}, {"oid": "aa59da8f816644aa0bd0008444d229327cced13b", "url": "https://github.com/confluentinc/ksql/commit/aa59da8f816644aa0bd0008444d229327cced13b", "message": "fix exception inheritance", "committedDate": "2020-08-07T09:30:48Z", "type": "commit"}, {"oid": "c75732d686bf570b6e6639a902766fd2707096f7", "url": "https://github.com/confluentinc/ksql/commit/c75732d686bf570b6e6639a902766fd2707096f7", "message": "fix var naming", "committedDate": "2020-08-07T09:30:49Z", "type": "commit"}, {"oid": "0cbcefffddb85da52e6a226703271c9ebb68e17c", "url": "https://github.com/confluentinc/ksql/commit/0cbcefffddb85da52e6a226703271c9ebb68e17c", "message": "comment when offset and meta == null", "committedDate": "2020-08-07T09:30:49Z", "type": "commit"}, {"oid": "d8063f189f40949742cdf0aa935478f222a7df91", "url": "https://github.com/confluentinc/ksql/commit/d8063f189f40949742cdf0aa935478f222a7df91", "message": "move time suffix", "committedDate": "2020-08-07T09:30:49Z", "type": "commit"}, {"oid": "f1dad37e8ed5e6befeea8503252d9d9328ebc1e6", "url": "https://github.com/confluentinc/ksql/commit/f1dad37e8ed5e6befeea8503252d9d9328ebc1e6", "message": "test query app id builder", "committedDate": "2020-08-07T09:30:49Z", "type": "commit"}, {"oid": "e6e4ab6cabeaa0404d2eb2d8d4c817fcf0d2e6c8", "url": "https://github.com/confluentinc/ksql/commit/e6e4ab6cabeaa0404d2eb2d8d4c817fcf0d2e6c8", "message": "fix: remove commented out code", "committedDate": "2020-08-07T09:30:50Z", "type": "commit"}, {"oid": "bd0d51754929de356a0dafb1c112974255ad2563", "url": "https://github.com/confluentinc/ksql/commit/bd0d51754929de356a0dafb1c112974255ad2563", "message": "validate objects non null", "committedDate": "2020-08-07T09:30:50Z", "type": "commit"}, {"oid": "fba0a7552fd7c0d16b9ba61c7ce120b46d9acfc0", "url": "https://github.com/confluentinc/ksql/commit/fba0a7552fd7c0d16b9ba61c7ce120b46d9acfc0", "message": "add tests to list offsets", "committedDate": "2020-08-07T09:30:50Z", "type": "commit"}, {"oid": "e34555f99f830a34e0008cb85b6bf93789d90898", "url": "https://github.com/confluentinc/ksql/commit/e34555f99f830a34e0008cb85b6bf93789d90898", "message": "add test when offsets differ", "committedDate": "2020-08-07T09:30:50Z", "type": "commit"}, {"oid": "46c547306c6bda5652343f9c64ba7f8a993d2596", "url": "https://github.com/confluentinc/ksql/commit/46c547306c6bda5652343f9c64ba7f8a993d2596", "message": "converge tests", "committedDate": "2020-08-07T09:30:51Z", "type": "commit"}, {"oid": "04f517a6b9455e32256a07eab3566da62eef3b21", "url": "https://github.com/confluentinc/ksql/commit/04f517a6b9455e32256a07eab3566da62eef3b21", "message": "fix checkstyle", "committedDate": "2020-08-07T09:30:51Z", "type": "commit"}, {"oid": "480e7c4c970cc0e87283f646166c42bd809abe8c", "url": "https://github.com/confluentinc/ksql/commit/480e7c4c970cc0e87283f646166c42bd809abe8c", "message": "fix issues with long instantiation", "committedDate": "2020-08-07T09:30:51Z", "type": "commit"}, {"oid": "c3f5eb44e00f0695aaa3ce706ed30902ae35b8da", "url": "https://github.com/confluentinc/ksql/commit/c3f5eb44e00f0695aaa3ce706ed30902ae35b8da", "message": "fix checkstyle", "committedDate": "2020-08-07T09:30:51Z", "type": "commit"}, {"oid": "dc97ede39b8d64bc19b668ae4c9a8261292035a9", "url": "https://github.com/confluentinc/ksql/commit/dc97ede39b8d64bc19b668ae4c9a8261292035a9", "message": "map consumer groups to topics", "committedDate": "2020-08-07T09:30:52Z", "type": "commit"}, {"oid": "03d0cfce38c81edb5b2b38d1d22a85c081aee471", "url": "https://github.com/confluentinc/ksql/commit/03d0cfce38c81edb5b2b38d1d22a85c081aee471", "message": "fix console tests", "committedDate": "2020-08-07T09:30:52Z", "type": "commit"}, {"oid": "32b3df5adb764f9741d45e07206467ab8bb7b287", "url": "https://github.com/confluentinc/ksql/commit/32b3df5adb764f9741d45e07206467ab8bb7b287", "message": "add topic offsets summary", "committedDate": "2020-08-07T09:30:52Z", "type": "commit"}, {"oid": "8e4612f322e51f91f7874bd2e41a3bd678bbcf45", "url": "https://github.com/confluentinc/ksql/commit/8e4612f322e51f91f7874bd2e41a3bd678bbcf45", "message": "fix unused import", "committedDate": "2020-08-07T09:30:52Z", "type": "commit"}, {"oid": "1976131610ecd78453962327ca4b87fb5148de71", "url": "https://github.com/confluentinc/ksql/commit/1976131610ecd78453962327ca4b87fb5148de71", "message": "fix console test", "committedDate": "2020-08-07T09:30:53Z", "type": "commit"}, {"oid": "6f213a86749a033019ae8a26932f999f27f6b800", "url": "https://github.com/confluentinc/ksql/commit/6f213a86749a033019ae8a26932f999f27f6b800", "message": "fix json format", "committedDate": "2020-08-07T10:50:00Z", "type": "commit"}, {"oid": "6f213a86749a033019ae8a26932f999f27f6b800", "url": "https://github.com/confluentinc/ksql/commit/6f213a86749a033019ae8a26932f999f27f6b800", "message": "fix json format", "committedDate": "2020-08-07T10:50:00Z", "type": "forcePushed"}, {"oid": "40a484a21fbcb542ab8da24d80aa9562c05cd06d", "url": "https://github.com/confluentinc/ksql/commit/40a484a21fbcb542ab8da24d80aa9562c05cd06d", "message": "fix groupId mapping", "committedDate": "2020-08-07T11:38:39Z", "type": "commit"}, {"oid": "082b5d243b9c5bac48ba6406a91619046cea77fa", "url": "https://github.com/confluentinc/ksql/commit/082b5d243b9c5bac48ba6406a91619046cea77fa", "message": "fix import", "committedDate": "2020-08-07T11:47:24Z", "type": "commit"}, {"oid": "ca007da99c0c787a346154a5c7f9b474cca1c163", "url": "https://github.com/confluentinc/ksql/commit/ca007da99c0c787a346154a5c7f9b474cca1c163", "message": "fix json order", "committedDate": "2020-08-07T12:41:45Z", "type": "commit"}]}