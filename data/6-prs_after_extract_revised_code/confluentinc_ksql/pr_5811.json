{"pr_number": 5811, "pr_title": "test:  Adds another test case for pull queries for active restoring state", "pr_createdAt": "2020-07-10T17:11:15Z", "pr_url": "https://github.com/confluentinc/ksql/pull/5811", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NTM1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r452985356", "bodyText": "This method checks that the clusterStatus response contains lag reporting? Not that there is actual lag, right?", "author": "vpapavas", "createdAt": "2020-07-10T17:46:09Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java", "diffHunk": "@@ -264,5 +270,121 @@ public static void sendLagReportingRequest(\n           .get();\n     }\n   }\n+\n+  static String extractQueryId(final String outputString) {\n+    final java.util.regex.Matcher matcher = QUERY_ID_PATTERN.matcher(outputString);\n+    if (!matcher.find()) {\n+      throw new AssertionError(\"Could not find query id in: \" + outputString);\n+    }\n+    return matcher.group(1);\n+  }\n+\n+  // Ensures that lags exist for the cluster.  Makes the simplified assumption that there's just one\n+  // state store.\n+  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>", "originalCommit": "67b17d59d38de14886f913fe9988b0733356d7cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAxMjM1Ng==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453012356", "bodyText": "Correct.  The name and comments weren't great.  Changed to lagsReported. The latter version checks the actual values.", "author": "AlanConfluent", "createdAt": "2020-07-10T18:41:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NTM1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\nindex b21b6c82f2..5bdd857c74 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\n\n@@ -270,121 +264,5 @@ class HighAvailabilityTestUtil {\n           .get();\n     }\n   }\n-\n-  static String extractQueryId(final String outputString) {\n-    final java.util.regex.Matcher matcher = QUERY_ID_PATTERN.matcher(outputString);\n-    if (!matcher.find()) {\n-      throw new AssertionError(\"Could not find query id in: \" + outputString);\n-    }\n-    return matcher.group(1);\n-  }\n-\n-  // Ensures that lags exist for the cluster.  Makes the simplified assumption that there's just one\n-  // state store.\n-  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n-  lagsExist(\n-      final int expectedClusterSize\n-  ) {\n-    return (remoteServer, clusterStatus) -> {\n-      if (clusterStatus.size() == expectedClusterSize) {\n-        int numWithLag = 0;\n-        for (Entry<KsqlHostInfoEntity, HostStatusEntity> e : clusterStatus.entrySet()) {\n-          if (e.getValue().getHostStoreLags().getStateStoreLags().size() > 0) {\n-            numWithLag++;\n-          }\n-        }\n-        if (numWithLag >= Math.min(expectedClusterSize, 2)) {\n-          LOG.info(\"Found expected lags: {}\", clusterStatus.toString());\n-          return true;\n-        }\n-      }\n-      LOG.info(\"Didn't yet find expected lags: {}\", clusterStatus.toString());\n-      return false;\n-    };\n-  }\n-\n-  // Ensures that lags exist for the given host.  Makes the simplified assumption that there's just\n-  // one state store.\n-  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n-  lagsExist(\n-      final int clusterSize,\n-      final KsqlHostInfoEntity server,\n-      final long currentOffset,\n-      final long endOffset\n-  ) {\n-    return (remote, clusterStatus) -> {\n-      if (clusterStatus.size() == clusterSize) {\n-        HostStatusEntity hostStatusEntity = clusterStatus.get(server);\n-        if (hostStatusEntity == null) {\n-          LOG.info(\"Didn't find {}\", server.toString());\n-          return false;\n-        }\n-        Pair<Long, Long> pair = getOffsets(server,clusterStatus);\n-        long current = pair.left;\n-        long end = pair.right;\n-        if ((currentOffset < 0 || current >= currentOffset) && end >= endOffset) {\n-          LOG.info(\"Found expected end offset {} for {}: {}\", endOffset, server,\n-              clusterStatus.toString());\n-          return true;\n-        }\n-      }\n-      LOG.info(\"Didn't yet find expected end offset {} for {}: {}\", endOffset, server,\n-          clusterStatus.toString());\n-      return false;\n-    };\n-  }\n-\n-  // Gets (current, end) offsets for the given host.  Makes the simplified assumption that there's\n-  // just one state store.\n-  public static Pair<Long, Long> getOffsets(\n-      final KsqlHostInfoEntity server,\n-      final Map<KsqlHostInfoEntity, HostStatusEntity> clusterStatus) {\n-    HostStatusEntity hostStatusEntity = clusterStatus.get(server);\n-    long end = hostStatusEntity.getHostStoreLags().getStateStoreLags().values().stream()\n-        .flatMap(stateStoreLags -> stateStoreLags.getLagByPartition().values().stream())\n-        .mapToLong(LagInfoEntity::getEndOffsetPosition)\n-        .max()\n-        .orElse(0);\n-    long current = hostStatusEntity.getHostStoreLags().getStateStoreLags().values().stream()\n-        .flatMap(stateStoreLags -> stateStoreLags.getLagByPartition().values().stream())\n-        .mapToLong(LagInfoEntity::getCurrentOffsetPosition)\n-        .max()\n-        .orElse(0);\n-    return Pair.of(current, end);\n-  }\n-\n-  // A class that holds shutoff switches for various network components in our system to simulate\n-  // slowdowns or partitions.\n-  public static class Shutoffs {\n-    private final AtomicBoolean ksqlOutgoing = new AtomicBoolean(false);\n-    private final AtomicInteger kafkaPauseOffset = new AtomicInteger(-1);\n-\n-    public void shutOffAll() {\n-      ksqlOutgoing.set(true);\n-      kafkaPauseOffset.set(0);\n-    }\n-\n-    public void reset() {\n-      ksqlOutgoing.set(false);\n-      kafkaPauseOffset.set(-1);\n-    }\n-\n-    public void setKsqlOutgoing(boolean ksqlOutgoingPaused) {\n-      ksqlOutgoing.set(ksqlOutgoingPaused);\n-    }\n-\n-    public void setKafkaPauseOffset(int pauseOffset) {\n-      kafkaPauseOffset.set(pauseOffset);\n-    }\n-\n-    public Boolean getKsqlOutgoing() {\n-      return ksqlOutgoing.get();\n-    }\n-\n-    public Integer getKafkaPauseOffset() {\n-      return kafkaPauseOffset.get();\n-    }\n-\n-  }\n }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NjM0OQ==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r452986349", "bodyText": "This method checks for a specific server if there is actual lag reported? I don't understand the if condition: currentOffset < 0  what does this mean?", "author": "vpapavas", "createdAt": "2020-07-10T17:48:13Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java", "diffHunk": "@@ -264,5 +270,121 @@ public static void sendLagReportingRequest(\n           .get();\n     }\n   }\n+\n+  static String extractQueryId(final String outputString) {\n+    final java.util.regex.Matcher matcher = QUERY_ID_PATTERN.matcher(outputString);\n+    if (!matcher.find()) {\n+      throw new AssertionError(\"Could not find query id in: \" + outputString);\n+    }\n+    return matcher.group(1);\n+  }\n+\n+  // Ensures that lags exist for the cluster.  Makes the simplified assumption that there's just one\n+  // state store.\n+  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n+  lagsExist(\n+      final int expectedClusterSize\n+  ) {\n+    return (remoteServer, clusterStatus) -> {\n+      if (clusterStatus.size() == expectedClusterSize) {\n+        int numWithLag = 0;\n+        for (Entry<KsqlHostInfoEntity, HostStatusEntity> e : clusterStatus.entrySet()) {\n+          if (e.getValue().getHostStoreLags().getStateStoreLags().size() > 0) {\n+            numWithLag++;\n+          }\n+        }\n+        if (numWithLag >= Math.min(expectedClusterSize, 2)) {\n+          LOG.info(\"Found expected lags: {}\", clusterStatus.toString());\n+          return true;\n+        }\n+      }\n+      LOG.info(\"Didn't yet find expected lags: {}\", clusterStatus.toString());\n+      return false;\n+    };\n+  }\n+\n+  // Ensures that lags exist for the given host.  Makes the simplified assumption that there's just\n+  // one state store.\n+  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n+  lagsExist(\n+      final int clusterSize,\n+      final KsqlHostInfoEntity server,\n+      final long currentOffset,\n+      final long endOffset\n+  ) {\n+    return (remote, clusterStatus) -> {\n+      if (clusterStatus.size() == clusterSize) {\n+        HostStatusEntity hostStatusEntity = clusterStatus.get(server);\n+        if (hostStatusEntity == null) {\n+          LOG.info(\"Didn't find {}\", server.toString());\n+          return false;\n+        }\n+        Pair<Long, Long> pair = getOffsets(server,clusterStatus);\n+        long current = pair.left;\n+        long end = pair.right;\n+        if ((currentOffset < 0 || current >= currentOffset) && end >= endOffset) {", "originalCommit": "67b17d59d38de14886f913fe9988b0733356d7cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAwMjM2NA==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453002364", "bodyText": "If offset currentOffset < 0, we don't check it.  It's just mean as a special value since some tests don't care about this.  I'll make this optional instead to make it more obvious.", "author": "AlanConfluent", "createdAt": "2020-07-10T18:22:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NjM0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\nindex b21b6c82f2..5bdd857c74 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/HighAvailabilityTestUtil.java\n\n@@ -270,121 +264,5 @@ class HighAvailabilityTestUtil {\n           .get();\n     }\n   }\n-\n-  static String extractQueryId(final String outputString) {\n-    final java.util.regex.Matcher matcher = QUERY_ID_PATTERN.matcher(outputString);\n-    if (!matcher.find()) {\n-      throw new AssertionError(\"Could not find query id in: \" + outputString);\n-    }\n-    return matcher.group(1);\n-  }\n-\n-  // Ensures that lags exist for the cluster.  Makes the simplified assumption that there's just one\n-  // state store.\n-  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n-  lagsExist(\n-      final int expectedClusterSize\n-  ) {\n-    return (remoteServer, clusterStatus) -> {\n-      if (clusterStatus.size() == expectedClusterSize) {\n-        int numWithLag = 0;\n-        for (Entry<KsqlHostInfoEntity, HostStatusEntity> e : clusterStatus.entrySet()) {\n-          if (e.getValue().getHostStoreLags().getStateStoreLags().size() > 0) {\n-            numWithLag++;\n-          }\n-        }\n-        if (numWithLag >= Math.min(expectedClusterSize, 2)) {\n-          LOG.info(\"Found expected lags: {}\", clusterStatus.toString());\n-          return true;\n-        }\n-      }\n-      LOG.info(\"Didn't yet find expected lags: {}\", clusterStatus.toString());\n-      return false;\n-    };\n-  }\n-\n-  // Ensures that lags exist for the given host.  Makes the simplified assumption that there's just\n-  // one state store.\n-  static BiFunction<KsqlHostInfoEntity, Map<KsqlHostInfoEntity, HostStatusEntity>, Boolean>\n-  lagsExist(\n-      final int clusterSize,\n-      final KsqlHostInfoEntity server,\n-      final long currentOffset,\n-      final long endOffset\n-  ) {\n-    return (remote, clusterStatus) -> {\n-      if (clusterStatus.size() == clusterSize) {\n-        HostStatusEntity hostStatusEntity = clusterStatus.get(server);\n-        if (hostStatusEntity == null) {\n-          LOG.info(\"Didn't find {}\", server.toString());\n-          return false;\n-        }\n-        Pair<Long, Long> pair = getOffsets(server,clusterStatus);\n-        long current = pair.left;\n-        long end = pair.right;\n-        if ((currentOffset < 0 || current >= currentOffset) && end >= endOffset) {\n-          LOG.info(\"Found expected end offset {} for {}: {}\", endOffset, server,\n-              clusterStatus.toString());\n-          return true;\n-        }\n-      }\n-      LOG.info(\"Didn't yet find expected end offset {} for {}: {}\", endOffset, server,\n-          clusterStatus.toString());\n-      return false;\n-    };\n-  }\n-\n-  // Gets (current, end) offsets for the given host.  Makes the simplified assumption that there's\n-  // just one state store.\n-  public static Pair<Long, Long> getOffsets(\n-      final KsqlHostInfoEntity server,\n-      final Map<KsqlHostInfoEntity, HostStatusEntity> clusterStatus) {\n-    HostStatusEntity hostStatusEntity = clusterStatus.get(server);\n-    long end = hostStatusEntity.getHostStoreLags().getStateStoreLags().values().stream()\n-        .flatMap(stateStoreLags -> stateStoreLags.getLagByPartition().values().stream())\n-        .mapToLong(LagInfoEntity::getEndOffsetPosition)\n-        .max()\n-        .orElse(0);\n-    long current = hostStatusEntity.getHostStoreLags().getStateStoreLags().values().stream()\n-        .flatMap(stateStoreLags -> stateStoreLags.getLagByPartition().values().stream())\n-        .mapToLong(LagInfoEntity::getCurrentOffsetPosition)\n-        .max()\n-        .orElse(0);\n-    return Pair.of(current, end);\n-  }\n-\n-  // A class that holds shutoff switches for various network components in our system to simulate\n-  // slowdowns or partitions.\n-  public static class Shutoffs {\n-    private final AtomicBoolean ksqlOutgoing = new AtomicBoolean(false);\n-    private final AtomicInteger kafkaPauseOffset = new AtomicInteger(-1);\n-\n-    public void shutOffAll() {\n-      ksqlOutgoing.set(true);\n-      kafkaPauseOffset.set(0);\n-    }\n-\n-    public void reset() {\n-      ksqlOutgoing.set(false);\n-      kafkaPauseOffset.set(-1);\n-    }\n-\n-    public void setKsqlOutgoing(boolean ksqlOutgoingPaused) {\n-      ksqlOutgoing.set(ksqlOutgoingPaused);\n-    }\n-\n-    public void setKafkaPauseOffset(int pauseOffset) {\n-      kafkaPauseOffset.set(pauseOffset);\n-    }\n-\n-    public Boolean getKsqlOutgoing() {\n-      return ksqlOutgoing.get();\n-    }\n-\n-    public Integer getKafkaPauseOffset() {\n-      return kafkaPauseOffset.get();\n-    }\n-\n-  }\n }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NjcxOA==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r452986718", "bodyText": "Why do we need the lagsExists check here?", "author": "vpapavas", "createdAt": "2020-07-10T17:48:58Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java", "diffHunk": "@@ -258,19 +249,13 @@ public void cleanUp() {\n     APP_SHUTOFFS_2.reset();\n   }\n \n-  @AfterClass\n-  public static void classTearDown() {\n-    TMP.delete();\n-  }\n-\n   @Test\n   public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n     // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(REST_APP_0, REST_APP_1, REST_APP_2);\n+    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n     waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n     waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        PullQueryRoutingFunctionalTest::lagsExist);\n+        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsExist(3));", "originalCommit": "67b17d59d38de14886f913fe9988b0733356d7cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAwMTkwMg==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453001902", "bodyText": "The intention is to ensure that all lags have been reported since we won't forward requests if we have no lag info by default.  If we sneak in a pull query before this happens, it will fail, so we just wait for all lags to be reported.", "author": "AlanConfluent", "createdAt": "2020-07-10T18:21:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk4NjcxOA=="}], "type": "inlineReview", "revised_code": {"commit": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\nindex bdb2952343..c56eaf6fc0 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n\n@@ -129,284 +130,450 @@ public class PullQueryRoutingFunctionalTest {\n       .put(KsqlConfig.KSQL_SHUTDOWN_TIMEOUT_MS_CONFIG, 1000)\n       .build();\n \n-  private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n-\n-  private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n-  private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n-  private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n-  private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n-\n-  @Rule\n-  public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_0::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  private static class CommonState {\n+\n+    private final AtomicLong timestampSupplier = new AtomicLong(BASE_TIME);\n+    String output;\n+    String queryId;\n+    String sql;\n+    String topic;\n+\n+    public void setUp(final IntegrationTestHarness testHarness, final TestKsqlRestApp restApp) {\n+      //Create topic with 1 partition to control who is active and standby\n+      topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      testHarness.ensureTopics(1, topic);\n+\n+      testHarness.produceRows(\n+          topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          timestampSupplier::getAndIncrement\n+      );\n+\n+      //Create stream\n+      makeAdminRequest(\n+          restApp,\n+          \"CREATE STREAM \" + USERS_STREAM\n+              + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n+              + \" WITH (\"\n+              + \"   kafka_topic='\" + topic + \"', \"\n+              + \"   value_format='JSON');\"\n+      );\n+      //Create table\n+      output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n+      List<KsqlEntity> res = makeAdminRequestWithResponse(\n+          restApp,\n+          \"CREATE TABLE \" + output + \" AS\"\n+              + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n+              + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n+      );\n+      queryId = extractQueryId(res.get(0).toString());\n+      queryId = queryId.substring(0, queryId.length() - 1);\n+      waitForTableRows(testHarness);\n+    }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_1::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+    private void waitForTableRows(final IntegrationTestHarness testHarness) {\n+      testHarness.verifyAvailableUniqueRows(\n+          output.toUpperCase(),\n+          USER_PROVIDER.data().size(),\n+          FormatFactory.JSON,\n+          AGGREGATE_SCHEMA\n+      );\n+    }\n+  }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_2::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class ThreeNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n+\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n+    private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_1.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_2.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n+    public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n+    public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+      FaultyKafkaConsumer1.setPause(APP_SHUTOFFS_1.kafkaIncoming::get);\n+      FaultyKafkaConsumer2.setPause(APP_SHUTOFFS_2.kafkaIncoming::get);\n+    }\n \n-  public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n-  public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n-  public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n \n-  @ClassRule\n-  public static final RuleChain CHAIN = RuleChain\n-      .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n-      .around(TEST_HARNESS).around(TMP);\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), commonState.queryId);\n+    }\n \n-  @Rule\n-  public final Timeout timeout = Timeout.builder()\n-      .withTimeout(1, TimeUnit.MINUTES)\n-      .withLookingForStuckThread(true)\n-      .build();\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+      APP_SHUTOFFS_1.reset();\n+      APP_SHUTOFFS_2.reset();\n+    }\n \n-  @BeforeClass\n-  public static void setUpClass() {\n-    FaultyKafkaConsumer0.setPauseOffset(APP_SHUTOFFS_0::getKafkaPauseOffset);\n-    FaultyKafkaConsumer1.setPauseOffset(APP_SHUTOFFS_1::getKafkaPauseOffset);\n-    FaultyKafkaConsumer2.setPauseOffset(APP_SHUTOFFS_2::getKafkaPauseOffset);\n-  }\n+//    @AfterClass\n+//    public static void classTearDown() {\n+//      TMP.delete();\n+//    }\n+\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.standBy.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      // When:\n+      List<StreamedRow> rows_0 =\n+          makePullQueryRequest(clusterFormation.standBy.getApp(), commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Before\n-  public void setUp() {\n-    //Create topic with 1 partition to control who is active and standby\n-    topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    TEST_HARNESS.ensureTopics(1, topic);\n-\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    //Create stream\n-    makeAdminRequest(\n-        REST_APP_0,\n-        \"CREATE STREAM \" + USERS_STREAM\n-            + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n-            + \" WITH (\"\n-            + \"   kafka_topic='\" + topic + \"', \"\n-            + \"   value_format='JSON');\"\n-    );\n-    //Create table\n-    output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n-    List<KsqlEntity> res = makeAdminRequestWithResponse(\n-        REST_APP_0,\n-        \"CREATE TABLE \" + output + \" AS\"\n-            + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n-            + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n-    );\n-    queryId = extractQueryId(res.get(0).toString());\n-    queryId = queryId.substring(0, queryId.length() - 1);\n-    waitForTableRows();\n-\n-    waitForStreamsMetadataToInitialize(\n-        REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), queryId);\n-  }\n \n-  @After\n-  public void cleanUp() {\n-    REST_APP_0.closePersistentQueries();\n-    REST_APP_0.dropSourcesExcept();\n-    APP_SHUTOFFS_0.reset();\n-    APP_SHUTOFFS_1.reset();\n-    APP_SHUTOFFS_2.reset();\n-  }\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the standby\n+      clusterFormation.standBy.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n+\n+    @Test\n+    public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the active\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsExist(3));\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.standBy.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    // When:\n-    List<StreamedRow> rows_0 =\n-        makePullQueryRequest(clusterFormation.standBy.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    @Test\n+    public void shouldFilterLaggyServers() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.standBy.getHost(), 5));\n+\n+      // Cut off standby from Kafka to simulate lag\n+      clusterFormation.standBy.getShutoffs().kafkaIncoming.set(true);\n+      Thread.sleep(2000);\n+\n+      // Produce more data that will now only be available on active since standby is cut off\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // Make sure that the lags get reported before we kill active\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.active.getHost(), 10));\n+\n+      // Partition active off\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.router.getApp(), commonState.sql,\n+          LAG_FILTER_6);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      // This line ensures that we've not processed the new data\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n+          clusterFormation.router.getApp(),\n+          commonState.sql, LAG_FILTER_3);\n+      Assert.assertEquals(40001, errorMessage.getErrorCode());\n+      Assert.assertTrue(\n+          errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    }\n   }\n \n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class OneNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity host0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(host0, REST_APP_0, APP_SHUTOFFS_0);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsExist(3));\n-\n-    // Partition off the standby\n-    clusterFormation.standBy.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+    }\n \n-  @Test\n-  public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsExist(3));\n-\n-    // Partition off the active\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n+\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+    }\n \n-  @Test\n-  public void shouldFilterLaggyServers() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsExist(3));\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsExist(3, clusterFormation.standBy.getHost(), -1, 5));\n-\n-    // Cut off standby from Kafka to simulate lag\n-    clusterFormation.standBy.getShutoffs().setKafkaPauseOffset(0);\n-    Thread.sleep(2000);\n-\n-    // Produce more data that will now only be available on active since standby is cut off\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    // Make sure that the lags get reported before we kill active\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsExist(3, clusterFormation.active.getHost(), -1, 10));\n-\n-    // Partition active off\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_6);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    // This line ensures that we've not processed the new data\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-\n-    KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_3);\n-    Assert.assertEquals(40001, errorMessage.getErrorCode());\n-    Assert.assertTrue(\n-        errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    @Test\n+    public void singleNewNode() throws Exception {\n+      waitForStreamsMetadataToInitialize(REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+      ClusterFormation clusterFormation = findClusterFormation(commonState.queryId, TEST_APP_0);\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 5));\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Pause incoming kafka consumption\n+      APP_SHUTOFFS_0.kafkaIncoming.set(true);\n+\n+      // Produce more data\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // It should be cut off, so these new rows shouldn't be reflected, but give that some time\n+      // anyway to show it won't be reflected.\n+      Thread.sleep(2000);\n+\n+      final List<StreamedRow> sameRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = sameRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(sameRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Still haven't gotten the update yet\n+      assertThat(sameRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Unpause incoming kafka consumption. We then expect active to catch back up.\n+      APP_SHUTOFFS_0.kafkaIncoming.set(false);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 10));\n+\n+      final List<StreamedRow> updatedRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = updatedRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(updatedRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Got the update now!\n+      assertThat(updatedRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 2)));\n+    }\n   }\n \n   private static List<StreamedRow> makePullQueryRequest(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAyNjI0OA==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453026248", "bodyText": "This checks that the currentOffset is 3 and endOffset is 5, right?", "author": "vpapavas", "createdAt": "2020-07-10T19:00:24Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java", "diffHunk": "@@ -376,10 +358,11 @@ public void shouldFilterLaggyServers() throws Exception {\n \n     waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n         clusterFormation.router.getHost(),\n-        PullQueryRoutingFunctionalTest.lagsExist(clusterFormation.standBy.getHost(), 5));\n+        HighAvailabilityTestUtil.lagsReported(3, clusterFormation.standBy.getHost(),", "originalCommit": "34b1b9f322d27f108070f642924ea3b059880456", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAyOTE3Mw==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453029173", "bodyText": "This checks that there are three lag reports.  It was hardcoded before.  currentOffset is Optional.empty() in this case.  Maybe I should change this method to ignore the number of reports since HighAvailabilityTestUtil.lagsReported(3) does that above.", "author": "AlanConfluent", "createdAt": "2020-07-10T19:06:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAyNjI0OA=="}], "type": "inlineReview", "revised_code": {"commit": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\nindex ff33a7083b..c56eaf6fc0 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n\n@@ -129,286 +130,450 @@ public class PullQueryRoutingFunctionalTest {\n       .put(KsqlConfig.KSQL_SHUTDOWN_TIMEOUT_MS_CONFIG, 1000)\n       .build();\n \n-  private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n-\n-  private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n-  private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n-  private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n-  private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n-\n-  @Rule\n-  public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_0::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  private static class CommonState {\n+\n+    private final AtomicLong timestampSupplier = new AtomicLong(BASE_TIME);\n+    String output;\n+    String queryId;\n+    String sql;\n+    String topic;\n+\n+    public void setUp(final IntegrationTestHarness testHarness, final TestKsqlRestApp restApp) {\n+      //Create topic with 1 partition to control who is active and standby\n+      topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      testHarness.ensureTopics(1, topic);\n+\n+      testHarness.produceRows(\n+          topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          timestampSupplier::getAndIncrement\n+      );\n+\n+      //Create stream\n+      makeAdminRequest(\n+          restApp,\n+          \"CREATE STREAM \" + USERS_STREAM\n+              + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n+              + \" WITH (\"\n+              + \"   kafka_topic='\" + topic + \"', \"\n+              + \"   value_format='JSON');\"\n+      );\n+      //Create table\n+      output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n+      List<KsqlEntity> res = makeAdminRequestWithResponse(\n+          restApp,\n+          \"CREATE TABLE \" + output + \" AS\"\n+              + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n+              + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n+      );\n+      queryId = extractQueryId(res.get(0).toString());\n+      queryId = queryId.substring(0, queryId.length() - 1);\n+      waitForTableRows(testHarness);\n+    }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_1::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+    private void waitForTableRows(final IntegrationTestHarness testHarness) {\n+      testHarness.verifyAvailableUniqueRows(\n+          output.toUpperCase(),\n+          USER_PROVIDER.data().size(),\n+          FormatFactory.JSON,\n+          AGGREGATE_SCHEMA\n+      );\n+    }\n+  }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_2::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class ThreeNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n+\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n+    private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_1.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_2.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n+    public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n+    public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+      FaultyKafkaConsumer1.setPause(APP_SHUTOFFS_1.kafkaIncoming::get);\n+      FaultyKafkaConsumer2.setPause(APP_SHUTOFFS_2.kafkaIncoming::get);\n+    }\n \n-  public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n-  public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n-  public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n \n-  @ClassRule\n-  public static final RuleChain CHAIN = RuleChain\n-      .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n-      .around(TEST_HARNESS).around(TMP);\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), commonState.queryId);\n+    }\n \n-  @Rule\n-  public final Timeout timeout = Timeout.builder()\n-      .withTimeout(1, TimeUnit.MINUTES)\n-      .withLookingForStuckThread(true)\n-      .build();\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+      APP_SHUTOFFS_1.reset();\n+      APP_SHUTOFFS_2.reset();\n+    }\n \n-  @BeforeClass\n-  public static void setUpClass() {\n-    FaultyKafkaConsumer0.setPauseOffset(APP_SHUTOFFS_0::getKafkaPauseOffset);\n-    FaultyKafkaConsumer1.setPauseOffset(APP_SHUTOFFS_1::getKafkaPauseOffset);\n-    FaultyKafkaConsumer2.setPauseOffset(APP_SHUTOFFS_2::getKafkaPauseOffset);\n-  }\n+//    @AfterClass\n+//    public static void classTearDown() {\n+//      TMP.delete();\n+//    }\n+\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.standBy.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      // When:\n+      List<StreamedRow> rows_0 =\n+          makePullQueryRequest(clusterFormation.standBy.getApp(), commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Before\n-  public void setUp() {\n-    //Create topic with 1 partition to control who is active and standby\n-    topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    TEST_HARNESS.ensureTopics(1, topic);\n-\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    //Create stream\n-    makeAdminRequest(\n-        REST_APP_0,\n-        \"CREATE STREAM \" + USERS_STREAM\n-            + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n-            + \" WITH (\"\n-            + \"   kafka_topic='\" + topic + \"', \"\n-            + \"   value_format='JSON');\"\n-    );\n-    //Create table\n-    output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n-    List<KsqlEntity> res = makeAdminRequestWithResponse(\n-        REST_APP_0,\n-        \"CREATE TABLE \" + output + \" AS\"\n-            + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n-            + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n-    );\n-    queryId = extractQueryId(res.get(0).toString());\n-    queryId = queryId.substring(0, queryId.length() - 1);\n-    waitForTableRows();\n-\n-    waitForStreamsMetadataToInitialize(\n-        REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), queryId);\n-  }\n \n-  @After\n-  public void cleanUp() {\n-    REST_APP_0.closePersistentQueries();\n-    REST_APP_0.dropSourcesExcept();\n-    APP_SHUTOFFS_0.reset();\n-    APP_SHUTOFFS_1.reset();\n-    APP_SHUTOFFS_2.reset();\n-  }\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the standby\n+      clusterFormation.standBy.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n+\n+    @Test\n+    public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the active\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.standBy.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    // When:\n-    List<StreamedRow> rows_0 =\n-        makePullQueryRequest(clusterFormation.standBy.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    @Test\n+    public void shouldFilterLaggyServers() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.standBy.getHost(), 5));\n+\n+      // Cut off standby from Kafka to simulate lag\n+      clusterFormation.standBy.getShutoffs().kafkaIncoming.set(true);\n+      Thread.sleep(2000);\n+\n+      // Produce more data that will now only be available on active since standby is cut off\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // Make sure that the lags get reported before we kill active\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.active.getHost(), 10));\n+\n+      // Partition active off\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.router.getApp(), commonState.sql,\n+          LAG_FILTER_6);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      // This line ensures that we've not processed the new data\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n+          clusterFormation.router.getApp(),\n+          commonState.sql, LAG_FILTER_3);\n+      Assert.assertEquals(40001, errorMessage.getErrorCode());\n+      Assert.assertTrue(\n+          errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    }\n   }\n \n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class OneNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity host0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(host0, REST_APP_0, APP_SHUTOFFS_0);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    // Partition off the standby\n-    clusterFormation.standBy.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+    }\n \n-  @Test\n-  public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    // Partition off the active\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n+\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+    }\n \n-  @Test\n-  public void shouldFilterLaggyServers() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsReported(3, clusterFormation.standBy.getHost(),\n-            Optional.empty(), 5));\n-\n-    // Cut off standby from Kafka to simulate lag\n-    clusterFormation.standBy.getShutoffs().setKafkaPauseOffset(0);\n-    Thread.sleep(2000);\n-\n-    // Produce more data that will now only be available on active since standby is cut off\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    // Make sure that the lags get reported before we kill active\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsReported(3, clusterFormation.active.getHost(), Optional.empty(),\n-            10));\n-\n-    // Partition active off\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_6);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    // This line ensures that we've not processed the new data\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-\n-    KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_3);\n-    Assert.assertEquals(40001, errorMessage.getErrorCode());\n-    Assert.assertTrue(\n-        errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    @Test\n+    public void singleNewNode() throws Exception {\n+      waitForStreamsMetadataToInitialize(REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+      ClusterFormation clusterFormation = findClusterFormation(commonState.queryId, TEST_APP_0);\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 5));\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Pause incoming kafka consumption\n+      APP_SHUTOFFS_0.kafkaIncoming.set(true);\n+\n+      // Produce more data\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // It should be cut off, so these new rows shouldn't be reflected, but give that some time\n+      // anyway to show it won't be reflected.\n+      Thread.sleep(2000);\n+\n+      final List<StreamedRow> sameRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = sameRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(sameRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Still haven't gotten the update yet\n+      assertThat(sameRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Unpause incoming kafka consumption. We then expect active to catch back up.\n+      APP_SHUTOFFS_0.kafkaIncoming.set(false);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 10));\n+\n+      final List<StreamedRow> updatedRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = updatedRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(updatedRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Got the update now!\n+      assertThat(updatedRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 2)));\n+    }\n   }\n \n   private static List<StreamedRow> makePullQueryRequest(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzMDY2Ng==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453030666", "bodyText": "I am confused by the title and description of the PR. You say you add a new test case for restoring the active after it restarts but that's not what's happening here: the active gets killed and we query the standby that has lag. Am I missing something?", "author": "vpapavas", "createdAt": "2020-07-10T19:10:28Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java", "diffHunk": "@@ -360,11 +343,10 @@ public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() th\n   @Test", "originalCommit": "34b1b9f322d27f108070f642924ea3b059880456", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzMTk5OQ==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453031999", "bodyText": "Yes.  There is no standby here.  There's just one server.  It gets up to date with its state store, it gets killed, has it's state store wiped out, and then restarts.  Then that same server is queried.", "author": "AlanConfluent", "createdAt": "2020-07-10T19:13:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzMDY2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzNTY1Mg==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453035652", "bodyText": "Sorry, this file hasn't been changed materially.  The new file has a new test that does what I described.", "author": "AlanConfluent", "createdAt": "2020-07-10T19:21:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzAzMDY2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\nindex ff33a7083b..c56eaf6fc0 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQueryRoutingFunctionalTest.java\n\n@@ -129,286 +130,450 @@ public class PullQueryRoutingFunctionalTest {\n       .put(KsqlConfig.KSQL_SHUTDOWN_TIMEOUT_MS_CONFIG, 1000)\n       .build();\n \n-  private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n-  private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n-\n-  private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n-  private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n-  private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n-  private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n-  private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n-\n-  @Rule\n-  public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_0::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  private static class CommonState {\n+\n+    private final AtomicLong timestampSupplier = new AtomicLong(BASE_TIME);\n+    String output;\n+    String queryId;\n+    String sql;\n+    String topic;\n+\n+    public void setUp(final IntegrationTestHarness testHarness, final TestKsqlRestApp restApp) {\n+      //Create topic with 1 partition to control who is active and standby\n+      topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      testHarness.ensureTopics(1, topic);\n+\n+      testHarness.produceRows(\n+          topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          timestampSupplier::getAndIncrement\n+      );\n+\n+      //Create stream\n+      makeAdminRequest(\n+          restApp,\n+          \"CREATE STREAM \" + USERS_STREAM\n+              + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n+              + \" WITH (\"\n+              + \"   kafka_topic='\" + topic + \"', \"\n+              + \"   value_format='JSON');\"\n+      );\n+      //Create table\n+      output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n+      sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n+      List<KsqlEntity> res = makeAdminRequestWithResponse(\n+          restApp,\n+          \"CREATE TABLE \" + output + \" AS\"\n+              + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n+              + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n+      );\n+      queryId = extractQueryId(res.get(0).toString());\n+      queryId = queryId.substring(0, queryId.length() - 1);\n+      waitForTableRows(testHarness);\n+    }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_1::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+    private void waitForTableRows(final IntegrationTestHarness testHarness) {\n+      testHarness.verifyAvailableUniqueRows(\n+          output.toUpperCase(),\n+          USER_PROVIDER.data().size(),\n+          FormatFactory.JSON,\n+          AGGREGATE_SCHEMA\n+      );\n+    }\n+  }\n \n-  @Rule\n-  public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n-      .builder(TEST_HARNESS::kafkaBootstrapServers)\n-      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir())\n-      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n-      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n-      .withFaultyKsqlClient(APP_SHUTOFFS_2::getKsqlOutgoing)\n-      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n-          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n-      .withProperties(COMMON_CONFIG)\n-      .build();\n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class ThreeNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_1 = new Shutoffs();\n+    private static final Shutoffs APP_SHUTOFFS_2 = new Shutoffs();\n+\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_1 = TestUtils.findFreeLocalPort();\n+    private static final int INT_PORT_2 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity HOST0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final KsqlHostInfoEntity HOST1 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_1);\n+    private static final KsqlHostInfoEntity HOST2 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_2);\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_1 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_1)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_1.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer1.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_2 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_2)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_2.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer2.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n+    public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n+    public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+      FaultyKafkaConsumer1.setPause(APP_SHUTOFFS_1.kafkaIncoming::get);\n+      FaultyKafkaConsumer2.setPause(APP_SHUTOFFS_2.kafkaIncoming::get);\n+    }\n \n-  public final TestApp TEST_APP_0 = new TestApp(HOST0, REST_APP_0, APP_SHUTOFFS_0);\n-  public final TestApp TEST_APP_1 = new TestApp(HOST1, REST_APP_1, APP_SHUTOFFS_1);\n-  public final TestApp TEST_APP_2 = new TestApp(HOST2, REST_APP_2, APP_SHUTOFFS_2);\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n \n-  @ClassRule\n-  public static final RuleChain CHAIN = RuleChain\n-      .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n-      .around(TEST_HARNESS).around(TMP);\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), commonState.queryId);\n+    }\n \n-  @Rule\n-  public final Timeout timeout = Timeout.builder()\n-      .withTimeout(1, TimeUnit.MINUTES)\n-      .withLookingForStuckThread(true)\n-      .build();\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+      APP_SHUTOFFS_1.reset();\n+      APP_SHUTOFFS_2.reset();\n+    }\n \n-  @BeforeClass\n-  public static void setUpClass() {\n-    FaultyKafkaConsumer0.setPauseOffset(APP_SHUTOFFS_0::getKafkaPauseOffset);\n-    FaultyKafkaConsumer1.setPauseOffset(APP_SHUTOFFS_1::getKafkaPauseOffset);\n-    FaultyKafkaConsumer2.setPauseOffset(APP_SHUTOFFS_2::getKafkaPauseOffset);\n-  }\n+//    @AfterClass\n+//    public static void classTearDown() {\n+//      TMP.delete();\n+//    }\n+\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.standBy.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      // When:\n+      List<StreamedRow> rows_0 =\n+          makePullQueryRequest(clusterFormation.standBy.getApp(), commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Before\n-  public void setUp() {\n-    //Create topic with 1 partition to control who is active and standby\n-    topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    TEST_HARNESS.ensureTopics(1, topic);\n-\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    //Create stream\n-    makeAdminRequest(\n-        REST_APP_0,\n-        \"CREATE STREAM \" + USERS_STREAM\n-            + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n-            + \" WITH (\"\n-            + \"   kafka_topic='\" + topic + \"', \"\n-            + \"   value_format='JSON');\"\n-    );\n-    //Create table\n-    output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n-    sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n-    List<KsqlEntity> res = makeAdminRequestWithResponse(\n-        REST_APP_0,\n-        \"CREATE TABLE \" + output + \" AS\"\n-            + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n-            + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n-    );\n-    queryId = extractQueryId(res.get(0).toString());\n-    queryId = queryId.substring(0, queryId.length() - 1);\n-    waitForTableRows();\n-\n-    waitForStreamsMetadataToInitialize(\n-        REST_APP_0, ImmutableList.of(HOST0, HOST1, HOST2), queryId);\n-  }\n \n-  @After\n-  public void cleanUp() {\n-    REST_APP_0.closePersistentQueries();\n-    REST_APP_0.dropSourcesExcept();\n-    APP_SHUTOFFS_0.reset();\n-    APP_SHUTOFFS_1.reset();\n-    APP_SHUTOFFS_2.reset();\n-  }\n+    @Test\n+    public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the standby\n+      clusterFormation.standBy.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n+\n+    @Test\n+    public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+\n+      // Partition off the active\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(),\n+          commonState.sql);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveQueryIssuedToStandby() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.standBy.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.standBy.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    // When:\n-    List<StreamedRow> rows_0 =\n-        makePullQueryRequest(clusterFormation.standBy.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+    @Test\n+    public void shouldFilterLaggyServers() throws Exception {\n+      // Given:\n+      ClusterFormation clusterFormation = findClusterFormation(\n+          commonState.queryId, TEST_APP_0, TEST_APP_1, TEST_APP_2);\n+      waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(), lagsExist(3));\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.standBy.getHost(), 5));\n+\n+      // Cut off standby from Kafka to simulate lag\n+      clusterFormation.standBy.getShutoffs().kafkaIncoming.set(true);\n+      Thread.sleep(2000);\n+\n+      // Produce more data that will now only be available on active since standby is cut off\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // Make sure that the lags get reported before we kill active\n+      waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n+          clusterFormation.router.getHost(),\n+          lagsExist(3, clusterFormation.active.getHost(), 10));\n+\n+      // Partition active off\n+      clusterFormation.active.getShutoffs().shutOffAll();\n+\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.standBy.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsUp);\n+      waitForRemoteServerToChangeStatus(\n+          clusterFormation.router.getApp(),\n+          clusterFormation.active.getHost(),\n+          HighAvailabilityTestUtil::remoteServerIsDown);\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.router.getApp(), commonState.sql,\n+          LAG_FILTER_6);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      // This line ensures that we've not processed the new data\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n+          clusterFormation.router.getApp(),\n+          commonState.sql, LAG_FILTER_3);\n+      Assert.assertEquals(40001, errorMessage.getErrorCode());\n+      Assert.assertTrue(\n+          errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    }\n   }\n \n+  @RunWith(MockitoJUnitRunner.class)\n+  public static class OneNodeTests {\n+    private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+    private static final TemporaryFolder TMP = new TemporaryFolder();\n+    private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+    private static final KsqlHostInfoEntity host0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+    private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+\n+    @Rule\n+    public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+        .builder(TEST_HARNESS::kafkaBootstrapServers)\n+        .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+        .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+        .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+        .withFaultyKsqlClient(APP_SHUTOFFS_0.ksqlOutgoing::get)\n+        .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+            + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+        .withProperties(COMMON_CONFIG)\n+        .build();\n+\n+    public final TestApp TEST_APP_0 = new TestApp(host0, REST_APP_0, APP_SHUTOFFS_0);\n+\n+    @ClassRule\n+    public static final RuleChain CHAIN = RuleChain\n+        .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+        .around(TEST_HARNESS).around(TMP);\n+\n+    @Rule\n+    public final Timeout timeout = Timeout.builder()\n+        .withTimeout(1, TimeUnit.MINUTES)\n+        .withLookingForStuckThread(true)\n+        .build();\n+\n+    private CommonState commonState;\n+\n+    @BeforeClass\n+    public static void setUpClass() {\n+      FaultyKafkaConsumer0.setPause(APP_SHUTOFFS_0.kafkaIncoming::get);\n+    }\n \n-  @Test\n-  public void shouldQueryActiveWhenActiveAliveStandbyDeadQueryIssuedToRouter() {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    // Partition off the standby\n-    clusterFormation.standBy.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @After\n+    public void cleanUp() {\n+      REST_APP_0.closePersistentQueries();\n+      REST_APP_0.dropSourcesExcept();\n+      APP_SHUTOFFS_0.reset();\n+    }\n \n-  @Test\n-  public void shouldQueryStandbyWhenActiveDeadStandbyAliveQueryIssuedToRouter() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-\n-    // Partition off the active\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(clusterFormation.router.getApp(), sql);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-  }\n+    @Before\n+    public void setUp() {\n+      commonState = new CommonState();\n+      commonState.setUp(TEST_HARNESS, REST_APP_0);\n+\n+      waitForStreamsMetadataToInitialize(\n+          REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+    }\n \n-  @Test\n-  public void shouldFilterLaggyServers() throws Exception {\n-    // Given:\n-    ClusterFormation clusterFormation = findClusterFormation(TEST_APP_0, TEST_APP_1, TEST_APP_2);\n-    waitForClusterToBeDiscovered(clusterFormation.router.getApp(), 3);\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(), HighAvailabilityTestUtil.lagsReported(3));\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsReported(3, clusterFormation.standBy.getHost(),\n-            Optional.empty(), 5));\n-\n-    // Cut off standby from Kafka to simulate lag\n-    clusterFormation.standBy.getShutoffs().setKafkaPauseOffset(0);\n-    Thread.sleep(2000);\n-\n-    // Produce more data that will now only be available on active since standby is cut off\n-    TEST_HARNESS.produceRows(\n-        topic,\n-        USER_PROVIDER,\n-        FormatFactory.JSON,\n-        timestampSupplier::getAndIncrement\n-    );\n-\n-    // Make sure that the lags get reported before we kill active\n-    waitForRemoteServerToChangeStatus(clusterFormation.router.getApp(),\n-        clusterFormation.router.getHost(),\n-        HighAvailabilityTestUtil.lagsReported(3, clusterFormation.active.getHost(), Optional.empty(),\n-            10));\n-\n-    // Partition active off\n-    clusterFormation.active.getShutoffs().shutOffAll();\n-\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.standBy.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsUp);\n-    waitForRemoteServerToChangeStatus(\n-        clusterFormation.router.getApp(),\n-        clusterFormation.active.getHost(),\n-        HighAvailabilityTestUtil::remoteServerIsDown);\n-\n-    // When:\n-    final List<StreamedRow> rows_0 = makePullQueryRequest(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_6);\n-\n-    // Then:\n-    assertThat(rows_0, hasSize(HEADER + 1));\n-    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n-    assertThat(host.getHost(), is(clusterFormation.standBy.getHost().getHost()));\n-    assertThat(host.getPort(), is(clusterFormation.standBy.getHost().getPort()));\n-    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n-    // This line ensures that we've not processed the new data\n-    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n-\n-    KsqlErrorMessage errorMessage = makePullQueryRequestWithError(\n-        clusterFormation.router.getApp(), sql, LAG_FILTER_3);\n-    Assert.assertEquals(40001, errorMessage.getErrorCode());\n-    Assert.assertTrue(\n-        errorMessage.getMessage().contains(\"All nodes are dead or exceed max allowed lag.\"));\n+    @Test\n+    public void singleNewNode() throws Exception {\n+      waitForStreamsMetadataToInitialize(REST_APP_0, ImmutableList.of(host0), commonState.queryId);\n+      ClusterFormation clusterFormation = findClusterFormation(commonState.queryId, TEST_APP_0);\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 5));\n+\n+      // When:\n+      final List<StreamedRow> rows_0 = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      // Then:\n+      assertThat(rows_0, hasSize(HEADER + 1));\n+      KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+      assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Pause incoming kafka consumption\n+      APP_SHUTOFFS_0.kafkaIncoming.set(true);\n+\n+      // Produce more data\n+      TEST_HARNESS.produceRows(\n+          commonState.topic,\n+          USER_PROVIDER,\n+          FormatFactory.JSON,\n+          commonState.timestampSupplier::getAndIncrement\n+      );\n+\n+      // It should be cut off, so these new rows shouldn't be reflected, but give that some time\n+      // anyway to show it won't be reflected.\n+      Thread.sleep(2000);\n+\n+      final List<StreamedRow> sameRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = sameRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(sameRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Still haven't gotten the update yet\n+      assertThat(sameRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+      // Unpause incoming kafka consumption. We then expect active to catch back up.\n+      APP_SHUTOFFS_0.kafkaIncoming.set(false);\n+\n+      waitForRemoteServerToChangeStatus(clusterFormation.active.getApp(),\n+          clusterFormation.active.getHost(), lagsExist(1, clusterFormation.active.getHost(), 10));\n+\n+      final List<StreamedRow> updatedRows = makePullQueryRequest(\n+          clusterFormation.active.getApp(), commonState.sql,\n+          LAG_FILTER_3);\n+\n+      host = updatedRows.get(1).getSourceHost().get();\n+      assertThat(host.getHost(), is(clusterFormation.active.getHost().getHost()));\n+      assertThat(host.getPort(), is(clusterFormation.active.getHost().getPort()));\n+      assertThat(updatedRows.get(1).getRow(), is(not(Optional.empty())));\n+      // Got the update now!\n+      assertThat(updatedRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 2)));\n+    }\n   }\n \n   private static List<StreamedRow> makePullQueryRequest(\n"}}, {"oid": "5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "url": "https://github.com/confluentinc/ksql/commit/5fdf11eecf09d378a536df49c0ad4c82d4c1b515", "message": "test: Adds another test case for single active to pull query correctness tests", "committedDate": "2020-07-10T19:46:51Z", "type": "commit"}, {"oid": "e2c57b36a201f751fec0fc03891bcf5c52a1c6b2", "url": "https://github.com/confluentinc/ksql/commit/e2c57b36a201f751fec0fc03891bcf5c52a1c6b2", "message": "Working state", "committedDate": "2020-07-10T19:46:51Z", "type": "commit"}, {"oid": "092b78dea6c12e898ef93569a9d1daa29ad4a574", "url": "https://github.com/confluentinc/ksql/commit/092b78dea6c12e898ef93569a9d1daa29ad4a574", "message": "Splits test into another file", "committedDate": "2020-07-10T19:46:51Z", "type": "commit"}, {"oid": "413c9293e03d0a509ccf2b6927766800bbd32109", "url": "https://github.com/confluentinc/ksql/commit/413c9293e03d0a509ccf2b6927766800bbd32109", "message": "Feedback", "committedDate": "2020-07-10T19:46:51Z", "type": "commit"}, {"oid": "1af6cf792f4feb94d00330a3f3f4ae4eed4fb9a8", "url": "https://github.com/confluentinc/ksql/commit/1af6cf792f4feb94d00330a3f3f4ae4eed4fb9a8", "message": "Feedback", "committedDate": "2020-07-10T19:46:51Z", "type": "commit"}, {"oid": "1af6cf792f4feb94d00330a3f3f4ae4eed4fb9a8", "url": "https://github.com/confluentinc/ksql/commit/1af6cf792f4feb94d00330a3f3f4ae4eed4fb9a8", "message": "Feedback", "committedDate": "2020-07-10T19:46:51Z", "type": "forcePushed"}, {"oid": "746400f3e755d05770d849c3eea1a1019d7d7d0c", "url": "https://github.com/confluentinc/ksql/commit/746400f3e755d05770d849c3eea1a1019d7d7d0c", "message": "Adds annotation", "committedDate": "2020-07-10T21:03:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4OTczMw==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453089733", "bodyText": "Now the title makes sense :)", "author": "vpapavas", "createdAt": "2020-07-10T21:41:39Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.getOffsets;\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.waitForRemoteServerToChangeStatus;\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.waitForStreamsMetadataToInitialize;\n+import static io.confluent.ksql.util.KsqlConfig.KSQL_STREAMS_PREFIX;\n+import static org.apache.kafka.streams.StreamsConfig.CONSUMER_PREFIX;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.integration.Retry;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.rest.entity.ClusterStatusResponse;\n+import io.confluent.ksql.rest.entity.KsqlEntity;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.rest.integration.FaultyKafkaConsumer.FaultyKafkaConsumer0;\n+import io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.Shutoffs;\n+import io.confluent.ksql.rest.server.KsqlRestConfig;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.rest.server.utils.TestUtils;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.serde.SerdeOption;\n+import io.confluent.ksql.test.util.KsqlIdentifierTestUtil;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlRequestConfig;\n+import io.confluent.ksql.util.Pair;\n+import io.confluent.ksql.util.UserDataProvider;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import kafka.zookeeper.ZooKeeperClientException;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({IntegrationTest.class})\n+public class PullQuerySingleNodeFunctionalTest {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQuerySingleNodeFunctionalTest.class);\n+\n+  private static final Pattern QUERY_ID_PATTERN = Pattern.compile(\"query with ID (\\\\S+)\");\n+  private static final String USER_TOPIC = \"user_topic_\";\n+  private static final String USERS_STREAM = \"users\";\n+  private static final UserDataProvider USER_PROVIDER = new UserDataProvider();\n+  private static final int HEADER = 1;\n+  private static final int BASE_TIME = 1_000_000;\n+  private final static String KEY = Iterables.get(USER_PROVIDER.data().keySet(), 0);\n+  private final static String KEY_3 = Iterables.get(USER_PROVIDER.data().keySet(), 3);\n+  private static final Map<String, ?> LAG_FILTER_3 =\n+      ImmutableMap.of(KsqlConfig.KSQL_QUERY_PULL_MAX_ALLOWED_OFFSET_LAG_CONFIG, \"3\");\n+\n+  private static final PhysicalSchema AGGREGATE_SCHEMA = PhysicalSchema.from(\n+      LogicalSchema.builder()\n+          .keyColumn(ColumnName.of(\"USERID\"), SqlTypes.STRING)\n+          .valueColumn(ColumnName.of(\"COUNT\"), SqlTypes.BIGINT)\n+          .build(),\n+      SerdeOption.none()\n+  );\n+\n+  private static final Map<String, Object> COMMON_CONFIG = ImmutableMap.<String, Object>builder()\n+      .put(KSQL_STREAMS_PREFIX + StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_ENABLE_CONFIG, true)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_SEND_INTERVAL_MS_CONFIG, 500)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_CHECK_INTERVAL_MS_CONFIG, 1000)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_DISCOVER_CLUSTER_MS_CONFIG, 2000)\n+      .put(KsqlRestConfig.KSQL_LAG_REPORTING_ENABLE_CONFIG, true)\n+      .put(KsqlRestConfig.KSQL_LAG_REPORTING_SEND_INTERVAL_MS_CONFIG, 3000)\n+      .put(KsqlConfig.KSQL_QUERY_PULL_ENABLE_STANDBY_READS, true)\n+      .put(KsqlConfig.KSQL_STREAMS_PREFIX + \"num.standby.replicas\", 1)\n+      .put(KsqlConfig.KSQL_SHUTDOWN_TIMEOUT_MS_CONFIG, 1000)\n+      .build();\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+  private static final TemporaryFolder TMP = new TemporaryFolder();\n+  private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+  private static final KsqlHostInfoEntity host0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+  private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+\n+  @Rule\n+  public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+      .withFaultyKsqlClient(APP_SHUTOFFS_0::getKsqlOutgoing)\n+      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX + \"max.poll.records\", 1)\n+      .withProperties(COMMON_CONFIG)\n+      .build();\n+\n+  private final AtomicLong timestampSupplier = new AtomicLong(BASE_TIME);\n+  private String output;\n+  private String queryId;\n+  private String sql;\n+  private String sqlKey3;\n+  private String topic;\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain\n+      .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+      .around(TEST_HARNESS).around(TMP);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(1, TimeUnit.MINUTES)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    FaultyKafkaConsumer0.setPauseOffset(APP_SHUTOFFS_0::getKafkaPauseOffset);\n+  }\n+\n+  @After\n+  public void cleanUp() {\n+    REST_APP_0.closePersistentQueries();\n+    REST_APP_0.dropSourcesExcept();\n+    APP_SHUTOFFS_0.reset();\n+  }\n+\n+  @Before\n+  public void setUp() {\n+    //Create topic with 1 partition to control who is active and standby\n+    topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n+    TEST_HARNESS.ensureTopics(1, topic);\n+\n+    TEST_HARNESS.produceRows(\n+        topic,\n+        USER_PROVIDER,\n+        FormatFactory.JSON,\n+        timestampSupplier::getAndIncrement\n+    );\n+\n+    //Create stream\n+    makeAdminRequest(\n+        REST_APP_0,\n+        \"CREATE STREAM \" + USERS_STREAM\n+            + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n+            + \" WITH (\"\n+            + \"   kafka_topic='\" + topic + \"', \"\n+            + \"   value_format='JSON');\"\n+    );\n+    //Create table\n+    output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n+    sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n+    List<KsqlEntity> res = makeAdminRequestWithResponse(\n+        REST_APP_0,\n+        \"CREATE TABLE \" + output + \" AS\"\n+            + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n+            + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n+    );\n+    queryId = extractQueryId(res.get(0).toString());\n+    queryId = queryId.substring(0, queryId.length() - 1);\n+    waitForTableRows(TEST_HARNESS);\n+\n+    sqlKey3 = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY_3\n+        + \"';\";\n+    waitForStreamsMetadataToInitialize(\n+        REST_APP_0, ImmutableList.of(host0), queryId);\n+  }\n+\n+  @Test", "originalCommit": "746400f3e755d05770d849c3eea1a1019d7d7d0c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aa22d115045f641459a371059219899f4c7be346", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\nindex 1bee7024bd..dd6730f54d 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\n\n@@ -56,6 +56,7 @@ import java.io.IOException;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.nio.file.Paths;\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4OTk3OQ==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453089979", "bodyText": "These are common methods with the PullQueryRoutingFunctionalTest. Maybe move them to the HATestUtil class so that they are in one place?", "author": "vpapavas", "createdAt": "2020-07-10T21:42:29Z", "path": "ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java", "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.rest.integration;\n+\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.getOffsets;\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.waitForRemoteServerToChangeStatus;\n+import static io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.waitForStreamsMetadataToInitialize;\n+import static io.confluent.ksql.util.KsqlConfig.KSQL_STREAMS_PREFIX;\n+import static org.apache.kafka.streams.StreamsConfig.CONSUMER_PREFIX;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.hasSize;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import io.confluent.common.utils.IntegrationTest;\n+import io.confluent.ksql.integration.IntegrationTestHarness;\n+import io.confluent.ksql.integration.Retry;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.rest.entity.ClusterStatusResponse;\n+import io.confluent.ksql.rest.entity.KsqlEntity;\n+import io.confluent.ksql.rest.entity.KsqlHostInfoEntity;\n+import io.confluent.ksql.rest.entity.StreamedRow;\n+import io.confluent.ksql.rest.integration.FaultyKafkaConsumer.FaultyKafkaConsumer0;\n+import io.confluent.ksql.rest.integration.HighAvailabilityTestUtil.Shutoffs;\n+import io.confluent.ksql.rest.server.KsqlRestConfig;\n+import io.confluent.ksql.rest.server.TestKsqlRestApp;\n+import io.confluent.ksql.rest.server.utils.TestUtils;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.serde.SerdeOption;\n+import io.confluent.ksql.test.util.KsqlIdentifierTestUtil;\n+import io.confluent.ksql.util.KsqlConfig;\n+import io.confluent.ksql.util.KsqlRequestConfig;\n+import io.confluent.ksql.util.Pair;\n+import io.confluent.ksql.util.UserDataProvider;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import kafka.zookeeper.ZooKeeperClientException;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.RuleChain;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@Category({IntegrationTest.class})\n+public class PullQuerySingleNodeFunctionalTest {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PullQuerySingleNodeFunctionalTest.class);\n+\n+  private static final Pattern QUERY_ID_PATTERN = Pattern.compile(\"query with ID (\\\\S+)\");\n+  private static final String USER_TOPIC = \"user_topic_\";\n+  private static final String USERS_STREAM = \"users\";\n+  private static final UserDataProvider USER_PROVIDER = new UserDataProvider();\n+  private static final int HEADER = 1;\n+  private static final int BASE_TIME = 1_000_000;\n+  private final static String KEY = Iterables.get(USER_PROVIDER.data().keySet(), 0);\n+  private final static String KEY_3 = Iterables.get(USER_PROVIDER.data().keySet(), 3);\n+  private static final Map<String, ?> LAG_FILTER_3 =\n+      ImmutableMap.of(KsqlConfig.KSQL_QUERY_PULL_MAX_ALLOWED_OFFSET_LAG_CONFIG, \"3\");\n+\n+  private static final PhysicalSchema AGGREGATE_SCHEMA = PhysicalSchema.from(\n+      LogicalSchema.builder()\n+          .keyColumn(ColumnName.of(\"USERID\"), SqlTypes.STRING)\n+          .valueColumn(ColumnName.of(\"COUNT\"), SqlTypes.BIGINT)\n+          .build(),\n+      SerdeOption.none()\n+  );\n+\n+  private static final Map<String, Object> COMMON_CONFIG = ImmutableMap.<String, Object>builder()\n+      .put(KSQL_STREAMS_PREFIX + StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_ENABLE_CONFIG, true)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_SEND_INTERVAL_MS_CONFIG, 500)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_CHECK_INTERVAL_MS_CONFIG, 1000)\n+      .put(KsqlRestConfig.KSQL_HEARTBEAT_DISCOVER_CLUSTER_MS_CONFIG, 2000)\n+      .put(KsqlRestConfig.KSQL_LAG_REPORTING_ENABLE_CONFIG, true)\n+      .put(KsqlRestConfig.KSQL_LAG_REPORTING_SEND_INTERVAL_MS_CONFIG, 3000)\n+      .put(KsqlConfig.KSQL_QUERY_PULL_ENABLE_STANDBY_READS, true)\n+      .put(KsqlConfig.KSQL_STREAMS_PREFIX + \"num.standby.replicas\", 1)\n+      .put(KsqlConfig.KSQL_SHUTDOWN_TIMEOUT_MS_CONFIG, 1000)\n+      .build();\n+\n+  private static final IntegrationTestHarness TEST_HARNESS = IntegrationTestHarness.build();\n+  private static final TemporaryFolder TMP = new TemporaryFolder();\n+  private static final int INT_PORT_0 = TestUtils.findFreeLocalPort();\n+  private static final KsqlHostInfoEntity host0 = new KsqlHostInfoEntity(\"localhost\", INT_PORT_0);\n+  private static final Shutoffs APP_SHUTOFFS_0 = new Shutoffs();\n+\n+  @Rule\n+  public final TestKsqlRestApp REST_APP_0 = TestKsqlRestApp\n+      .builder(TEST_HARNESS::kafkaBootstrapServers)\n+      .withProperty(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG, getNewStateDir(TMP))\n+      .withProperty(KsqlRestConfig.LISTENERS_CONFIG, \"http://localhost:0\")\n+      .withProperty(KsqlRestConfig.INTERNAL_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+      .withProperty(KsqlRestConfig.ADVERTISED_LISTENER_CONFIG, \"http://localhost:\" + INT_PORT_0)\n+      .withFaultyKsqlClient(APP_SHUTOFFS_0::getKsqlOutgoing)\n+      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX\n+          + ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, FaultyKafkaConsumer0.class.getName())\n+      .withProperty(KSQL_STREAMS_PREFIX + CONSUMER_PREFIX + \"max.poll.records\", 1)\n+      .withProperties(COMMON_CONFIG)\n+      .build();\n+\n+  private final AtomicLong timestampSupplier = new AtomicLong(BASE_TIME);\n+  private String output;\n+  private String queryId;\n+  private String sql;\n+  private String sqlKey3;\n+  private String topic;\n+\n+  @ClassRule\n+  public static final RuleChain CHAIN = RuleChain\n+      .outerRule(Retry.of(3, ZooKeeperClientException.class, 3, TimeUnit.SECONDS))\n+      .around(TEST_HARNESS).around(TMP);\n+\n+  @Rule\n+  public final Timeout timeout = Timeout.builder()\n+      .withTimeout(1, TimeUnit.MINUTES)\n+      .withLookingForStuckThread(true)\n+      .build();\n+\n+\n+  @BeforeClass\n+  public static void setUpClass() {\n+    FaultyKafkaConsumer0.setPauseOffset(APP_SHUTOFFS_0::getKafkaPauseOffset);\n+  }\n+\n+  @After\n+  public void cleanUp() {\n+    REST_APP_0.closePersistentQueries();\n+    REST_APP_0.dropSourcesExcept();\n+    APP_SHUTOFFS_0.reset();\n+  }\n+\n+  @Before\n+  public void setUp() {\n+    //Create topic with 1 partition to control who is active and standby\n+    topic = USER_TOPIC + KsqlIdentifierTestUtil.uniqueIdentifierName();\n+    TEST_HARNESS.ensureTopics(1, topic);\n+\n+    TEST_HARNESS.produceRows(\n+        topic,\n+        USER_PROVIDER,\n+        FormatFactory.JSON,\n+        timestampSupplier::getAndIncrement\n+    );\n+\n+    //Create stream\n+    makeAdminRequest(\n+        REST_APP_0,\n+        \"CREATE STREAM \" + USERS_STREAM\n+            + \" (\" + USER_PROVIDER.ksqlSchemaString(false) + \")\"\n+            + \" WITH (\"\n+            + \"   kafka_topic='\" + topic + \"', \"\n+            + \"   value_format='JSON');\"\n+    );\n+    //Create table\n+    output = KsqlIdentifierTestUtil.uniqueIdentifierName();\n+    sql = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY + \"';\";\n+    List<KsqlEntity> res = makeAdminRequestWithResponse(\n+        REST_APP_0,\n+        \"CREATE TABLE \" + output + \" AS\"\n+            + \" SELECT \" + USER_PROVIDER.key() + \", COUNT(1) AS COUNT FROM \" + USERS_STREAM\n+            + \" GROUP BY \" + USER_PROVIDER.key() + \";\"\n+    );\n+    queryId = extractQueryId(res.get(0).toString());\n+    queryId = queryId.substring(0, queryId.length() - 1);\n+    waitForTableRows(TEST_HARNESS);\n+\n+    sqlKey3 = \"SELECT * FROM \" + output + \" WHERE USERID = '\" + KEY_3\n+        + \"';\";\n+    waitForStreamsMetadataToInitialize(\n+        REST_APP_0, ImmutableList.of(host0), queryId);\n+  }\n+\n+  @Test\n+  public void restoreAfterClearState() throws Exception {\n+    waitForStreamsMetadataToInitialize(REST_APP_0, ImmutableList.of(host0), queryId);\n+    waitForRemoteServerToChangeStatus(REST_APP_0, host0, HighAvailabilityTestUtil\n+        .lagsReported(host0, Optional.empty(), 5));\n+\n+    // When:\n+    final List<StreamedRow> rows_0 = makePullQueryRequest(\n+        REST_APP_0, sql, LAG_FILTER_3);\n+\n+    // Then:\n+    assertThat(rows_0, hasSize(HEADER + 1));\n+    KsqlHostInfoEntity host = rows_0.get(1).getSourceHost().get();\n+    assertThat(host.getHost(), is(host0.getHost()));\n+    assertThat(host.getPort(), is(host0.getPort()));\n+    assertThat(rows_0.get(1).getRow(), is(not(Optional.empty())));\n+    assertThat(rows_0.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+    // Stop the server and blow away the state\n+    LOG.info(\"Shutting down the server \" + host0.toString());\n+    REST_APP_0.stop();\n+    String stateDir = (String)REST_APP_0.getBaseConfig()\n+        .get(KSQL_STREAMS_PREFIX + StreamsConfig.STATE_DIR_CONFIG);\n+    clearDir(stateDir);\n+\n+    // Pause incoming kafka consumption\n+    APP_SHUTOFFS_0.setKafkaPauseOffset(2);\n+\n+    LOG.info(\"Restarting the server \" + host0.toString());\n+    REST_APP_0.start();\n+\n+    waitForStreamsMetadataToInitialize(REST_APP_0, ImmutableList.of(host0), queryId);\n+    waitForRemoteServerToChangeStatus(REST_APP_0, host0, HighAvailabilityTestUtil\n+        .lagsReported(host0, Optional.of(2L), 5));\n+\n+    ClusterStatusResponse clusterStatusResponse = HighAvailabilityTestUtil\n+        .sendClusterStatusRequest(REST_APP_0);\n+    Pair<Long, Long> pair = getOffsets(host0, clusterStatusResponse.getClusterStatus());\n+    assertThat(pair.left, is(2L));\n+    assertThat(pair.right, is(5L));\n+\n+    final List<StreamedRow> sameRows = makePullQueryRequest(\n+        REST_APP_0, sql, LAG_FILTER_3);\n+\n+    host = sameRows.get(1).getSourceHost().get();\n+    assertThat(host.getHost(), is(host0.getHost()));\n+    assertThat(host.getPort(), is(host0.getPort()));\n+    assertThat(sameRows.get(1).getRow(), is(not(Optional.empty())));\n+    // Still haven't gotten the update yet\n+    assertThat(sameRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY, 1)));\n+\n+    // Row not found!\n+    final List<StreamedRow> headerOnly = makePullQueryRequest(\n+        REST_APP_0, sqlKey3, LAG_FILTER_3);\n+    assertThat(headerOnly.size(), is(1));\n+\n+    // Unpause incoming kafka consumption. We then expect active to catch back up.\n+    APP_SHUTOFFS_0.setKafkaPauseOffset(-1);\n+\n+    waitForRemoteServerToChangeStatus(REST_APP_0, host0, HighAvailabilityTestUtil\n+        .lagsReported(host0, Optional.of(5L), 5));\n+\n+    clusterStatusResponse = HighAvailabilityTestUtil\n+        .sendClusterStatusRequest(REST_APP_0);\n+    pair = getOffsets(host0, clusterStatusResponse.getClusterStatus());\n+    assertThat(pair.left, is(5L));\n+    assertThat(pair.right, is(5L));\n+\n+    final List<StreamedRow> updatedRows = makePullQueryRequest(\n+        REST_APP_0, sqlKey3, LAG_FILTER_3);\n+\n+    // Now it is found!\n+    host = updatedRows.get(1).getSourceHost().get();\n+    assertThat(host.getHost(), is(host0.getHost()));\n+    assertThat(host.getPort(), is(host0.getPort()));\n+    assertThat(updatedRows.get(1).getRow(), is(not(Optional.empty())));\n+    assertThat(updatedRows.get(1).getRow().get().values(), is(ImmutableList.of(KEY_3, 1)));\n+  }\n+\n+  private static String extractQueryId(final String outputString) {\n+    final java.util.regex.Matcher matcher = QUERY_ID_PATTERN.matcher(outputString);\n+    assertThat(\"Could not find query id in: \" + outputString, matcher.find());\n+    return matcher.group(1);\n+  }\n+\n+  private static void makeAdminRequest(TestKsqlRestApp restApp, final String sql) {\n+    RestIntegrationTestUtil.makeKsqlRequest(restApp, sql, Optional.empty());\n+  }\n+\n+  private static List<KsqlEntity> makeAdminRequestWithResponse(", "originalCommit": "746400f3e755d05770d849c3eea1a1019d7d7d0c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyODY3Mg==", "url": "https://github.com/confluentinc/ksql/pull/5811#discussion_r453128672", "bodyText": "Sure, sounds good.", "author": "AlanConfluent", "createdAt": "2020-07-11T00:14:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA4OTk3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "aa22d115045f641459a371059219899f4c7be346", "chunk": "diff --git a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\nindex 1bee7024bd..dd6730f54d 100644\n--- a/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\n+++ b/ksqldb-rest-app/src/test/java/io/confluent/ksql/rest/integration/PullQuerySingleNodeFunctionalTest.java\n\n@@ -56,6 +56,7 @@ import java.io.IOException;\n import java.nio.file.Files;\n import java.nio.file.Path;\n import java.nio.file.Paths;\n+import java.util.Comparator;\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n"}}, {"oid": "aa22d115045f641459a371059219899f4c7be346", "url": "https://github.com/confluentinc/ksql/commit/aa22d115045f641459a371059219899f4c7be346", "message": "Fixes warning", "committedDate": "2020-07-10T21:42:49Z", "type": "commit"}, {"oid": "e25df51f64d3558b9f02ca0e7e9c54eae2ea463d", "url": "https://github.com/confluentinc/ksql/commit/e25df51f64d3558b9f02ca0e7e9c54eae2ea463d", "message": "More feedback", "committedDate": "2020-07-11T00:27:03Z", "type": "commit"}]}