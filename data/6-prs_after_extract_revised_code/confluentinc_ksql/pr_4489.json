{"pr_number": 4489, "pr_title": "fix: csas/ctas with timestamp column is used for output rowtime", "pr_createdAt": "2020-02-07T22:46:30Z", "pr_url": "https://github.com/confluentinc/ksql/pull/4489", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r376643236", "bodyText": "I think the logic here should write the System.currenttimeMillis() in case there is no timestamp column set, right? Otherwise, the output ROWTIME is derived from the source topics.", "author": "spena", "createdAt": "2020-02-07T22:47:47Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          if (timestampColumnIndex >= 0 && row.get(timestampColumnIndex) instanceof Long) {\n+            processorContext.forward(\n+                key,\n+                row,\n+                To.all().withTimestamp((long) row.get(timestampColumnIndex))\n+            );", "originalCommit": "a63d42f59e78d6b41f8b2046b1c697b1d6a93341", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY4NzU0OQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r376687549", "bodyText": "I don't think so. From my understanding the WITH ROWTIME clause in CTAS/CSAS statement overwrite the existing event-timestamp with something different. Hence, if no WITH ROWTIME column is specified the input key and value should be forwarded without a modified timestamp?\nIt seem atm, the transform() would drop the record if no WITH ROWTIME clause is present?", "author": "mjsax", "createdAt": "2020-02-08T04:53:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMTExNQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380821115", "bodyText": "Is this an out of date comment? It doesn't look to match the current impl...", "author": "big-andy-coates", "createdAt": "2020-02-18T17:24:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjY0MzIzNg=="}], "type": "inlineReview", "revised_code": {"commit": "95d22bfcda7cfc462a40beddd2456c6b9ac14bc8", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex 2f65a480f3..f4b1edad02 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -115,9 +115,11 @@ public final class SinkBuilder {\n                 row,\n                 To.all().withTimestamp((long) row.get(timestampColumnIndex))\n             );\n+\n+            return null;\n           }\n \n-          return null;\n+          return KeyValue.pair(key, row);\n         }\n \n \n"}}, {"oid": "95d22bfcda7cfc462a40beddd2456c6b9ac14bc8", "url": "https://github.com/confluentinc/ksql/commit/95d22bfcda7cfc462a40beddd2456c6b9ac14bc8", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-10T22:34:22Z", "type": "forcePushed"}, {"oid": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "url": "https://github.com/confluentinc/ksql/commit/2a587f17b5c22837a4bee556d86a12b4bb742abe", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-11T14:01:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjA3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380802070", "bodyText": "why would an Optional field be required?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  @JsonProperty(value = \"timestampColumn\", required = true)\n          \n          \n            \n                  @JsonProperty(value = \"timestampColumn\")", "author": "big-andy-coates", "createdAt": "2020-02-18T16:52:41Z", "path": "ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java", "diffHunk": "@@ -16,26 +16,34 @@\n \n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.errorprone.annotations.Immutable;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+\n import java.util.Collections;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Optional;\n \n @Immutable\n public class StreamSink<K> implements ExecutionStep<KStreamHolder<K>> {\n   private final ExecutionStepPropertiesV1 properties;\n   private final ExecutionStep<KStreamHolder<K>>  source;\n   private final Formats formats;\n   private final String topicName;\n+  private final Optional<TimestampColumn> timestampColumn;\n \n   public StreamSink(\n       @JsonProperty(value = \"properties\", required = true) final ExecutionStepPropertiesV1 props,\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KStreamHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n-      @JsonProperty(value = \"topicName\", required = true) final String topicName) {\n+      @JsonProperty(value = \"topicName\", required = true) final String topicName,\n+      @JsonProperty(value = \"timestampColumn\", required = true)", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMjc1Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383412756", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjA3MA=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java b/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java\nindex 43118713a4..955d60e36e 100644\n--- a/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java\n+++ b/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/StreamSink.java\n\n@@ -36,8 +36,7 @@ public class StreamSink<K> implements ExecutionStep<KStreamHolder<K>> {\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KStreamHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n       @JsonProperty(value = \"topicName\", required = true) final String topicName,\n-      @JsonProperty(value = \"timestampColumn\", required = true)\n-      final Optional<TimestampColumn> timestampColumn\n+      @JsonProperty(value = \"timestampColumn\") final Optional<TimestampColumn> timestampColumn\n   ) {\n     this.properties = Objects.requireNonNull(props, \"props\");\n     this.formats = Objects.requireNonNull(formats, \"formats\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjM3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380802370", "bodyText": "as above\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  @JsonProperty(value = \"timestampColumn\", required = true)\n          \n          \n            \n                  @JsonProperty(value = \"timestampColumn\")", "author": "big-andy-coates", "createdAt": "2020-02-18T16:53:14Z", "path": "ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java", "diffHunk": "@@ -17,27 +17,34 @@\n import com.fasterxml.jackson.annotation.JsonIgnore;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.google.errorprone.annotations.Immutable;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+\n import java.util.Collections;\n import java.util.List;\n import java.util.Objects;\n+import java.util.Optional;\n \n @Immutable\n public class TableSink<K> implements ExecutionStep<KTableHolder<K>> {\n   private final ExecutionStepPropertiesV1 properties;\n   private final ExecutionStep<KTableHolder<K>> source;\n   private final Formats formats;\n   private final String topicName;\n+  private final Optional<TimestampColumn> timestampColumn;\n \n   public TableSink(\n       @JsonProperty(value = \"properties\", required = true) final ExecutionStepPropertiesV1 props,\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KTableHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n-      @JsonProperty(value = \"topicName\", required = true) final String topicName\n+      @JsonProperty(value = \"topicName\", required = true) final String topicName,\n+      @JsonProperty(value = \"timestampColumn\", required = true)", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMjgwMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383412803", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:40:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgwMjM3MA=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java b/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java\nindex a4a52d29f2..744911c88e 100644\n--- a/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java\n+++ b/ksql-execution/src/main/java/io/confluent/ksql/execution/plan/TableSink.java\n\n@@ -37,8 +37,7 @@ public class TableSink<K> implements ExecutionStep<KTableHolder<K>> {\n       @JsonProperty(value = \"source\", required = true) final ExecutionStep<KTableHolder<K>> source,\n       @JsonProperty(value = \"formats\", required = true) final Formats formats,\n       @JsonProperty(value = \"topicName\", required = true) final String topicName,\n-      @JsonProperty(value = \"timestampColumn\", required = true)\n-      final Optional<TimestampColumn> timestampColumn\n+      @JsonProperty(value = \"timestampColumn\") final Optional<TimestampColumn> timestampColumn\n   ) {\n     this.properties = Objects.requireNonNull(props, \"props\");\n     this.source = Objects.requireNonNull(source, \"source\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMTgxNw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380811817", "bodyText": "Can we only add the transform step if there is a timestamp column specified?\n   final Optional<TransformTimestamp> tsTransformer = timestampColumn\n        .map(TimestampColumn::getColumn)\n        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n        .map(Column::index)\n        .map(TransformTimestamp::new);\n\n   final KStream<K, GenericRow> transformed = tsTransformer\n        .map(stream::transform)\n        .orElse(stream);\n\n   return stream.to(topicName, Produced.with(keySerde, valueSerde));", "author": "big-andy-coates", "createdAt": "2020-02-18T17:08:22Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzIxMA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413210", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMTgxNw=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex f4b1edad02..ae34584111 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -65,13 +65,17 @@ public final class SinkBuilder {\n         queryContext\n     );\n \n-    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampColumn\n+        .map(TimestampColumn::getColumn)\n         .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n         .map(Column::index)\n-        .orElse(-1);\n+        .map(TransformTimestamp::new);\n \n-    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n-        .to(topicName, Produced.with(keySerde, valueSerde));\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n   }\n \n   static class TransformTimestamp<K>\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMjE2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380812166", "bodyText": "Do we need equals and hashCode for this class?", "author": "big-andy-coates", "createdAt": "2020-02-18T17:09:00Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzc3MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413770", "bodyText": "Not needed anymore. I was using it for mocking tests when I used the assertThat methods to verify the same TransformTimestamp was called. I changed the approach and they're not used anymore.", "author": "spena", "createdAt": "2020-02-24T17:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxMjE2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex f4b1edad02..ae34584111 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -65,13 +65,17 @@ public final class SinkBuilder {\n         queryContext\n     );\n \n-    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampColumn\n+        .map(TimestampColumn::getColumn)\n         .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n         .map(Column::index)\n-        .orElse(-1);\n+        .map(TransformTimestamp::new);\n \n-    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n-        .to(topicName, Produced.with(keySerde, valueSerde));\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n   }\n \n   static class TransformTimestamp<K>\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxODY1NA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380818654", "bodyText": "nit: validate params that will be stored in object state; ensuring object does not get into an invalid state, i.e. throw if negative.", "author": "big-andy-coates", "createdAt": "2020-02-18T17:20:10Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzg2Ng==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413866", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxODY1NA=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex f4b1edad02..ae34584111 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -65,13 +65,17 @@ public final class SinkBuilder {\n         queryContext\n     );\n \n-    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampColumn\n+        .map(TimestampColumn::getColumn)\n         .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n         .map(Column::index)\n-        .orElse(-1);\n+        .map(TransformTimestamp::new);\n \n-    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n-        .to(topicName, Produced.with(keySerde, valueSerde));\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n   }\n \n   static class TransformTimestamp<K>\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDYzOQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380820639", "bodyText": "The timestampColumnIndex >= 0 bit can be removed if we're not introducing this step if not timestamp column is supplied.\nThe row.get(timestampColumnIndex) instanceof Long bit should be removed as prior code should already have checked the timestamp column exists and is of the right type.\nBTW, I believe CTAS/CSAS support WITH(TIMESTAMP_FORMAT) (the docs say they do), so the value may well be a string!", "author": "big-andy-coates", "createdAt": "2020-02-18T17:23:37Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+\n+import java.util.Objects;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+        .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .orElse(-1);\n+\n+    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n+        .to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    private final int timestampColumnIndex;\n+\n+    TransformTimestamp(final int timestampColumnIndex) {\n+      this.timestampColumnIndex = timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object o) {\n+      if (o == null || !(o instanceof TransformTimestamp)) {\n+        return false;\n+      }\n+\n+      final TransformTimestamp that = (TransformTimestamp)o;\n+      return timestampColumnIndex == that.timestampColumnIndex;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hashCode(timestampColumnIndex);\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          if (timestampColumnIndex >= 0 && row.get(timestampColumnIndex) instanceof Long) {", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxMzkxNA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383413914", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDYzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "ab711d526cf993b0b8102a7250ac7b96e978e706", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex f4b1edad02..ae34584111 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -65,13 +65,17 @@ public final class SinkBuilder {\n         queryContext\n     );\n \n-    final int timestampColumnIndex = timestampColumn.map(TimestampColumn::getColumn)\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampColumn\n+        .map(TimestampColumn::getColumn)\n         .map(c -> schema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n         .map(Column::index)\n-        .orElse(-1);\n+        .map(TransformTimestamp::new);\n \n-    stream.transform(new TransformTimestamp<>(timestampColumnIndex))\n-        .to(topicName, Produced.with(keySerde, valueSerde));\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n   }\n \n   static class TransformTimestamp<K>\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDI2MQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r380824261", "bodyText": "This test class and TableSinkBuilderTest are now both essentially just testing the new SinkBuilder class.\nMove the tests from these two test classes into SinkBuilderTest and remove them.", "author": "big-andy-coates", "createdAt": "2020-02-18T17:30:02Z", "path": "ksql-streams/src/test/java/io/confluent/ksql/execution/streams/StreamSinkBuilderTest.java", "diffHunk": "@@ -90,12 +92,14 @@ public void setup() {\n     when(keySerdeFactory.buildKeySerde(any(), any(), any())).thenReturn(keySerde);\n     when(queryBuilder.buildValueSerde(any(), any(), any())).thenReturn(valSerde);\n     when(source.build(any())).thenReturn(new KStreamHolder<>(kStream, SCHEMA, keySerdeFactory));\n+    doReturn(kStream).when(kStream).transform(any());", "originalCommit": "2a587f17b5c22837a4bee556d86a12b4bb742abe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQxNDAwNQ==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383414005", "bodyText": "Done", "author": "spena", "createdAt": "2020-02-24T17:42:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDI2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0e10dcb916ef02fa5eca439c1a0d69915cfccfcd", "chunk": "diff --git a/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/StreamSinkBuilderTest.java b/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/StreamSinkBuilderTest.java\ndeleted file mode 100644\nindex 321ba3bd43..0000000000\n--- a/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/StreamSinkBuilderTest.java\n+++ /dev/null\n\n@@ -1,151 +0,0 @@\n-/*\n- * Copyright 2019 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.execution.streams;\n-\n-import static org.mockito.ArgumentMatchers.any;\n-import static org.mockito.ArgumentMatchers.anyString;\n-import static org.mockito.ArgumentMatchers.eq;\n-import static org.mockito.Mockito.doReturn;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.verify;\n-import static org.mockito.Mockito.when;\n-\n-import io.confluent.ksql.GenericRow;\n-import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n-import io.confluent.ksql.execution.context.QueryContext;\n-import io.confluent.ksql.execution.plan.ExecutionStep;\n-import io.confluent.ksql.execution.plan.ExecutionStepPropertiesV1;\n-import io.confluent.ksql.execution.plan.KStreamHolder;\n-import io.confluent.ksql.execution.plan.KeySerdeFactory;\n-import io.confluent.ksql.execution.plan.PlanBuilder;\n-import io.confluent.ksql.execution.plan.StreamSink;\n-import io.confluent.ksql.name.ColumnName;\n-import io.confluent.ksql.schema.ksql.LogicalSchema;\n-import io.confluent.ksql.schema.ksql.PhysicalSchema;\n-import io.confluent.ksql.schema.ksql.types.SqlTypes;\n-import io.confluent.ksql.serde.FormatFactory;\n-import io.confluent.ksql.serde.FormatInfo;\n-import io.confluent.ksql.serde.SerdeOption;\n-import java.util.Optional;\n-import org.apache.kafka.common.serialization.Serde;\n-import org.apache.kafka.connect.data.Struct;\n-import org.apache.kafka.streams.kstream.KStream;\n-import org.apache.kafka.streams.kstream.Produced;\n-import org.apache.kafka.streams.kstream.ValueMapper;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.mockito.ArgumentCaptor;\n-import org.mockito.Captor;\n-import org.mockito.Mock;\n-import org.mockito.junit.MockitoJUnitRunner;\n-\n-@RunWith(MockitoJUnitRunner.class)\n-public class StreamSinkBuilderTest {\n-\n-  private static final String TOPIC = \"TOPIC\";\n-  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n-      .valueColumn(ColumnName.of(\"BLUE\"), SqlTypes.BIGINT)\n-      .valueColumn(ColumnName.of(\"GREEN\"), SqlTypes.STRING)\n-      .build();\n-\n-  private static final PhysicalSchema PHYSICAL_SCHEMA =\n-      PhysicalSchema.from(SCHEMA.withoutMetaAndKeyColsInValue(), SerdeOption.none());\n-  private static final FormatInfo KEY_FORMAT = FormatInfo.of(FormatFactory.KAFKA.name());\n-  private static final FormatInfo VALUE_FORMAT = FormatInfo.of(FormatFactory.JSON.name());\n-\n-  @Mock\n-  private KsqlQueryBuilder queryBuilder;\n-  @Mock\n-  private KeySerdeFactory<Struct> keySerdeFactory;\n-  @Mock\n-  private KStream<Struct, GenericRow> kStream;\n-  @Mock\n-  private ExecutionStep<KStreamHolder<Struct>> source;\n-  @Mock\n-  private Serde<Struct> keySerde;\n-  @Mock\n-  private Serde<GenericRow> valSerde;\n-  @Captor\n-  private ArgumentCaptor<ValueMapper<GenericRow, GenericRow>> mapperCaptor;\n-  @Mock\n-  private QueryContext queryContext;\n-\n-  private PlanBuilder planBuilder;\n-  private StreamSink<Struct> sink;\n-\n-  @Before\n-  public void setup() {\n-    when(keySerdeFactory.buildKeySerde(any(), any(), any())).thenReturn(keySerde);\n-    when(queryBuilder.buildValueSerde(any(), any(), any())).thenReturn(valSerde);\n-    when(source.build(any())).thenReturn(new KStreamHolder<>(kStream, SCHEMA, keySerdeFactory));\n-    doReturn(kStream).when(kStream).transform(any());\n-\n-    sink = new StreamSink<>(\n-        new ExecutionStepPropertiesV1(queryContext),\n-        source,\n-        io.confluent.ksql.execution.plan.Formats.of(KEY_FORMAT, VALUE_FORMAT, SerdeOption.none()),\n-        TOPIC,\n-        Optional.empty()\n-    );\n-    planBuilder = new KSPlanBuilder(\n-        queryBuilder,\n-        mock(SqlPredicateFactory.class),\n-        mock(AggregateParamsFactory.class),\n-        mock(StreamsFactories.class)\n-    );\n-  }\n-\n-  @Test\n-  public void shouldWriteOutStreamToCorrectTopic() {\n-    // When:\n-    sink.build(planBuilder);\n-\n-    // Then:\n-    verify(kStream).to(eq(TOPIC), any());\n-  }\n-\n-  @Test\n-  public void shouldBuildKeySerdeCorrectly() {\n-    // When:\n-    sink.build(planBuilder);\n-\n-    // Then:\n-    verify(keySerdeFactory).buildKeySerde(KEY_FORMAT, PHYSICAL_SCHEMA, queryContext);\n-  }\n-\n-  @Test\n-  public void shouldBuildValueSerdeCorrectly() {\n-    // When:\n-    sink.build(planBuilder);\n-\n-    // Then:\n-    verify(queryBuilder).buildValueSerde(\n-        VALUE_FORMAT,\n-        PHYSICAL_SCHEMA,\n-        queryContext\n-    );\n-  }\n-\n-  @Test\n-  public void shouldWriteOutStreamWithCorrectSerdes() {\n-    // When:\n-    sink.build(planBuilder);\n-\n-    // Then:\n-    verify(kStream).to(anyString(), eq(Produced.with(keySerde, valSerde)));\n-  }\n-}\n"}}, {"oid": "ab711d526cf993b0b8102a7250ac7b96e978e706", "url": "https://github.com/confluentinc/ksql/commit/ab711d526cf993b0b8102a7250ac7b96e978e706", "message": "fix: address Andy's feedback (run TopologyFileWriter)", "committedDate": "2020-02-19T19:45:45Z", "type": "forcePushed"}, {"oid": "0e10dcb916ef02fa5eca439c1a0d69915cfccfcd", "url": "https://github.com/confluentinc/ksql/commit/0e10dcb916ef02fa5eca439c1a0d69915cfccfcd", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-24T16:40:16Z", "type": "forcePushed"}, {"oid": "d4989d2cc4bddc2174b304a11a661f212077a61f", "url": "https://github.com/confluentinc/ksql/commit/d4989d2cc4bddc2174b304a11a661f212077a61f", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-24T17:38:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzOTkxMw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383439913", "bodyText": "Can we used the variant of transform that takes a Named please, so that we can give the step in the topology a more descriptive name than just KSTREAM-TRANSFORM-0003, e.g. name it Apply-timestamp or similar.\n(You'll need to re-write the topologies again once you've done this)", "author": "big-andy-coates", "createdAt": "2020-02-24T18:35:02Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex e5bb9545e4..63f96dcb92 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -19,20 +19,23 @@ import static java.util.Objects.requireNonNull;\n import io.confluent.ksql.GenericRow;\n import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.context.QueryLoggerUtil;\n import io.confluent.ksql.execution.plan.Formats;\n import io.confluent.ksql.execution.plan.KeySerdeFactory;\n-import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.KsqlTimestampExtractor;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.execution.util.EngineProcessingLogMessageFactory;\n+import io.confluent.ksql.logging.processing.ProcessingLogger;\n import io.confluent.ksql.schema.ksql.Column;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n import io.confluent.ksql.schema.ksql.PhysicalSchema;\n-import io.confluent.ksql.util.KsqlConfig;\n import java.util.Optional;\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.streams.KeyValue;\n import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Named;\n import org.apache.kafka.streams.kstream.Produced;\n import org.apache.kafka.streams.kstream.Transformer;\n import org.apache.kafka.streams.kstream.TransformerSupplier;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ0NjgxNA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383446814", "bodyText": "Remove this filter (and cast), by passing making TimestampExtractionPolicy.create return KsqlTimestampExtraction, where KsqlTimestampExtraction extends TimestampExtraction.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:48:26Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  private static  <K> Optional<TransformTimestamp<K>> timestampTransformer(\n+      final KsqlConfig ksqlConfig,\n+      final LogicalSchema sourceSchema,\n+      final Optional<TimestampColumn> timestampColumn\n+  ) {\n+    if (!timestampColumn.isPresent()) {\n+      return Optional.empty();\n+    }\n+\n+    final TimestampExtractionPolicy timestampPolicy = TimestampExtractionPolicyFactory.create(\n+        ksqlConfig,\n+        sourceSchema,\n+        timestampColumn\n+    );\n+\n+    return timestampColumn\n+        .map(TimestampColumn::getColumn)\n+        .map(c -> sourceSchema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .map(timestampPolicy::create)\n+        .filter(te -> te instanceof AbstractColumnTimestampExtractor)", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex e5bb9545e4..63f96dcb92 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -19,20 +19,23 @@ import static java.util.Objects.requireNonNull;\n import io.confluent.ksql.GenericRow;\n import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.context.QueryLoggerUtil;\n import io.confluent.ksql.execution.plan.Formats;\n import io.confluent.ksql.execution.plan.KeySerdeFactory;\n-import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.KsqlTimestampExtractor;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.execution.util.EngineProcessingLogMessageFactory;\n+import io.confluent.ksql.logging.processing.ProcessingLogger;\n import io.confluent.ksql.schema.ksql.Column;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n import io.confluent.ksql.schema.ksql.PhysicalSchema;\n-import io.confluent.ksql.util.KsqlConfig;\n import java.util.Optional;\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.streams.KeyValue;\n import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Named;\n import org.apache.kafka.streams.kstream.Produced;\n import org.apache.kafka.streams.kstream.Transformer;\n import org.apache.kafka.streams.kstream.TransformerSupplier;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ0ODQ5MA==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383448490", "bodyText": "I'd consider just making this an interface:\npublic interface KsqlTimestampExtractor implements TimestampExtractor {\n\n  long extract(GenericRow row);\n\n  default long extract(final ConsumerRecord<Object, Object> record, final long previousTimestamp) {\n       return extract((GenericRow) record.value());\n  }\n}\nThis is because the base class can't actually handle getting the column at index timetampColumnIndex out of the row for the subclasses, so the abstract class isn't adding much.\nNote:\n\nThis impl throws if the value is not a GenericRow - as it should always be a GenericRow and it should fail if its not.  Return a timestamp of 0 just doesn't make any sense.\nI've removed the common exception handling. Why? Because we'd want the same exception handling when calling extreact(row) directly, so it seems to me we still need the exception handling in the subclasses.", "author": "big-andy-coates", "createdAt": "2020-02-24T18:51:46Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/timestamp/AbstractColumnTimestampExtractor.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams.timestamp;\n+\n+import com.google.common.base.Preconditions;\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.util.KsqlException;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.streams.processor.TimestampExtractor;\n+\n+public abstract class AbstractColumnTimestampExtractor implements TimestampExtractor {\n+  protected final int timetampColumnIndex;\n+\n+  AbstractColumnTimestampExtractor(final int timestampColumnindex) {\n+    Preconditions.checkArgument(timestampColumnindex >= 0, \"timestampColumnindex must be >= 0\");\n+    this.timetampColumnIndex = timestampColumnindex;\n+  }\n+\n+  @Override\n+  public long extract(final ConsumerRecord<Object, Object> record, final long previousTimestamp) {\n+    if (record.value() instanceof GenericRow) {\n+      try {\n+        return extract((GenericRow) record.value());\n+      } catch (final Exception e) {\n+        throw new KsqlException(\"Unable to extract timestamp from record.\"\n+            + \" record=\" + record,\n+            e);\n+      }\n+    }\n+\n+    return 0;\n+  }\n+\n+  public abstract long extract(GenericRow row);\n+}", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/timestamp/AbstractColumnTimestampExtractor.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/timestamp/AbstractColumnTimestampExtractor.java\ndeleted file mode 100644\nindex 3589083780..0000000000\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/timestamp/AbstractColumnTimestampExtractor.java\n+++ /dev/null\n\n@@ -1,48 +0,0 @@\n-/*\n- * Copyright 2020 Confluent Inc.\n- *\n- * Licensed under the Confluent Community License (the \"License\"); you may not use\n- * this file except in compliance with the License.  You may obtain a copy of the\n- * License at\n- *\n- * http://www.confluent.io/confluent-community-license\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n- * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations under the License.\n- */\n-\n-package io.confluent.ksql.execution.streams.timestamp;\n-\n-import com.google.common.base.Preconditions;\n-import io.confluent.ksql.GenericRow;\n-import io.confluent.ksql.util.KsqlException;\n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n-import org.apache.kafka.streams.processor.TimestampExtractor;\n-\n-public abstract class AbstractColumnTimestampExtractor implements TimestampExtractor {\n-  protected final int timetampColumnIndex;\n-\n-  AbstractColumnTimestampExtractor(final int timestampColumnindex) {\n-    Preconditions.checkArgument(timestampColumnindex >= 0, \"timestampColumnindex must be >= 0\");\n-    this.timetampColumnIndex = timestampColumnindex;\n-  }\n-\n-  @Override\n-  public long extract(final ConsumerRecord<Object, Object> record, final long previousTimestamp) {\n-    if (record.value() instanceof GenericRow) {\n-      try {\n-        return extract((GenericRow) record.value());\n-      } catch (final Exception e) {\n-        throw new KsqlException(\"Unable to extract timestamp from record.\"\n-            + \" record=\" + record,\n-            e);\n-      }\n-    }\n-\n-    return 0;\n-  }\n-\n-  public abstract long extract(GenericRow row);\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1MDA5Mw==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383450093", "bodyText": "If this throws.... does the exception get logged?  Does it kill the streams task?\n@rodesai what do you think should be the behaviour here?  I guess it should be inline with other parts of the processing. What happens with (de)serialization errors?", "author": "big-andy-coates", "createdAt": "2020-02-24T18:54:47Z", "path": "ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n+import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.schema.ksql.Column;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.util.KsqlConfig;\n+import java.util.Optional;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.kstream.TransformerSupplier;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+\n+public final class SinkBuilder {\n+  private SinkBuilder() {\n+  }\n+\n+  public static  <K> void build(\n+      final LogicalSchema schema,\n+      final Formats formats,\n+      final Optional<TimestampColumn> timestampColumn,\n+      final String topicName,\n+      final KStream<K, GenericRow> stream,\n+      final KeySerdeFactory<K> keySerdeFactory,\n+      final QueryContext queryContext,\n+      final KsqlQueryBuilder queryBuilder\n+  ) {\n+    final PhysicalSchema physicalSchema = PhysicalSchema.from(schema, formats.getOptions());\n+\n+    final Serde<K> keySerde = keySerdeFactory.buildKeySerde(\n+        formats.getKeyFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Serde<GenericRow> valueSerde = queryBuilder.buildValueSerde(\n+        formats.getValueFormat(),\n+        physicalSchema,\n+        queryContext\n+    );\n+\n+    final Optional<TransformTimestamp<K>> tsTransformer = timestampTransformer(\n+        queryBuilder.getKsqlConfig(),\n+        schema,\n+        timestampColumn\n+    );\n+\n+    final KStream<K, GenericRow> transformed = tsTransformer\n+        .map(t -> stream.transform(t))\n+        .orElse(stream);\n+\n+    transformed.to(topicName, Produced.with(keySerde, valueSerde));\n+  }\n+\n+  private static  <K> Optional<TransformTimestamp<K>> timestampTransformer(\n+      final KsqlConfig ksqlConfig,\n+      final LogicalSchema sourceSchema,\n+      final Optional<TimestampColumn> timestampColumn\n+  ) {\n+    if (!timestampColumn.isPresent()) {\n+      return Optional.empty();\n+    }\n+\n+    final TimestampExtractionPolicy timestampPolicy = TimestampExtractionPolicyFactory.create(\n+        ksqlConfig,\n+        sourceSchema,\n+        timestampColumn\n+    );\n+\n+    return timestampColumn\n+        .map(TimestampColumn::getColumn)\n+        .map(c -> sourceSchema.findValueColumn(c).orElseThrow(IllegalStateException::new))\n+        .map(Column::index)\n+        .map(timestampPolicy::create)\n+        .filter(te -> te instanceof AbstractColumnTimestampExtractor)\n+        .map(te -> new TransformTimestamp<>((AbstractColumnTimestampExtractor)te));\n+  }\n+\n+  static class TransformTimestamp<K>\n+      implements TransformerSupplier<K, GenericRow, KeyValue<K, GenericRow>> {\n+    final AbstractColumnTimestampExtractor timestampExtractor;\n+\n+    TransformTimestamp(final AbstractColumnTimestampExtractor timestampExtractor) {\n+      this.timestampExtractor = requireNonNull(timestampExtractor, \"timestampExtractor\");\n+    }\n+\n+    @Override\n+    public Transformer<K, GenericRow, KeyValue<K, GenericRow>> get() {\n+      return new Transformer<K, GenericRow, KeyValue<K, GenericRow>>() {\n+        private ProcessorContext processorContext;\n+\n+        @Override\n+        public void init(final ProcessorContext processorContext) {\n+          this.processorContext = requireNonNull(processorContext, \"processorContext\");\n+        }\n+\n+        @Override\n+        public KeyValue<K, GenericRow> transform(final K key, final GenericRow row) {\n+          processorContext.forward(\n+              key,\n+              row,\n+              To.all().withTimestamp(timestampExtractor.extract(row))", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "chunk": "diff --git a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\nindex e5bb9545e4..63f96dcb92 100644\n--- a/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n+++ b/ksql-streams/src/main/java/io/confluent/ksql/execution/streams/SinkBuilder.java\n\n@@ -19,20 +19,23 @@ import static java.util.Objects.requireNonNull;\n import io.confluent.ksql.GenericRow;\n import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.context.QueryLoggerUtil;\n import io.confluent.ksql.execution.plan.Formats;\n import io.confluent.ksql.execution.plan.KeySerdeFactory;\n-import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.KsqlTimestampExtractor;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicy;\n import io.confluent.ksql.execution.streams.timestamp.TimestampExtractionPolicyFactory;\n import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.execution.util.EngineProcessingLogMessageFactory;\n+import io.confluent.ksql.logging.processing.ProcessingLogger;\n import io.confluent.ksql.schema.ksql.Column;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n import io.confluent.ksql.schema.ksql.PhysicalSchema;\n-import io.confluent.ksql.util.KsqlConfig;\n import java.util.Optional;\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.streams.KeyValue;\n import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Named;\n import org.apache.kafka.streams.kstream.Produced;\n import org.apache.kafka.streams.kstream.Transformer;\n import org.apache.kafka.streams.kstream.TransformerSupplier;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1MDc0Mg==", "url": "https://github.com/confluentinc/ksql/pull/4489#discussion_r383450742", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public void shouldThrowIfNegativeProcessorContext() {\n          \n          \n            \n              public void shouldThrowOnNullProcessorContext() {", "author": "big-andy-coates", "createdAt": "2020-02-24T18:56:05Z", "path": "ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java", "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License; you may not use this file\n+ * except in compliance with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.ksql.execution.streams;\n+\n+import io.confluent.ksql.GenericRow;\n+import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n+import io.confluent.ksql.execution.context.QueryContext;\n+import io.confluent.ksql.execution.plan.Formats;\n+import io.confluent.ksql.execution.plan.KeySerdeFactory;\n+import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.schema.ksql.LogicalSchema;\n+import io.confluent.ksql.schema.ksql.PhysicalSchema;\n+import io.confluent.ksql.schema.ksql.types.SqlTypes;\n+import io.confluent.ksql.serde.FormatFactory;\n+import io.confluent.ksql.serde.FormatInfo;\n+import io.confluent.ksql.serde.SerdeOption;\n+import org.apache.kafka.common.serialization.Serde;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Produced;\n+import org.apache.kafka.streams.kstream.Transformer;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.apache.kafka.streams.processor.To;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.ArgumentCaptor;\n+import org.mockito.Captor;\n+import org.mockito.InOrder;\n+import org.mockito.Mock;\n+import org.mockito.Mockito;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+import java.util.Optional;\n+\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyString;\n+import static org.mockito.ArgumentMatchers.eq;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class SinkBuilderTest {\n+  private static final String TOPIC = \"TOPIC\";\n+\n+  private static final LogicalSchema SCHEMA = LogicalSchema.builder()\n+      .valueColumn(ColumnName.of(\"BLUE\"), SqlTypes.BIGINT)\n+      .valueColumn(ColumnName.of(\"GREEN\"), SqlTypes.STRING)\n+      .build();\n+\n+  private static final FormatInfo KEY_FORMAT = FormatInfo.of(FormatFactory.KAFKA.name());\n+  private static final FormatInfo VALUE_FORMAT = FormatInfo.of(FormatFactory.JSON.name());\n+  private static final PhysicalSchema PHYSICAL_SCHEMA =\n+      PhysicalSchema.from(SCHEMA.withoutMetaAndKeyColsInValue(), SerdeOption.none());\n+\n+  @Mock\n+  private KsqlQueryBuilder queryBuilder;\n+  @Mock\n+  private KeySerdeFactory<Struct> keySerdeFactory;\n+  @Mock\n+  private KStream<Struct, GenericRow> kStream;\n+  @Mock\n+  private Serde<Struct> keySerde;\n+  @Mock\n+  private Serde<GenericRow> valSerde;\n+  @Mock\n+  private QueryContext queryContext;\n+  @Mock\n+  private GenericRow row;\n+  @Captor\n+  private ArgumentCaptor<To> toCaptor;\n+\n+  @Before\n+  public void setup() {\n+    when(keySerdeFactory.buildKeySerde(any(), any(), any())).thenReturn(keySerde);\n+    when(queryBuilder.buildValueSerde(any(), any(), any())).thenReturn(valSerde);\n+    doReturn(kStream).when(kStream).transform(any());\n+  }\n+\n+  @Test\n+  public void shouldBuildKeySerdeCorrectly() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then:\n+    verify(keySerdeFactory).buildKeySerde(KEY_FORMAT, PHYSICAL_SCHEMA, queryContext);\n+  }\n+\n+  @Test\n+  public void shouldBuildValueSerdeCorrectly() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then:\n+    verify(queryBuilder).buildValueSerde(\n+        VALUE_FORMAT,\n+        PHYSICAL_SCHEMA,\n+        queryContext\n+    );\n+  }\n+\n+  @Test\n+  public void shouldWriteOutStreamWithCorrectSerdes() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    verify(kStream).to(anyString(), eq(Produced.with(keySerde, valSerde)));\n+  }\n+\n+  @Test\n+  public void shouldWriteOutStreamToCorrectTopic() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    verify(kStream).to(eq(TOPIC), any());\n+  }\n+\n+  @Test\n+  public void shouldBuildStreamUsingTransformTimestampWhenTimestampIsSpecified() {\n+    // Given/When\n+    SinkBuilder.build(\n+        SCHEMA,\n+        Formats.of(KEY_FORMAT, VALUE_FORMAT, SerdeOption.none()),\n+        Optional.of(new TimestampColumn(ColumnName.of(\"BLUE\"), Optional.empty())),\n+        TOPIC,\n+        kStream,\n+        keySerdeFactory,\n+        queryContext,\n+        queryBuilder\n+    );\n+\n+    // Then\n+    final InOrder inOrder = Mockito.inOrder(kStream);\n+    inOrder.verify(kStream).transform(any());\n+    inOrder.verify(kStream).to(anyString(), any());\n+    inOrder.verifyNoMoreInteractions();\n+  }\n+\n+  @Test\n+  public void shouldBuildStreamWithoutTransformTimestampWhenNoTimestampIsSpecified() {\n+    // Given/When\n+    buildDefaultSinkBuilder();\n+\n+    // Then\n+    final InOrder inOrder = Mockito.inOrder(kStream);\n+    inOrder.verify(kStream).to(anyString(), any());\n+    inOrder.verifyNoMoreInteractions();\n+  }\n+\n+  @Test\n+  public void shouldTransformTimestampRow() {\n+    // Given\n+    final long timestampColumnValue = 10001;\n+    final ProcessorContext context = mock(ProcessorContext.class);\n+    final AbstractColumnTimestampExtractor timestampExtractor\n+        = mock(AbstractColumnTimestampExtractor.class);\n+    when(timestampExtractor.extract(any())).thenReturn(timestampColumnValue);\n+\n+    // When\n+    final Transformer<String, GenericRow, KeyValue<String, GenericRow>> transformer =\n+        new SinkBuilder.TransformTimestamp<String>(timestampExtractor).get();\n+    transformer.init(context);\n+    final KeyValue<String, GenericRow> kv = transformer.transform(\"key\", row);\n+\n+    // Then\n+    assertNull(kv);\n+    verify(timestampExtractor).extract(row);\n+    verify(context, Mockito.times(1))\n+        .forward(eq(\"key\"), eq(row), toCaptor.capture());\n+    assertTrue(toCaptor.getValue().equals(To.all().withTimestamp(timestampColumnValue)));\n+  }\n+\n+  @Test(expected = NullPointerException.class)\n+  public void shouldThrowIfNegativeProcessorContext() {", "originalCommit": "d4989d2cc4bddc2174b304a11a661f212077a61f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "chunk": "diff --git a/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java b/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java\nindex b521ec312f..b0d59c59d3 100644\n--- a/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java\n+++ b/ksql-streams/src/test/java/io/confluent/ksql/execution/streams/SinkBuilderTest.java\n\n@@ -19,19 +19,25 @@ import io.confluent.ksql.execution.builder.KsqlQueryBuilder;\n import io.confluent.ksql.execution.context.QueryContext;\n import io.confluent.ksql.execution.plan.Formats;\n import io.confluent.ksql.execution.plan.KeySerdeFactory;\n-import io.confluent.ksql.execution.streams.timestamp.AbstractColumnTimestampExtractor;\n+import io.confluent.ksql.execution.streams.timestamp.KsqlTimestampExtractor;\n import io.confluent.ksql.execution.timestamp.TimestampColumn;\n+import io.confluent.ksql.logging.processing.ProcessingLogContext;\n+import io.confluent.ksql.logging.processing.ProcessingLogger;\n+import io.confluent.ksql.logging.processing.ProcessingLoggerFactory;\n import io.confluent.ksql.name.ColumnName;\n+import io.confluent.ksql.query.QueryId;\n import io.confluent.ksql.schema.ksql.LogicalSchema;\n import io.confluent.ksql.schema.ksql.PhysicalSchema;\n import io.confluent.ksql.schema.ksql.types.SqlTypes;\n import io.confluent.ksql.serde.FormatFactory;\n import io.confluent.ksql.serde.FormatInfo;\n import io.confluent.ksql.serde.SerdeOption;\n+import io.confluent.ksql.util.KsqlException;\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.connect.data.Struct;\n import org.apache.kafka.streams.KeyValue;\n import org.apache.kafka.streams.kstream.KStream;\n+import org.apache.kafka.streams.kstream.Named;\n import org.apache.kafka.streams.kstream.Produced;\n import org.apache.kafka.streams.kstream.Transformer;\n import org.apache.kafka.streams.processor.ProcessorContext;\n"}}, {"oid": "6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "url": "https://github.com/confluentinc/ksql/commit/6c86eb88eabf9ab2f8a2e9e381b08c897635542b", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T15:06:45Z", "type": "forcePushed"}, {"oid": "5df1126f2a99a6e13c8b2fc22e0dea483233eaba", "url": "https://github.com/confluentinc/ksql/commit/5df1126f2a99a6e13c8b2fc22e0dea483233eaba", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T15:31:37Z", "type": "forcePushed"}, {"oid": "2eece4260a94dc68940ed5b0bd6277c5fc261630", "url": "https://github.com/confluentinc/ksql/commit/2eece4260a94dc68940ed5b0bd6277c5fc261630", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-25T23:27:34Z", "type": "forcePushed"}, {"oid": "4ca53c138161b23be67a82c0aa59818933657460", "url": "https://github.com/confluentinc/ksql/commit/4ca53c138161b23be67a82c0aa59818933657460", "message": "feat: csas/ctas with timestamp column is used for output rowtime", "committedDate": "2020-02-26T16:10:43Z", "type": "commit"}, {"oid": "89aecd6c389feeb8642c493e438079a35af23d38", "url": "https://github.com/confluentinc/ksql/commit/89aecd6c389feeb8642c493e438079a35af23d38", "message": "fix: return record when WITH TIMESTAMP is not set", "committedDate": "2020-02-26T16:10:44Z", "type": "commit"}, {"oid": "6b26d669e8643022055bab41f3a945bbe0199bb4", "url": "https://github.com/confluentinc/ksql/commit/6b26d669e8643022055bab41f3a945bbe0199bb4", "message": "fix: address Andy's feedback", "committedDate": "2020-02-26T16:10:46Z", "type": "commit"}, {"oid": "66f027ce347c3ec681d8fc4be4726a46921638ed", "url": "https://github.com/confluentinc/ksql/commit/66f027ce347c3ec681d8fc4be4726a46921638ed", "message": "fix: address Andy's feedback (run TopologyFileWriter)", "committedDate": "2020-02-26T16:10:47Z", "type": "commit"}, {"oid": "965dace44db6ae359e933071d73149b9ff9c8a5d", "url": "https://github.com/confluentinc/ksql/commit/965dace44db6ae359e933071d73149b9ff9c8a5d", "message": "fix: support string/long/any timestamp, add new QTT tests", "committedDate": "2020-02-26T16:10:48Z", "type": "commit"}, {"oid": "489cc33084014a9b8f9220683de7b069c82be108", "url": "https://github.com/confluentinc/ksql/commit/489cc33084014a9b8f9220683de7b069c82be108", "message": "fix: send transform forward errors to processing log", "committedDate": "2020-02-26T16:10:49Z", "type": "commit"}, {"oid": "5b4058c52804163cc756141d241ebc3db2164927", "url": "https://github.com/confluentinc/ksql/commit/5b4058c52804163cc756141d241ebc3db2164927", "message": "fix: convert AbstractColumnTimestampExtractor to KsqlTimestampExtractor", "committedDate": "2020-02-26T16:10:50Z", "type": "commit"}, {"oid": "aa40d7bca748268f45ea2769adeffb6ba4d8028e", "url": "https://github.com/confluentinc/ksql/commit/aa40d7bca748268f45ea2769adeffb6ba4d8028e", "message": "fix: regenerate query plans for join-with-custom-timestamp", "committedDate": "2020-02-26T16:27:57Z", "type": "commit"}, {"oid": "aa40d7bca748268f45ea2769adeffb6ba4d8028e", "url": "https://github.com/confluentinc/ksql/commit/aa40d7bca748268f45ea2769adeffb6ba4d8028e", "message": "fix: regenerate query plans for join-with-custom-timestamp", "committedDate": "2020-02-26T16:27:57Z", "type": "forcePushed"}]}