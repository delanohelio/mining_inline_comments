{"pr_number": 14634, "pr_title": "Optimize shuffle efficiency for Presto on Spark", "pr_createdAt": "2020-06-11T04:52:11Z", "pr_url": "https://github.com/prestodb/presto/pull/14634", "timeline": [{"oid": "b7ac2fbb14356fed0e148f483cc68e68e1a5eb55", "url": "https://github.com/prestodb/presto/commit/b7ac2fbb14356fed0e148f483cc68e68e1a5eb55", "message": "Add PrestoSparkMaterializedRow", "committedDate": "2020-06-12T10:35:54Z", "type": "forcePushed"}, {"oid": "cfb34d3c2ee3e20c9860b464714f56d50615ca2d", "url": "https://github.com/prestodb/presto/commit/cfb34d3c2ee3e20c9860b464714f56d50615ca2d", "message": "Add PrestoSparkMaterializedRow", "committedDate": "2020-06-12T10:59:58Z", "type": "forcePushed"}, {"oid": "53942effc737a79000776da1978623f8db24a3a7", "url": "https://github.com/prestodb/presto/commit/53942effc737a79000776da1978623f8db24a3a7", "message": "Optimize PrestoSparkOutputOperator\n\nAvoid allocating a lot of small objects", "committedDate": "2020-06-15T22:25:08Z", "type": "forcePushed"}, {"oid": "fa003ce7647ea3693882870a3979aae77c509af2", "url": "https://github.com/prestodb/presto/commit/fa003ce7647ea3693882870a3979aae77c509af2", "message": "Optimize PrestoSparkOutputOperator\n\nAvoid allocating a lot of small objects", "committedDate": "2020-06-15T22:29:54Z", "type": "forcePushed"}, {"oid": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "url": "https://github.com/prestodb/presto/commit/a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "message": "Avoid unnecessary page to row conversions\n\nConvert to row only if shuffle is needed", "committedDate": "2020-06-16T14:20:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzM0NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441023344", "bodyText": "nit: i think this class should be renamed to be something like PrestoSparkPartitioner now \ud83d\ude03", "author": "wenleix", "createdAt": "2020-06-16T17:31:36Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IntegerIdentityPartitioner.java", "diffHunk": "@@ -37,7 +37,9 @@ public int numPartitions()\n     @Override\n     public int getPartition(Object key)\n     {\n-        int partition = requireNonNull((Integer) key, \"key is null\");\n+        requireNonNull(key, \"key is null\");\n+        MutablePartitionId mutablePartitionId = (MutablePartitionId) key;", "originalCommit": "4925720e2dd6c5b42bc8726e3a292a4d599fc620", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzkzNQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441023935", "bodyText": "actually use MutablePartitionId instead of Integer make generic type parameters slightly easier to understand ;)", "author": "wenleix", "createdAt": "2020-06-16T17:32:40Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskInputs.java", "diffHunk": "@@ -27,18 +27,18 @@\n public class PrestoSparkTaskInputs\n {\n     // fragmentId -> Iterator<[partitionId, page]>\n-    private final Map<String, Iterator<Tuple2<Integer, PrestoSparkRow>>> shuffleInputs;\n+    private final Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>>> shuffleInputs;", "originalCommit": "4925720e2dd6c5b42bc8726e3a292a4d599fc620", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskInputs.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskInputs.java\nindex e4012e0412..dabd9e4a43 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskInputs.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskInputs.java\n\n@@ -27,18 +27,21 @@ import static java.util.Objects.requireNonNull;\n public class PrestoSparkTaskInputs\n {\n     // fragmentId -> Iterator<[partitionId, page]>\n-    private final Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>>> shuffleInputs;\n+    private final Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>> shuffleInputs;\n     private final Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs;\n+    private final Map<String, List<PrestoSparkSerializedPage>> inMemoryInputs;\n \n     public PrestoSparkTaskInputs(\n-            Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>>> shuffleInputs,\n-            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs)\n+            Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>> shuffleInputs,\n+            Map<String, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs,\n+            Map<String, List<PrestoSparkSerializedPage>> inMemoryInputs)\n     {\n         this.shuffleInputs = unmodifiableMap(new HashMap<>(requireNonNull(shuffleInputs, \"shuffleInputs is null\")));\n         this.broadcastInputs = unmodifiableMap(new HashMap<>(requireNonNull(broadcastInputs, \"broadcastInputs is null\")));\n+        this.inMemoryInputs = unmodifiableMap(new HashMap<>(requireNonNull(inMemoryInputs, \"inMemoryInputs is null\")));\n     }\n \n-    public Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>>> getShuffleInputs()\n+    public Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>> getShuffleInputs()\n     {\n         return shuffleInputs;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1MzQwMA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441053400", "bodyText": "At this moment you we still allocate one PrestoSparkMutableRow per row. Is this going to be addressed in later commit? (or in the future? :)", "author": "wenleix", "createdAt": "2020-06-16T18:20:55Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java", "diffHunk": "@@ -259,13 +260,19 @@ public void addInput(Page page)\n             byte[] rowBytes = output.size() == 0 ? new byte[0] : output.getUnderlyingSlice().byteArray();\n             if (shouldReplicate) {\n                 for (int i = 0; i < partitionFunction.getPartitionCount(); i++) {\n-                    appendRow(new PrestoSparkMutableRow(i, output.size(), rowBytes));\n+                    PrestoSparkMutableRow row = new PrestoSparkMutableRow();", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwNzkyMw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441107923", "bodyText": "Yeah. This  will be addressed in the next two commits.", "author": "arhimondr", "createdAt": "2020-06-16T20:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1MzQwMA=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\nsimilarity index 71%\nrename from presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java\nrename to presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\nindex 7fe3e1923d..4037c289d0 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\n\n@@ -244,53 +233,45 @@ public class PrestoSparkOutputOperator\n     public void addInput(Page page)\n     {\n         page = pagePreprocessor.apply(page);\n+\n         int positionCount = page.getPositionCount();\n+        if (positionCount == 0) {\n+            return;\n+        }\n+\n+        int partitionCount = partitionFunction.getPartitionCount();\n+\n+        if (rowBatchBuilder == null) {\n+            rowBatchBuilder = PrestoSparkRowBatch.builder(partitionCount);\n+        }\n+\n         int channelCount = page.getChannelCount();\n-        int averageRowSizeInBytes = min(toIntExact(page.getLogicalSizeInBytes() / positionCount), 10);\n         Page partitionFunctionArguments = getPartitionFunctionArguments(page);\n         for (int position = 0; position < positionCount; position++) {\n-            SliceOutput output = new DynamicSliceOutput(averageRowSizeInBytes * 2);\n+            if (rowBatchBuilder.isFull()) {\n+                outputBuffer.enqueue(rowBatchBuilder.build());\n+                rowBatchBuilder = PrestoSparkRowBatch.builder(partitionCount);\n+            }\n+\n+            SliceOutput output = rowBatchBuilder.beginRowEntry();\n             for (int channel = 0; channel < channelCount; channel++) {\n                 Block block = page.getBlock(channel);\n                 block.writePositionTo(position, output);\n             }\n-\n             boolean shouldReplicate = (replicateNullsAndAny && !hasAnyRowBeenReplicated) ||\n                     nullChannel.isPresent() && page.getBlock(nullChannel.getAsInt()).isNull(position);\n-            byte[] rowBytes = output.size() == 0 ? new byte[0] : output.getUnderlyingSlice().byteArray();\n             if (shouldReplicate) {\n-                for (int i = 0; i < partitionFunction.getPartitionCount(); i++) {\n-                    PrestoSparkMutableRow row = new PrestoSparkMutableRow();\n-                    row.setPartition(i);\n-                    row.setBuffer(ByteBuffer.wrap(rowBytes, 0, output.size()));\n-                    appendRow(row);\n-                }\n                 hasAnyRowBeenReplicated = true;\n+                rowBatchBuilder.closeEntryForReplicatedRow();\n             }\n             else {\n                 int partition = getPartition(partitionFunctionArguments, position);\n-                PrestoSparkMutableRow row = new PrestoSparkMutableRow();\n-                row.setPartition(partition);\n-                row.setBuffer(ByteBuffer.wrap(rowBytes, 0, output.size()));\n-                appendRow(row);\n+                rowBatchBuilder.closeEntryForNonReplicatedRow(partition);\n             }\n         }\n         updateMemoryContext();\n     }\n \n-    private void appendRow(PrestoSparkMutableRow row)\n-    {\n-        long rowSize = row.getRetainedSize();\n-        if (currentBatchSize + rowSize > BATCH_SIZE) {\n-            flush();\n-        }\n-        if (currentBatch == null) {\n-            currentBatch = ImmutableList.builderWithExpectedSize(EXPECTED_ROWS_COUNT_PER_BATCH);\n-        }\n-        currentBatch.add(row);\n-        currentBatchSize += rowSize;\n-    }\n-\n     private int getPartition(Page partitionFunctionArgs, int position)\n     {\n         return partitionFunction.getPartition(partitionFunctionArgs, position);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1NzI2Mg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441057262", "bodyText": "Now I understand why @sameeragarwal  once recommended to use Scala. Spark JavaRDD API is much more restricted so we often have to \"unwrap\" JavaRDD , do something on Scala RDD, and then \"re-wrap\" into a Java RDD...", "author": "wenleix", "createdAt": "2020-06-16T18:26:17Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -206,6 +208,17 @@ private PlanFragment configureOutputPartitioning(Session session, PlanFragment f\n         return fragment;\n     }\n \n+    private static JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow> partitionBy(JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow> rdd, Partitioner partitioner)\n+    {\n+        JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow> javaPairRdd = rdd.partitionBy(partitioner);\n+        ShuffledRDD<MutablePartitionId, PrestoSparkMutableRow, PrestoSparkMutableRow> shuffledRdd = (ShuffledRDD<MutablePartitionId, PrestoSparkMutableRow, PrestoSparkMutableRow>) javaPairRdd.rdd();", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwODUzOQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441108539", "bodyText": "Yeah. I also notices that transforming Scala iterators to Java iterators and back is inefficient, as every transform produces one more frame on the stack (virtual functions cannot be inlined). I'm going to address the iterators problem in the next PR.", "author": "arhimondr", "createdAt": "2020-06-16T20:01:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1NzI2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 7d451ec2e9..d3a6ad65e7 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -222,20 +225,20 @@ public class PrestoSparkRddFactory\n     private Partitioner createPartitioner(Session session, PartitioningHandle partitioning)\n     {\n         if (partitioning.equals(SINGLE_DISTRIBUTION)) {\n-            return new IntegerIdentityPartitioner(1);\n+            return new PrestoSparkPartitioner(1);\n         }\n         if (partitioning.equals(FIXED_HASH_DISTRIBUTION)) {\n             int hashPartitionCount = getHashPartitionCount(session);\n-            return new IntegerIdentityPartitioner(hashPartitionCount);\n+            return new PrestoSparkPartitioner(hashPartitionCount);\n         }\n         if (partitioning.getConnectorId().isPresent()) {\n             int connectorPartitionCount = getPartitionCount(session, partitioning);\n-            return new IntegerIdentityPartitioner(connectorPartitionCount);\n+            return new PrestoSparkPartitioner(connectorPartitionCount);\n         }\n         throw new IllegalArgumentException(format(\"Unexpected fragment partitioning %s\", partitioning));\n     }\n \n-    private JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow> createRdd(\n+    private <T extends PrestoSparkTaskOutput> JavaPairRDD<MutablePartitionId, T> createRdd(\n             JavaSparkContext sparkContext,\n             Session session,\n             PlanFragment fragment,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2OTM4OQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441069389", "bodyText": "the data byte array is not copied here.\nSo , if we have a PrestoSparkMaterializedRow r1 and convert to PrestoSparkMutableRow r2, and then do changes over r2, would that affect data in r1?", "author": "wenleix", "createdAt": "2020-06-16T18:47:45Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMaterializedRow.java", "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkMaterializedRow\n+        implements Serializable\n+{\n+    private final byte[] data;\n+\n+    public PrestoSparkMaterializedRow(byte[] data)\n+    {\n+        this.data = requireNonNull(data, \"data is null\");\n+    }\n+\n+    public byte[] getData()\n+    {\n+        return data;\n+    }\n+\n+    public PrestoSparkMutableRow toPrestoSparkMutableRow()\n+    {\n+        PrestoSparkMutableRow prestoSparkMutableRow = new PrestoSparkMutableRow();\n+        prestoSparkMutableRow.setBuffer(ByteBuffer.wrap(data));", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwODg2OQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441108869", "bodyText": "As I mentioned you can think of PrestoSparkMaterializedRow just as a transitional think. It is removed in the last commit. You don't have to spend much time reviewing this.", "author": "arhimondr", "createdAt": "2020-06-16T20:01:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2OTM4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMaterializedRow.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMaterializedRow.java\ndeleted file mode 100644\nindex 9a36add08c..0000000000\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMaterializedRow.java\n+++ /dev/null\n\n@@ -1,42 +0,0 @@\n-/*\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package com.facebook.presto.spark.classloader_interface;\n-\n-import java.io.Serializable;\n-import java.nio.ByteBuffer;\n-\n-import static java.util.Objects.requireNonNull;\n-\n-public class PrestoSparkMaterializedRow\n-        implements Serializable\n-{\n-    private final byte[] data;\n-\n-    public PrestoSparkMaterializedRow(byte[] data)\n-    {\n-        this.data = requireNonNull(data, \"data is null\");\n-    }\n-\n-    public byte[] getData()\n-    {\n-        return data;\n-    }\n-\n-    public PrestoSparkMutableRow toPrestoSparkMutableRow()\n-    {\n-        PrestoSparkMutableRow prestoSparkMutableRow = new PrestoSparkMutableRow();\n-        prestoSparkMutableRow.setBuffer(ByteBuffer.wrap(data));\n-        return prestoSparkMutableRow;\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MDA3Mg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441070072", "bodyText": "nit: PrestoSparkMutableRow , also for line 95 and 97.", "author": "wenleix", "createdAt": "2020-06-16T18:49:00Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java", "diffHunk": "@@ -13,48 +13,87 @@\n  */\n package com.facebook.presto.spark.classloader_interface;\n \n-import java.io.Serializable;\n+import com.esotericsoftware.kryo.Kryo;\n+import com.esotericsoftware.kryo.KryoSerializable;\n+import com.esotericsoftware.kryo.io.Input;\n+import com.esotericsoftware.kryo.io.Output;\n \n-import static java.util.Objects.requireNonNull;\n+import java.io.Externalizable;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Serializable\n+        implements Externalizable, KryoSerializable\n {\n-    private static final int INSTANCE_SIZE = Long.BYTES * 2 /* headers */\n-            + Integer.BYTES /* partition */\n-            + Integer.BYTES /* length */\n-            + Long.BYTES /* bytes pointer */\n-            + Long.BYTES * 2 /* bytes headers */\n-            + Integer.BYTES /* bytes length */;\n+    private int partition;\n+    private ByteBuffer buffer;\n \n-    private final int partition;\n-    private final int length;\n-    private final byte[] bytes;\n+    public int getPartition()\n+    {\n+        return partition;\n+    }\n \n-    public PrestoSparkMutableRow(int partition, int length, byte[] bytes)\n+    public void setPartition(int partition)\n     {\n         this.partition = partition;\n-        this.length = length;\n-        this.bytes = requireNonNull(bytes, \"bytes is null\");\n     }\n \n-    public int getPartition()\n+    public ByteBuffer getBuffer()\n     {\n-        return partition;\n+        return buffer;\n+    }\n+\n+    public void setBuffer(ByteBuffer buffer)\n+    {\n+        this.buffer = buffer;\n+    }\n+\n+    /**\n+     * TODO: Transitional method. Will be removed in the next commit.\n+     */\n+    public int getRetainedSize()\n+    {\n+        return Integer.SIZE + buffer.remaining();\n+    }\n+\n+    public PrestoSparkMaterializedRow toMaterializedRow()\n+    {\n+        byte[] copy = new byte[buffer.remaining()];\n+        System.arraycopy(buffer.array(), buffer.arrayOffset() + buffer.position(), copy, 0, buffer.remaining());\n+        return new PrestoSparkMaterializedRow(copy);\n+    }\n+\n+    @Override\n+    public void write(Kryo kryo, Output output)\n+    {\n+        throw serializationNotSupportedException();\n+    }\n+\n+    @Override\n+    public void read(Kryo kryo, Input input)\n+    {\n+        throw serializationNotSupportedException();\n     }\n \n-    public int getLength()\n+    @Override\n+    public void writeExternal(ObjectOutput output)\n     {\n-        return length;\n+        throw serializationNotSupportedException();\n     }\n \n-    public byte[] getBytes()\n+    @Override\n+    public void readExternal(ObjectInput input)\n     {\n-        return bytes;\n+        throw serializationNotSupportedException();\n     }\n \n-    public long getRetainedSize()\n+    private static RuntimeException serializationNotSupportedException()\n     {\n-        return INSTANCE_SIZE + bytes.length;\n+        // PrestoSparkUnsafeRow is expected to be serialized only during shuffle.", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\nindex 377df8fcac..82caf5d6ea 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n\n@@ -24,21 +24,10 @@ import java.io.ObjectOutput;\n import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Externalizable, KryoSerializable\n+        implements Externalizable, KryoSerializable, PrestoSparkTaskOutput\n {\n-    private int partition;\n     private ByteBuffer buffer;\n \n-    public int getPartition()\n-    {\n-        return partition;\n-    }\n-\n-    public void setPartition(int partition)\n-    {\n-        this.partition = partition;\n-    }\n-\n     public ByteBuffer getBuffer()\n     {\n         return buffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MDcwMA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441070700", "bodyText": "curious: in that case what's the motivation of having both PrestoSparkMutableRow and PrestoSparkMaterializedRow? :)  ? For example, can we have one type and both have customized shuffle serializer , and Kyro serialization?", "author": "wenleix", "createdAt": "2020-06-16T18:50:10Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java", "diffHunk": "@@ -13,48 +13,87 @@\n  */\n package com.facebook.presto.spark.classloader_interface;\n \n-import java.io.Serializable;\n+import com.esotericsoftware.kryo.Kryo;\n+import com.esotericsoftware.kryo.KryoSerializable;\n+import com.esotericsoftware.kryo.io.Input;\n+import com.esotericsoftware.kryo.io.Output;\n \n-import static java.util.Objects.requireNonNull;\n+import java.io.Externalizable;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Serializable\n+        implements Externalizable, KryoSerializable\n {\n-    private static final int INSTANCE_SIZE = Long.BYTES * 2 /* headers */\n-            + Integer.BYTES /* partition */\n-            + Integer.BYTES /* length */\n-            + Long.BYTES /* bytes pointer */\n-            + Long.BYTES * 2 /* bytes headers */\n-            + Integer.BYTES /* bytes length */;\n+    private int partition;\n+    private ByteBuffer buffer;\n \n-    private final int partition;\n-    private final int length;\n-    private final byte[] bytes;\n+    public int getPartition()\n+    {\n+        return partition;\n+    }\n \n-    public PrestoSparkMutableRow(int partition, int length, byte[] bytes)\n+    public void setPartition(int partition)\n     {\n         this.partition = partition;\n-        this.length = length;\n-        this.bytes = requireNonNull(bytes, \"bytes is null\");\n     }\n \n-    public int getPartition()\n+    public ByteBuffer getBuffer()\n     {\n-        return partition;\n+        return buffer;\n+    }\n+\n+    public void setBuffer(ByteBuffer buffer)\n+    {\n+        this.buffer = buffer;\n+    }\n+\n+    /**\n+     * TODO: Transitional method. Will be removed in the next commit.\n+     */\n+    public int getRetainedSize()\n+    {\n+        return Integer.SIZE + buffer.remaining();\n+    }\n+\n+    public PrestoSparkMaterializedRow toMaterializedRow()\n+    {\n+        byte[] copy = new byte[buffer.remaining()];\n+        System.arraycopy(buffer.array(), buffer.arrayOffset() + buffer.position(), copy, 0, buffer.remaining());\n+        return new PrestoSparkMaterializedRow(copy);\n+    }\n+\n+    @Override\n+    public void write(Kryo kryo, Output output)\n+    {\n+        throw serializationNotSupportedException();\n+    }\n+\n+    @Override\n+    public void read(Kryo kryo, Input input)\n+    {\n+        throw serializationNotSupportedException();\n     }\n \n-    public int getLength()\n+    @Override\n+    public void writeExternal(ObjectOutput output)\n     {\n-        return length;\n+        throw serializationNotSupportedException();\n     }\n \n-    public byte[] getBytes()\n+    @Override\n+    public void readExternal(ObjectInput input)\n     {\n-        return bytes;\n+        throw serializationNotSupportedException();\n     }\n \n-    public long getRetainedSize()\n+    private static RuntimeException serializationNotSupportedException()\n     {\n-        return INSTANCE_SIZE + bytes.length;\n+        // PrestoSparkUnsafeRow is expected to be serialized only during shuffle.\n+        // Shuffle rows are always serialized with PrestoSparkShuffleSerializer.\n+        // PrestoSparkUnsafeRow must be converted to PrestoSparkMaterializedRow before", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwOTE1MA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441109150", "bodyText": "I forgot about this comment. Let me remove it in the last commit.", "author": "arhimondr", "createdAt": "2020-06-16T20:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MDcwMA=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\nindex 377df8fcac..82caf5d6ea 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n\n@@ -24,21 +24,10 @@ import java.io.ObjectOutput;\n import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Externalizable, KryoSerializable\n+        implements Externalizable, KryoSerializable, PrestoSparkTaskOutput\n {\n-    private int partition;\n     private ByteBuffer buffer;\n \n-    public int getPartition()\n-    {\n-        return partition;\n-    }\n-\n-    public void setPartition(int partition)\n-    {\n-        this.partition = partition;\n-    }\n-\n     public ByteBuffer getBuffer()\n     {\n         return buffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MTIwMw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441071203", "bodyText": "So essentially, although PrestoSparkMutableRow claims it implements KryoSerializable interface, it just tricks the type system. As the serialization is actually done by PrestoSparkShuffleSerializer", "author": "wenleix", "createdAt": "2020-06-16T18:51:03Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java", "diffHunk": "@@ -13,48 +13,87 @@\n  */\n package com.facebook.presto.spark.classloader_interface;\n \n-import java.io.Serializable;\n+import com.esotericsoftware.kryo.Kryo;\n+import com.esotericsoftware.kryo.KryoSerializable;\n+import com.esotericsoftware.kryo.io.Input;\n+import com.esotericsoftware.kryo.io.Output;\n \n-import static java.util.Objects.requireNonNull;\n+import java.io.Externalizable;\n+import java.io.ObjectInput;\n+import java.io.ObjectOutput;\n+import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Serializable\n+        implements Externalizable, KryoSerializable\n {\n-    private static final int INSTANCE_SIZE = Long.BYTES * 2 /* headers */\n-            + Integer.BYTES /* partition */\n-            + Integer.BYTES /* length */\n-            + Long.BYTES /* bytes pointer */\n-            + Long.BYTES * 2 /* bytes headers */\n-            + Integer.BYTES /* bytes length */;\n+    private int partition;\n+    private ByteBuffer buffer;\n \n-    private final int partition;\n-    private final int length;\n-    private final byte[] bytes;\n+    public int getPartition()\n+    {\n+        return partition;\n+    }\n \n-    public PrestoSparkMutableRow(int partition, int length, byte[] bytes)\n+    public void setPartition(int partition)\n     {\n         this.partition = partition;\n-        this.length = length;\n-        this.bytes = requireNonNull(bytes, \"bytes is null\");\n     }\n \n-    public int getPartition()\n+    public ByteBuffer getBuffer()\n     {\n-        return partition;\n+        return buffer;\n+    }\n+\n+    public void setBuffer(ByteBuffer buffer)\n+    {\n+        this.buffer = buffer;\n+    }\n+\n+    /**\n+     * TODO: Transitional method. Will be removed in the next commit.\n+     */\n+    public int getRetainedSize()\n+    {\n+        return Integer.SIZE + buffer.remaining();\n+    }\n+\n+    public PrestoSparkMaterializedRow toMaterializedRow()\n+    {\n+        byte[] copy = new byte[buffer.remaining()];\n+        System.arraycopy(buffer.array(), buffer.arrayOffset() + buffer.position(), copy, 0, buffer.remaining());\n+        return new PrestoSparkMaterializedRow(copy);\n+    }\n+\n+    @Override\n+    public void write(Kryo kryo, Output output)\n+    {\n+        throw serializationNotSupportedException();\n+    }\n+\n+    @Override\n+    public void read(Kryo kryo, Input input)\n+    {\n+        throw serializationNotSupportedException();\n     }\n \n-    public int getLength()\n+    @Override\n+    public void writeExternal(ObjectOutput output)\n     {\n-        return length;\n+        throw serializationNotSupportedException();\n     }\n \n-    public byte[] getBytes()\n+    @Override\n+    public void readExternal(ObjectInput input)\n     {\n-        return bytes;\n+        throw serializationNotSupportedException();\n     }\n \n-    public long getRetainedSize()\n+    private static RuntimeException serializationNotSupportedException()\n     {\n-        return INSTANCE_SIZE + bytes.length;\n+        // PrestoSparkUnsafeRow is expected to be serialized only during shuffle.\n+        // Shuffle rows are always serialized with PrestoSparkShuffleSerializer.", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\nindex 377df8fcac..82caf5d6ea 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkMutableRow.java\n\n@@ -24,21 +24,10 @@ import java.io.ObjectOutput;\n import java.nio.ByteBuffer;\n \n public class PrestoSparkMutableRow\n-        implements Externalizable, KryoSerializable\n+        implements Externalizable, KryoSerializable, PrestoSparkTaskOutput\n {\n-    private int partition;\n     private ByteBuffer buffer;\n \n-    public int getPartition()\n-    {\n-        return partition;\n-    }\n-\n-    public void setPartition(int partition)\n-    {\n-        this.partition = partition;\n-    }\n-\n     public ByteBuffer getBuffer()\n     {\n         return buffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MTkxNw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441071917", "bodyText": "What's this default key?", "author": "wenleix", "createdAt": "2020-06-16T18:52:20Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkShuffleSerializer.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.classloader_interface;\n+\n+import org.apache.spark.serializer.DeserializationStream;\n+import org.apache.spark.serializer.SerializationStream;\n+import org.apache.spark.serializer.Serializer;\n+import org.apache.spark.serializer.SerializerInstance;\n+import scala.Tuple2;\n+import scala.collection.Iterator;\n+import scala.reflect.ClassTag;\n+\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.nio.ByteBuffer;\n+import java.util.NoSuchElementException;\n+\n+import static java.util.Objects.requireNonNull;\n+import static scala.collection.JavaConversions.asScalaIterator;\n+\n+public class PrestoSparkShuffleSerializer\n+        extends Serializer\n+        implements Serializable\n+{\n+    private static final Integer DEFAULT_KEY = -1;\n+\n+    @Override\n+    public SerializerInstance newInstance()\n+    {\n+        return new PrestoSparkShuffleSerializerInstance();\n+    }\n+\n+    @Override\n+    public boolean supportsRelocationOfSerializedObjects()\n+    {\n+        return true;\n+    }\n+\n+    public static class PrestoSparkShuffleSerializerInstance\n+            extends SerializerInstance\n+    {\n+        @Override\n+        public SerializationStream serializeStream(OutputStream outputStream)\n+        {\n+            return new PrestoSparkShuffleSerializationStream(outputStream);\n+        }\n+\n+        @Override\n+        public DeserializationStream deserializeStream(InputStream inputStream)\n+        {\n+            return new PrestoSparkShuffleDeserializationStream(inputStream);\n+        }\n+\n+        @Override\n+        public <T> ByteBuffer serialize(T tuple, ClassTag<T> classTag)\n+        {\n+            throw new UnsupportedOperationException(\"this method is never used by shuffle\");\n+        }\n+\n+        @Override\n+        public <T> T deserialize(ByteBuffer buffer, ClassTag<T> classTag)\n+        {\n+            throw new UnsupportedOperationException(\"this method is never used by shuffle\");\n+        }\n+\n+        @Override\n+        public <T> T deserialize(ByteBuffer bytes, ClassLoader loader, ClassTag<T> classTag)\n+        {\n+            throw new UnsupportedOperationException(\"this method is never used by shuffle\");\n+        }\n+    }\n+\n+    public static class PrestoSparkShuffleSerializationStream\n+            extends SerializationStream\n+    {\n+        private final DataOutputStream outputStream;\n+\n+        public PrestoSparkShuffleSerializationStream(OutputStream outputStream)\n+        {\n+            this.outputStream = new DataOutputStream(requireNonNull(outputStream, \"outputStream is null\"));\n+        }\n+\n+        @Override\n+        public <T> SerializationStream writeKey(T key, ClassTag<T> classTag)\n+        {\n+            // key is only needed to select partition\n+            return this;\n+        }\n+\n+        @Override\n+        public <T> SerializationStream writeValue(T value, ClassTag<T> classTag)\n+        {\n+            PrestoSparkMutableRow row = (PrestoSparkMutableRow) value;\n+            ByteBuffer buffer = row.getBuffer();\n+            int length = buffer.remaining();\n+            try {\n+                outputStream.writeInt(length);\n+                outputStream.write(buffer.array(), buffer.arrayOffset() + buffer.position(), length);\n+            }\n+            catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+            return this;\n+        }\n+\n+        @Override\n+        public void flush()\n+        {\n+            try {\n+                outputStream.flush();\n+            }\n+            catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+        }\n+\n+        @Override\n+        public void close()\n+        {\n+            try {\n+                outputStream.close();\n+            }\n+            catch (IOException e) {\n+                throw new UncheckedIOException(e);\n+            }\n+        }\n+\n+        @Override\n+        public <T> SerializationStream writeObject(T tuple, ClassTag<T> classTag)\n+        {\n+            throw new UnsupportedOperationException(\"this method is never used by shuffle\");\n+        }\n+\n+        @Override\n+        public <T> SerializationStream writeAll(Iterator<T> iterator, ClassTag<T> classTag)\n+        {\n+            throw new UnsupportedOperationException(\"this method is never used by shuffle\");\n+        }\n+    }\n+\n+    public static class PrestoSparkShuffleDeserializationStream\n+            extends DeserializationStream\n+    {\n+        private final DataInputStream inputStream;\n+\n+        private final MutablePartitionId mutablePartitionId = new MutablePartitionId();\n+        private final PrestoSparkMutableRow row = new PrestoSparkMutableRow();\n+        private final Tuple2<Object, Object> tuple = new Tuple2<>(mutablePartitionId, row);\n+\n+        private byte[] bytes;\n+        private ByteBuffer buffer;\n+\n+        public PrestoSparkShuffleDeserializationStream(InputStream inputStream)\n+        {\n+            this.inputStream = new DataInputStream(requireNonNull(inputStream, \"inputStream is null\"));\n+        }\n+\n+        @Override\n+        public Iterator<Tuple2<Object, Object>> asKeyValueIterator()\n+        {\n+            return asScalaIterator(new java.util.Iterator<Tuple2<Object, Object>>()\n+            {\n+                private Tuple2<Object, Object> next;\n+\n+                @Override\n+                public boolean hasNext()\n+                {\n+                    if (next == null) {\n+                        next = tryComputeNext();\n+                    }\n+                    return next != null;\n+                }\n+\n+                @Override\n+                public Tuple2<Object, Object> next()\n+                {\n+                    if (next == null) {\n+                        next = tryComputeNext();\n+                    }\n+                    if (next == null) {\n+                        throw new NoSuchElementException();\n+                    }\n+                    Tuple2<Object, Object> result = next;\n+                    next = null;\n+                    return result;\n+                }\n+\n+                private Tuple2<Object, Object> tryComputeNext()\n+                {\n+                    int length;\n+                    try {\n+                        length = inputStream.readInt();\n+                    }\n+                    catch (EOFException e) {\n+                        return null;\n+                    }\n+                    catch (IOException e) {\n+                        throw new UncheckedIOException(e);\n+                    }\n+\n+                    try {\n+                        readRowData(length);\n+                    }\n+                    catch (IOException e) {\n+                        throw new UncheckedIOException(e);\n+                    }\n+                    return tuple;\n+                }\n+            });\n+        }\n+\n+        @Override\n+        public <T> T readKey(ClassTag<T> classTag)\n+        {\n+            return (T) DEFAULT_KEY;", "originalCommit": "659f49113904133e527181bc097a18a90c40cae2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwOTQyMA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441109420", "bodyText": "readKey and readValue methods are actually not used by the shuffle. Let me remove them.", "author": "arhimondr", "createdAt": "2020-06-16T20:03:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MTkxNw=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkShuffleSerializer.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkShuffleSerializer.java\nindex b1aa4c5942..790171c678 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkShuffleSerializer.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkShuffleSerializer.java\n\n@@ -39,8 +39,6 @@ public class PrestoSparkShuffleSerializer\n         extends Serializer\n         implements Serializable\n {\n-    private static final Integer DEFAULT_KEY = -1;\n-\n     @Override\n     public SerializerInstance newInstance()\n     {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTExMDI5MA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441110290", "bodyText": "nit : why not output.slice().getInput()? (I learned this pattern from your previous commits.. )", "author": "wenleix", "createdAt": "2020-06-16T20:05:09Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkMutableRowPageInput.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.common.Page;\n+import com.facebook.presto.common.PageBuilder;\n+import com.facebook.presto.common.block.BlockBuilder;\n+import com.facebook.presto.common.type.Type;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.ImmutableList;\n+import io.airlift.slice.BasicSliceInput;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceInput;\n+import io.airlift.slice.SliceOutput;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import static com.google.common.base.Verify.verify;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkMutableRowPageInput\n+        implements PrestoSparkPageInput\n+{\n+    private static final int TARGET_SIZE = 1024 * 1024;\n+    private static final int BUFFER_SIZE = (int) (TARGET_SIZE * 1.2f);\n+    private static final int MAX_ROWS_PER_ZERO_COLUMN_PAGE = 10_000;\n+\n+    private final List<Type> types;\n+    private final Iterator<PrestoSparkMutableRow> rowsIterator;\n+\n+    public PrestoSparkMutableRowPageInput(List<Type> types, Iterator<PrestoSparkMutableRow> rowsIterator)\n+    {\n+        this.types = ImmutableList.copyOf(requireNonNull(types, \"types is null\"));\n+        this.rowsIterator = requireNonNull(rowsIterator, \"rowsIterator is null\");\n+    }\n+\n+    @Override\n+    public Page getNextPage()\n+    {\n+        // zero columns page\n+        if (types.isEmpty()) {\n+            int rowsCount = 0;\n+            synchronized (rowsIterator) {\n+                if (!rowsIterator.hasNext()) {\n+                    return null;\n+                }\n+                while (rowsIterator.hasNext() && rowsCount < MAX_ROWS_PER_ZERO_COLUMN_PAGE) {\n+                    rowsIterator.next();\n+                    rowsCount++;\n+                }\n+            }\n+            return new Page(rowsCount);\n+        }\n+\n+        SliceOutput output = new DynamicSliceOutput(BUFFER_SIZE);\n+        int rowCount = 0;\n+        synchronized (rowsIterator) {\n+            if (!rowsIterator.hasNext()) {\n+                return null;\n+            }\n+            while (rowsIterator.hasNext() && output.size() < TARGET_SIZE) {\n+                PrestoSparkMutableRow row = rowsIterator.next();\n+                ByteBuffer buffer = row.getBuffer();\n+                output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+                rowCount++;\n+            }\n+        }\n+        SliceInput sliceInput = new BasicSliceInput(output.slice());", "originalCommit": "ce30ae12c53bc950b0e243eb315371b31e7d118c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkMutableRowPageInput.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkMutableRowPageInput.java\nindex 34d0ce7336..6ab17b0798 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkMutableRowPageInput.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkMutableRowPageInput.java\n\n@@ -19,7 +19,6 @@ import com.facebook.presto.common.block.BlockBuilder;\n import com.facebook.presto.common.type.Type;\n import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n import com.google.common.collect.ImmutableList;\n-import io.airlift.slice.BasicSliceInput;\n import io.airlift.slice.DynamicSliceOutput;\n import io.airlift.slice.SliceInput;\n import io.airlift.slice.SliceOutput;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4OTY5MQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441189691", "bodyText": "A nice refactor to have named PrestoSparkPageInput class (instead of a general iterator).\nCurious why now syntonization is not required?", "author": "wenleix", "createdAt": "2020-06-16T23:04:42Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceOperator.java", "diffHunk": "@@ -70,13 +69,12 @@ public Page getOutput()\n             return null;\n         }\n \n-        synchronized (iterator) {\n-            if (!iterator.hasNext()) {\n-                finished = true;\n-                return null;\n-            }\n-            return iterator.next();\n+        Page page = pageInput.getNextPage();", "originalCommit": "ce30ae12c53bc950b0e243eb315371b31e7d118c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIwMTM2Mw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441201363", "bodyText": "Now the pageInput ensures synchronization. The PrestoSparkPageInput implementation is expected to be thread safe.", "author": "arhimondr", "createdAt": "2020-06-16T23:43:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE4OTY5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceOperator.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceOperator.java\nindex f4fce0bc27..22382f4589 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceOperator.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceOperator.java\n\n@@ -69,12 +70,13 @@ public class PrestoSparkRemoteSourceOperator\n             return null;\n         }\n \n-        Page page = pageInput.getNextPage();\n-        if (page == null) {\n-            finished = true;\n-            return null;\n+        synchronized (iterator) {\n+            if (!iterator.hasNext()) {\n+                finished = true;\n+                return null;\n+            }\n+            return iterator.next();\n         }\n-        return page;\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MjgwMg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441292802", "bodyText": "Instead of setting rowPartitions[rowCount] to a special negative value, I personally prefer to have a separate replicate boolean array. This also aligns with the null bit design in Block.", "author": "wenleix", "createdAt": "2020-06-17T05:42:31Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    {\n+        this.rowCount = rowCount;\n+        this.rowData = requireNonNull(rowData, \"rowData is null\");\n+        this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+        this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+        this.retainedSizeInBytes = INSTANCE_SIZE\n+                + sizeOf(rowData)\n+                + sizeOf(rowPartitions)\n+                + sizeOf(rowSizes);\n+    }\n+\n+    public Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> createRowTupleIterator()\n+    {\n+        return new RowTupleIterator(rowCount, rowData, rowPartitions, rowSizes);\n+    }\n+\n+    public long getRetainedSizeInBytes()\n+    {\n+        return retainedSizeInBytes;\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder()\n+    {\n+        return new PrestoSparkRowBatchBuilder(DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder(int targetSizeInBytes, int expectedRowsCount)\n+    {\n+        return new PrestoSparkRowBatchBuilder(targetSizeInBytes, expectedRowsCount);\n+    }\n+\n+    public static class PrestoSparkRowBatchBuilder\n+    {\n+        private static final int BUILDER_INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatchBuilder.class).instanceSize();\n+\n+        private final int targetSizeInBytes;\n+        private final DynamicSliceOutput sliceOutput;\n+        private int[] rowSizes;\n+        private int[] rowPartitions;\n+        private int rowCount;\n+\n+        private int currentRowOffset;\n+        private boolean openEntry;\n+\n+        public PrestoSparkRowBatchBuilder(int targetSizeInBytes, int expectedRowsCount)\n+        {\n+            this.targetSizeInBytes = targetSizeInBytes;\n+            sliceOutput = new DynamicSliceOutput((int) (targetSizeInBytes * 1.2f));\n+            rowSizes = new int[expectedRowsCount];\n+            rowPartitions = new int[expectedRowsCount];\n+        }\n+\n+        public long getRetainedSizeInBytes()\n+        {\n+            return BUILDER_INSTANCE_SIZE + sliceOutput.getRetainedSize() + sizeOf(rowSizes) + sizeOf(rowPartitions);\n+        }\n+\n+        public boolean isFull()\n+        {\n+            return sliceOutput.size() >= targetSizeInBytes;\n+        }\n+\n+        public boolean isEmpty()\n+        {\n+            return rowCount == 0;\n+        }\n+\n+        public SliceOutput beginRowEntry()\n+        {\n+            checkState(!openEntry, \"previous entry must be closed before creating a new entry\");\n+            openEntry = true;\n+            currentRowOffset = sliceOutput.size();\n+            return sliceOutput;\n+        }\n+\n+        public void closeEntry(int partition, int partitionCount, boolean replicate)\n+        {\n+            checkState(openEntry, \"entry must be opened first\");\n+            openEntry = false;\n+\n+            rowSizes = ensureCapacity(rowSizes, rowCount + 1);\n+            rowSizes[rowCount] = sliceOutput.size() - currentRowOffset;\n+\n+            rowPartitions = ensureCapacity(rowPartitions, rowCount + 1);\n+            if (replicate) {\n+                checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+                rowPartitions[rowCount] = -partitionCount;", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0MDg4MQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441640881", "bodyText": "Although it is less readable it saves us extra memory and extra allocations. I would prefer to keep it this way. It should be covered well with unit test.", "author": "arhimondr", "createdAt": "2020-06-17T15:37:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MjgwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0MzQ2NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r442243464", "bodyText": "Now with partitionCount set as a parameter we use a simple marker instead of (-partitionCount). Now it should be more readable.", "author": "arhimondr", "createdAt": "2020-06-18T13:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MjgwMg=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5Mjg2Mw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441292863", "bodyText": "Can we pass in the partitionCount when creating the PrestoSparkRowBatchBuilder? -- thus we don't need to pass partitionCount in closeEntry.", "author": "wenleix", "createdAt": "2020-06-17T05:42:44Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    {\n+        this.rowCount = rowCount;\n+        this.rowData = requireNonNull(rowData, \"rowData is null\");\n+        this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+        this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+        this.retainedSizeInBytes = INSTANCE_SIZE\n+                + sizeOf(rowData)\n+                + sizeOf(rowPartitions)\n+                + sizeOf(rowSizes);\n+    }\n+\n+    public Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> createRowTupleIterator()\n+    {\n+        return new RowTupleIterator(rowCount, rowData, rowPartitions, rowSizes);\n+    }\n+\n+    public long getRetainedSizeInBytes()\n+    {\n+        return retainedSizeInBytes;\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder()\n+    {\n+        return new PrestoSparkRowBatchBuilder(DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder(int targetSizeInBytes, int expectedRowsCount)\n+    {\n+        return new PrestoSparkRowBatchBuilder(targetSizeInBytes, expectedRowsCount);\n+    }\n+\n+    public static class PrestoSparkRowBatchBuilder\n+    {\n+        private static final int BUILDER_INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatchBuilder.class).instanceSize();\n+\n+        private final int targetSizeInBytes;\n+        private final DynamicSliceOutput sliceOutput;\n+        private int[] rowSizes;\n+        private int[] rowPartitions;\n+        private int rowCount;\n+\n+        private int currentRowOffset;\n+        private boolean openEntry;\n+\n+        public PrestoSparkRowBatchBuilder(int targetSizeInBytes, int expectedRowsCount)\n+        {\n+            this.targetSizeInBytes = targetSizeInBytes;\n+            sliceOutput = new DynamicSliceOutput((int) (targetSizeInBytes * 1.2f));\n+            rowSizes = new int[expectedRowsCount];\n+            rowPartitions = new int[expectedRowsCount];\n+        }\n+\n+        public long getRetainedSizeInBytes()\n+        {\n+            return BUILDER_INSTANCE_SIZE + sliceOutput.getRetainedSize() + sizeOf(rowSizes) + sizeOf(rowPartitions);\n+        }\n+\n+        public boolean isFull()\n+        {\n+            return sliceOutput.size() >= targetSizeInBytes;\n+        }\n+\n+        public boolean isEmpty()\n+        {\n+            return rowCount == 0;\n+        }\n+\n+        public SliceOutput beginRowEntry()\n+        {\n+            checkState(!openEntry, \"previous entry must be closed before creating a new entry\");\n+            openEntry = true;\n+            currentRowOffset = sliceOutput.size();\n+            return sliceOutput;\n+        }\n+\n+        public void closeEntry(int partition, int partitionCount, boolean replicate)", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5Mjg4NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441292884", "bodyText": "Also note partition parameter is only useful when replicate is set to be false. Another thought is seperating closeEntry for replicate and non-replicate row:\n\ncloseEntryForReplicateRow()\ncloseEntryForNonReplicateRow(int partition)\n\nFeel free to consider this a a possible future refactor.", "author": "wenleix", "createdAt": "2020-06-17T05:42:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5Mjg2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0MTQ1OQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441641459", "bodyText": "Can we pass in the partitionCount when creating the PrestoSparkRowBatchBuilder? -- thus we don't need to pass partitionCount in closeEntry.\n\nMakes sense. Let me do that.", "author": "arhimondr", "createdAt": "2020-06-17T15:37:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5Mjg2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MzI4OA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441293288", "bodyText": "When partition is negative, it also encodes the number of total partitions. Can we just pass the number of total partitions in constructor so it doesn't have to be encoded in partition? \ud83d\ude03", "author": "wenleix", "createdAt": "2020-06-17T05:44:07Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    {\n+        this.rowCount = rowCount;\n+        this.rowData = requireNonNull(rowData, \"rowData is null\");\n+        this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+        this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+        this.retainedSizeInBytes = INSTANCE_SIZE\n+                + sizeOf(rowData)\n+                + sizeOf(rowPartitions)\n+                + sizeOf(rowSizes);\n+    }\n+\n+    public Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> createRowTupleIterator()\n+    {\n+        return new RowTupleIterator(rowCount, rowData, rowPartitions, rowSizes);\n+    }\n+\n+    public long getRetainedSizeInBytes()\n+    {\n+        return retainedSizeInBytes;\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder()\n+    {\n+        return new PrestoSparkRowBatchBuilder(DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder(int targetSizeInBytes, int expectedRowsCount)\n+    {\n+        return new PrestoSparkRowBatchBuilder(targetSizeInBytes, expectedRowsCount);\n+    }\n+\n+    public static class PrestoSparkRowBatchBuilder\n+    {\n+        private static final int BUILDER_INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatchBuilder.class).instanceSize();\n+\n+        private final int targetSizeInBytes;\n+        private final DynamicSliceOutput sliceOutput;\n+        private int[] rowSizes;\n+        private int[] rowPartitions;\n+        private int rowCount;\n+\n+        private int currentRowOffset;\n+        private boolean openEntry;\n+\n+        public PrestoSparkRowBatchBuilder(int targetSizeInBytes, int expectedRowsCount)\n+        {\n+            this.targetSizeInBytes = targetSizeInBytes;\n+            sliceOutput = new DynamicSliceOutput((int) (targetSizeInBytes * 1.2f));\n+            rowSizes = new int[expectedRowsCount];\n+            rowPartitions = new int[expectedRowsCount];\n+        }\n+\n+        public long getRetainedSizeInBytes()\n+        {\n+            return BUILDER_INSTANCE_SIZE + sliceOutput.getRetainedSize() + sizeOf(rowSizes) + sizeOf(rowPartitions);\n+        }\n+\n+        public boolean isFull()\n+        {\n+            return sliceOutput.size() >= targetSizeInBytes;\n+        }\n+\n+        public boolean isEmpty()\n+        {\n+            return rowCount == 0;\n+        }\n+\n+        public SliceOutput beginRowEntry()\n+        {\n+            checkState(!openEntry, \"previous entry must be closed before creating a new entry\");\n+            openEntry = true;\n+            currentRowOffset = sliceOutput.size();\n+            return sliceOutput;\n+        }\n+\n+        public void closeEntry(int partition, int partitionCount, boolean replicate)\n+        {\n+            checkState(openEntry, \"entry must be opened first\");\n+            openEntry = false;\n+\n+            rowSizes = ensureCapacity(rowSizes, rowCount + 1);\n+            rowSizes[rowCount] = sliceOutput.size() - currentRowOffset;\n+\n+            rowPartitions = ensureCapacity(rowPartitions, rowCount + 1);\n+            if (replicate) {\n+                checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+                rowPartitions[rowCount] = -partitionCount;\n+            }\n+            else {\n+                rowPartitions[rowCount] = partition;\n+            }\n+\n+            rowCount++;\n+        }\n+\n+        private static int[] ensureCapacity(int[] array, int capacity)\n+        {\n+            if (array.length >= capacity) {\n+                return array;\n+            }\n+            return Arrays.copyOf(array, capacity * 2);\n+        }\n+\n+        public PrestoSparkRowBatch build()\n+        {\n+            checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+            return new PrestoSparkRowBatch(rowCount, sliceOutput.getUnderlyingSlice().byteArray(), rowPartitions, rowSizes);\n+        }\n+    }\n+\n+    private static class RowTupleIterator\n+            extends AbstractIterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>\n+    {\n+        private final int rowCount;\n+        private final int[] rowPartitions;\n+        private final int[] rowSizes;\n+\n+        private int replicatePartition = -1;\n+        private int currentRow;\n+        private int currentOffset;\n+        private final ByteBuffer rowData;\n+        private final MutablePartitionId mutablePartitionId;\n+        private final Tuple2<MutablePartitionId, PrestoSparkMutableRow> tuple;\n+\n+        private RowTupleIterator(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+        {\n+            this.rowCount = rowCount;\n+            this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+            this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+\n+            this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            mutablePartitionId = new MutablePartitionId();\n+            PrestoSparkMutableRow row = new PrestoSparkMutableRow();\n+            row.setBuffer(this.rowData);\n+            tuple = new Tuple2<>(mutablePartitionId, row);\n+        }\n+\n+        @Override\n+        protected Tuple2<MutablePartitionId, PrestoSparkMutableRow> computeNext()\n+        {\n+            if (currentRow >= rowCount) {\n+                return endOfData();\n+            }\n+\n+            int rowSize = rowSizes[currentRow];\n+            rowData.limit(currentOffset + rowSize);\n+            rowData.position(currentOffset);\n+\n+            int partition = rowPartitions[currentRow];\n+            if (partition < 0) {\n+                if (replicatePartition < 0) {\n+                    replicatePartition = -partition;", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0NzIzNg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441647236", "bodyText": "That makes sense, let me do that", "author": "arhimondr", "createdAt": "2020-06-17T15:46:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MzI4OA=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5MzgyOA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441293828", "bodyText": "nit: What about remainingReplicantCount? And make the default value to be 0 instead of -1", "author": "wenleix", "createdAt": "2020-06-17T05:45:44Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    {\n+        this.rowCount = rowCount;\n+        this.rowData = requireNonNull(rowData, \"rowData is null\");\n+        this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+        this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+        this.retainedSizeInBytes = INSTANCE_SIZE\n+                + sizeOf(rowData)\n+                + sizeOf(rowPartitions)\n+                + sizeOf(rowSizes);\n+    }\n+\n+    public Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> createRowTupleIterator()\n+    {\n+        return new RowTupleIterator(rowCount, rowData, rowPartitions, rowSizes);\n+    }\n+\n+    public long getRetainedSizeInBytes()\n+    {\n+        return retainedSizeInBytes;\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder()\n+    {\n+        return new PrestoSparkRowBatchBuilder(DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder(int targetSizeInBytes, int expectedRowsCount)\n+    {\n+        return new PrestoSparkRowBatchBuilder(targetSizeInBytes, expectedRowsCount);\n+    }\n+\n+    public static class PrestoSparkRowBatchBuilder\n+    {\n+        private static final int BUILDER_INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatchBuilder.class).instanceSize();\n+\n+        private final int targetSizeInBytes;\n+        private final DynamicSliceOutput sliceOutput;\n+        private int[] rowSizes;\n+        private int[] rowPartitions;\n+        private int rowCount;\n+\n+        private int currentRowOffset;\n+        private boolean openEntry;\n+\n+        public PrestoSparkRowBatchBuilder(int targetSizeInBytes, int expectedRowsCount)\n+        {\n+            this.targetSizeInBytes = targetSizeInBytes;\n+            sliceOutput = new DynamicSliceOutput((int) (targetSizeInBytes * 1.2f));\n+            rowSizes = new int[expectedRowsCount];\n+            rowPartitions = new int[expectedRowsCount];\n+        }\n+\n+        public long getRetainedSizeInBytes()\n+        {\n+            return BUILDER_INSTANCE_SIZE + sliceOutput.getRetainedSize() + sizeOf(rowSizes) + sizeOf(rowPartitions);\n+        }\n+\n+        public boolean isFull()\n+        {\n+            return sliceOutput.size() >= targetSizeInBytes;\n+        }\n+\n+        public boolean isEmpty()\n+        {\n+            return rowCount == 0;\n+        }\n+\n+        public SliceOutput beginRowEntry()\n+        {\n+            checkState(!openEntry, \"previous entry must be closed before creating a new entry\");\n+            openEntry = true;\n+            currentRowOffset = sliceOutput.size();\n+            return sliceOutput;\n+        }\n+\n+        public void closeEntry(int partition, int partitionCount, boolean replicate)\n+        {\n+            checkState(openEntry, \"entry must be opened first\");\n+            openEntry = false;\n+\n+            rowSizes = ensureCapacity(rowSizes, rowCount + 1);\n+            rowSizes[rowCount] = sliceOutput.size() - currentRowOffset;\n+\n+            rowPartitions = ensureCapacity(rowPartitions, rowCount + 1);\n+            if (replicate) {\n+                checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+                rowPartitions[rowCount] = -partitionCount;\n+            }\n+            else {\n+                rowPartitions[rowCount] = partition;\n+            }\n+\n+            rowCount++;\n+        }\n+\n+        private static int[] ensureCapacity(int[] array, int capacity)\n+        {\n+            if (array.length >= capacity) {\n+                return array;\n+            }\n+            return Arrays.copyOf(array, capacity * 2);\n+        }\n+\n+        public PrestoSparkRowBatch build()\n+        {\n+            checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+            return new PrestoSparkRowBatch(rowCount, sliceOutput.getUnderlyingSlice().byteArray(), rowPartitions, rowSizes);\n+        }\n+    }\n+\n+    private static class RowTupleIterator\n+            extends AbstractIterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>\n+    {\n+        private final int rowCount;\n+        private final int[] rowPartitions;\n+        private final int[] rowSizes;\n+\n+        private int replicatePartition = -1;", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5NDMzMQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441294331", "bodyText": "With this replicatePartition--, today the semantic is:\n\nThere will be no more replicant when replicatePartition == -1\n\nIf we don't have this replicatePartition--, does the semantic change to\n\nThere will be no more replicant when replicatePartition == 0\n\nWould this be easier to understand? :)", "author": "wenleix", "createdAt": "2020-06-17T05:47:22Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    {\n+        this.rowCount = rowCount;\n+        this.rowData = requireNonNull(rowData, \"rowData is null\");\n+        this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+        this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+        this.retainedSizeInBytes = INSTANCE_SIZE\n+                + sizeOf(rowData)\n+                + sizeOf(rowPartitions)\n+                + sizeOf(rowSizes);\n+    }\n+\n+    public Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> createRowTupleIterator()\n+    {\n+        return new RowTupleIterator(rowCount, rowData, rowPartitions, rowSizes);\n+    }\n+\n+    public long getRetainedSizeInBytes()\n+    {\n+        return retainedSizeInBytes;\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder()\n+    {\n+        return new PrestoSparkRowBatchBuilder(DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+    }\n+\n+    public static PrestoSparkRowBatchBuilder builder(int targetSizeInBytes, int expectedRowsCount)\n+    {\n+        return new PrestoSparkRowBatchBuilder(targetSizeInBytes, expectedRowsCount);\n+    }\n+\n+    public static class PrestoSparkRowBatchBuilder\n+    {\n+        private static final int BUILDER_INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatchBuilder.class).instanceSize();\n+\n+        private final int targetSizeInBytes;\n+        private final DynamicSliceOutput sliceOutput;\n+        private int[] rowSizes;\n+        private int[] rowPartitions;\n+        private int rowCount;\n+\n+        private int currentRowOffset;\n+        private boolean openEntry;\n+\n+        public PrestoSparkRowBatchBuilder(int targetSizeInBytes, int expectedRowsCount)\n+        {\n+            this.targetSizeInBytes = targetSizeInBytes;\n+            sliceOutput = new DynamicSliceOutput((int) (targetSizeInBytes * 1.2f));\n+            rowSizes = new int[expectedRowsCount];\n+            rowPartitions = new int[expectedRowsCount];\n+        }\n+\n+        public long getRetainedSizeInBytes()\n+        {\n+            return BUILDER_INSTANCE_SIZE + sliceOutput.getRetainedSize() + sizeOf(rowSizes) + sizeOf(rowPartitions);\n+        }\n+\n+        public boolean isFull()\n+        {\n+            return sliceOutput.size() >= targetSizeInBytes;\n+        }\n+\n+        public boolean isEmpty()\n+        {\n+            return rowCount == 0;\n+        }\n+\n+        public SliceOutput beginRowEntry()\n+        {\n+            checkState(!openEntry, \"previous entry must be closed before creating a new entry\");\n+            openEntry = true;\n+            currentRowOffset = sliceOutput.size();\n+            return sliceOutput;\n+        }\n+\n+        public void closeEntry(int partition, int partitionCount, boolean replicate)\n+        {\n+            checkState(openEntry, \"entry must be opened first\");\n+            openEntry = false;\n+\n+            rowSizes = ensureCapacity(rowSizes, rowCount + 1);\n+            rowSizes[rowCount] = sliceOutput.size() - currentRowOffset;\n+\n+            rowPartitions = ensureCapacity(rowPartitions, rowCount + 1);\n+            if (replicate) {\n+                checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+                rowPartitions[rowCount] = -partitionCount;\n+            }\n+            else {\n+                rowPartitions[rowCount] = partition;\n+            }\n+\n+            rowCount++;\n+        }\n+\n+        private static int[] ensureCapacity(int[] array, int capacity)\n+        {\n+            if (array.length >= capacity) {\n+                return array;\n+            }\n+            return Arrays.copyOf(array, capacity * 2);\n+        }\n+\n+        public PrestoSparkRowBatch build()\n+        {\n+            checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+            return new PrestoSparkRowBatch(rowCount, sliceOutput.getUnderlyingSlice().byteArray(), rowPartitions, rowSizes);\n+        }\n+    }\n+\n+    private static class RowTupleIterator\n+            extends AbstractIterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>\n+    {\n+        private final int rowCount;\n+        private final int[] rowPartitions;\n+        private final int[] rowSizes;\n+\n+        private int replicatePartition = -1;\n+        private int currentRow;\n+        private int currentOffset;\n+        private final ByteBuffer rowData;\n+        private final MutablePartitionId mutablePartitionId;\n+        private final Tuple2<MutablePartitionId, PrestoSparkMutableRow> tuple;\n+\n+        private RowTupleIterator(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+        {\n+            this.rowCount = rowCount;\n+            this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n+            this.rowSizes = requireNonNull(rowSizes, \"rowSizes is null\");\n+\n+            this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            mutablePartitionId = new MutablePartitionId();\n+            PrestoSparkMutableRow row = new PrestoSparkMutableRow();\n+            row.setBuffer(this.rowData);\n+            tuple = new Tuple2<>(mutablePartitionId, row);\n+        }\n+\n+        @Override\n+        protected Tuple2<MutablePartitionId, PrestoSparkMutableRow> computeNext()\n+        {\n+            if (currentRow >= rowCount) {\n+                return endOfData();\n+            }\n+\n+            int rowSize = rowSizes[currentRow];\n+            rowData.limit(currentOffset + rowSize);\n+            rowData.position(currentOffset);\n+\n+            int partition = rowPartitions[currentRow];\n+            if (partition < 0) {\n+                if (replicatePartition < 0) {\n+                    replicatePartition = -partition;\n+                    replicatePartition--;", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY1MTU0OQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441651549", "bodyText": "Yeah, I aggree. Good suggestion. Changed.", "author": "arhimondr", "createdAt": "2020-06-17T15:53:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5NDMzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5NTA4Mg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441295082", "bodyText": "nit: why change the variable name? I thought shouldReplicate is a good name? ;)", "author": "wenleix", "createdAt": "2020-06-17T05:49:44Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java", "diffHunk": "@@ -244,53 +233,44 @@ public boolean needsInput()\n     public void addInput(Page page)\n     {\n         page = pagePreprocessor.apply(page);\n+\n         int positionCount = page.getPositionCount();\n+        if (positionCount == 0) {\n+            return;\n+        }\n+\n+        if (rowBatchBuilder == null) {\n+            rowBatchBuilder = PrestoSparkRowBatch.builder();\n+        }\n+\n         int channelCount = page.getChannelCount();\n-        int averageRowSizeInBytes = min(toIntExact(page.getLogicalSizeInBytes() / positionCount), 10);\n         Page partitionFunctionArguments = getPartitionFunctionArguments(page);\n+        int partitionCount = partitionFunction.getPartitionCount();\n         for (int position = 0; position < positionCount; position++) {\n-            SliceOutput output = new DynamicSliceOutput(averageRowSizeInBytes * 2);\n+            if (rowBatchBuilder.isFull()) {\n+                rowBuffer.enqueue(rowBatchBuilder.build());\n+                rowBatchBuilder = PrestoSparkRowBatch.builder();\n+            }\n+\n+            SliceOutput output = rowBatchBuilder.beginRowEntry();\n             for (int channel = 0; channel < channelCount; channel++) {\n                 Block block = page.getBlock(channel);\n                 block.writePositionTo(position, output);\n             }\n-\n-            boolean shouldReplicate = (replicateNullsAndAny && !hasAnyRowBeenReplicated) ||\n+            boolean replicate = (replicateNullsAndAny && !hasAnyRowBeenReplicated) ||", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY1MTc3MQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441651771", "bodyText": "Probably by mistake. Let me revert it.", "author": "arhimondr", "createdAt": "2020-06-17T15:53:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5NTA4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\nsimilarity index 84%\nrename from presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java\nrename to presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\nindex 337cac2875..4037c289d0 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputOperator.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\n\n@@ -239,17 +239,18 @@ public class PrestoSparkOutputOperator\n             return;\n         }\n \n+        int partitionCount = partitionFunction.getPartitionCount();\n+\n         if (rowBatchBuilder == null) {\n-            rowBatchBuilder = PrestoSparkRowBatch.builder();\n+            rowBatchBuilder = PrestoSparkRowBatch.builder(partitionCount);\n         }\n \n         int channelCount = page.getChannelCount();\n         Page partitionFunctionArguments = getPartitionFunctionArguments(page);\n-        int partitionCount = partitionFunction.getPartitionCount();\n         for (int position = 0; position < positionCount; position++) {\n             if (rowBatchBuilder.isFull()) {\n-                rowBuffer.enqueue(rowBatchBuilder.build());\n-                rowBatchBuilder = PrestoSparkRowBatch.builder();\n+                outputBuffer.enqueue(rowBatchBuilder.build());\n+                rowBatchBuilder = PrestoSparkRowBatch.builder(partitionCount);\n             }\n \n             SliceOutput output = rowBatchBuilder.beginRowEntry();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5NjcxMw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441296713", "bodyText": "nit: should this constructor be private or package private? since it should only get constructed by the builder.", "author": "wenleix", "createdAt": "2020-06-17T05:54:44Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.facebook.presto.spark.execution;\n+\n+import com.facebook.presto.spark.classloader_interface.MutablePartitionId;\n+import com.facebook.presto.spark.classloader_interface.PrestoSparkMutableRow;\n+import com.google.common.collect.AbstractIterator;\n+import io.airlift.slice.DynamicSliceOutput;\n+import io.airlift.slice.SliceOutput;\n+import org.openjdk.jol.info.ClassLayout;\n+import scala.Tuple2;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.base.Preconditions.checkState;\n+import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.util.Objects.requireNonNull;\n+\n+public class PrestoSparkRowBatch\n+{\n+    private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n+\n+    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+\n+    private final int rowCount;\n+    private final byte[] rowData;\n+    private final int[] rowPartitions;\n+    private final int[] rowSizes;\n+    private final long retainedSizeInBytes;\n+\n+    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex f097f2aa83..5b84870e09 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -31,20 +31,24 @@ import static io.airlift.slice.SizeOf.sizeOf;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n+        implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n     private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n+    private static final int REPLICATED_ROW_PARTITION_ID = -1;\n \n+    private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowSizes;\n     private final long retainedSizeInBytes;\n \n-    public PrestoSparkRowBatch(int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n     {\n+        this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n         this.rowData = requireNonNull(rowData, \"rowData is null\");\n         this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5OTU5NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441299594", "bodyText": "This is unrelated to this PR: but maybe add some comments explain it is intended that we reduce the memory accounting eagerly when the rowBatch is polled.  But at most one single row batch will be under counted with about ~1MB so it's a good enough approximation. (as discussed in #14522 (comment))", "author": "wenleix", "createdAt": "2020-06-17T06:03:14Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBuffer.java", "diffHunk": "@@ -66,48 +62,21 @@ public void setNoMoreRows()\n         }\n     }\n \n-    public List<PrestoSparkMutableRow> get()\n+    public PrestoSparkRowBatch get()\n             throws InterruptedException\n     {\n-        BufferedRows bufferedRows = null;\n+        PrestoSparkRowBatch rowBatch = null;\n         synchronized (monitor) {\n             while (buffer.isEmpty() && !finished) {\n                 monitor.wait();\n             }\n             if (!buffer.isEmpty()) {\n-                bufferedRows = buffer.poll();\n+                rowBatch = buffer.poll();\n             }\n-            if (bufferedRows != null) {\n-                memoryManager.updateMemoryUsage(-bufferedRows.getRetainedSizeInBytes());\n+            if (rowBatch != null) {\n+                memoryManager.updateMemoryUsage(-rowBatch.getRetainedSizeInBytes());", "originalCommit": "b938ed175cfe80e8851fff6388ab1abb1a7b33a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY1MzQ5NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441653494", "bodyText": "Added a comment", "author": "arhimondr", "createdAt": "2020-06-17T15:55:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI5OTU5NA=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBuffer.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputBuffer.java\nsimilarity index 80%\nrename from presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBuffer.java\nrename to presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputBuffer.java\nindex 96a3124058..44e0547c60 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBuffer.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkOutputBuffer.java\n\n@@ -62,10 +62,10 @@ public class PrestoSparkRowBuffer\n         }\n     }\n \n-    public PrestoSparkRowBatch get()\n+    public T get()\n             throws InterruptedException\n     {\n-        PrestoSparkRowBatch rowBatch = null;\n+        T rowBatch = null;\n         synchronized (monitor) {\n             while (buffer.isEmpty() && !finished) {\n                 monitor.wait();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMDg1Mw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441300853", "bodyText": "I prefer the old variable name (shuffleInputs and broadcastInputs). Otherwise, it's not immediately clear why we need two type of inputs.", "author": "wenleix", "createdAt": "2020-06-17T06:07:08Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java", "diffHunk": "@@ -36,38 +36,38 @@\n         implements RemoteSourceFactory\n {\n     private final PagesSerde pagesSerde;\n-    private final Map<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs;\n-    private final Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs;", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2MTUxOQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441661519", "bodyText": "I had to add third input type, localInputs. This is needed to provide inputs for the fragment that is running on the driver (COORDINATOR_ONLY fragment). Now broadcastInputs is a superset of both, local and broadcast inputs. That's why I decided to make this rename, as currently there's no 1-1 mapping.", "author": "arhimondr", "createdAt": "2020-06-17T16:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMDg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4MDIzMA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441980230", "bodyText": "@arhimondr : What about remoteBroadcastInputs? \ud83d\ude03\nAlso I would still argue, even with slightly confusion, shuffleInputs and broadcastInputs are better than rowInputs and pageInputs. rowInputs and pageInputs doesn't contain any extra information -- the fact that the input type is Page/Row can be read from its type.", "author": "wenleix", "createdAt": "2020-06-18T05:42:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMDg1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0NTcyOA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r442245728", "bodyText": "It feels like at the RemoteSourceFactory level of abstraction it doesn't matter where the inputs come from. What matters is the type of the input. Whether it supplies rows or pages. In theory, in the future, shuffle input could also supply pages (for example if the downstream stage is single node and all rows are written to the same partition there's no need to convert pages to rows).", "author": "arhimondr", "createdAt": "2020-06-18T13:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMDg1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java\nindex 43e2e2e49d..cd99428eb3 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRemoteSourceFactory.java\n\n@@ -29,45 +29,48 @@ import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n \n+import static com.facebook.presto.spark.util.PrestoSparkUtils.toSerializedPage;\n+import static com.facebook.presto.spark.util.PrestoSparkUtils.transformRowsToPages;\n import static com.google.common.base.Preconditions.checkArgument;\n+import static com.google.common.collect.Iterators.transform;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRemoteSourceFactory\n         implements RemoteSourceFactory\n {\n     private final PagesSerde pagesSerde;\n-    private final Map<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs;\n-    private final Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs;\n+    private final Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs;\n \n     public PrestoSparkRemoteSourceFactory(\n             PagesSerde pagesSerde,\n-            Map<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs,\n-            Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs)\n+            Map<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs,\n+            Map<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs)\n     {\n         this.pagesSerde = requireNonNull(pagesSerde, \"pagesSerde is null\");\n-        this.rowInputs = ImmutableMap.copyOf(requireNonNull(rowInputs, \"rowInputs is null\"));\n-        this.pageInputs = ImmutableMap.copyOf(requireNonNull(pageInputs, \"pageInputs is null\"));\n+        this.shuffleInputs = ImmutableMap.copyOf(requireNonNull(shuffleInputs, \"shuffleInputs is null\"));\n+        this.broadcastInputs = ImmutableMap.copyOf(requireNonNull(broadcastInputs, \"broadcastInputs is null\"));\n     }\n \n     @Override\n     public OperatorFactory createRemoteSource(Session session, int operatorId, PlanNodeId planNodeId, List<Type> types)\n     {\n-        Iterator<PrestoSparkMutableRow> rowInput = rowInputs.get(planNodeId);\n-        Iterator<PrestoSparkSerializedPage> pageInput = pageInputs.get(planNodeId);\n-        checkArgument(rowInput != null || pageInput != null, \"input not found for plan node with id %s\", planNodeId);\n-        checkArgument(rowInput == null || pageInput == null, \"single remote source cannot accept both, row and page inputs\");\n+        Iterator<PrestoSparkRow> shuffleInput = shuffleInputs.get(planNodeId);\n+        Iterator<PrestoSparkSerializedPage> broadcastInput = broadcastInputs.get(planNodeId);\n+        checkArgument(shuffleInput != null || broadcastInput != null, \"input not found for plan node with id %s\", planNodeId);\n+        checkArgument(shuffleInput == null || broadcastInput == null, \"single remote source cannot accept both, broadcast and shuffle inputs\");\n \n-        if (pageInput != null) {\n+        if (broadcastInput != null) {\n             return new SparkRemoteSourceOperatorFactory(\n                     operatorId,\n                     planNodeId,\n-                    new PrestoSparkSerializedPageInput(pagesSerde, pageInput));\n+                    transform(broadcastInput, sparkSerializedPage -> pagesSerde.deserialize(toSerializedPage(sparkSerializedPage))));\n         }\n \n         return new SparkRemoteSourceOperatorFactory(\n                 operatorId,\n                 planNodeId,\n-                new PrestoSparkMutableRowPageInput(types, rowInput));\n+                transformRowsToPages(shuffleInput, types));\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzA1Nw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441303057", "bodyText": "ditto", "author": "wenleix", "createdAt": "2020-06-17T06:13:19Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -276,59 +281,74 @@ public IPrestoSparkTaskExecutor doCreate(\n                 allocationTrackingEnabled,\n                 false);\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n-\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\nindex 574e21d152..b97fd6749f 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n\n@@ -289,17 +290,17 @@ public class PrestoSparkTaskExecutorFactory\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                List<PrestoSparkSerializedPage> localInput = inputs.getLocalInputs().get(sourceFragmentId.toString());\n+                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n \n                 if (shuffleInput != null) {\n                     checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n                     continue;\n                 }\n \n                 if (broadcastInput != null) {\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzI3MQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441303271", "bodyText": "what's local input? Input from \"VALUES\" node? -- maybe add a comment to it.", "author": "wenleix", "createdAt": "2020-06-17T06:13:57Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -276,59 +281,74 @@ public IPrestoSparkTaskExecutor doCreate(\n                 allocationTrackingEnabled,\n                 false);\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n-\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n-                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n+                List<PrestoSparkSerializedPage> localInput = inputs.getLocalInputs().get(sourceFragmentId.toString());", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2MjI0Nw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441662247", "bodyText": "Those are \"inmemory\" inputs. For the COORDINATOR_ONLY fragment we first collect the inputs on the Driver, and provide them to the fragment.", "author": "arhimondr", "createdAt": "2020-06-17T16:09:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4NDM2NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441984364", "bodyText": "@arhimondr : Add comment about this \"localInputs\" some where then.", "author": "wenleix", "createdAt": "2020-06-18T05:56:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0NzUzOQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r442247539", "bodyText": "Going to add a comment in PrestoSparkTaskInputs", "author": "arhimondr", "createdAt": "2020-06-18T13:58:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzI3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\nindex 574e21d152..b97fd6749f 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n\n@@ -289,17 +290,17 @@ public class PrestoSparkTaskExecutorFactory\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                List<PrestoSparkSerializedPage> localInput = inputs.getLocalInputs().get(sourceFragmentId.toString());\n+                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n \n                 if (shuffleInput != null) {\n                     checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n                     continue;\n                 }\n \n                 if (broadcastInput != null) {\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzc3OA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441303778", "bodyText": "Any reason why this block of code is now moved after the for-loop?", "author": "wenleix", "createdAt": "2020-06-17T06:15:21Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -276,59 +281,74 @@ public IPrestoSparkTaskExecutor doCreate(\n                 allocationTrackingEnabled,\n                 false);\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n-\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n-                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n+                List<PrestoSparkSerializedPage> localInput = inputs.getLocalInputs().get(sourceFragmentId.toString());\n+\n                 if (shuffleInput != null) {\n-                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    continue;\n                 }\n+\n                 if (broadcastInput != null) {\n+                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n-                    // broadcastRemoteSourceInputs.add(getNullifyingIterator(broadcastInput.value()));\n-                    broadcastRemoteSourceInputs.add(broadcastInput.value().iterator());\n+                    // remoteSourcePageInputs.add(getNullifyingIterator(broadcastInput.value()));\n+                    remoteSourcePageInputs.add(broadcastInput.value().iterator());\n+                    continue;\n                 }\n+\n+                if (localInput != null) {\n+                    remoteSourcePageInputs.add(localInput.iterator());\n+                    continue;\n+                }\n+\n+                throw new IllegalArgumentException(\"Input not found for sourceFragmentId: \" + sourceFragmentId);\n             }\n-            if (!shuffleRemoteSourceInputs.isEmpty()) {\n-                shuffleInputs.put(remoteSource.getId(), Iterators.concat(shuffleRemoteSourceInputs.iterator()));\n+            if (!remoteSourceRowInputs.isEmpty()) {\n+                rowInputs.put(remoteSource.getId(), Iterators.concat(remoteSourceRowInputs.iterator()));\n             }\n-            if (!broadcastRemoteSourceInputs.isEmpty()) {\n-                broadcastInputs.put(remoteSource.getId(), Iterators.concat(broadcastRemoteSourceInputs.iterator()));\n+            if (!remoteSourcePageInputs.isEmpty()) {\n+                pageInputs.put(remoteSource.getId(), Iterators.concat(remoteSourcePageInputs.iterator()));\n             }\n         }\n \n+        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2MzYwMw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441663603", "bodyText": "I don't know why it was before the loop. It is not needed in the loop. It is usually keep the variables that are of the same scope closer together, as it is more readable.", "author": "arhimondr", "createdAt": "2020-06-17T16:11:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwMzc3OA=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\nindex 574e21d152..b97fd6749f 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n\n@@ -289,17 +290,17 @@ public class PrestoSparkTaskExecutorFactory\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                List<PrestoSparkSerializedPage> localInput = inputs.getLocalInputs().get(sourceFragmentId.toString());\n+                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n \n                 if (shuffleInput != null) {\n                     checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n                     continue;\n                 }\n \n                 if (broadcastInput != null) {\n-                    checkArgument(localInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNDgwNg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441304806", "bodyText": "nit: why not toImmutableList()?", "author": "wenleix", "createdAt": "2020-06-17T06:18:08Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -443,76 +437,68 @@ private PrestoSparkQueryExecution(\n                         tableWriteInfo);\n                 SerializedPrestoSparkTaskDescriptor serializedTaskDescriptor = new SerializedPrestoSparkTaskDescriptor(sparkTaskDescriptorJsonCodec.toJsonBytes(taskDescriptor));\n \n-                Map<PlanFragmentId, RddAndMore> inputRdds = root.getChildren().stream()\n-                        .collect(toImmutableMap(child -> child.getFragment().getId(), this::createRdd));\n+                Map<PlanFragmentId, RddAndMore<PrestoSparkSerializedPage>> inputRdds = root.getChildren().stream()\n+                        .collect(toImmutableMap(child -> child.getFragment().getId(), child -> createRdd(child, PrestoSparkSerializedPage.class)));\n \n-                Map<String, Future<List<Tuple2<MutablePartitionId, PrestoSparkMaterializedRow>>>> inputFutures = inputRdds.entrySet().stream()\n-                        .collect(toImmutableMap(\n-                                entry -> entry.getKey().toString(),\n-                                entry -> entry.getValue().getRdd()\n-                                        .mapValues(new PrestoSparkToMaterializedRowFunction())\n-                                        .collectAsync()));\n+                Map<String, Future<List<Tuple2<MutablePartitionId, PrestoSparkSerializedPage>>>> inputFutures = inputRdds.entrySet().stream()\n+                        .collect(toImmutableMap(entry -> entry.getKey().toString(), entry -> entry.getValue().getRdd().collectAsync()));\n \n                 waitFor(inputFutures.values());\n \n-                Map<String, Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>> inputs = inputFutures.entrySet().stream()\n+                Map<String, List<PrestoSparkSerializedPage>> inputs = inputFutures.entrySet().stream()\n                         .collect(toImmutableMap(\n                                 Map.Entry::getKey,\n-                                entry -> transform(\n-                                        getUnchecked(entry.getValue()).iterator(),\n-                                        tuple -> new Tuple2<>(tuple._1, tuple._2.toPrestoSparkMutableRow()))));\n+                                entry -> getUnchecked(entry.getValue()).stream().map(Tuple2::_2).collect(toImmutableList())));\n \n-                IPrestoSparkTaskExecutor prestoSparkTaskExecutor = executorFactoryProvider.get(SparkProcessType.DRIVER).create(\n+                IPrestoSparkTaskExecutor<PrestoSparkSerializedPage> prestoSparkTaskExecutor = executorFactoryProvider.get(SparkProcessType.DRIVER).create(\n                         0,\n                         0,\n                         serializedTaskDescriptor,\n                         emptyIterator(),\n-                        new PrestoSparkTaskInputs(inputs, ImmutableMap.of()),\n-                        taskStatsCollector);\n-                return ImmutableList.copyOf(transform(prestoSparkTaskExecutor, tuple -> new Tuple2<>(tuple._1, tuple._2.toMaterializedRow())));\n+                        new PrestoSparkTaskInputs(ImmutableMap.of(), ImmutableMap.of(), inputs),\n+                        taskStatsCollector,\n+                        PrestoSparkSerializedPage.class);\n+                return ImmutableList.copyOf(prestoSparkTaskExecutor);\n             }\n \n-            RddAndMore rootRdd = createRdd(root);\n+            RddAndMore<PrestoSparkSerializedPage> rootRdd = createRdd(root, PrestoSparkSerializedPage.class);\n             return rootRdd.collectAndDestroyDependencies();\n         }\n \n-        private RddAndMore createRdd(SubPlan subPlan)\n+        private <T> RddAndMore<T> createRdd(SubPlan subPlan, Class<T> outputType)\n         {\n             ImmutableMap.Builder<PlanFragmentId, JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow>> rddInputs = ImmutableMap.builder();\n             ImmutableMap.Builder<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs = ImmutableMap.builder();\n             ImmutableList.Builder<Broadcast<?>> broadcastDependencies = ImmutableList.builder();\n \n             for (SubPlan child : subPlan.getChildren()) {\n                 PlanFragment childFragment = child.getFragment();\n-                RddAndMore childRdd = createRdd(child);\n                 if (childFragment.getPartitioningScheme().getPartitioning().getHandle().equals(FIXED_BROADCAST_DISTRIBUTION)) {\n-                    List<Tuple2<MutablePartitionId, PrestoSparkMaterializedRow>> broadcastRows = childRdd.collectAndDestroyDependencies();\n-                    // TODO: Transform rows to pages on executors (using `RDD#map` function)\n-                    // TODO: Transforming it on coordinator results in 2x memory utilization as both,\n-                    // TODO: rows and pages have to be kept in memory all at the same time\n-                    Iterator<Page> pagesIterator = transformMaterializedRowsToPages(transform(broadcastRows.iterator(), Tuple2::_2), childFragment.getTypes());\n-                    Iterator<PrestoSparkSerializedPage> serializedPagesIterator = transform(transform(pagesIterator, pagesSerde::serialize), PrestoSparkUtils::toPrestoSparkSerializedPage);\n-                    List<PrestoSparkSerializedPage> serializedPages = new ArrayList<>();\n-                    serializedPagesIterator.forEachRemaining(serializedPages::add);\n-                    Broadcast<List<PrestoSparkSerializedPage>> broadcast = sparkContext.broadcast(serializedPages);\n+                    RddAndMore<PrestoSparkSerializedPage> childRdd = createRdd(child, PrestoSparkSerializedPage.class);\n+                    List<PrestoSparkSerializedPage> broadcastPages = childRdd.collectAndDestroyDependencies().stream()\n+                            .map(Tuple2::_2)\n+                            .collect(toList());", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2NDAzOQ==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441664039", "bodyText": "Broadcast won't be able to serialize it, Guava is loaded by the Presto class loader", "author": "arhimondr", "createdAt": "2020-06-17T16:12:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNDgwNg=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\nindex 4b86ff9930..b583054dd8 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\n\n@@ -465,7 +466,7 @@ public class PrestoSparkQueryExecutionFactory\n             return rootRdd.collectAndDestroyDependencies();\n         }\n \n-        private <T> RddAndMore<T> createRdd(SubPlan subPlan, Class<T> outputType)\n+        private <T extends PrestoSparkTaskOutput> RddAndMore<T> createRdd(SubPlan subPlan, Class<T> outputType)\n         {\n             ImmutableMap.Builder<PlanFragmentId, JavaPairRDD<MutablePartitionId, PrestoSparkMutableRow>> rddInputs = ImmutableMap.builder();\n             ImmutableMap.Builder<PlanFragmentId, Broadcast<List<PrestoSparkSerializedPage>>> broadcastInputs = ImmutableMap.builder();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNTMwOA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441305308", "bodyText": "Add a comment that T should only be PrestoSparkMutableRow or PrestoSparkSerializedPage when in use.", "author": "wenleix", "createdAt": "2020-06-17T06:19:35Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IPrestoSparkTaskExecutor.java", "diffHunk": "@@ -17,7 +17,7 @@\n \n import java.util.Iterator;\n \n-public interface IPrestoSparkTaskExecutor\n-        extends Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>\n+public interface IPrestoSparkTaskExecutor<T>", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2NDU2NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441664564", "bodyText": "Actually let me better add a marker interface and let the compiler do the job", "author": "arhimondr", "createdAt": "2020-06-17T16:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNTMwOA=="}], "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IPrestoSparkTaskExecutor.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IPrestoSparkTaskExecutor.java\nindex 2bfc6e2a88..aa5049b78f 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IPrestoSparkTaskExecutor.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/IPrestoSparkTaskExecutor.java\n\n@@ -17,7 +17,7 @@ import scala.Tuple2;\n \n import java.util.Iterator;\n \n-public interface IPrestoSparkTaskExecutor<T>\n-        extends Iterator<Tuple2<MutablePartitionId, T>>\n+public interface IPrestoSparkTaskExecutor\n+        extends Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>>\n {\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNTczMw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441305733", "bodyText": "Add a comment that T should only be PrestoSparkMutableRow or PrestoSparkSerializedPage when in use.", "author": "wenleix", "createdAt": "2020-06-17T06:20:38Z", "path": "presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java", "diffHunk": "@@ -56,20 +56,20 @@\n  * <p>\n  * The broadcast inputs are encapsulated in taskProcessor.\n  */\n-public class PrestoSparkTaskRdd\n-        extends ZippedPartitionsBaseRDD<Tuple2<MutablePartitionId, PrestoSparkMutableRow>>\n+public class PrestoSparkTaskRdd<T>", "originalCommit": "a1bd2a27ea149a5f23bd4c6edc5fabd9f56fb802", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY2ODQyNg==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441668426", "bodyText": "Added a marker interface, PrestoSparkTaskOutput. Now it should be enforced by the compiler.", "author": "arhimondr", "createdAt": "2020-06-17T16:20:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTMwNTczMw=="}], "type": "inlineReview", "revised_code": {"commit": "de0bc338361d5206205709fea1f1683c0516072e", "chunk": "diff --git a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java\nindex 7b2798f5d8..fcc9350942 100644\n--- a/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java\n+++ b/presto-spark-classloader-interface/src/main/java/com/facebook/presto/spark/classloader_interface/PrestoSparkTaskRdd.java\n\n@@ -56,7 +56,7 @@ import static scala.collection.JavaConversions.seqAsJavaList;\n  * <p>\n  * The broadcast inputs are encapsulated in taskProcessor.\n  */\n-public class PrestoSparkTaskRdd<T>\n+public class PrestoSparkTaskRdd<T extends PrestoSparkTaskOutput>\n         extends ZippedPartitionsBaseRDD<Tuple2<MutablePartitionId, T>>\n {\n     private List<String> shuffleInputFragmentIds;\n"}}, {"oid": "de0bc338361d5206205709fea1f1683c0516072e", "url": "https://github.com/prestodb/presto/commit/de0bc338361d5206205709fea1f1683c0516072e", "message": "Avoid unnecessary page to row conversions\n\nConvert to row only if shuffle is needed", "committedDate": "2020-06-17T16:23:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4NTI4Nw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441985287", "bodyText": "maybe add a comment about what is \"inMemoryInputinPrestoSparkTaskInputs.java\" ? (you explained in #14634 (comment))", "author": "wenleix", "createdAt": "2020-06-18T05:59:11Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -276,59 +282,74 @@ public IPrestoSparkTaskExecutor doCreate(\n                 allocationTrackingEnabled,\n                 false);\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n-\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n-                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n+                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());", "originalCommit": "de0bc338361d5206205709fea1f1683c0516072e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\nindex b97fd6749f..43f7382163 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n\n@@ -282,74 +276,59 @@ public class PrestoSparkTaskExecutorFactory\n                 allocationTrackingEnabled,\n                 false);\n \n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n+        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n+                sinkMaxBufferSize.toBytes(),\n+                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n+                notificationExecutor);\n+        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n+\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n-                Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n+                Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n-\n+                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n+                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n                 if (shuffleInput != null) {\n-                    checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n-                    continue;\n+                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n                 }\n-\n                 if (broadcastInput != null) {\n-                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n-                    // remoteSourcePageInputs.add(getNullifyingIterator(broadcastInput.value()));\n-                    remoteSourcePageInputs.add(broadcastInput.value().iterator());\n-                    continue;\n-                }\n-\n-                if (inMemoryInput != null) {\n-                    remoteSourcePageInputs.add(inMemoryInput.iterator());\n-                    continue;\n+                    // broadcastRemoteSourceInputs.add(getNullifyingIterator(broadcastInput.value()));\n+                    broadcastRemoteSourceInputs.add(broadcastInput.value().iterator());\n                 }\n-\n-                throw new IllegalArgumentException(\"Input not found for sourceFragmentId: \" + sourceFragmentId);\n             }\n-            if (!remoteSourceRowInputs.isEmpty()) {\n-                rowInputs.put(remoteSource.getId(), Iterators.concat(remoteSourceRowInputs.iterator()));\n+            if (!shuffleRemoteSourceInputs.isEmpty()) {\n+                shuffleInputs.put(remoteSource.getId(), Iterators.concat(shuffleRemoteSourceInputs.iterator()));\n             }\n-            if (!remoteSourcePageInputs.isEmpty()) {\n-                pageInputs.put(remoteSource.getId(), Iterators.concat(remoteSourcePageInputs.iterator()));\n+            if (!broadcastRemoteSourceInputs.isEmpty()) {\n+                broadcastInputs.put(remoteSource.getId(), Iterators.concat(broadcastRemoteSourceInputs.iterator()));\n             }\n         }\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PagesSerde pagesSerde = new PagesSerde(blockEncodingManager, Optional.empty(), Optional.empty(), Optional.empty());\n-        Output<T> output = configureOutput(outputType, pagesSerde, memoryManager);\n-        PrestoSparkOutputBuffer<?> outputBuffer = output.getOutputBuffer();\n-\n         LocalExecutionPlan localExecutionPlan = localExecutionPlanner.plan(\n                 taskContext,\n                 fragment.getRoot(),\n                 fragment.getPartitioningScheme(),\n                 fragment.getStageExecutionDescriptor(),\n                 fragment.getTableScanSchedulingOrder(),\n-                output.getOutputFactory(),\n+                new PrestoSparkOutputFactory(rowBuffer),\n                 new PrestoSparkRemoteSourceFactory(\n-                        pagesSerde,\n-                        rowInputs.build(),\n-                        pageInputs.build()),\n+                        new PagesSerde(blockEncodingManager, Optional.empty(), Optional.empty(), Optional.empty()),\n+                        shuffleInputs.build(),\n+                        broadcastInputs.build()),\n                 taskDescriptor.getTableWriteInfo(),\n                 true);\n \n         TaskStateMachine taskStateMachine = new TaskStateMachine(taskId, notificationExecutor);\n         taskStateMachine.addStateChangeListener(state -> {\n             if (state.isDone()) {\n-                outputBuffer.setNoMoreRows();\n+                rowBuffer.setNoMoreRows();\n             }\n         });\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4NTM4NA==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r441985384", "bodyText": "Is this comment still relevant?", "author": "wenleix", "createdAt": "2020-06-18T05:59:30Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java", "diffHunk": "@@ -276,59 +282,74 @@ public IPrestoSparkTaskExecutor doCreate(\n                 allocationTrackingEnabled,\n                 false);\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n-\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> shuffleInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n-                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n+                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n+\n                 if (shuffleInput != null) {\n-                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n+                    remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n+                    continue;\n                 }\n+\n                 if (broadcastInput != null) {\n+                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model", "originalCommit": "de0bc338361d5206205709fea1f1683c0516072e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI0NzA3Mw==", "url": "https://github.com/prestodb/presto/pull/14634#discussion_r442247073", "bodyText": "Yes, it is still relevant. Transitioning from rows to pages saves memory on the driver. NullifyingIterator is needed to save memory on executors.", "author": "arhimondr", "createdAt": "2020-06-18T13:58:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTk4NTM4NA=="}], "type": "inlineReview", "revised_code": {"commit": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\nindex b97fd6749f..43f7382163 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkTaskExecutorFactory.java\n\n@@ -282,74 +276,59 @@ public class PrestoSparkTaskExecutorFactory\n                 allocationTrackingEnabled,\n                 false);\n \n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkMutableRow>> rowInputs = ImmutableMap.builder();\n-        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> pageInputs = ImmutableMap.builder();\n+        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n+                sinkMaxBufferSize.toBytes(),\n+                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n+                notificationExecutor);\n+        PrestoSparkRowBuffer rowBuffer = new PrestoSparkRowBuffer(memoryManager);\n+\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkRow>> shuffleInputs = ImmutableMap.builder();\n+        ImmutableMap.Builder<PlanNodeId, Iterator<PrestoSparkSerializedPage>> broadcastInputs = ImmutableMap.builder();\n         for (RemoteSourceNode remoteSource : fragment.getRemoteSourceNodes()) {\n-            List<Iterator<PrestoSparkMutableRow>> remoteSourceRowInputs = new ArrayList<>();\n-            List<Iterator<PrestoSparkSerializedPage>> remoteSourcePageInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkRow>> shuffleRemoteSourceInputs = new ArrayList<>();\n+            List<Iterator<PrestoSparkSerializedPage>> broadcastRemoteSourceInputs = new ArrayList<>();\n             for (PlanFragmentId sourceFragmentId : remoteSource.getSourceFragmentIds()) {\n-                Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n+                Iterator<Tuple2<MutablePartitionId, PrestoSparkRow>> shuffleInput = inputs.getShuffleInputs().get(sourceFragmentId.toString());\n                 Broadcast<List<PrestoSparkSerializedPage>> broadcastInput = inputs.getBroadcastInputs().get(sourceFragmentId.toString());\n-                List<PrestoSparkSerializedPage> inMemoryInput = inputs.getInMemoryInputs().get(sourceFragmentId.toString());\n-\n+                checkArgument(shuffleInput != null || broadcastInput != null, \"Input not found for sourceFragmentId: %s\", sourceFragmentId);\n+                checkArgument(shuffleInput == null || broadcastInput == null, \"Single remote source cannot accept both, broadcast and shuffle inputs\");\n                 if (shuffleInput != null) {\n-                    checkArgument(broadcastInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n-                    remoteSourceRowInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n-                    continue;\n+                    shuffleRemoteSourceInputs.add(Iterators.transform(shuffleInput, tuple -> tuple._2));\n                 }\n-\n                 if (broadcastInput != null) {\n-                    checkArgument(inMemoryInput == null, \"single remote source is not expected to accept different kind of inputs\");\n                     // TODO: Enable NullifyingIterator once migrated to one task per JVM model\n                     // NullifyingIterator removes element from the list upon return\n                     // This allows GC to gradually reclaim memory\n-                    // remoteSourcePageInputs.add(getNullifyingIterator(broadcastInput.value()));\n-                    remoteSourcePageInputs.add(broadcastInput.value().iterator());\n-                    continue;\n-                }\n-\n-                if (inMemoryInput != null) {\n-                    remoteSourcePageInputs.add(inMemoryInput.iterator());\n-                    continue;\n+                    // broadcastRemoteSourceInputs.add(getNullifyingIterator(broadcastInput.value()));\n+                    broadcastRemoteSourceInputs.add(broadcastInput.value().iterator());\n                 }\n-\n-                throw new IllegalArgumentException(\"Input not found for sourceFragmentId: \" + sourceFragmentId);\n             }\n-            if (!remoteSourceRowInputs.isEmpty()) {\n-                rowInputs.put(remoteSource.getId(), Iterators.concat(remoteSourceRowInputs.iterator()));\n+            if (!shuffleRemoteSourceInputs.isEmpty()) {\n+                shuffleInputs.put(remoteSource.getId(), Iterators.concat(shuffleRemoteSourceInputs.iterator()));\n             }\n-            if (!remoteSourcePageInputs.isEmpty()) {\n-                pageInputs.put(remoteSource.getId(), Iterators.concat(remoteSourcePageInputs.iterator()));\n+            if (!broadcastRemoteSourceInputs.isEmpty()) {\n+                broadcastInputs.put(remoteSource.getId(), Iterators.concat(broadcastRemoteSourceInputs.iterator()));\n             }\n         }\n \n-        OutputBufferMemoryManager memoryManager = new OutputBufferMemoryManager(\n-                sinkMaxBufferSize.toBytes(),\n-                () -> queryContext.getTaskContextByTaskId(taskId).localSystemMemoryContext(),\n-                notificationExecutor);\n-        PagesSerde pagesSerde = new PagesSerde(blockEncodingManager, Optional.empty(), Optional.empty(), Optional.empty());\n-        Output<T> output = configureOutput(outputType, pagesSerde, memoryManager);\n-        PrestoSparkOutputBuffer<?> outputBuffer = output.getOutputBuffer();\n-\n         LocalExecutionPlan localExecutionPlan = localExecutionPlanner.plan(\n                 taskContext,\n                 fragment.getRoot(),\n                 fragment.getPartitioningScheme(),\n                 fragment.getStageExecutionDescriptor(),\n                 fragment.getTableScanSchedulingOrder(),\n-                output.getOutputFactory(),\n+                new PrestoSparkOutputFactory(rowBuffer),\n                 new PrestoSparkRemoteSourceFactory(\n-                        pagesSerde,\n-                        rowInputs.build(),\n-                        pageInputs.build()),\n+                        new PagesSerde(blockEncodingManager, Optional.empty(), Optional.empty(), Optional.empty()),\n+                        shuffleInputs.build(),\n+                        broadcastInputs.build()),\n                 taskDescriptor.getTableWriteInfo(),\n                 true);\n \n         TaskStateMachine taskStateMachine = new TaskStateMachine(taskId, notificationExecutor);\n         taskStateMachine.addStateChangeListener(state -> {\n             if (state.isDone()) {\n-                outputBuffer.setNoMoreRows();\n+                rowBuffer.setNoMoreRows();\n             }\n         });\n \n"}}, {"oid": "d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "url": "https://github.com/prestodb/presto/commit/d2bfc2a8398d002ecfe5ec36f9ef2fadeda1e851", "message": "Implement MutablePartitionId\n\nTo avoid allocating new Integer for every row", "committedDate": "2020-06-18T13:51:48Z", "type": "commit"}, {"oid": "77bc66c8255b2c3a0ddc7b9cc0b1f7ad0ffd5bc9", "url": "https://github.com/prestodb/presto/commit/77bc66c8255b2c3a0ddc7b9cc0b1f7ad0ffd5bc9", "message": "Rename PrestoSparkRow to PrestoSparkMutableRow\n\nIn next commits an unsafe serialization algorithm will be introduced for\nbetter efficiency. The unsafe serialization algorithm tries to avoid\nunnecessary allocations by mutating an existing instance of the\nPrestoSparkMutableRow.", "committedDate": "2020-06-18T13:51:48Z", "type": "commit"}, {"oid": "1e5f42f65152da306767da69958880054831d062", "url": "https://github.com/prestodb/presto/commit/1e5f42f65152da306767da69958880054831d062", "message": "Add custom shuffle serialization for Presto on Spark", "committedDate": "2020-06-18T13:51:48Z", "type": "commit"}, {"oid": "16c7279b733e4d773be580402ae3239ca6d06986", "url": "https://github.com/prestodb/presto/commit/16c7279b733e4d773be580402ae3239ca6d06986", "message": "Optimize PrestoSparkRemoteSourceOperator\n\nAvoid rows decoding under the lock", "committedDate": "2020-06-18T13:51:48Z", "type": "commit"}, {"oid": "b0f39096a18749f2bb49d651d73a5086d957c121", "url": "https://github.com/prestodb/presto/commit/b0f39096a18749f2bb49d651d73a5086d957c121", "message": "Optimize PrestoSparkOutputOperator\n\nAvoid allocating a lot of small objects", "committedDate": "2020-06-18T13:51:48Z", "type": "commit"}, {"oid": "5ff668e515219a4b52c69c64cb269d19fa7ccbe6", "url": "https://github.com/prestodb/presto/commit/5ff668e515219a4b52c69c64cb269d19fa7ccbe6", "message": "Avoid unnecessary page to row conversions\n\nConvert to row only if shuffle is needed", "committedDate": "2020-06-18T14:03:42Z", "type": "commit"}, {"oid": "5ff668e515219a4b52c69c64cb269d19fa7ccbe6", "url": "https://github.com/prestodb/presto/commit/5ff668e515219a4b52c69c64cb269d19fa7ccbe6", "message": "Avoid unnecessary page to row conversions\n\nConvert to row only if shuffle is needed", "committedDate": "2020-06-18T14:03:42Z", "type": "forcePushed"}]}