{"pr_number": 14989, "pr_title": "Batch presto spark rows", "pr_createdAt": "2020-08-07T16:54:18Z", "pr_url": "https://github.com/prestodb/presto/pull/14989", "timeline": [{"oid": "99a3724921617370bdc90147bf7429ac7520f16a", "url": "https://github.com/prestodb/presto/commit/99a3724921617370bdc90147bf7429ac7520f16a", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-11T03:54:13Z", "type": "forcePushed"}, {"oid": "5efc3982ea46e269f0d24f480691ce3d754935a5", "url": "https://github.com/prestodb/presto/commit/5efc3982ea46e269f0d24f480691ce3d754935a5", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-11T20:44:26Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2ODY2MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468768661", "bodyText": "nit. totalSizeInBytes", "author": "viczhang861", "createdAt": "2020-08-11T18:07:37Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;", "originalCommit": "fd40afa0853b9c593e63e54b4cccf5cb3644e450", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 42acb75784..09f360af96 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -35,9 +41,15 @@ public class PrestoSparkRowBatch\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;\n+    public static final byte MULTI_ROW_ENTRY_MARKER = 2;\n+\n+    private static final int MIN_TARGET_SIZE_IN_BYTES = 1024 * 1024;\n+    private static final int MAX_TARGET_SIZE_IN_BYTES = 10 * 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n     private static final int REPLICATED_ROW_PARTITION_ID = -1;\n+    private static final short MULTI_ROW_ENTRY_MAX_SIZE_IN_BYTES = 10 * 1024;\n+    private static final short MULTI_ROW_ENTRY_MAX_ROW_COUNT = 10 * 1024;\n \n     private final int partitionCount;\n     private final int rowCount;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg2ODk2MA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468868960", "bodyText": "Put comment separately in the end or use a new line?", "author": "viczhang861", "createdAt": "2020-08-11T21:15:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -385,6 +372,33 @@ public class PrestoSparkRowBatch\n         }\n     }\n \n+    /*\n+     * Partitions rows into disjoint sets based on the partitions assigned\n+     *\n+     * int[] rowIndex - links rows that belong for the same partition\n+     *\n+     * For example for 3 rows with partitions assigned [2, 1, 2, 1] the\n+     * row index will look like:\n+     *\n+     * [2, 3, -1, -1]\n+     *\n+     * int[] nextRow - contains the pointers to the next row for each partition:\n+     *\n+     * [-1, 1, 0]\n+     *\n+     * note: there's no rows with partition 0\n+     *\n+     * To get all rows for a single partition first we check what is the tip of the\n+     * list of rows for that partition at the moment:\n+     *\n+     * int row = nextRow[partition]\n+     *\n+     * And then we iterate over the linked list to get all the rows that belong to\n+     * the same partition:\n+     *\n+     * while (rowIndex[row] != -1)\n+     *      row = rowIndex[row]\n+     */\n     public static class RowIndex\n     {\n         private static final int NIL = -1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMTI3Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468901272", "bodyText": "peekRow(partition) != NIL", "author": "viczhang861", "createdAt": "2020-08-11T22:31:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex\n+    {\n+        private static final int NIL = -1;\n+\n+        private final int[] nextRow;\n+        private final int[] rowIndex;\n+\n+        public static RowIndex create(int rowCount, int partitionCount, int[] partitions)\n+        {\n+            int[] nextRow = new int[partitionCount + 1 /*one more slot for replicated partition*/];\n+            fill(nextRow, NIL);\n+            int[] rowIndex = new int[rowCount];\n+            fill(rowIndex, NIL);\n+\n+            for (int row = rowCount - 1; row >= 0; row--) {\n+                int partition = partitions[row];\n+                int partitionIndex = getPartitionIndex(partition, nextRow);\n+                int currentPointer = nextRow[partitionIndex];\n+                nextRow[partitionIndex] = row;\n+                rowIndex[row] = currentPointer;\n+            }\n+\n+            return new RowIndex(nextRow, rowIndex);\n+        }\n+\n+        private RowIndex(int[] nextRow, int[] rowIndex)\n+        {\n+            this.nextRow = requireNonNull(nextRow, \"nextRow is null\");\n+            this.rowIndex = requireNonNull(rowIndex, \"rowIndex is null\");\n+        }\n+\n+        public boolean hasNextRow(int partition)\n+        {\n+            return nextRow[getPartitionIndex(partition, nextRow)] != NIL;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -385,6 +372,33 @@ public class PrestoSparkRowBatch\n         }\n     }\n \n+    /*\n+     * Partitions rows into disjoint sets based on the partitions assigned\n+     *\n+     * int[] rowIndex - links rows that belong for the same partition\n+     *\n+     * For example for 3 rows with partitions assigned [2, 1, 2, 1] the\n+     * row index will look like:\n+     *\n+     * [2, 3, -1, -1]\n+     *\n+     * int[] nextRow - contains the pointers to the next row for each partition:\n+     *\n+     * [-1, 1, 0]\n+     *\n+     * note: there's no rows with partition 0\n+     *\n+     * To get all rows for a single partition first we check what is the tip of the\n+     * list of rows for that partition at the moment:\n+     *\n+     * int row = nextRow[partition]\n+     *\n+     * And then we iterate over the linked list to get all the rows that belong to\n+     * the same partition:\n+     *\n+     * while (rowIndex[row] != -1)\n+     *      row = rowIndex[row]\n+     */\n     public static class RowIndex\n     {\n         private static final int NIL = -1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468796482", "bodyText": "when would this happen? essentially it's an empty row batch?", "author": "wenleix", "createdAt": "2020-08-11T18:53:54Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,6 +220,21 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {", "originalCommit": "99a3724921617370bdc90147bf7429ac7520f16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NzI1Nw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469387257", "bodyText": "In theory it should never happen. We create the builder lazily. It should never be created if theres no rows to be appended. However I added it here for completeness, as the interface in theory allows you to create a zero rows batch.", "author": "arhimondr", "createdAt": "2020-08-12T16:27:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc5NjQ4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -225,7 +222,7 @@ public class PrestoSparkRowBatch\n                 return createDirectRowBatch();\n             }\n \n-            int averageRowSize = totalSize / rowCount;\n+            int averageRowSize = totalSizeInBytes / rowCount;\n             if (averageRowSize < targetAverageRowSizeInBytes) {\n                 return createGroupedRowBatch();\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468903642", "bodyText": "Add some comment like \"Array-simulated linked-list to find the row offset for each partition\" assume i read the intention of this class right :)", "author": "wenleix", "createdAt": "2020-08-11T22:38:15Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5NTk2Mg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469395962", "bodyText": "Added more descriptive comment", "author": "arhimondr", "createdAt": "2020-08-12T16:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkwMzY0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -385,6 +372,33 @@ public class PrestoSparkRowBatch\n         }\n     }\n \n+    /*\n+     * Partitions rows into disjoint sets based on the partitions assigned\n+     *\n+     * int[] rowIndex - links rows that belong for the same partition\n+     *\n+     * For example for 3 rows with partitions assigned [2, 1, 2, 1] the\n+     * row index will look like:\n+     *\n+     * [2, 3, -1, -1]\n+     *\n+     * int[] nextRow - contains the pointers to the next row for each partition:\n+     *\n+     * [-1, 1, 0]\n+     *\n+     * note: there's no rows with partition 0\n+     *\n+     * To get all rows for a single partition first we check what is the tip of the\n+     * list of rows for that partition at the moment:\n+     *\n+     * int row = nextRow[partition]\n+     *\n+     * And then we iterate over the linked list to get all the rows that belong to\n+     * the same partition:\n+     *\n+     * while (rowIndex[row] != -1)\n+     *      row = rowIndex[row]\n+     */\n     public static class RowIndex\n     {\n         private static final int NIL = -1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911068", "bodyText": "hmm? what's this for? :)", "author": "wenleix", "createdAt": "2020-08-11T23:00:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -204,8 +333,9 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             this.totalSize = totalSize;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n+            this.rowData.order(LITTLE_ENDIAN);", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM0OTU4Mw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469349583", "bodyText": "To make sure we are writing short in LITTLE_ENDIAN byte order, as the default byte order is BIG_ENDIAN that is different that the machine byte order.", "author": "arhimondr", "createdAt": "2020-08-12T15:34:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTA2OA=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -324,13 +320,13 @@ public class PrestoSparkRowBatch\n         private final PrestoSparkMutableRow row;\n         private final Tuple2<MutablePartitionId, PrestoSparkMutableRow> tuple;\n \n-        private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)\n+        private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSizeInBytes)\n         {\n             this.partitionCount = partitionCount;\n             this.rowCount = rowCount;\n             this.rowPartitions = requireNonNull(rowPartitions, \"rowPartitions is null\");\n             this.rowOffsets = requireNonNull(rowOffsets, \"rowSizes is null\");\n-            this.totalSize = totalSize;\n+            this.totalSizeInBytes = totalSizeInBytes;\n \n             this.rowData = ByteBuffer.wrap(requireNonNull(rowData, \"rowData is null\"));\n             this.rowData.order(LITTLE_ENDIAN);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468911401", "bodyText": "is this while try to skip empty partition? -- in that case can it be an if statement instead ? :)", "author": "wenleix", "createdAt": "2020-08-11T23:01:02Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3MzM5NA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468973394", "bodyText": "I think one partition may receive multiple row batches (entries), thus requires a while loop.", "author": "viczhang861", "createdAt": "2020-08-12T02:46:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxMTQwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -241,7 +238,7 @@ public class PrestoSparkRowBatch\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n         }\n \n         private PrestoSparkRowBatch createGroupedRowBatch()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNDQ1NQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468914455", "bodyText": "nit: new line.", "author": "wenleix", "createdAt": "2020-08-11T23:10:23Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java", "diffHunk": "@@ -170,7 +179,7 @@ public OperatorFactory duplicate()\n                     partitionChannels,\n                     partitionConstants,\n                     replicateNullsAndAny,\n-                    nullChannel);\n+                    nullChannel, targetAverageRowSizeInBytes);", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\nindex 3f5351a653..6ad9aaade1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowOutputOperator.java\n\n@@ -179,7 +179,8 @@ public class PrestoSparkRowOutputOperator\n                     partitionChannels,\n                     partitionConstants,\n                     replicateNullsAndAny,\n-                    nullChannel, targetAverageRowSizeInBytes);\n+                    nullChannel,\n+                    targetAverageRowSizeInBytes);\n         }\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468915736", "bodyText": "lol", "author": "wenleix", "createdAt": "2020-08-11T23:14:42Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java", "diffHunk": "@@ -76,32 +78,76 @@ public Page getNextPage()\n             while (currentIteratorIndex < shuffleInputs.size()) {\n                 PrestoSparkShuffleInput input = shuffleInputs.get(currentIteratorIndex);\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n-                long processedBytes = 0;\n+                long currentIteratorProcessedBytes = 0;\n+                long currentIteratorProcessedRows = 0;\n+                long currentIteratorProcessedEntries = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n+                    currentIteratorProcessedEntries++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                        processedBytes += buffer.remaining();\n+                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MDA5MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469350091", "bodyText": "Now even a zero columns row will contain at least a single byte, that is a marker.", "author": "arhimondr", "createdAt": "2020-08-12T15:35:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODkxNTczNg=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java\nindex 20161b12cc..cb609e253e 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkShufflePageInput.java\n\n@@ -80,56 +78,28 @@ public class PrestoSparkShufflePageInput\n                 Iterator<Tuple2<MutablePartitionId, PrestoSparkMutableRow>> iterator = input.getIterator();\n                 long currentIteratorProcessedBytes = 0;\n                 long currentIteratorProcessedRows = 0;\n-                long currentIteratorProcessedEntries = 0;\n+                long currentIteratorProcessedRowBatches = 0;\n                 long start = System.currentTimeMillis();\n                 while (iterator.hasNext() && output.size() <= TARGET_SIZE && rowCount <= MAX_ROWS_PER_PAGE) {\n-                    currentIteratorProcessedEntries++;\n+                    currentIteratorProcessedRowBatches++;\n                     PrestoSparkMutableRow row = iterator.next()._2;\n                     if (row.getBuffer() != null) {\n                         ByteBuffer buffer = row.getBuffer();\n-                        verify(buffer.remaining() >= 1, \"row must contain at least a single byte\");\n+                        verify(buffer.remaining() >= 2, \"row data is expected to be at least 2 bytes long\");\n                         currentIteratorProcessedBytes += buffer.remaining();\n-                        byte marker = buffer.get();\n-                        switch (marker) {\n-                            case SINGLE_ROW_ENTRY_MARKER: {\n-                                rowCount++;\n-                                currentIteratorProcessedRows++;\n-                                output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                                break;\n-                            }\n-                            case MULTI_ROW_ENTRY_MARKER: {\n-                                short entryRowCount = getShortLittleEndian(buffer);\n-                                rowCount += entryRowCount;\n-                                currentIteratorProcessedRows += entryRowCount;\n-                                buffer.position(buffer.position() + 2);\n-                                output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n-                                break;\n-                            }\n-                            default:\n-                                throw new IllegalArgumentException(\"Unexpected row marker: \" + marker);\n-                        }\n+                        short entryRowCount = getShortLittleEndian(buffer);\n+                        rowCount += entryRowCount;\n+                        currentIteratorProcessedRows += entryRowCount;\n+                        buffer.position(buffer.position() + 2);\n+                        output.writeBytes(buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n                     }\n                     else if (row.getArray() != null) {\n+                        verify(row.getLength() >= 2, \"row data is expected to be at least 2 bytes long\");\n                         currentIteratorProcessedBytes += row.getLength();\n-                        verify(row.getLength() >= 1, \"row must contain at least a single byte\");\n-                        byte marker = row.getArray()[row.getOffset()];\n-                        switch (marker) {\n-                            case SINGLE_ROW_ENTRY_MARKER: {\n-                                rowCount++;\n-                                currentIteratorProcessedRows++;\n-                                output.writeBytes(row.getArray(), row.getOffset() + 1, row.getLength() - 1);\n-                                break;\n-                            }\n-                            case MULTI_ROW_ENTRY_MARKER: {\n-                                short entryRowCount = getShortLittleEndian(row.getArray(), row.getOffset() + 1);\n-                                rowCount += entryRowCount;\n-                                currentIteratorProcessedRows += entryRowCount;\n-                                output.writeBytes(row.getArray(), row.getOffset() + 3, row.getLength() - 3);\n-                                break;\n-                            }\n-                            default:\n-                                throw new IllegalArgumentException(\"Unexpected row marker: \" + marker);\n-                        }\n+                        short entryRowCount = getShortLittleEndian(row.getArray(), row.getOffset());\n+                        rowCount += entryRowCount;\n+                        currentIteratorProcessedRows += entryRowCount;\n+                        output.writeBytes(row.getArray(), row.getOffset() + 2, row.getLength() - 2);\n                     }\n                     else {\n                         throw new IllegalArgumentException(\"Unexpected PrestoSparkMutableRow: 'buffer' and 'array' fields are both null\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468959278", "bodyText": "This is a nice implementation but a little difficult to understand, rowIndex[] is next next row I would prefer to rename nextRow to currentRow and rowIndex to nextRow. If you convert this implementation to an array of stacks, it could be simplified a lot. Are you worried about creating a lot objects when partitionCount is more than a few thousands.", "author": "viczhang861", "createdAt": "2020-08-12T01:50:54Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -242,4 +384,62 @@ private RowTupleSupplier(int partitionCount, int rowCount, byte[] rowData, int[]\n             return tuple;\n         }\n     }\n+\n+    public static class RowIndex", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5MDk4Mw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469390983", "bodyText": "The main reason for implementing RowIndex was to avoid allocating extra objects along the way.\nLet me add a comment explaining how it works at the top of the class.", "author": "arhimondr", "createdAt": "2020-08-12T16:33:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk1OTI3OA=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -385,6 +372,33 @@ public class PrestoSparkRowBatch\n         }\n     }\n \n+    /*\n+     * Partitions rows into disjoint sets based on the partitions assigned\n+     *\n+     * int[] rowIndex - links rows that belong for the same partition\n+     *\n+     * For example for 3 rows with partitions assigned [2, 1, 2, 1] the\n+     * row index will look like:\n+     *\n+     * [2, 3, -1, -1]\n+     *\n+     * int[] nextRow - contains the pointers to the next row for each partition:\n+     *\n+     * [-1, 1, 0]\n+     *\n+     * note: there's no rows with partition 0\n+     *\n+     * To get all rows for a single partition first we check what is the tip of the\n+     * list of rows for that partition at the moment:\n+     *\n+     * int row = nextRow[partition]\n+     *\n+     * And then we iterate over the linked list to get all the rows that belong to\n+     * the same partition:\n+     *\n+     * while (rowIndex[row] != -1)\n+     *      row = rowIndex[row]\n+     */\n     public static class RowIndex\n     {\n         private static final int NIL = -1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2NDUzOA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468964538", "bodyText": "Entries is a little difficult to guess what it means without reading the code. Have you consider to use totalProcessedRowBatches, at least for logging.", "author": "viczhang861", "createdAt": "2020-08-12T02:11:20Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java", "diffHunk": "@@ -819,33 +819,44 @@ private void processShuffleStats()\n         private void logShuffleStatsSummary(ShuffleStatsKey key, List<PrestoSparkShuffleStats> statsList)\n         {\n             long totalProcessedRows = 0;\n+            long totalProcessedEntries = 0;\n             long totalProcessedBytes = 0;\n             long totalElapsedWallTimeMills = 0;\n             for (PrestoSparkShuffleStats stats : statsList) {\n                 totalProcessedRows += stats.getProcessedRows();\n+                totalProcessedEntries += stats.getProcessedEntries();\n                 totalProcessedBytes += stats.getProcessedBytes();\n                 totalElapsedWallTimeMills += stats.getElapsedWallTimeMills();\n             }\n             long totalElapsedWallTimeSeconds = totalElapsedWallTimeMills / 1000;\n             long rowsPerSecond = totalProcessedRows;\n+            long entriesPerSecond = totalProcessedEntries;\n             long bytesPerSecond = totalProcessedBytes;\n             if (totalElapsedWallTimeSeconds > 0) {\n                 rowsPerSecond = totalProcessedRows / totalElapsedWallTimeSeconds;\n+                entriesPerSecond = totalProcessedEntries / totalElapsedWallTimeSeconds;\n                 bytesPerSecond = totalProcessedBytes / totalElapsedWallTimeSeconds;\n             }\n             long averageRowSize = 0;\n             if (totalProcessedRows > 0) {\n                 averageRowSize = totalProcessedBytes / totalProcessedRows;\n             }\n+            long averageEntrySize = 0;\n+            if (totalProcessedEntries > 0) {\n+                averageEntrySize = totalProcessedBytes / totalProcessedEntries;\n+            }\n             log.info(\n-                    \"Fragment: %s, Operation: %s, Rows: %s, Size: %s, Avg Row Size: %s, Time: %s, %srows/s, %s/s\",\n+                    \"Fragment: %s, Operation: %s, Rows: %s, Entries: %s, Size: %s, Avg Row Size: %s, Avg Entry Size: %s, Time: %s, %s rows/s, %s entries/s, %s/s\",", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\nindex 3386da991a..8f7052cfd2 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkQueryExecutionFactory.java\n\n@@ -819,44 +819,44 @@ public class PrestoSparkQueryExecutionFactory\n         private void logShuffleStatsSummary(ShuffleStatsKey key, List<PrestoSparkShuffleStats> statsList)\n         {\n             long totalProcessedRows = 0;\n-            long totalProcessedEntries = 0;\n+            long totalProcessedRowBatches = 0;\n             long totalProcessedBytes = 0;\n             long totalElapsedWallTimeMills = 0;\n             for (PrestoSparkShuffleStats stats : statsList) {\n                 totalProcessedRows += stats.getProcessedRows();\n-                totalProcessedEntries += stats.getProcessedEntries();\n+                totalProcessedRowBatches += stats.getProcessedRowBatches();\n                 totalProcessedBytes += stats.getProcessedBytes();\n                 totalElapsedWallTimeMills += stats.getElapsedWallTimeMills();\n             }\n             long totalElapsedWallTimeSeconds = totalElapsedWallTimeMills / 1000;\n             long rowsPerSecond = totalProcessedRows;\n-            long entriesPerSecond = totalProcessedEntries;\n+            long rowBatchesPerSecond = totalProcessedRowBatches;\n             long bytesPerSecond = totalProcessedBytes;\n             if (totalElapsedWallTimeSeconds > 0) {\n                 rowsPerSecond = totalProcessedRows / totalElapsedWallTimeSeconds;\n-                entriesPerSecond = totalProcessedEntries / totalElapsedWallTimeSeconds;\n+                rowBatchesPerSecond = totalProcessedRowBatches / totalElapsedWallTimeSeconds;\n                 bytesPerSecond = totalProcessedBytes / totalElapsedWallTimeSeconds;\n             }\n             long averageRowSize = 0;\n             if (totalProcessedRows > 0) {\n                 averageRowSize = totalProcessedBytes / totalProcessedRows;\n             }\n-            long averageEntrySize = 0;\n-            if (totalProcessedEntries > 0) {\n-                averageEntrySize = totalProcessedBytes / totalProcessedEntries;\n+            long averageRowBatchSize = 0;\n+            if (totalProcessedRowBatches > 0) {\n+                averageRowBatchSize = totalProcessedBytes / totalProcessedRowBatches;\n             }\n             log.info(\n-                    \"Fragment: %s, Operation: %s, Rows: %s, Entries: %s, Size: %s, Avg Row Size: %s, Avg Entry Size: %s, Time: %s, %s rows/s, %s entries/s, %s/s\",\n+                    \"Fragment: %s, Operation: %s, Rows: %s, Row Batches: %s, Size: %s, Avg Row Size: %s, Avg Row Batch Size: %s, Time: %s, %s rows/s, %s batches/s, %s/s\",\n                     key.getFragmentId(),\n                     key.getOperation(),\n                     totalProcessedRows,\n-                    totalProcessedEntries,\n+                    totalProcessedRowBatches,\n                     DataSize.succinctBytes(totalProcessedBytes),\n                     DataSize.succinctBytes(averageRowSize),\n-                    DataSize.succinctBytes(averageEntrySize),\n+                    DataSize.succinctBytes(averageRowBatchSize),\n                     Duration.succinctDuration(totalElapsedWallTimeMills, MILLISECONDS),\n                     rowsPerSecond,\n-                    entriesPerSecond,\n+                    rowBatchesPerSecond,\n                     DataSize.succinctBytes(bytesPerSecond));\n         }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468968220", "bodyText": "Ideally each partition is expected to receive at least one row before doing a shuffle to make a shuffle meaningful ?", "author": "viczhang861", "createdAt": "2020-08-12T02:25:59Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -77,14 +89,38 @@ public int getPositionCount()\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n     {\n-        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n+        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n+        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MTcwMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469351701", "bodyText": "I'm trying to figure out an average here. It is fine when one partitions receives more than the other.", "author": "arhimondr", "createdAt": "2020-08-12T15:37:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk2ODIyMA=="}], "type": "inlineReview", "revised_code": {"commit": "6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..42acb75784 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -89,38 +77,14 @@ public class PrestoSparkRowBatch\n         return rowCount;\n     }\n \n-    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetAverageRowSizeInBytes)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount)\n     {\n-        checkArgument(partitionCount > 0, \"partitionCount must be greater then zero: %s\", partitionCount);\n-        int targetSizeInBytes = partitionCount * targetAverageRowSizeInBytes;\n-        targetSizeInBytes = max(targetSizeInBytes, MIN_TARGET_SIZE_IN_BYTES);\n-        targetSizeInBytes = min(targetSizeInBytes, MAX_TARGET_SIZE_IN_BYTES);\n-        targetAverageRowSizeInBytes = min(targetSizeInBytes / partitionCount, targetAverageRowSizeInBytes);\n-        return builder(\n-                partitionCount,\n-                targetSizeInBytes,\n-                DEFAULT_EXPECTED_ROWS_COUNT,\n-                targetAverageRowSizeInBytes,\n-                MULTI_ROW_ENTRY_MAX_SIZE_IN_BYTES,\n-                MULTI_ROW_ENTRY_MAX_ROW_COUNT);\n+        return new PrestoSparkRowBatchBuilder(partitionCount, DEFAULT_TARGET_SIZE, DEFAULT_EXPECTED_ROWS_COUNT);\n     }\n \n-    @VisibleForTesting\n-    static PrestoSparkRowBatchBuilder builder(\n-            int partitionCount,\n-            int targetSizeInBytes,\n-            int expectedRowsCount,\n-            int targetAverageRowSizeInBytes,\n-            int maxEntrySizeInBytes,\n-            int maxRowsPerEntry)\n+    public static PrestoSparkRowBatchBuilder builder(int partitionCount, int targetSizeInBytes, int expectedRowsCount)\n     {\n-        return new PrestoSparkRowBatchBuilder(\n-                partitionCount,\n-                targetSizeInBytes,\n-                expectedRowsCount,\n-                targetAverageRowSizeInBytes,\n-                maxEntrySizeInBytes,\n-                maxRowsPerEntry);\n+        return new PrestoSparkRowBatchBuilder(partitionCount, targetSizeInBytes, expectedRowsCount);\n     }\n \n     public static class PrestoSparkRowBatchBuilder\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468975634", "bodyText": "Constructor parameter names for PrestoSparkRowBatch is not updated.", "author": "viczhang861", "createdAt": "2020-08-12T02:55:17Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -179,6 +243,70 @@ public PrestoSparkRowBatch build()\n                     rowOffsets,\n                     totalSize);\n         }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSize * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSize / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    output.writeByte(MULTI_ROW_ENTRY_MARKER);\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSize;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;\n+                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n+\n+                        // skip the marker\n+                        rowOffset++;\n+                        rowSize--;\n+\n+                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n+                            break;\n+                        }\n+\n+                        output.writeBytes(data, rowOffset, rowSize);\n+                        currentEntrySize += rowSize;\n+                        currentEntryRowCount++;\n+\n+                        rowIndex.nextRow(partition);\n+                    }\n+\n+                    // entry is done\n+                    output.getUnderlyingSlice().setShort(currentEntryOffset + 1, currentEntryRowCount);\n+                    entriesCount++;\n+                }\n+            }\n+\n+            return new PrestoSparkRowBatch(", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5ODcyMQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469398721", "bodyText": "Yeah, I deliberately decided not to do a more extensive rename, as this code should be pretty transitional", "author": "arhimondr", "createdAt": "2020-08-12T16:46:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NTYzNA=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -241,7 +238,7 @@ public class PrestoSparkRowBatch\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n         }\n \n         private PrestoSparkRowBatch createGroupedRowBatch()\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468976199", "bodyText": "Have you considered to not distinguish SINGLE and MULTIPLE.  For SINGLE_ROW, one byte (marker) is changed to short (rowCount),  for MULTIPLE row, three bytes (marker and rowCount) becomes two bytes (rowCount).", "author": "viczhang861", "createdAt": "2020-08-12T02:57:34Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -27,17 +28,28 @@\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Preconditions.checkState;\n+import static com.google.common.base.Verify.verify;\n import static io.airlift.slice.SizeOf.sizeOf;\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static java.nio.ByteOrder.LITTLE_ENDIAN;\n+import static java.util.Arrays.fill;\n import static java.util.Objects.requireNonNull;\n \n public class PrestoSparkRowBatch\n         implements PrestoSparkBufferedResult\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;", "originalCommit": "5efc3982ea46e269f0d24f480691ce3d754935a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM1MzI0Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469353246", "bodyText": "That is a very good point. Let me refactor it.", "author": "arhimondr", "createdAt": "2020-08-12T15:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MTM5Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469361396", "bodyText": "Actually now I realize that we might not even need short to store number of rows.\nWe can use unsigned byte instead that gives us a good 0 - 255 range and allows us to store 255 rows per batch. The theoretical minimum row size is 2 bytes (1 byte for null marker and 1 byte for data). With 255 rows per batch we can get to moderately decent 510 bytes. It is still less that expected 1kb, but with a single short column row we are already getting very close to it (3 * 255 = 765 bytes). And with a single integer column rows we are even above the threshold (5 * 255 = 1275)\nOn the other end one byte of overhead per row (to store short) is also nothing terrible. We can do that as well.\nWhat do you think?", "author": "arhimondr", "createdAt": "2020-08-12T15:47:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3NjE4Ng==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469376186", "bodyText": "Actually let me proceed with short, as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb", "author": "arhimondr", "createdAt": "2020-08-12T16:09:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcyNDUxNg==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469724516", "bodyText": "as 1 vs 2 bytes won't make a big difference with the target entry size ~1kb\n\nYeah, it's also valuable to make encoding simple :).  Just make sure I understand correctly -- for now a single row is considered as a \"row batch\" with rowCount = 1?", "author": "wenleix", "createdAt": "2020-08-13T06:28:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAwNjU3NQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r470006575", "bodyText": "Yeah", "author": "arhimondr", "createdAt": "2020-08-13T14:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NjE5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 09f360af96..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -41,9 +41,6 @@ public class PrestoSparkRowBatch\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    public static final byte SINGLE_ROW_ENTRY_MARKER = 1;\n-    public static final byte MULTI_ROW_ENTRY_MARKER = 2;\n-\n     private static final int MIN_TARGET_SIZE_IN_BYTES = 1024 * 1024;\n     private static final int MAX_TARGET_SIZE_IN_BYTES = 10 * 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r468977539", "bodyText": "entryData, entryOffsets, entryPartitions", "author": "viczhang861", "createdAt": "2020-08-12T03:02:41Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -43,25 +43,27 @@\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n-    private final int[] rowSizes;\n+    private final int[] rowOffsets;\n+    private final int totalSize;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowSizes)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)", "originalCommit": "707081b6641904d0c71fcf0ff65d90c3ecff6f59", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM2MzE2MQ==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469363161", "bodyText": "I deliberately didn't want to do the extensive renames, as this might potentially go to a very large scope. Since this code is transitional I would like to keep it the way it is, as then when we revert this patch we would have to rename everything back.", "author": "arhimondr", "createdAt": "2020-08-12T15:50:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk3NzUzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 42acb75784..782b84d4f1 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -35,19 +41,22 @@ public class PrestoSparkRowBatch\n {\n     private static final int INSTANCE_SIZE = ClassLayout.parseClass(PrestoSparkRowBatch.class).instanceSize();\n \n-    private static final int DEFAULT_TARGET_SIZE = 1024 * 1024;\n+    private static final int MIN_TARGET_SIZE_IN_BYTES = 1024 * 1024;\n+    private static final int MAX_TARGET_SIZE_IN_BYTES = 10 * 1024 * 1024;\n     private static final int DEFAULT_EXPECTED_ROWS_COUNT = 10000;\n     private static final int REPLICATED_ROW_PARTITION_ID = -1;\n+    private static final short MULTI_ROW_ENTRY_MAX_SIZE_IN_BYTES = 10 * 1024;\n+    private static final short MULTI_ROW_ENTRY_MAX_ROW_COUNT = 10 * 1024;\n \n     private final int partitionCount;\n     private final int rowCount;\n     private final byte[] rowData;\n     private final int[] rowPartitions;\n     private final int[] rowOffsets;\n-    private final int totalSize;\n+    private final int totalSizeInBytes;\n     private final long retainedSizeInBytes;\n \n-    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSize)\n+    private PrestoSparkRowBatch(int partitionCount, int rowCount, byte[] rowData, int[] rowPartitions, int[] rowOffsets, int totalSizeInBytes)\n     {\n         this.partitionCount = partitionCount;\n         this.rowCount = rowCount;\n"}}, {"oid": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "url": "https://github.com/prestodb/presto/commit/e77a7081b87d3e8bd67a469b317a2a18db676a81", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-12T16:50:27Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTkxMw==", "url": "https://github.com/prestodb/presto/pull/14989#discussion_r469569913", "bodyText": "Out of dated comment", "author": "viczhang861", "createdAt": "2020-08-12T21:57:41Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java", "diffHunk": "@@ -171,13 +217,91 @@ private void closeEntry(int partitionId)\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n+\n+            if (rowCount == 0) {\n+                return createDirectRowBatch();\n+            }\n+\n+            int averageRowSize = totalSizeInBytes / rowCount;\n+            if (averageRowSize < targetAverageRowSizeInBytes) {\n+                return createGroupedRowBatch();\n+            }\n+\n+            return createDirectRowBatch();\n+        }\n+\n+        private PrestoSparkRowBatch createDirectRowBatch()\n+        {\n             return new PrestoSparkRowBatch(\n                     partitionCount,\n                     rowCount,\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSize);\n+                    totalSizeInBytes);\n+        }\n+\n+        private PrestoSparkRowBatch createGroupedRowBatch()\n+        {\n+            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n+            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n+\n+            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSizeInBytes * 1.2f));\n+            int expectedEntriesCount = (int) ((totalSizeInBytes / targetAverageRowSizeInBytes) * 1.2f);\n+            int[] entryOffsets = new int[expectedEntriesCount];\n+            int[] entryPartitions = new int[expectedEntriesCount];\n+            int entriesCount = 0;\n+            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n+                while (rowIndex.hasNextRow(partition)) {\n+                    // start entry\n+                    short currentEntrySize = 0;\n+                    short currentEntryRowCount = 0;\n+                    int currentEntryOffset = output.size();\n+                    // Reserve space for the row count, the actual row count will be set later\n+                    output.writeShort(0);\n+\n+                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n+                    entryOffsets[entriesCount] = currentEntryOffset;\n+                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n+                    entryPartitions[entriesCount] = partition;\n+\n+                    while (rowIndex.hasNextRow(partition)) {\n+                        int row = rowIndex.peekRow(partition);\n+                        int followingRow = row + 1;\n+                        int rowOffset = rowOffsets[row];\n+                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSizeInBytes;\n+                        int rowSize = followingRowOffset - rowOffset;\n+                        // each row entry should contain at least a marker;", "originalCommit": "e77a7081b87d3e8bd67a469b317a2a18db676a81", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\nindex 782b84d4f1..42acb75784 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/execution/PrestoSparkRowBatch.java\n\n@@ -217,91 +171,13 @@ public class PrestoSparkRowBatch\n         public PrestoSparkRowBatch build()\n         {\n             checkState(!openEntry, \"entry must be closed before creating a row batch\");\n-\n-            if (rowCount == 0) {\n-                return createDirectRowBatch();\n-            }\n-\n-            int averageRowSize = totalSizeInBytes / rowCount;\n-            if (averageRowSize < targetAverageRowSizeInBytes) {\n-                return createGroupedRowBatch();\n-            }\n-\n-            return createDirectRowBatch();\n-        }\n-\n-        private PrestoSparkRowBatch createDirectRowBatch()\n-        {\n             return new PrestoSparkRowBatch(\n                     partitionCount,\n                     rowCount,\n                     sliceOutput.getUnderlyingSlice().byteArray(),\n                     rowPartitions,\n                     rowOffsets,\n-                    totalSizeInBytes);\n-        }\n-\n-        private PrestoSparkRowBatch createGroupedRowBatch()\n-        {\n-            RowIndex rowIndex = RowIndex.create(rowCount, partitionCount, rowPartitions);\n-            byte[] data = sliceOutput.getUnderlyingSlice().byteArray();\n-\n-            DynamicSliceOutput output = new DynamicSliceOutput((int) (totalSizeInBytes * 1.2f));\n-            int expectedEntriesCount = (int) ((totalSizeInBytes / targetAverageRowSizeInBytes) * 1.2f);\n-            int[] entryOffsets = new int[expectedEntriesCount];\n-            int[] entryPartitions = new int[expectedEntriesCount];\n-            int entriesCount = 0;\n-            for (int partition = REPLICATED_ROW_PARTITION_ID; partition < partitionCount; partition++) {\n-                while (rowIndex.hasNextRow(partition)) {\n-                    // start entry\n-                    short currentEntrySize = 0;\n-                    short currentEntryRowCount = 0;\n-                    int currentEntryOffset = output.size();\n-                    // Reserve space for the row count, the actual row count will be set later\n-                    output.writeShort(0);\n-\n-                    entryOffsets = ensureCapacity(entryOffsets, entriesCount + 1);\n-                    entryOffsets[entriesCount] = currentEntryOffset;\n-                    entryPartitions = ensureCapacity(entryPartitions, entriesCount + 1);\n-                    entryPartitions[entriesCount] = partition;\n-\n-                    while (rowIndex.hasNextRow(partition)) {\n-                        int row = rowIndex.peekRow(partition);\n-                        int followingRow = row + 1;\n-                        int rowOffset = rowOffsets[row];\n-                        int followingRowOffset = followingRow < rowCount ? rowOffsets[followingRow] : totalSizeInBytes;\n-                        int rowSize = followingRowOffset - rowOffset;\n-                        // each row entry should contain at least a marker;\n-                        verify(rowSize >= 1, \"rowSize is expected to be greater than or equal to zero: %s\", rowSize);\n-\n-                        // skip the rows count\n-                        rowOffset += 2;\n-                        rowSize -= 2;\n-\n-                        if (currentEntryRowCount > 0 && (currentEntrySize + rowSize > maxEntrySizeInBytes || currentEntryRowCount + 1 > maxRowsPerEntry)) {\n-                            break;\n-                        }\n-\n-                        output.writeBytes(data, rowOffset, rowSize);\n-                        currentEntrySize += rowSize;\n-                        currentEntryRowCount++;\n-\n-                        rowIndex.nextRow(partition);\n-                    }\n-\n-                    // entry is done\n-                    output.getUnderlyingSlice().setShort(currentEntryOffset, currentEntryRowCount);\n-                    entriesCount++;\n-                }\n-            }\n-\n-            return new PrestoSparkRowBatch(\n-                    partitionCount,\n-                    entriesCount,\n-                    output.getUnderlyingSlice().byteArray(),\n-                    entryPartitions,\n-                    entryOffsets,\n-                    output.size());\n+                    totalSize);\n         }\n     }\n \n"}}, {"oid": "6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "url": "https://github.com/prestodb/presto/commit/6c35eeee2c5a000c7f0f6770aca24812b5c3239a", "message": "Use offset instead of size in PrestoSparkRowBatch", "committedDate": "2020-08-13T02:17:11Z", "type": "commit"}, {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-13T02:18:51Z", "type": "commit"}, {"oid": "0d8ba5cc581f395de9ab2301e627ede08c239ca1", "url": "https://github.com/prestodb/presto/commit/0d8ba5cc581f395de9ab2301e627ede08c239ca1", "message": "Implement batching of tiny rows for Presto on Spark", "committedDate": "2020-08-13T02:18:51Z", "type": "forcePushed"}]}