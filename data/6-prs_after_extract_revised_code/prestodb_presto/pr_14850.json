{"pr_number": 14850, "pr_title": "Distribute splits to presto on spark tasks based on size", "pr_createdAt": "2020-07-16T22:30:38Z", "pr_url": "https://github.com/prestodb/presto/pull/14850", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUzNTkyNA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456535924", "bodyText": "nit: use OptionalLong", "author": "wenleix", "createdAt": "2020-07-17T16:06:06Z", "path": "presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java", "diffHunk": "@@ -282,6 +282,12 @@ public Object getInfo()\n                 .build();\n     }\n \n+    @Override\n+    public Optional<Long> getSplitSizeInBytes()", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java b/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java\nindex 86d43a2766..08d331ca06 100644\n--- a/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java\n+++ b/presto-hive/src/main/java/com/facebook/presto/hive/HiveSplit.java\n\n@@ -283,9 +284,9 @@ public class HiveSplit\n     }\n \n     @Override\n-    public Optional<Long> getSplitSizeInBytes()\n+    public OptionalLong getSplitSizeInBytes()\n     {\n-        return Optional.of(getLength());\n+        return OptionalLong.of(getLength());\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTEyOA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585128", "bodyText": "Let's return Optional.empty() here.", "author": "arhimondr", "createdAt": "2020-07-17T17:44:58Z", "path": "presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java", "diffHunk": "@@ -37,4 +38,9 @@\n     List<HostAddress> getPreferredNodes(List<HostAddress> sortedCandidates);\n \n     Object getInfo();\n+\n+    default Optional<Long> getSplitSizeInBytes()\n+    {\n+        throw new UnsupportedOperationException();", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java\nindex 33f2124cd0..b974a9563a 100644\n--- a/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java\n+++ b/presto-spi/src/main/java/com/facebook/presto/spi/ConnectorSplit.java\n\n@@ -39,8 +39,8 @@ public interface ConnectorSplit\n \n     Object getInfo();\n \n-    default Optional<Long> getSplitSizeInBytes()\n+    default OptionalLong getSplitSizeInBytes()\n     {\n-        throw new UnsupportedOperationException();\n+        return OptionalLong.empty();\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTQ5Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585493", "bodyText": "add checkArgument(minSplitsPerSparkPartition > 0 , \"minSplitsPerSparkPartition must be greater than zero: %s\", minSplitsPerSparkPartition)", "author": "arhimondr", "createdAt": "2020-07-17T17:45:37Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -387,10 +389,12 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        boolean splitSizeMissing = splits.stream()\n-                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        if (splitSizeMissing) {\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n             int taskCount = getSparkInitialPartitionCount(session);\n             checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n             for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NTgzOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456585839", "bodyText": "ditto here", "author": "arhimondr", "createdAt": "2020-07-17T17:46:19Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -387,10 +389,12 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        boolean splitSizeMissing = splits.stream()\n-                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        if (splitSizeMissing) {\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n             int taskCount = getSparkInitialPartitionCount(session);\n             checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n             for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjIyMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586221", "bodyText": "nit: move it inside the loop", "author": "arhimondr", "createdAt": "2020-07-17T17:47:03Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +387,44 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        boolean splitSizeMissing = splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+        if (splitSizeMissing) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % taskCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n+            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n+\n+            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().get())\n+                    .sum();\n+            int taskCountBySplitsDataSizePerPartition = max(1, toIntExact(totalSizeInBytes / estimatedSizeInBytesPerPartition));\n+\n+            int taskCount = min(taskCountBySplitsPerPartition, taskCountBySplitsDataSizePerPartition);\n+\n+            PriorityQueue<SparkPartition> pq = new PriorityQueue();\n+            int partitionId;", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -387,10 +389,12 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        boolean splitSizeMissing = splits.stream()\n-                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        if (splitSizeMissing) {\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n             int taskCount = getSparkInitialPartitionCount(session);\n             checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n             for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NjM2NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586364", "bodyText": "maybe assignSplit?", "author": "arhimondr", "createdAt": "2020-07-17T17:47:21Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -519,23 +527,17 @@ public class PrestoSparkRddFactory\n     private static class SparkPartition\n             implements Comparable<SparkPartition>\n     {\n-        private final int maxSplitSlot;\n         private final int partitionId;\n-        private int usedSplitSlot;\n         private long splitsSizeInBytes;\n \n-        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        public SparkPartition(int partitionId)\n         {\n             this.partitionId = partitionId;\n-            this.maxSplitSlot = minSplitsPerSparkPartition;\n         }\n \n         @Override\n         public int compareTo(SparkPartition o)\n         {\n-            if (usedSplitSlot != o.usedSplitSlot) {\n-                return usedSplitSlot - o.usedSplitSlot;\n-            }\n             return splitsSizeInBytes == o.splitsSizeInBytes ?\n                     0 :\n                     splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456586940", "bodyText": "I would simply sort it by size. We should always add a next split to the partition that has the least data assigned (not the least number of splits).", "author": "arhimondr", "createdAt": "2020-07-17T17:48:30Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcyOTM5MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456729390", "bodyText": "Make sense,  spark.min-splits-per-partition is only used to determine total number of spark task:  avoid spawn new task that has few splits.  In most cases,  spark task count should be determined by data size per task.", "author": "viczhang861", "createdAt": "2020-07-18T01:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDMzOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660339", "bodyText": "simply this. splitsSizeInBytes  - that.splitsSizeInBytes", "author": "arhimondr", "createdAt": "2020-07-20T20:04:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjM0NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802344", "bodyText": "This avoids overflow when converting Long to Int.", "author": "viczhang861", "createdAt": "2020-07-21T02:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4Njk0MA=="}], "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -519,23 +527,17 @@ public class PrestoSparkRddFactory\n     private static class SparkPartition\n             implements Comparable<SparkPartition>\n     {\n-        private final int maxSplitSlot;\n         private final int partitionId;\n-        private int usedSplitSlot;\n         private long splitsSizeInBytes;\n \n-        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        public SparkPartition(int partitionId)\n         {\n             this.partitionId = partitionId;\n-            this.maxSplitSlot = minSplitsPerSparkPartition;\n         }\n \n         @Override\n         public int compareTo(SparkPartition o)\n         {\n-            if (usedSplitSlot != o.usedSplitSlot) {\n-                return usedSplitSlot - o.usedSplitSlot;\n-            }\n             return splitsSizeInBytes == o.splitsSizeInBytes ?\n                     0 :\n                     splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4NzI0MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r456587241", "bodyText": "splitSize is not available", "author": "arhimondr", "createdAt": "2020-07-17T17:49:05Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +515,42 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int maxSplitSlot;\n+        private final int partitionId;\n+        private int usedSplitSlot;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        {\n+            this.partitionId = partitionId;\n+            this.maxSplitSlot = minSplitsPerSparkPartition;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            if (usedSplitSlot != o.usedSplitSlot) {\n+                return usedSplitSlot - o.usedSplitSlot;\n+            }\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void addSplit(ScheduledSplit scheduledSplit)\n+        {\n+            usedSplitSlot = min(usedSplitSlot + 1, maxSplitSlot);\n+            Optional<Long> splitSize = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSize.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes not available\"));", "originalCommit": "ca11baba29aef6b8eaa743aa4369e64083a5c202", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 652627313e..71be5272ec 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -519,23 +527,17 @@ public class PrestoSparkRddFactory\n     private static class SparkPartition\n             implements Comparable<SparkPartition>\n     {\n-        private final int maxSplitSlot;\n         private final int partitionId;\n-        private int usedSplitSlot;\n         private long splitsSizeInBytes;\n \n-        public SparkPartition(int partitionId, int minSplitsPerSparkPartition)\n+        public SparkPartition(int partitionId)\n         {\n             this.partitionId = partitionId;\n-            this.maxSplitSlot = minSplitsPerSparkPartition;\n         }\n \n         @Override\n         public int compareTo(SparkPartition o)\n         {\n-            if (usedSplitSlot != o.usedSplitSlot) {\n-                return usedSplitSlot - o.usedSplitSlot;\n-            }\n             return splitsSizeInBytes == o.splitsSizeInBytes ?\n                     0 :\n                     splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n"}}, {"oid": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "url": "https://github.com/prestodb/presto/commit/c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size.", "committedDate": "2020-07-18T00:52:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMTEyNg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457511126", "bodyText": "nit: should this be allMatch? -- Since if some split has size, some other splits don't, we still cannot do auto tune right?", "author": "wenleix", "createdAt": "2020-07-20T15:47:52Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());", "originalCommit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU2Nzc3Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457567773", "bodyText": "I removed double negation to improve readability.", "author": "viczhang861", "createdAt": "2020-07-20T17:16:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMTEyNg=="}], "type": "inlineReview", "revised_code": {"commit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 71be5272ec..47f8ec0c25 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -389,17 +389,16 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n+        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n         boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n-        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n-                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        autoTunePartitionCount = autoTunePartitionCount && splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+\n         if (!autoTunePartitionCount) {\n-            int taskCount = getSparkInitialPartitionCount(session);\n+            taskCount = getSparkInitialPartitionCount(session);\n             checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n-            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                result.put(splitIndex % taskCount, splits.get(splitIndex));\n-            }\n         }\n         else {\n             int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxMjAyNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457512025", "bodyText": "Even the partition count is not decided automatically, we can still use the greedy algorithm to have a balanced split assignment.", "author": "wenleix", "createdAt": "2020-07-20T15:48:59Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,11 +389,50 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n+                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!autoTunePartitionCount) {\n+            int taskCount = getSparkInitialPartitionCount(session);\n+            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {", "originalCommit": "c41b38f3f5514d35a22f16b3fbe429ca7f7224a7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 71be5272ec..47f8ec0c25 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -389,17 +389,16 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n+        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n         boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n-        autoTunePartitionCount = autoTunePartitionCount && !splits.stream()\n-                .anyMatch(split -> !split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        autoTunePartitionCount = autoTunePartitionCount && splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+\n         if (!autoTunePartitionCount) {\n-            int taskCount = getSparkInitialPartitionCount(session);\n+            taskCount = getSparkInitialPartitionCount(session);\n             checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n-            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                result.put(splitIndex % taskCount, splits.get(splitIndex));\n-            }\n         }\n         else {\n             int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457516151", "bodyText": "Curious: Will this be the total number of physical cores on the Spark workers (instead of Spark container CPUs? ). (E.g. the worker has 16 cores, but the Spark container only has 4 CPUs, and this will set minSplitsPerSparkPartition to 16? )", "author": "wenleix", "createdAt": "2020-07-20T15:53:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;\n     private int initialSparkPartitionCount = 16;\n+    private int minSplitsPerSparkPartition = Runtime.getRuntime().availableProcessors();", "originalCommit": "2303a390bd13591864af0cc311277b407833e532", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU2ODU0Ng==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457568546", "bodyText": "@arhimondr Do you know the answer?  I can run some test to verify.", "author": "viczhang861", "createdAt": "2020-07-20T17:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MTIxMg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457661212", "bodyText": "Yeah, I think this default doesn't make a lot of sense.\nI think it is even better to do not set any defaults and keep the autotune disabled. Then it can be manually enabled and the correct settings will be set.", "author": "arhimondr", "createdAt": "2020-07-20T20:06:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzUxNjE1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\nindex 93cad43804..9e02e61106 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\n\n@@ -21,21 +21,20 @@ import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n-    private boolean autoTuneSparkPartitionCount = true;\n+    private boolean sparkInitialPartitionCountAutoTuneEnabled = true;\n     private int initialSparkPartitionCount = 16;\n-    private int minSplitsPerSparkPartition = Runtime.getRuntime().availableProcessors();\n-    private DataSize splitsDataSizePerSparkPartition = new DataSize(2, GIGABYTE);\n+    private DataSize maxSplitsDataSizePerSparkPartition = new DataSize(2, GIGABYTE);\n \n-    public boolean isAutoTuneSparkPartitionCount()\n+    public boolean isSparkInitialPartitionCountAutoTuneEnabled()\n     {\n-        return autoTuneSparkPartitionCount;\n+        return sparkInitialPartitionCountAutoTuneEnabled;\n     }\n \n-    @Config(\"spark.auto-tune-partition-count\")\n-    @ConfigDescription(\"Automatic tuning of spark partition count based on splits size per partition\")\n-    public PrestoSparkConfig setAutoTuneSparkPartitionCount(boolean autoTuneSparkPartitionCount)\n+    @Config(\"spark.initial-partition-count-auto-tune-enabled\")\n+    @ConfigDescription(\"Automatic tuning of spark partition count based on max splits data size per partition\")\n+    public PrestoSparkConfig setSparkInitialPartitionCountAutoTuneEnabled(boolean sparkInitialPartitionCountAutoTuneEnabled)\n     {\n-        this.autoTuneSparkPartitionCount = autoTuneSparkPartitionCount;\n+        this.sparkInitialPartitionCountAutoTuneEnabled = sparkInitialPartitionCountAutoTuneEnabled;\n         return this;\n     }\n \n"}}, {"oid": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "url": "https://github.com/prestodb/presto/commit/7f267d86eb8656c8f852c931baffb0d0b00b8025", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split size.", "committedDate": "2020-07-20T17:14:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1ODUxOQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457658519", "bodyText": "maybe sparkInitialPartitionCountAutoTuneEnabled?\ncc: @wenleix", "author": "arhimondr", "createdAt": "2020-07-20T20:00:58Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java", "diffHunk": "@@ -15,10 +15,29 @@\n \n import com.facebook.airlift.configuration.Config;\n import com.facebook.airlift.configuration.ConfigDescription;\n+import io.airlift.units.DataSize;\n+\n+import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n+    private boolean autoTuneSparkPartitionCount = true;", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\nindex 93cad43804..9e02e61106 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/PrestoSparkConfig.java\n\n@@ -21,21 +21,20 @@ import static io.airlift.units.DataSize.Unit.GIGABYTE;\n \n public class PrestoSparkConfig\n {\n-    private boolean autoTuneSparkPartitionCount = true;\n+    private boolean sparkInitialPartitionCountAutoTuneEnabled = true;\n     private int initialSparkPartitionCount = 16;\n-    private int minSplitsPerSparkPartition = Runtime.getRuntime().availableProcessors();\n-    private DataSize splitsDataSizePerSparkPartition = new DataSize(2, GIGABYTE);\n+    private DataSize maxSplitsDataSizePerSparkPartition = new DataSize(2, GIGABYTE);\n \n-    public boolean isAutoTuneSparkPartitionCount()\n+    public boolean isSparkInitialPartitionCountAutoTuneEnabled()\n     {\n-        return autoTuneSparkPartitionCount;\n+        return sparkInitialPartitionCountAutoTuneEnabled;\n     }\n \n-    @Config(\"spark.auto-tune-partition-count\")\n-    @ConfigDescription(\"Automatic tuning of spark partition count based on splits size per partition\")\n-    public PrestoSparkConfig setAutoTuneSparkPartitionCount(boolean autoTuneSparkPartitionCount)\n+    @Config(\"spark.initial-partition-count-auto-tune-enabled\")\n+    @ConfigDescription(\"Automatic tuning of spark partition count based on max splits data size per partition\")\n+    public PrestoSparkConfig setSparkInitialPartitionCountAutoTuneEnabled(boolean sparkInitialPartitionCountAutoTuneEnabled)\n     {\n-        this.autoTuneSparkPartitionCount = autoTuneSparkPartitionCount;\n+        this.sparkInitialPartitionCountAutoTuneEnabled = sparkInitialPartitionCountAutoTuneEnabled;\n         return this;\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1OTQwNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457659405", "bodyText": "Let's extract this into a variable\nboolean splitsDataSizeAvailable  = splits.stream() .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent())\nThen simply boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session) && splitsDataSizeAvailable", "author": "arhimondr", "createdAt": "2020-07-20T20:02:49Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +389,51 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n+\n+        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n+        autoTunePartitionCount = autoTunePartitionCount && splits.stream()", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 47f8ec0c25..5691b3a446 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -389,49 +386,66 @@ public class PrestoSparkRddFactory\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount;\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n-        boolean autoTunePartitionCount = isAutoTuneSparkPartitionCount(session);\n-        autoTunePartitionCount = autoTunePartitionCount && splits.stream()\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n                 .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n \n-        if (!autoTunePartitionCount) {\n-            taskCount = getSparkInitialPartitionCount(session);\n-            checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n         }\n         else {\n-            int minSplitsPerSparkPartition = getMinSplitsPerSparkPartition(session);\n-            checkArgument(minSplitsPerSparkPartition > 0,\n-                    \"minSplitsPerSparkPartition must be greater than zero: %s\", minSplitsPerSparkPartition);\n-            int taskCountBySplitsPerPartition = max(1, splits.size() / minSplitsPerSparkPartition);\n-\n-            long estimatedSizeInBytesPerPartition = getSplitsDataSizePerSparkPartition(session).toBytes();\n-            checkArgument(estimatedSizeInBytesPerPartition > 0,\n-                    \"estimatedSizeInBytesPerPartition must be greater than zero: %s\", estimatedSizeInBytesPerPartition);\n             long totalSizeInBytes = splits.stream()\n                     .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n                     .sum();\n-            int taskCountBySplitsDataSizePerPartition = max(1, toIntExact(totalSizeInBytes / estimatedSizeInBytesPerPartition));\n-\n-            taskCount = min(taskCountBySplitsPerPartition, taskCountBySplitsDataSizePerPartition);\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n         }\n \n-        PriorityQueue<SparkPartition> pq = new PriorityQueue();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            int partitionId;\n-            int currentSize = pq.size();\n-            if (currentSize < taskCount) {\n-                partitionId = currentSize;\n-                pq.add(new SparkPartition(partitionId));\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n             }\n-            else {\n-                SparkPartition partition = pq.poll();\n-                partitionId = partition.getPartitionId();\n-                partition.assignSplit(splits.get(splitIndex));\n-                pq.add(partition);\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);\n+                        newPartition.assignSplitWithSize(splitSizeInBytes);\n+                    }\n+                }\n+\n+                result.put(partitionId, splits.get(splitIndex));\n             }\n-            result.put(partitionId, splits.get(splitIndex));\n         }\n \n         return result.build();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDYyNQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457660625", "bodyText": "This breakes the non auto tune path. It may start throwing if the split size is not available", "author": "arhimondr", "createdAt": "2020-07-20T20:05:11Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +523,35 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long splitsSizeInBytes;\n+\n+        public SparkPartition(int partitionId)\n+        {\n+            this.partitionId = partitionId;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+                    0 :\n+                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+        }\n+\n+        public int getPartitionId()\n+        {\n+            return partitionId;\n+        }\n+\n+        public void assignSplit(ScheduledSplit scheduledSplit)\n+        {\n+            OptionalLong splitSizeInBytes = scheduledSplit.getSplit().getConnectorSplit().getSplitSizeInBytes();\n+            splitsSizeInBytes += splitSizeInBytes.orElseThrow(() -> new IllegalArgumentException(\"splitSizeInBytes is not present\"));", "originalCommit": "7f267d86eb8656c8f852c931baffb0d0b00b8025", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2OTM5Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457669397", "bodyText": "Good catch ! Is it possible that some splits have size available and some don't, for this case, a combination of even distribution + random distribution will be used.", "author": "viczhang861", "createdAt": "2020-07-20T20:22:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY2MDYyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 47f8ec0c25..5691b3a446 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -528,19 +542,20 @@ public class PrestoSparkRddFactory\n             implements Comparable<SparkPartition>\n     {\n         private final int partitionId;\n-        private long splitsSizeInBytes;\n+        private long availableCapacityInBytes;\n \n-        public SparkPartition(int partitionId)\n+        public SparkPartition(int partitionId, long availableCapacityInBytes)\n         {\n             this.partitionId = partitionId;\n+            this.availableCapacityInBytes = availableCapacityInBytes;\n         }\n \n         @Override\n         public int compareTo(SparkPartition o)\n         {\n-            return splitsSizeInBytes == o.splitsSizeInBytes ?\n+            return availableCapacityInBytes == o.availableCapacityInBytes ?\n                     0 :\n-                    splitsSizeInBytes < o.splitsSizeInBytes ? -1 : 1;\n+                    availableCapacityInBytes > o.availableCapacityInBytes ? -1 : 1;\n         }\n \n         public int getPartitionId()\n"}}, {"oid": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "url": "https://github.com/prestodb/presto/commit/aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-21T02:18:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDE0OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800149", "bodyText": "(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition", "author": "arhimondr", "createdAt": "2020-07-21T02:33:22Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMDQwMw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457800403", "bodyText": "Can splits be immutable here?", "author": "arhimondr", "createdAt": "2020-07-21T02:34:14Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801177", "bodyText": "What if split is larger? Does it mean that we are not going to schedule it? We are going to break correctness.", "author": "arhimondr", "createdAt": "2020-07-21T02:37:04Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NzYyOA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547628", "bodyText": "This last line of code inside for loop guarantees each split will be assigned to a partition decided by variable partitionId.", "author": "viczhang861", "createdAt": "2020-07-22T05:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NTYxMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458845611", "bodyText": "Oh, nevermind. I missread", "author": "arhimondr", "createdAt": "2020-07-22T14:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0ODYyMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458848621", "bodyText": "So basically if the split size is larger than the split size per partition theres no need to add the partition to the queue, as it will get full after adding this split, right?", "author": "arhimondr", "createdAt": "2020-07-22T14:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTE3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801298", "bodyText": "Should this partition be placed into the queue?", "author": "arhimondr", "createdAt": "2020-07-21T02:37:29Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0Nzc5NA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547794", "bodyText": "Not needed, this partition cannot accept more splits.", "author": "viczhang861", "createdAt": "2020-07-22T05:44:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg0NzU2Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458847567", "bodyText": "Could you please elaborate how it works?\nfrom what I see in the upper branch we check if !queue.isEmpty(), so if queue is empty it will not enter the upper branch. But we never place anything into the queue outside of the upper branch?", "author": "arhimondr", "createdAt": "2020-07-22T14:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3ODg0MA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458978840", "bodyText": "With the redesign of original thought,  we removed min_splits_per_partition. The goal is now simple:   (1) Avoid skew by setting a max limit per partition. If a single split's data size exceeds this limit, the only thing we can do is to not add any more splits into this partition to make maximal partition size small.  (2) Make splits balanced, this is achieved by heuristic algorithm to always assign a split to partition with smallest size.\nWhat is partitionCount >= initialPartitionCount?\n-- initialPartitionCount is the lower bound of partitions (based on calculation total_size / max_partition_size), so if this is false, always create a new partition to accept any split\nWhat is !queue.isEmpty() && queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes\n-- If this condition is false,  there is no partition with enough capacity to accept this split, create a new partition.\nInvariance for partition in the queue\n-- This partition is not full and contains at least one split", "author": "viczhang861", "createdAt": "2020-07-22T17:56:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTI5OA=="}], "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTQ1Mw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801453", "bodyText": "This code is getting non trivial. I would strongly suggest adding a unit test.", "author": "arhimondr", "createdAt": "2020-07-21T02:38:05Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >\n+                            o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() ? -1 : 1);\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&\n+                        !queue.isEmpty() &&\n+                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getAvailableCapacityInBytes() > 0) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionCount++;\n+                    partitionId = partitionCount - 1;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n+                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);\n+                        newPartition.assignSplitWithSize(splitSizeInBytes);", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0Nzg2OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458547869", "bodyText": "Unit test added, repeated for 1000 times.", "author": "viczhang861", "createdAt": "2020-07-22T05:44:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTQ1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTU3MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801571", "bodyText": "why not simply this. availableCapacityInBytes  - that. availableCapacityInBytes ? (or the vice versa, depending on the order you are trying to achieve)", "author": "arhimondr", "createdAt": "2020-07-21T02:38:30Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -476,4 +537,40 @@ private static void checkInputs(\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n+\n+    private static class SparkPartition\n+            implements Comparable<SparkPartition>\n+    {\n+        private final int partitionId;\n+        private long availableCapacityInBytes;\n+\n+        public SparkPartition(int partitionId, long availableCapacityInBytes)\n+        {\n+            this.partitionId = partitionId;\n+            this.availableCapacityInBytes = availableCapacityInBytes;\n+        }\n+\n+        @Override\n+        public int compareTo(SparkPartition o)\n+        {\n+            return availableCapacityInBytes == o.availableCapacityInBytes ?", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjczMw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802733", "bodyText": "Nevermind. DIdn't realize we are dealing with a long.", "author": "arhimondr", "createdAt": "2020-07-21T02:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTU3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..c91679b6aa 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -537,40 +476,4 @@ public class PrestoSparkRddFactory\n     {\n         return scala.reflect.ClassTag$.MODULE$.apply(clazz);\n     }\n-\n-    private static class SparkPartition\n-            implements Comparable<SparkPartition>\n-    {\n-        private final int partitionId;\n-        private long availableCapacityInBytes;\n-\n-        public SparkPartition(int partitionId, long availableCapacityInBytes)\n-        {\n-            this.partitionId = partitionId;\n-            this.availableCapacityInBytes = availableCapacityInBytes;\n-        }\n-\n-        @Override\n-        public int compareTo(SparkPartition o)\n-        {\n-            return availableCapacityInBytes == o.availableCapacityInBytes ?\n-                    0 :\n-                    availableCapacityInBytes > o.availableCapacityInBytes ? -1 : 1;\n-        }\n-\n-        public int getPartitionId()\n-        {\n-            return partitionId;\n-        }\n-\n-        public void assignSplitWithSize(long splitSizeInBytes)\n-        {\n-            availableCapacityInBytes -= splitSizeInBytes;\n-        }\n-\n-        public long getAvailableCapacityInBytes()\n-        {\n-            return availableCapacityInBytes;\n-        }\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTg5OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457801898", "bodyText": "You can simply do o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() - o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() (or the vice versa, depending on the order you are trying to achieve)", "author": "arhimondr", "createdAt": "2020-07-21T02:39:36Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -381,12 +386,68 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n \n     private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact(totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1 / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) ->\n+                    o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong() >", "originalCommit": "aebf6afd6234085b5fcb0e6a48f3de690ba0825e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMjY3Nw==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r457802677", "bodyText": "Nevermind. DIdn't realize we are dealing with a long.", "author": "arhimondr", "createdAt": "2020-07-21T02:42:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwMTg5OA=="}], "type": "inlineReview", "revised_code": {"commit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex 5691b3a446..afce2dfb44 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -384,7 +385,8 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n \n"}}, {"oid": "fcf8a651fd84280b60aca8b777ef32c26217b986", "url": "https://github.com/prestodb/presto/commit/fcf8a651fd84280b60aca8b777ef32c26217b986", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-22T05:37:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MDU2OA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458850568", "bodyText": "You don't have to create a HiveSplit. You can create a private class MockSplit that only implement the getSplitSizeInBytes method", "author": "arhimondr", "createdAt": "2020-07-22T14:48:45Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "chunk": "diff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\nindex fbf6d96a60..5b2d9a63af 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n\n@@ -176,71 +148,4 @@ public class TestPrestoSparkAbstractTestQueries\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n-\n-    @Test\n-    public void testAssignSourceDistributionSplits()\n-    {\n-        int maxSplitsSize = 2048;\n-        Session session = Session.builder(getSession())\n-                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n-                .build();\n-\n-        List<ScheduledSplit> splits = new ArrayList<>();\n-        long totalSizeInBytes = 0;\n-\n-        for (int i = 0; i < 1000; ++i) {\n-            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n-            totalSizeInBytes += splitSizeInBytes;\n-\n-            HiveSplit hiveSplit = new HiveSplit(\n-                    \"test_schema\",\n-                    \"test_table\",\n-                    \"\",\n-                    \"path\",\n-                    0,\n-                    splitSizeInBytes,\n-                    splitSizeInBytes,\n-                    new Storage(\n-                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n-                            \"location\",\n-                            Optional.empty(),\n-                            false,\n-                            ImmutableMap.of(),\n-                            ImmutableMap.of()),\n-                    ImmutableList.of(),\n-                    ImmutableList.of(),\n-                    OptionalInt.empty(),\n-                    OptionalInt.empty(),\n-                    NO_PREFERENCE,\n-                    1,\n-                    ImmutableMap.of(),\n-                    Optional.empty(),\n-                    false,\n-                    Optional.empty(),\n-                    NO_CACHE_REQUIREMENT,\n-                    Optional.empty());\n-            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n-            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n-            splits.add(scheduledSplit);\n-        }\n-\n-        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n-        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n-            if (scheduledSplits.size() > 1) {\n-                long totalPartitionSize = scheduledSplits.stream()\n-                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                        .sum();\n-                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));\n-            }\n-            else {\n-                assertTrue(scheduledSplits.size() == 1, \"A partition should hold at least one split\");\n-            }\n-        });\n-\n-        long actualTotalSizeInBytes = assignedSplits.values().stream()\n-                .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                .sum();\n-\n-        assertEquals(totalSizeInBytes, actualTotalSizeInBytes);\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MTA1MQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458851051", "bodyText": "It doesn't feel like this algorithm is worth to be tested with a fuzz testing. Instead i would recommend adding a number of deterministic test cases to verify all possible corner cases of the algorithm.", "author": "arhimondr", "createdAt": "2020-07-22T14:49:20Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "chunk": "diff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\nindex fbf6d96a60..5b2d9a63af 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n\n@@ -176,71 +148,4 @@ public class TestPrestoSparkAbstractTestQueries\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n-\n-    @Test\n-    public void testAssignSourceDistributionSplits()\n-    {\n-        int maxSplitsSize = 2048;\n-        Session session = Session.builder(getSession())\n-                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n-                .build();\n-\n-        List<ScheduledSplit> splits = new ArrayList<>();\n-        long totalSizeInBytes = 0;\n-\n-        for (int i = 0; i < 1000; ++i) {\n-            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n-            totalSizeInBytes += splitSizeInBytes;\n-\n-            HiveSplit hiveSplit = new HiveSplit(\n-                    \"test_schema\",\n-                    \"test_table\",\n-                    \"\",\n-                    \"path\",\n-                    0,\n-                    splitSizeInBytes,\n-                    splitSizeInBytes,\n-                    new Storage(\n-                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n-                            \"location\",\n-                            Optional.empty(),\n-                            false,\n-                            ImmutableMap.of(),\n-                            ImmutableMap.of()),\n-                    ImmutableList.of(),\n-                    ImmutableList.of(),\n-                    OptionalInt.empty(),\n-                    OptionalInt.empty(),\n-                    NO_PREFERENCE,\n-                    1,\n-                    ImmutableMap.of(),\n-                    Optional.empty(),\n-                    false,\n-                    Optional.empty(),\n-                    NO_CACHE_REQUIREMENT,\n-                    Optional.empty());\n-            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n-            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n-            splits.add(scheduledSplit);\n-        }\n-\n-        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n-        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n-            if (scheduledSplits.size() > 1) {\n-                long totalPartitionSize = scheduledSplits.stream()\n-                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                        .sum();\n-                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));\n-            }\n-            else {\n-                assertTrue(scheduledSplits.size() == 1, \"A partition should hold at least one split\");\n-            }\n-        });\n-\n-        long actualTotalSizeInBytes = assignedSplits.values().stream()\n-                .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                .sum();\n-\n-        assertEquals(totalSizeInBytes, actualTotalSizeInBytes);\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODg1MjAyNg==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r458852026", "bodyText": "Instead of having very generic assertions I would recommend adding more concrete assertions", "author": "arhimondr", "createdAt": "2020-07-22T14:50:32Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +176,71 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        int maxSplitsSize = 2048;\n+        Session session = Session.builder(getSession())\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .build();\n+\n+        List<ScheduledSplit> splits = new ArrayList<>();\n+        long totalSizeInBytes = 0;\n+\n+        for (int i = 0; i < 1000; ++i) {\n+            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n+            totalSizeInBytes += splitSizeInBytes;\n+\n+            HiveSplit hiveSplit = new HiveSplit(\n+                    \"test_schema\",\n+                    \"test_table\",\n+                    \"\",\n+                    \"path\",\n+                    0,\n+                    splitSizeInBytes,\n+                    splitSizeInBytes,\n+                    new Storage(\n+                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n+                            \"location\",\n+                            Optional.empty(),\n+                            false,\n+                            ImmutableMap.of(),\n+                            ImmutableMap.of()),\n+                    ImmutableList.of(),\n+                    ImmutableList.of(),\n+                    OptionalInt.empty(),\n+                    OptionalInt.empty(),\n+                    NO_PREFERENCE,\n+                    1,\n+                    ImmutableMap.of(),\n+                    Optional.empty(),\n+                    false,\n+                    Optional.empty(),\n+                    NO_CACHE_REQUIREMENT,\n+                    Optional.empty());\n+            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n+            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n+            splits.add(scheduledSplit);\n+        }\n+\n+        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n+        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n+            if (scheduledSplits.size() > 1) {\n+                long totalPartitionSize = scheduledSplits.stream()\n+                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                        .sum();\n+                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "chunk": "diff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\nindex fbf6d96a60..5b2d9a63af 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n\n@@ -176,71 +148,4 @@ public class TestPrestoSparkAbstractTestQueries\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n-\n-    @Test\n-    public void testAssignSourceDistributionSplits()\n-    {\n-        int maxSplitsSize = 2048;\n-        Session session = Session.builder(getSession())\n-                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n-                .build();\n-\n-        List<ScheduledSplit> splits = new ArrayList<>();\n-        long totalSizeInBytes = 0;\n-\n-        for (int i = 0; i < 1000; ++i) {\n-            long splitSizeInBytes = ThreadLocalRandom.current().nextInt((int) (maxSplitsSize * 1.1));\n-            totalSizeInBytes += splitSizeInBytes;\n-\n-            HiveSplit hiveSplit = new HiveSplit(\n-                    \"test_schema\",\n-                    \"test_table\",\n-                    \"\",\n-                    \"path\",\n-                    0,\n-                    splitSizeInBytes,\n-                    splitSizeInBytes,\n-                    new Storage(\n-                            StorageFormat.create(\"serde\", \"input\", \"output\"),\n-                            \"location\",\n-                            Optional.empty(),\n-                            false,\n-                            ImmutableMap.of(),\n-                            ImmutableMap.of()),\n-                    ImmutableList.of(),\n-                    ImmutableList.of(),\n-                    OptionalInt.empty(),\n-                    OptionalInt.empty(),\n-                    NO_PREFERENCE,\n-                    1,\n-                    ImmutableMap.of(),\n-                    Optional.empty(),\n-                    false,\n-                    Optional.empty(),\n-                    NO_CACHE_REQUIREMENT,\n-                    Optional.empty());\n-            Split testSplit = new Split(new ConnectorId(\"test\"), TestingTransactionHandle.create(), hiveSplit);\n-            ScheduledSplit scheduledSplit = new ScheduledSplit(0, new PlanNodeId(\"source\"), testSplit);\n-            splits.add(scheduledSplit);\n-        }\n-\n-        SetMultimap<Integer, ScheduledSplit> assignedSplits = assignSourceDistributionSplits(session, splits);\n-        asMap(assignedSplits).forEach((partitionId, scheduledSplits) -> {\n-            if (scheduledSplits.size() > 1) {\n-                long totalPartitionSize = scheduledSplits.stream()\n-                        .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                        .sum();\n-                assertTrue(totalPartitionSize <= maxSplitsSize, format(\"Total size for splits in one partition should be less than %d\", maxSplitsSize));\n-            }\n-            else {\n-                assertTrue(scheduledSplits.size() == 1, \"A partition should hold at least one split\");\n-            }\n-        });\n-\n-        long actualTotalSizeInBytes = assignedSplits.values().stream()\n-                .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                .sum();\n-\n-        assertEquals(totalSizeInBytes, actualTotalSizeInBytes);\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTY3OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459059679", "bodyText": "So if we look at this if-statement, it's like this:\n    if (queue is not empty and some other cnoditions) {\n        Do something\n        Add the partition into the queue\n    }\n    else {\n         // queue is empty\n         Do something, but doesn't append anything into the queue\n    }\n\nAs the queue is empty at the beginning, it looks like nothing will be added into the queue. Is there anything I am missing in the code flow? ~ -- or maybe just debug through it to see if the \"else\" branch will ever get triggered?", "author": "wenleix", "createdAt": "2020-07-22T20:21:16Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +385,72 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n+\n+        int initialPartitionCount;\n+        if (!autoTuneInitialPartitionCount) {\n+            initialPartitionCount = getSparkInitialPartitionCount(session);\n+            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+        }\n+        else {\n+            long totalSizeInBytes = splits.stream()\n+                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n+                    .sum();\n+            initialPartitionCount = max(1, toIntExact((totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition));\n+        }\n+\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n+                long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+                return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n+            });\n+\n+            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+            int partitionCount = 0;\n+\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount >= initialPartitionCount &&", "originalCommit": "fcf8a651fd84280b60aca8b777ef32c26217b986", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzYzMQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459167631", "bodyText": "Yes, I forgot to add it back to the queue, as a result every split is assigned to a new partition.  Unit test is updated with explicit examples to cover this case.", "author": "viczhang861", "createdAt": "2020-07-23T01:13:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTA1OTY3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex afce2dfb44..c91679b6aa 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -385,72 +379,14 @@ public class PrestoSparkRddFactory\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    @VisibleForTesting\n-    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n+        int taskCount = getSparkInitialPartitionCount(session);\n+        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-\n-        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n-        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n-                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n-\n-        boolean splitsDataSizeAvailable = splits.stream()\n-                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n-        boolean autoTuneInitialPartitionCount = isSparkInitialPartitionCountAutoTuneEnabled(session) && splitsDataSizeAvailable;\n-\n-        int initialPartitionCount;\n-        if (!autoTuneInitialPartitionCount) {\n-            initialPartitionCount = getSparkInitialPartitionCount(session);\n-            checkArgument(initialPartitionCount > 0, \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n-        }\n-        else {\n-            long totalSizeInBytes = splits.stream()\n-                    .mapToLong(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong())\n-                    .sum();\n-            initialPartitionCount = max(1, toIntExact((totalSizeInBytes + maxSplitsSizeInBytesPerPartition - 1) / maxSplitsSizeInBytesPerPartition));\n-        }\n-\n-        if (!splitsDataSizeAvailable) {\n-            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n-            }\n-        }\n-        else {\n-            splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n-                long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-                long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-                return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n-            });\n-\n-            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n-            int partitionCount = 0;\n-\n-            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                int partitionId;\n-                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-\n-                if (partitionCount >= initialPartitionCount &&\n-                        !queue.isEmpty() &&\n-                        queue.peek().getAvailableCapacityInBytes() >= splitSizeInBytes) {\n-                    SparkPartition partition = queue.poll();\n-                    partitionId = partition.getPartitionId();\n-                    partition.assignSplitWithSize(splitSizeInBytes);\n-                    if (partition.getAvailableCapacityInBytes() > 0) {\n-                        queue.add(partition);\n-                    }\n-                }\n-                else {\n-                    partitionId = partitionCount++;\n-                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n-                        SparkPartition newPartition = new SparkPartition(partitionId, maxSplitsSizeInBytesPerPartition);\n-                        newPartition.assignSplitWithSize(splitSizeInBytes);\n-                    }\n-                }\n-\n-                result.put(partitionId, splits.get(splitIndex));\n-            }\n+        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+            result.put(splitIndex % taskCount, splits.get(splitIndex));\n         }\n-\n         return result.build();\n     }\n \n"}}, {"oid": "bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "url": "https://github.com/prestodb/presto/commit/bac6dd0ee3bb3c2627f63d6d739b16bea0e3d01f", "message": "Add properties for Presto-on-Spark split distribution", "committedDate": "2020-07-22T23:27:13Z", "type": "commit"}, {"oid": "cb194a543593c4d344f68d2325672c972e99b30c", "url": "https://github.com/prestodb/presto/commit/cb194a543593c4d344f68d2325672c972e99b30c", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T01:05:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTM5OQ==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459181399", "bodyText": "nit: Let's simply return result.build(); instead of adding one more level of indentation", "author": "arhimondr", "createdAt": "2020-07-23T02:15:47Z", "path": "presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java", "diffHunk": "@@ -379,14 +383,85 @@ private PrestoSparkTaskSourceRdd createTaskSourcesRdd(\n         return assignPartitionedSplits(session, partitioning, splits);\n     }\n \n-    private static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n+    @VisibleForTesting\n+    public static SetMultimap<Integer, ScheduledSplit> assignSourceDistributionSplits(Session session, List<ScheduledSplit> splits)\n     {\n-        int taskCount = getSparkInitialPartitionCount(session);\n-        checkArgument(taskCount > 0, \"taskCount must be greater then zero: %s\", taskCount);\n         ImmutableSetMultimap.Builder<Integer, ScheduledSplit> result = ImmutableSetMultimap.builder();\n-        for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-            result.put(splitIndex % taskCount, splits.get(splitIndex));\n+\n+        long maxSplitsSizeInBytesPerPartition = getMaxSplitsDataSizePerSparkPartition(session).toBytes();\n+        checkArgument(maxSplitsSizeInBytesPerPartition > 0,\n+                \"maxSplitsSizeInBytesPerPartition must be greater than zero: %s\", maxSplitsSizeInBytesPerPartition);\n+        int initialPartitionCount = getSparkInitialPartitionCount(session);\n+        checkArgument(initialPartitionCount > 0,\n+                \"initialPartitionCount must be greater then zero: %s\", initialPartitionCount);\n+\n+        boolean splitsDataSizeAvailable = splits.stream()\n+                .allMatch(split -> split.getSplit().getConnectorSplit().getSplitSizeInBytes().isPresent());\n+        if (!splitsDataSizeAvailable) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n+            }\n+        }\n+        else {", "originalCommit": "cb194a543593c4d344f68d2325672c972e99b30c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "chunk": "diff --git a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\nindex fabeb52459..83e572b5da 100644\n--- a/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n+++ b/presto-spark-base/src/main/java/com/facebook/presto/spark/planner/PrestoSparkRddFactory.java\n\n@@ -401,64 +401,64 @@ public class PrestoSparkRddFactory\n             for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n                 result.put(splitIndex % initialPartitionCount, splits.get(splitIndex));\n             }\n+            return result.build();\n         }\n-        else {\n-            splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n-                long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-                long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-                return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n-            });\n-\n-            PriorityQueue<SparkPartition> queue = new PriorityQueue();\n-            int partitionCount = 0;\n-            boolean autoTunePartitionCount = isSparkPartitionCountAutoTuneEnabled(session);\n-            if (autoTunePartitionCount) {\n-                for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                    int partitionId;\n-                    long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n-\n-                    if (!queue.isEmpty() &&\n-                            queue.peek().getSplitsInBytes() + splitSizeInBytes <= maxSplitsSizeInBytesPerPartition) {\n-                        SparkPartition partition = queue.poll();\n-                        partitionId = partition.getPartitionId();\n-                        partition.assignSplitWithSize(splitSizeInBytes);\n-                        if (partition.getSplitsInBytes() < maxSplitsSizeInBytesPerPartition) {\n-                            queue.add(partition);\n-                        }\n-                    }\n-                    else {\n-                        partitionId = partitionCount++;\n-                        if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n-                            SparkPartition newPartition = new SparkPartition(partitionId);\n-                            newPartition.assignSplitWithSize(splitSizeInBytes);\n-                            queue.add(newPartition);\n-                        }\n-                    }\n \n-                    result.put(partitionId, splits.get(splitIndex));\n-                }\n-            }\n-            else {\n-                // partition count is fixed\n-                for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n-                    int partitionId;\n-                    long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+        splits.sort((ScheduledSplit o1, ScheduledSplit o2) -> {\n+            long size1 = o1.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+            long size2 = o2.getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+            return size1 == size2 ? 0 : size1 > size2 ? -1 : 1;\n+        });\n \n-                    if (partitionCount < initialPartitionCount) {\n-                        partitionId = partitionCount++;\n+        PriorityQueue<SparkPartition> queue = new PriorityQueue();\n+        int partitionCount = 0;\n+        boolean autoTunePartitionCount = isSparkPartitionCountAutoTuneEnabled(session);\n+        if (autoTunePartitionCount) {\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (!queue.isEmpty() &&\n+                        queue.peek().getSplitsInBytes() + splitSizeInBytes <= maxSplitsSizeInBytesPerPartition) {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    if (partition.getSplitsInBytes() < maxSplitsSizeInBytesPerPartition) {\n+                        queue.add(partition);\n+                    }\n+                }\n+                else {\n+                    partitionId = partitionCount++;\n+                    if (splitSizeInBytes < maxSplitsSizeInBytesPerPartition) {\n                         SparkPartition newPartition = new SparkPartition(partitionId);\n                         newPartition.assignSplitWithSize(splitSizeInBytes);\n                         queue.add(newPartition);\n                     }\n-                    else {\n-                        SparkPartition partition = queue.poll();\n-                        partitionId = partition.getPartitionId();\n-                        partition.assignSplitWithSize(splitSizeInBytes);\n-                        queue.add(partition);\n-                    }\n+                }\n \n-                    result.put(partitionId, splits.get(splitIndex));\n+                result.put(partitionId, splits.get(splitIndex));\n+            }\n+        }\n+        else {\n+            // partition count is fixed\n+            for (int splitIndex = 0; splitIndex < splits.size(); splitIndex++) {\n+                int partitionId;\n+                long splitSizeInBytes = splits.get(splitIndex).getSplit().getConnectorSplit().getSplitSizeInBytes().getAsLong();\n+\n+                if (partitionCount < initialPartitionCount) {\n+                    partitionId = partitionCount++;\n+                    SparkPartition newPartition = new SparkPartition(partitionId);\n+                    newPartition.assignSplitWithSize(splitSizeInBytes);\n+                    queue.add(newPartition);\n                 }\n+                else {\n+                    SparkPartition partition = queue.poll();\n+                    partitionId = partition.getPartitionId();\n+                    partition.assignSplitWithSize(splitSizeInBytes);\n+                    queue.add(partition);\n+                }\n+\n+                result.put(partitionId, splits.get(splitIndex));\n             }\n         }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjgwNA==", "url": "https://github.com/prestodb/presto/pull/14850#discussion_r459182804", "bodyText": "maxPartitionSize?", "author": "arhimondr", "createdAt": "2020-07-23T02:21:46Z", "path": "presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java", "diffHunk": "@@ -148,4 +178,158 @@ public void testExplainDdl()\n     {\n         // DDL statements are not supported by Presto on Spark\n     }\n+\n+    @Test\n+    public void testAssignSourceDistributionSplits()\n+    {\n+        // black box test\n+        testAssignSplitsToPartitionWithRandomSplitsSize(3);\n+\n+        // auto tune partition + splits with mixed size\n+        List<Long> testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L));\n+        Collections.shuffle(testSplitsSize);\n+        Map<Integer, List<Long>> actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        Map<Integer, List<Long>> expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(11L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(10L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(9L)));\n+        expectedResult.put(3, new ArrayList(Arrays.asList(8L, 1L)));\n+        expectedResult.put(4, new ArrayList(Arrays.asList(7L, 2L)));\n+        expectedResult.put(5, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(6, new ArrayList(Arrays.asList(5L, 4L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // enable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(true, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 3L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 4L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(2L, 1L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + small splits\n+        testSplitsSize = new ArrayList(Arrays.asList(1L, 2L, 3L, 4L, 5L, 6L));\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(6L, 1L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(5L, 2L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(4L, 3L)));\n+        assertEquals(actualResult, expectedResult);\n+\n+        // disable auto tune partition + large splits\n+        testSplitsSize = new ArrayList(Arrays.asList(5L, 6L, 7L, 8L, 9L, 10L));\n+        Collections.shuffle(testSplitsSize);\n+        actualResult = assignSplitsToPartition(false, 10, testSplitsSize);\n+\n+        expectedResult = new HashMap<>();\n+        expectedResult.put(0, new ArrayList(Arrays.asList(10L, 5L)));\n+        expectedResult.put(1, new ArrayList(Arrays.asList(9L, 6L)));\n+        expectedResult.put(2, new ArrayList(Arrays.asList(8L, 7L)));\n+        assertEquals(actualResult, expectedResult);\n+    }\n+\n+    private void testAssignSplitsToPartitionWithRandomSplitsSize(int repeatedTimes)\n+    {\n+        int maxSplitSizeInBytes = 2048;\n+        for (int i = 0; i < repeatedTimes; ++i) {\n+            List<Long> splitsSize = new ArrayList<>(1000);\n+            for (int j = 0; j < splitsSize.size(); j++) {\n+                splitsSize.set(j, ThreadLocalRandom.current().nextLong((long) (maxSplitSizeInBytes * 1.2)));\n+            }\n+            assignSplitsToPartition(true, maxSplitSizeInBytes, splitsSize);\n+            assignSplitsToPartition(false, maxSplitSizeInBytes, splitsSize);\n+        }\n+    }\n+\n+    private Map<Integer, List<Long>> assignSplitsToPartition(\n+            boolean autoTunePartitionCount,\n+            long maxSplitsSize,", "originalCommit": "cb194a543593c4d344f68d2325672c972e99b30c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "chunk": "diff --git a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\nindex d5446dc292..42e2cb75c8 100644\n--- a/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n+++ b/presto-spark-base/src/test/java/com/facebook/presto/spark/TestPrestoSparkAbstractTestQueries.java\n\n@@ -248,13 +248,13 @@ public class TestPrestoSparkAbstractTestQueries\n \n     private Map<Integer, List<Long>> assignSplitsToPartition(\n             boolean autoTunePartitionCount,\n-            long maxSplitsSize,\n+            long maxPartitionSize,\n             List<Long> splitsSize)\n     {\n         Session session = Session.builder(getSession())\n                 .setSystemProperty(SPARK_PARTITION_COUNT_AUTO_TUNE_ENABLED, Boolean.toString(autoTunePartitionCount))\n                 .setSystemProperty(SPARK_INITIAL_PARTITION_COUNT, \"3\")\n-                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxSplitsSize + \"B\")\n+                .setSystemProperty(MAX_SPLITS_DATA_SIZE_PER_SPARK_PARTITION, maxPartitionSize + \"B\")\n                 .build();\n \n         List<ScheduledSplit> splits = new ArrayList<>();\n"}}, {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T03:21:10Z", "type": "commit"}, {"oid": "508b1f86bac2451d7dd8be16de975b3415e2bf67", "url": "https://github.com/prestodb/presto/commit/508b1f86bac2451d7dd8be16de975b3415e2bf67", "message": "Add getSplitSizeInBytes in SPI ConnectorSplit\n\nPresto on Spark assigns splits to tasks based on split data size.\nThe total data size assigned to one split is controlled by session\nproperty \"max_splits_data_size_per_spark_partition\" unless a single\nsplit's data size exceeds this limit.", "committedDate": "2020-07-23T03:21:10Z", "type": "forcePushed"}]}