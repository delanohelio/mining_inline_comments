{"pr_number": 15074, "pr_title": "Add support in parquet reader for reading TIMESTAMP_MICROS type.", "pr_createdAt": "2020-08-24T22:43:49Z", "pr_url": "https://github.com/prestodb/presto/pull/15074", "timeline": [{"oid": "37d508c1ebc58d962dbb7cf723c076f66eb10f01", "url": "https://github.com/prestodb/presto/commit/37d508c1ebc58d962dbb7cf723c076f66eb10f01", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-25T15:39:10Z", "type": "forcePushed"}, {"oid": "bb0334ccda1384147a4792b288e355f397dcc87c", "url": "https://github.com/prestodb/presto/commit/bb0334ccda1384147a4792b288e355f397dcc87c", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-25T15:40:36Z", "type": "forcePushed"}, {"oid": "a3db6a90416cd4ccc3093302384de44e63ee9613", "url": "https://github.com/prestodb/presto/commit/a3db6a90416cd4ccc3093302384de44e63ee9613", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-26T20:31:54Z", "type": "forcePushed"}, {"oid": "48acc2878bd7221789df13fdb5564c54b4ebb22c", "url": "https://github.com/prestodb/presto/commit/48acc2878bd7221789df13fdb5564c54b4ebb22c", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-28T16:33:25Z", "type": "forcePushed"}, {"oid": "8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "url": "https://github.com/prestodb/presto/commit/8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-28T20:29:01Z", "type": "commit"}, {"oid": "8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "url": "https://github.com/prestodb/presto/commit/8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "message": "Add support in parquet reader for reading TIMESTAMP_MICROS type.\n\nSummary:\nRight now presto always assumes TIMESTAMP_MILLIS as the OriginalType when int64 is used to\nrepresent timestamp type in the schema. This causes an issue.\n\nWhen we use createParquetPageSource we use our own type convertor and this does not have a check\nfor TIMESTAMP_MICROS and this can fail the query\n\nFix: In this change I am adding three ValueDecoders one for plain and one for RLE compression format and\none for the non batch reader. All of these are in 3 new classes so that during creation we\ninstantiate the class based on the OriginalType. One Alternate approach is to do this check inside\nLongValueDecoders but that would make this check inside the most critical path and can affect query\nperformance. The fix simply divides the micros seconds by 1000 to get the milliseconds and this should\nbe ok because in presto we anyway operate at the millisecond granularity. Note that the non batch reader\nalso checks for whether timezone is present in the data while the batch readers don't because that\ninformation is not available.\n\nFor testing, the ValueDecoders have their own unit test. Apart from that have also added a new test in\nParquetTester to check if timestamp stored as int64 with OriginalType TIMESTAMP_MICROS works.\n\nNote: We use hive parquet writer and schema definition as defined in the presto-hive package\nwhich is a shaded jar with a old version of hive. This packs an old version of parquet-mr that does not\nhave the TIMESTAMP_MICROS type. However, in the read path, we use the independent parquet dependency\nwhich is more recent and so we use the version where the new enum is available. If we upgrade all the tests\nto use the new version of parquet, we would not be testing parquet version written\nby the packaged hive writer. We would need a version of the test that would work with the new definitions.\nFor now I have done this only for the timestamp type to test TIMESTAMP_MICROS. As part of seperate change\nwe can make sure all the test are tested with latest parquet writer. This change uses ExampleParquetWriter\nwhich is supposed to be used only for demo or test purposes. We cannot use presto's parquet writer as well because\nthe one used in test always stored timestamp as millis and does not store any annotated type and using\nthat we will not be able to test our code.", "committedDate": "2020-08-28T20:29:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwODI2OQ==", "url": "https://github.com/prestodb/presto/pull/15074#discussion_r479808269", "bodyText": "static import TIMESTAMP_MICROS", "author": "zhenxiao", "createdAt": "2020-08-30T19:45:55Z", "path": "presto-parquet/src/main/java/com/facebook/presto/parquet/ParquetTypeUtils.java", "diffHunk": "@@ -327,4 +329,9 @@ public static String pushdownColumnNameForSubfield(Subfield subfield)\n         columnPath.addAll(nestedColumnPath(subfield));\n         return columnPath.build();\n     }\n+\n+    public static boolean isTimeStampMicrosType(ColumnDescriptor descriptor)\n+    {\n+        return OriginalType.TIMESTAMP_MICROS.equals(descriptor.getPrimitiveType().getOriginalType());", "originalCommit": "8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgwODMxNQ==", "url": "https://github.com/prestodb/presto/pull/15074#discussion_r479808315", "bodyText": "static import TIMESTAMP_MICROS", "author": "zhenxiao", "createdAt": "2020-08-30T19:46:37Z", "path": "presto-parquet/src/main/java/com/facebook/presto/parquet/ColumnReaderFactory.java", "diffHunk": "@@ -76,6 +83,9 @@ public static ColumnReader createReader(RichColumnDescriptor descriptor, boolean\n             case INT32:\n                 return createDecimalColumnReader(descriptor).orElse(new IntColumnReader(descriptor));\n             case INT64:\n+                if (OriginalType.TIMESTAMP_MICROS.equals(descriptor.getPrimitiveType().getOriginalType())) {", "originalCommit": "8ab8b1786cc16583c79742fe2163c68ec62ea3ba", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}