{"pr_number": 283, "pr_title": "Add checkpoint index retention for multi entity detector", "pr_createdAt": "2020-10-20T17:06:05Z", "pr_url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283", "timeline": [{"oid": "2f7f33970177394ca26117de54ffbcd8a54a4a49", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/2f7f33970177394ca26117de54ffbcd8a54a4a49", "message": "Add checkpoint index retention for multi entity detector", "committedDate": "2020-10-20T17:03:23Z", "type": "commit"}, {"oid": "2773a92369f2f7cdb703efd5bd1d81afd0645438", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/2773a92369f2f7cdb703efd5bd1d81afd0645438", "message": "run splotlessJavaApply to fix style issues", "committedDate": "2020-10-20T17:31:05Z", "type": "commit"}, {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/a819d8d062a87fa897fcc819cf7360c8feb20760", "message": "set refresh as true", "committedDate": "2020-10-21T02:45:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxMTA5Mw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518911093", "bodyText": "Will this work with FGAC? We have moved AD indices to system indices list. May not be able to delete directly.\nRefer to AnomalyResultTransportAction.java#L218", "author": "ylwu-amzn", "createdAt": "2020-11-06T17:53:05Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.stats.CommonStats;\n+import org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest;\n+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.reindex.DeleteByQueryAction;\n+import org.elasticsearch.index.reindex.DeleteByQueryRequest;\n+import org.elasticsearch.index.store.StoreStats;\n+\n+import com.amazon.opendistroforelasticsearch.ad.util.ClientUtil;\n+\n+/**\n+ * Clean up the old docs for indices.\n+ */\n+public class IndexCleanup {\n+    private static final Logger LOG = LogManager.getLogger(IndexCleanup.class);\n+\n+    private final Client client;\n+    private final ClientUtil clientUtil;\n+    private final ClusterService clusterService;\n+\n+    public IndexCleanup(Client client, ClientUtil clientUtil, ClusterService clusterService) {\n+        this.client = client;\n+        this.clientUtil = clientUtil;\n+        this.clusterService = clusterService;\n+    }\n+\n+    /**\n+     * delete docs when shard size is bigger than max limitation.\n+     * @param indexName index name\n+     * @param maxShardSize max shard size\n+     * @param queryForDeleteByQueryRequest query request\n+     * @param listener action listener\n+     */\n+    public void deleteDocsBasedOnShardSize(\n+        String indexName,\n+        long maxShardSize,\n+        QueryBuilder queryForDeleteByQueryRequest,\n+        ActionListener<Boolean> listener\n+    ) {\n+\n+        if (!clusterService.state().getRoutingTable().hasIndex(indexName)) {\n+            LOG.debug(\"skip as the index:{} doesn't exist\", indexName);\n+            return;\n+        }\n+\n+        ActionListener<IndicesStatsResponse> indicesStatsResponseListener = ActionListener.wrap(indicesStatsResponse -> {\n+            // Check if any shard size is bigger than maxShardSize\n+            boolean cleanupNeeded = Arrays\n+                .stream(indicesStatsResponse.getShards())\n+                .map(ShardStats::getStats)\n+                .filter(Objects::nonNull)\n+                .map(CommonStats::getStore)\n+                .filter(Objects::nonNull)\n+                .map(StoreStats::getSizeInBytes)\n+                .anyMatch(size -> size > maxShardSize);\n+\n+            if (cleanupNeeded) {\n+                deleteDocsByQuery(\n+                    indexName,\n+                    queryForDeleteByQueryRequest,\n+                    ActionListener.wrap(r -> listener.onResponse(true), listener::onFailure)\n+                );\n+            } else {\n+                listener.onResponse(false);\n+            }\n+        }, listener::onFailure);\n+\n+        getCheckpointShardStoreStats(indexName, indicesStatsResponseListener);\n+    }\n+\n+    private void getCheckpointShardStoreStats(String indexName, ActionListener<IndicesStatsResponse> listener) {\n+        IndicesStatsRequest indicesStatsRequest = new IndicesStatsRequest();\n+        indicesStatsRequest.store();\n+        indicesStatsRequest.indices(indexName);\n+        client.admin().indices().stats(indicesStatsRequest, listener);\n+    }\n+\n+    /**\n+     * Delete docs based on query request\n+     * @param indexName index name\n+     * @param queryForDeleteByQueryRequest query request\n+     * @param listener action listener\n+     */\n+    public void deleteDocsByQuery(String indexName, QueryBuilder queryForDeleteByQueryRequest, ActionListener<Long> listener) {\n+        DeleteByQueryRequest deleteRequest = new DeleteByQueryRequest(indexName)\n+            .setQuery(queryForDeleteByQueryRequest)\n+            .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN)\n+            .setRefresh(true);\n+        clientUtil.execute(DeleteByQueryAction.INSTANCE, deleteRequest, ActionListener.wrap(response -> {", "originalCommit": "a819d8d062a87fa897fcc819cf7360c8feb20760", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTAzNjU0OQ==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r519036549", "bodyText": "Good catch, thanks.  It still works with current solution. But still add the stash-threadcontext logic to be safe.", "author": "weicongs-amazon", "createdAt": "2020-11-06T22:28:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxMTA5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "c9e13f26f1473181d1c2a85c14e2eef0b5af40bc", "chunk": "diff --git a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java\nindex d95c528..7044f45 100644\n--- a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java\n+++ b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java\n\n@@ -28,6 +28,7 @@ import org.elasticsearch.action.admin.indices.stats.ShardStats;\n import org.elasticsearch.action.support.IndicesOptions;\n import org.elasticsearch.client.Client;\n import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.util.concurrent.ThreadContext;\n import org.elasticsearch.index.query.QueryBuilder;\n import org.elasticsearch.index.reindex.DeleteByQueryAction;\n import org.elasticsearch.index.reindex.DeleteByQueryRequest;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjc4NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518926784", "bodyText": "Why need to minus 1 day for next cleanup TTL? If the defaultCheckpointTtl is 5 days at first, the TTL will become shorter and shorter until reach 1 day ?", "author": "ylwu-amzn", "createdAt": "2020-11-06T18:23:08Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.time.Clock;\n+import java.time.Duration;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.index.query.QueryBuilders;\n+\n+import com.amazon.opendistroforelasticsearch.ad.constant.CommonName;\n+import com.amazon.opendistroforelasticsearch.ad.ml.CheckpointDao;\n+\n+/**\n+ * Model checkpoints cleanup of multi-entity detectors.\n+ * <p> <b>Problem:</b>\n+ *     In multi-entity detectors, we can have thousands, even millions of entities, of which the model checkpoints will consume\n+ *     lots of disk resources. To protect the our disk usage, the checkpoint index size will be limited with specified threshold.\n+ *     Once its size exceeds the threshold, the model checkpoints cleanup process will be activated.\n+ * </p>\n+ * <p> <b>Solution:</b>\n+ *     Before multi-entity detectors, there is daily cron job to clean up the inactive checkpoints longer than some configurable days.\n+ *     We will keep the this logic, and add new clean up way based on shard size.\n+ * </p>\n+ */\n+public class ModelCheckpointIndexRetention implements Runnable {\n+    private static final Logger LOG = LogManager.getLogger(ModelCheckpointIndexRetention.class);\n+\n+    // The recommended max shard size is 50G, we don't wanna our index exceeds this number\n+    private static final long MAX_SHARD_SIZE_IN_BYTE = 50 * 1024 * 1024 * 1024L;\n+    // We can't clean up all of the checkpoints. At least keep models for 1 day\n+    private static final Duration MINIMUM_CHECKPOINT_TTL = Duration.ofDays(1);\n+\n+    private final Duration defaultCheckpointTtl;\n+    private final Clock clock;\n+    private final IndexCleanup indexCleanup;\n+\n+    public ModelCheckpointIndexRetention(Duration defaultCheckpointTtl, Clock clock, IndexCleanup indexCleanup) {\n+        this.defaultCheckpointTtl = defaultCheckpointTtl;\n+        this.clock = clock;\n+        this.indexCleanup = indexCleanup;\n+    }\n+\n+    @Override\n+    public void run() {\n+        indexCleanup\n+            .deleteDocsByQuery(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - defaultCheckpointTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener\n+                    .wrap(\n+                        response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                        exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)\n+                    )\n+            );\n+\n+    }\n+\n+    private void cleanupBasedOnShardSize(Duration cleanUpTtl) {\n+        indexCleanup\n+            .deleteDocsBasedOnShardSize(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                MAX_SHARD_SIZE_IN_BYTE,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - cleanUpTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener.wrap(cleanupNeeded -> {\n+                    if (cleanupNeeded) {\n+                        if (cleanUpTtl.equals(MINIMUM_CHECKPOINT_TTL)) {\n+                            return;\n+                        }\n+\n+                        Duration nextCleanupTtl = cleanUpTtl.minusDays(1);", "originalCommit": "a819d8d062a87fa897fcc819cf7360c8feb20760", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzMDE4NA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518930184", "bodyText": "Yes. But this logic is only triggered whey the shard size is bigger than 50G.", "author": "weicongs-amazon", "createdAt": "2020-11-06T18:29:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0OTY4OA==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518949688", "bodyText": "Got it, so the model checkpoint TTL is not hard constraint, we always prioritize the disk usage than checkpoint TTL. If the shard size still greater than 50GB after cleaning old checkpoints > 5days, will continue to clean old checkpoints older than 4 days, 3 days, until 1day.", "author": "ylwu-amzn", "createdAt": "2020-11-06T19:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjc4NA=="}], "type": "inlineReview", "revised_code": {"commit": "4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "chunk": "diff --git a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\nindex cc3f1ff..83c8fbc 100644\n--- a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\n+++ b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\n\n@@ -72,6 +72,7 @@ public class ModelCheckpointIndexRetention implements Runnable {\n                 ActionListener\n                     .wrap(\n                         response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                            //The docs will be deleted in next scheduled windows. No need for retrying.\n                         exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)\n                     )\n             );\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyODIxMg==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518928212", "bodyText": "If any exception happens, will stop clean up cron job? It's possible that high load causes failure and next run may succeed.\nSame question for line 108", "author": "ylwu-amzn", "createdAt": "2020-11-06T18:25:52Z", "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.time.Clock;\n+import java.time.Duration;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.index.query.QueryBuilders;\n+\n+import com.amazon.opendistroforelasticsearch.ad.constant.CommonName;\n+import com.amazon.opendistroforelasticsearch.ad.ml.CheckpointDao;\n+\n+/**\n+ * Model checkpoints cleanup of multi-entity detectors.\n+ * <p> <b>Problem:</b>\n+ *     In multi-entity detectors, we can have thousands, even millions of entities, of which the model checkpoints will consume\n+ *     lots of disk resources. To protect the our disk usage, the checkpoint index size will be limited with specified threshold.\n+ *     Once its size exceeds the threshold, the model checkpoints cleanup process will be activated.\n+ * </p>\n+ * <p> <b>Solution:</b>\n+ *     Before multi-entity detectors, there is daily cron job to clean up the inactive checkpoints longer than some configurable days.\n+ *     We will keep the this logic, and add new clean up way based on shard size.\n+ * </p>\n+ */\n+public class ModelCheckpointIndexRetention implements Runnable {\n+    private static final Logger LOG = LogManager.getLogger(ModelCheckpointIndexRetention.class);\n+\n+    // The recommended max shard size is 50G, we don't wanna our index exceeds this number\n+    private static final long MAX_SHARD_SIZE_IN_BYTE = 50 * 1024 * 1024 * 1024L;\n+    // We can't clean up all of the checkpoints. At least keep models for 1 day\n+    private static final Duration MINIMUM_CHECKPOINT_TTL = Duration.ofDays(1);\n+\n+    private final Duration defaultCheckpointTtl;\n+    private final Clock clock;\n+    private final IndexCleanup indexCleanup;\n+\n+    public ModelCheckpointIndexRetention(Duration defaultCheckpointTtl, Clock clock, IndexCleanup indexCleanup) {\n+        this.defaultCheckpointTtl = defaultCheckpointTtl;\n+        this.clock = clock;\n+        this.indexCleanup = indexCleanup;\n+    }\n+\n+    @Override\n+    public void run() {\n+        indexCleanup\n+            .deleteDocsByQuery(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - defaultCheckpointTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener\n+                    .wrap(\n+                        response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                        exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)", "originalCommit": "a819d8d062a87fa897fcc819cf7360c8feb20760", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzODQ2Mw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518938463", "bodyText": "It's possible. This error is acceptable since this is scheduled job. The process will continue in next schedule. Will add comment there", "author": "weicongs-amazon", "createdAt": "2020-11-06T18:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyODIxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk1MTYyMw==", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518951623", "bodyText": "Make sense.", "author": "ylwu-amzn", "createdAt": "2020-11-06T19:12:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyODIxMg=="}], "type": "inlineReview", "revised_code": {"commit": "4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "chunk": "diff --git a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\nindex cc3f1ff..83c8fbc 100644\n--- a/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\n+++ b/src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java\n\n@@ -72,6 +72,7 @@ public class ModelCheckpointIndexRetention implements Runnable {\n                 ActionListener\n                     .wrap(\n                         response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                            //The docs will be deleted in next scheduled windows. No need for retrying.\n                         exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)\n                     )\n             );\n"}}, {"oid": "c9e13f26f1473181d1c2a85c14e2eef0b5af40bc", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/c9e13f26f1473181d1c2a85c14e2eef0b5af40bc", "message": "stash context for index clean up", "committedDate": "2020-11-06T21:22:54Z", "type": "commit"}, {"oid": "facd5c5593e3ff3bfb86ef9bb5c66ae98560d77e", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/facd5c5593e3ff3bfb86ef9bb5c66ae98560d77e", "message": "remove useless import to fix checkstyle error", "committedDate": "2020-11-06T21:28:30Z", "type": "commit"}, {"oid": "4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "message": "Add comments for error handling", "committedDate": "2020-11-06T22:22:25Z", "type": "commit"}, {"oid": "7c24c8bcedb4654e97617a05f3bba448f793b362", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/7c24c8bcedb4654e97617a05f3bba448f793b362", "message": "Fix format error", "committedDate": "2020-11-06T22:29:20Z", "type": "commit"}]}