{"pr_number": 4697, "pr_title": "DHFPROD-6095: - Run connector against DHF 5.x", "pr_createdAt": "2020-10-09T21:50:49Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4697", "timeline": [{"oid": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "url": "https://github.com/marklogic/marklogic-data-hub/commit/e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "message": "DHFPROD-6095 - Run connector against DHF 5.x", "committedDate": "2020-10-09T21:48:22Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDM1NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502984354", "bodyText": "These imports should all be removed as they're not used", "author": "rjrudin", "createdAt": "2020-10-12T00:00:45Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java", "diffHunk": "@@ -15,15 +15,18 @@\n  */\n package com.marklogic.hub.spark.sql.sources.v2.writer;\n \n+import com.fasterxml.jackson.databind.node.ObjectNode;", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java\nindex d50d7740c..865856679 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriterFactory.java\n\n@@ -15,63 +15,39 @@\n  */\n package com.marklogic.hub.spark.sql.sources.v2.writer;\n \n-import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.fasterxml.jackson.databind.JsonNode;\n import com.marklogic.client.ext.helper.LoggingObject;\n import com.marklogic.hub.HubClient;\n-import com.marklogic.hub.impl.HubConfigImpl;\n-import com.marklogic.hub.spark.sql.sources.v2.IOUtil;\n-import com.marklogic.mgmt.util.SimplePropertySource;\n+import com.marklogic.hub.spark.sql.sources.v2.DefaultSource;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.sources.v2.writer.DataWriter;\n import org.apache.spark.sql.sources.v2.writer.DataWriterFactory;\n import org.apache.spark.sql.types.StructType;\n \n-import java.io.IOException;\n import java.util.Map;\n-import java.util.Properties;\n \n public class HubDataWriterFactory extends LoggingObject implements DataWriterFactory<InternalRow> {\n \n     private StructType schema;\n-    private HubClient hubClient;\n     private Map<String, String> options;\n-\n+    private JsonNode endpointParams;\n \n     /**\n      * @param options a map of options containing both DHF-supported properties (most likely prefixed with ml* or\n-     *               hub*) and connector-specific properties. The DHF-supported properties will be used to construct a\n-     *               HubClient for communicating with MarkLogic.\n+     *                hub*) and connector-specific properties. The DHF-supported properties will be used to construct a\n+     *                HubClient for communicating with MarkLogic.\n      * @param schema\n      */\n-    public HubDataWriterFactory(Map<String, String> options, StructType schema) {\n+    public HubDataWriterFactory(Map<String, String> options, StructType schema, JsonNode endpointParams) {\n         this.options = options;\n         this.schema = schema;\n-        HubConfigImpl hubConfig = buildHubConfig(options);\n-        logger.info(\"Creating HubClient for host: \" + hubConfig.getHost());\n-        this.hubClient = hubConfig.newHubClient();\n+        this.endpointParams = endpointParams;\n     }\n \n     @Override\n     public DataWriter<InternalRow> createDataWriter(int partitionId, long taskId, long epochId) {\n-        return new HubDataWriter(hubClient, schema, options);\n-    }\n-\n-    /**\n-     * This is a static method so that it can be easily tested without instantiating this class.\n-     *\n-     * @param options\n-     * @return\n-     */\n-    protected static HubConfigImpl buildHubConfig(Map<String, String> options) {\n-        Properties props = new Properties();\n-        // Assume DHS usage by default; the options map can override these\n-        props.setProperty(\"hubdhs\", \"true\");\n-        props.setProperty(\"hubssl\", \"true\");\n-        options.keySet().forEach(key -> props.setProperty(key, options.get(key)));\n-\n-        HubConfigImpl hubConfig = new HubConfigImpl();\n-        hubConfig.registerLowerCasedPropertyConsumers();\n-        hubConfig.applyProperties(new SimplePropertySource(props));\n-        return hubConfig;\n+        HubClient client = HubClient.withHubClientConfig(DefaultSource.buildHubClientConfig(options));\n+        logger.info(\"Creating HubClient for host: \" + client.getStagingClient().getHost());\n+        return new HubDataWriter(client, schema, options, endpointParams);\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502984670", "bodyText": "This PR should including deleting the two files from /data-hub/5/data-services/ingestion from git, as they're being moved to /marklogic-data-hub-spark-connector/", "author": "rjrudin", "createdAt": "2020-10-12T00:03:19Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -132,6 +132,20 @@ void invalidPermissionsString() {\n         assertTrue(ex.getCause().getMessage().contains(\"Unable to parse permissions: rest-reader,read,rest-writer\"), \"Unexpected error message: \" + ex.getCause().getMessage());\n     }\n \n+    @Test\n+    public void testEndpointsAreLoaded() throws Exception {\n+        TextDocumentManager modMgr = getHubClient().getModulesClient().newTextDocumentManager();\n+        modMgr.delete(\"/data-hub/5/data-services/ingestion/bulkIngester.api\", \"/data-hub/5/data-services/ingestion/bulkIngester.sjs\",", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5NTQ2MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r504095460", "bodyText": "Should we keep these in datahub modules too? Just in case if installer is installing these modules?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-13T16:32:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDE1NTAyNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r504155027", "bodyText": "The modules should still be in DHF core ,but under /marklogic-data-hub-spark-connector/", "author": "rjrudin", "createdAt": "2020-10-13T18:05:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NDY3MA=="}], "type": "inlineReview", "revised_code": {"commit": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\nindex f01553ab3..4b131c441 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\n\n@@ -125,6 +127,7 @@ public class WriteDataTest extends AbstractSparkConnectorTest {\n     }\n \n     @Test\n+    @Disabled(\"Error handling needs to be reworked based on Java Client 5.3\")\n     void invalidPermissionsString() {\n         DataWriter<InternalRow> writer = buildDataWriter(newFruitOptions().withPermissions(\"rest-reader,read,rest-writer\"));\n         RuntimeException ex = assertThrows(RuntimeException.class, () -> writer.write(buildRow(\"apple\", \"red\")));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTQwNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985405", "bodyText": "Pinging @SameeraPriyathamTadikonda and @ehennum  about this - I believe we should move the initialization to HubDataSourceWriter. This is based on my assumption that @SameeraPriyathamTadikonda  will be adding the initialization and finalization logic there for the job document.\nI think it makes sense though to not write this module if Ernie specifies his own ingestion path. Thus, it seems that all of the logic in this constructor should move into HubDataWriteSource. Does that seem right?", "author": "rjrudin", "createdAt": "2020-10-12T00:09:20Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 0631dea0b..968ab5e62 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -124,63 +113,4 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n         jacksonGenerator.flush();\n         return jsonObjectWriter.toString();\n     }\n-\n-    protected JsonNode determineIngestionEndpointParams(Map<String, String> options) {\n-        ObjectMapper objectMapper = new ObjectMapper();\n-\n-        ObjectNode endpointParams;\n-        if (options.containsKey(\"ingestendpointparams\")) {\n-            try {\n-                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"ingestendpointparams\"));\n-            } catch (IOException e) {\n-                throw new IllegalArgumentException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n-            }\n-        } else {\n-            endpointParams = objectMapper.createObjectNode();\n-        }\n-\n-        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n-        boolean hasWorkUnitOrEndpointState = endpointParams.hasNonNull(\"workUnit\") || endpointParams.hasNonNull(\"endpointState\");\n-        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n-            throw new IllegalArgumentException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n-        }\n-\n-        if (doesNotHaveApiPath) {\n-            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-\n-            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n-                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n-                try {\n-                    ioUtil.load(\"bulkIngester.api\");\n-                } catch (IOException e) {\n-                    throw new RuntimeException(\"Error occurred while loading default endpoints.\");\n-                }\n-                endpointParams.put(\"apiPath\", \"/marklogic-data-hub-spark-connector/bulkIngester.api\");\n-            }\n-            else\n-                endpointParams.put(\"apiPath\", apiPath);\n-\n-        }\n-\n-        // TODO : remove the below else block after java-client-api 5.3 release\n-        if (!endpointParams.hasNonNull(\"endpointState\")) {\n-            endpointParams.set(\"endpointState\", objectMapper.createObjectNode());\n-        }\n-\n-        if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            defaultWorkUnit = objectMapper.createObjectNode();\n-            buildDefaultWorkUnit(options);\n-            endpointParams.set(\"workUnit\", defaultWorkUnit);\n-        }\n-\n-        return endpointParams;\n-    }\n-\n-    protected void buildDefaultWorkUnit(Map<String, String> options) {\n-        Stream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n-            if (options.containsKey(key)) {\n-                defaultWorkUnit.put(key, options.get(key));\n-            }\n-        });\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTUxNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985516", "bodyText": "We don't want to lose the original exception here - e.g:\nthrow new RuntimeException(\"Unable to write default ingestion endpoint at path: \" + path + \"; cause: \" + e.getMessage(), e);", "author": "rjrudin", "createdAt": "2020-10-12T00:10:19Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+\n+            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n+                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n+                try {\n+                    ioUtil.load(\"bulkIngester.api\");\n+                } catch (IOException e) {\n+                    throw new RuntimeException(\"Error occurred while loading default endpoints.\");", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 0631dea0b..968ab5e62 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -124,63 +113,4 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n         jacksonGenerator.flush();\n         return jsonObjectWriter.toString();\n     }\n-\n-    protected JsonNode determineIngestionEndpointParams(Map<String, String> options) {\n-        ObjectMapper objectMapper = new ObjectMapper();\n-\n-        ObjectNode endpointParams;\n-        if (options.containsKey(\"ingestendpointparams\")) {\n-            try {\n-                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"ingestendpointparams\"));\n-            } catch (IOException e) {\n-                throw new IllegalArgumentException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n-            }\n-        } else {\n-            endpointParams = objectMapper.createObjectNode();\n-        }\n-\n-        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n-        boolean hasWorkUnitOrEndpointState = endpointParams.hasNonNull(\"workUnit\") || endpointParams.hasNonNull(\"endpointState\");\n-        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n-            throw new IllegalArgumentException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n-        }\n-\n-        if (doesNotHaveApiPath) {\n-            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-\n-            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n-                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n-                try {\n-                    ioUtil.load(\"bulkIngester.api\");\n-                } catch (IOException e) {\n-                    throw new RuntimeException(\"Error occurred while loading default endpoints.\");\n-                }\n-                endpointParams.put(\"apiPath\", \"/marklogic-data-hub-spark-connector/bulkIngester.api\");\n-            }\n-            else\n-                endpointParams.put(\"apiPath\", apiPath);\n-\n-        }\n-\n-        // TODO : remove the below else block after java-client-api 5.3 release\n-        if (!endpointParams.hasNonNull(\"endpointState\")) {\n-            endpointParams.set(\"endpointState\", objectMapper.createObjectNode());\n-        }\n-\n-        if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            defaultWorkUnit = objectMapper.createObjectNode();\n-            buildDefaultWorkUnit(options);\n-            endpointParams.set(\"workUnit\", defaultWorkUnit);\n-        }\n-\n-        return endpointParams;\n-    }\n-\n-    protected void buildDefaultWorkUnit(Map<String, String> options) {\n-        Stream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n-            if (options.containsKey(key)) {\n-                defaultWorkUnit.put(key, options.get(key));\n-            }\n-        });\n-    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r502985814", "bodyText": "We'll need to toss these files into the shadowJar, which is part of #4695 .\nOnce it's in the jar, the Spring class ClassPathResource can be used to retrieve it, no need for IOUtil:\nnew ClassPathResource(\"marklogic-data-hub-spark-connector/bulkIngester.api\").getInputStream()", "author": "rjrudin", "createdAt": "2020-10-12T00:12:51Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -142,7 +146,20 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> options)\n         }\n \n         if (doesNotHaveApiPath) {\n-            endpointParams.put(\"apiPath\", \"/data-hub/5/data-services/ingestion/bulkIngester.api\");\n+            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n+\n+            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n+                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n+                try {\n+                    ioUtil.load(\"bulkIngester.api\");", "originalCommit": "e1e2ff00baf8dace0af03ef6c8b187718e8137f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcwNDc5NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r503704795", "bodyText": "@rjrudin . I am guessing -\nnew ClassPathResource(\"marklogic-data-hub-spark-connector/bulkIngester.api\").getInputStream()\nis a replacement for IOUtil.testFileToString().\nThere are other operations in IOUtil as well like - adding permissions and writing to data-hub-MODULES database. Please let me know if there is a method in data-hub that handles the permissions part. Will go ahead and replace them.", "author": "anu3990", "createdAt": "2020-10-13T06:49:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzg3MDk4NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4697#discussion_r503870985", "bodyText": "@anu3990 @SameeraPriyathamTadikonda I think you'll want a separate method for building a DocumentWriteOperation for each endpoint module that needs to be written (it would be unusual, but it's possible that the connector only needs to write 1 or 2 of the 3 endpoint modules). And then another method will handle adding each DWO to a DocumentWriteSet, which can then be written to the modules database. I think all of that logic should be in HubDataWriterSource for now - there's no need for a separate utility class, the Java Client makes it pretty simple. We'd just want to avoid any duplication within HDWS.", "author": "rjrudin", "createdAt": "2020-10-13T11:22:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjk4NTgxNA=="}], "type": "inlineReview", "revised_code": {"commit": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 0631dea0b..968ab5e62 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -124,63 +113,4 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n         jacksonGenerator.flush();\n         return jsonObjectWriter.toString();\n     }\n-\n-    protected JsonNode determineIngestionEndpointParams(Map<String, String> options) {\n-        ObjectMapper objectMapper = new ObjectMapper();\n-\n-        ObjectNode endpointParams;\n-        if (options.containsKey(\"ingestendpointparams\")) {\n-            try {\n-                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"ingestendpointparams\"));\n-            } catch (IOException e) {\n-                throw new IllegalArgumentException(\"Unable to parse ingestendpointparams, cause: \" + e.getMessage(), e);\n-            }\n-        } else {\n-            endpointParams = objectMapper.createObjectNode();\n-        }\n-\n-        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n-        boolean hasWorkUnitOrEndpointState = endpointParams.hasNonNull(\"workUnit\") || endpointParams.hasNonNull(\"endpointState\");\n-        if (doesNotHaveApiPath && hasWorkUnitOrEndpointState) {\n-            throw new IllegalArgumentException(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\");\n-        }\n-\n-        if (doesNotHaveApiPath) {\n-            String apiPath = \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-\n-            if(hubClient.getModulesClient().newJSONDocumentManager().exists(apiPath) == null) {\n-                IOUtil ioUtil = new IOUtil(this.hubClient.getModulesClient());\n-                try {\n-                    ioUtil.load(\"bulkIngester.api\");\n-                } catch (IOException e) {\n-                    throw new RuntimeException(\"Error occurred while loading default endpoints.\");\n-                }\n-                endpointParams.put(\"apiPath\", \"/marklogic-data-hub-spark-connector/bulkIngester.api\");\n-            }\n-            else\n-                endpointParams.put(\"apiPath\", apiPath);\n-\n-        }\n-\n-        // TODO : remove the below else block after java-client-api 5.3 release\n-        if (!endpointParams.hasNonNull(\"endpointState\")) {\n-            endpointParams.set(\"endpointState\", objectMapper.createObjectNode());\n-        }\n-\n-        if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            defaultWorkUnit = objectMapper.createObjectNode();\n-            buildDefaultWorkUnit(options);\n-            endpointParams.set(\"workUnit\", defaultWorkUnit);\n-        }\n-\n-        return endpointParams;\n-    }\n-\n-    protected void buildDefaultWorkUnit(Map<String, String> options) {\n-        Stream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n-            if (options.containsKey(key)) {\n-                defaultWorkUnit.put(key, options.get(key));\n-            }\n-        });\n-    }\n }\n"}}, {"oid": "772e8eb137126e7cc01eaab1c873ae3f0261e757", "url": "https://github.com/marklogic/marklogic-data-hub/commit/772e8eb137126e7cc01eaab1c873ae3f0261e757", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T06:44:04Z", "type": "forcePushed"}, {"oid": "a7e6528112d6d687cdd22a56453643d9e2404b71", "url": "https://github.com/marklogic/marklogic-data-hub/commit/a7e6528112d6d687cdd22a56453643d9e2404b71", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T21:35:51Z", "type": "forcePushed"}, {"oid": "27dc710892e1cf3932e893b38aa95fb638fa6385", "url": "https://github.com/marklogic/marklogic-data-hub/commit/27dc710892e1cf3932e893b38aa95fb638fa6385", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-13T21:41:07Z", "type": "forcePushed"}, {"oid": "32e8b00e7e782027a9c851a48c035f87fe4b5077", "url": "https://github.com/marklogic/marklogic-data-hub/commit/32e8b00e7e782027a9c851a48c035f87fe4b5077", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-14T18:20:43Z", "type": "forcePushed"}, {"oid": "d27333597f30fde6888612ff8cddec40a20a9c5f", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d27333597f30fde6888612ff8cddec40a20a9c5f", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-14T21:00:14Z", "type": "forcePushed"}, {"oid": "60ff9d21af78718355afc02d44e11d839b55b1e3", "url": "https://github.com/marklogic/marklogic-data-hub/commit/60ff9d21af78718355afc02d44e11d839b55b1e3", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-15T20:51:58Z", "type": "forcePushed"}, {"oid": "f552ca6da03eb6b99ef0223da3477c930346c942", "url": "https://github.com/marklogic/marklogic-data-hub/commit/f552ca6da03eb6b99ef0223da3477c930346c942", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-16T15:47:04Z", "type": "forcePushed"}, {"oid": "18b9fc6cae177a213eaa57ee5f596a8afa58c3e5", "url": "https://github.com/marklogic/marklogic-data-hub/commit/18b9fc6cae177a213eaa57ee5f596a8afa58c3e5", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-16T15:50:20Z", "type": "forcePushed"}, {"oid": "a4264ba2a69f7b8451c68590698396580ab68073", "url": "https://github.com/marklogic/marklogic-data-hub/commit/a4264ba2a69f7b8451c68590698396580ab68073", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-19T22:42:21Z", "type": "forcePushed"}, {"oid": "d0705843a43bf1bda0fe498733258f6c6a0d8419", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d0705843a43bf1bda0fe498733258f6c6a0d8419", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T06:30:42Z", "type": "forcePushed"}, {"oid": "edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "url": "https://github.com/marklogic/marklogic-data-hub/commit/edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T16:47:53Z", "type": "commit"}, {"oid": "edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "url": "https://github.com/marklogic/marklogic-data-hub/commit/edebb3b6843bedf60bd8bf0c1bfd40b61116e0e9", "message": "DHFPROD-6095 - Run connector against DHF 5.2.x.", "committedDate": "2020-10-20T16:47:53Z", "type": "forcePushed"}]}