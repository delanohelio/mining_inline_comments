{"pr_number": 4675, "pr_title": "DHFPROD-6056: Structured Streaming in Spark", "pr_createdAt": "2020-10-06T17:43:03Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4675", "timeline": [{"oid": "637d5ee808c54e7df027506ab384733b9186a228", "url": "https://github.com/marklogic/marklogic-data-hub/commit/637d5ee808c54e7df027506ab384733b9186a228", "message": "DHFPROD-6056 - Structured Streaming in Spark", "committedDate": "2020-10-06T19:58:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUzMjAzNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4675#discussion_r500532037", "bodyText": "Given that we don't pass the OutputMode along, why log it? Its value has no impact on our connector.", "author": "rjrudin", "createdAt": "2020-10-06T19:06:10Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/HubDataSource.java", "diffHunk": "@@ -20,26 +20,39 @@\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.sources.v2.DataSourceOptions;\n+import org.apache.spark.sql.sources.v2.StreamWriteSupport;\n import org.apache.spark.sql.sources.v2.WriteSupport;\n import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n import org.apache.spark.sql.sources.v2.writer.DataWriterFactory;\n import org.apache.spark.sql.sources.v2.writer.WriterCommitMessage;\n+import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter;\n+import org.apache.spark.sql.streaming.OutputMode;\n import org.apache.spark.sql.types.StructType;\n \n import java.util.Map;\n import java.util.Optional;\n \n-public class HubDataSource extends LoggingObject implements WriteSupport {\n+public class HubDataSource extends LoggingObject implements WriteSupport, StreamWriteSupport {\n \n     @Override\n     public Optional<DataSourceWriter> createWriter(String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options) {\n         logger.info(\"Creating HubDataSourceWriter\");\n+        if(mode!=null && !mode.toString().equalsIgnoreCase(\"Save\"))", "originalCommit": "4e219a2472884da3b3ad5588b05ced4bff2776eb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "637d5ee808c54e7df027506ab384733b9186a228", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/HubDataSource.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/HubDataSource.java\nindex 535b8691c..b9308e7c5 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/HubDataSource.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/HubDataSource.java\n\n@@ -39,7 +39,7 @@ public class HubDataSource extends LoggingObject implements WriteSupport, Stream\n         logger.info(\"Creating HubDataSourceWriter\");\n         if(mode!=null && !mode.toString().equalsIgnoreCase(\"Save\"))\n             logger.warn(\"OutputMode is \"+mode.toString());\n-        return Optional.of(new HubDataSourceWriter(options.asMap(), schema){\n+        return Optional.of(new HubDataSourceWriter(options.asMap(), schema, false){\n \n         });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDU3OTI2Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4675#discussion_r500579263", "bodyText": "We don't need this test, as it's not testing anything new. The DataWriter that's being returned is identical to the one being returned by buildDataWriter - it's being constructed in the same fashion, since OutputMode doesn't matter. .\nWhat we need tests for are the commit/abort methods that take an epoch and don't take an epoch. But, we don't have any logic for those yet. That's where we need some research to figure out when Glue/Spark is invoking these methods - we should add some logging and then test out the connector to see when these methods are invoked, and what's in the WriterCommitMessage objects that are passed to it.", "author": "rjrudin", "createdAt": "2020-10-06T20:34:41Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/AbstractSparkConnectorTest.java", "diffHunk": "@@ -157,4 +159,24 @@ protected GenericInternalRow buildRow(String... values) {\n         }\n         return new GenericInternalRow(rowValues);\n     }\n+\n+    /**\n+     * @param options\n+     * @return\n+     * */\n+    protected DataWriter<InternalRow> buildStreamWriter(Options options) {", "originalCommit": "637d5ee808c54e7df027506ab384733b9186a228", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ed5e1332921ed89a900e2a327c8dbf57f9834920", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/AbstractSparkConnectorTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/AbstractSparkConnectorTest.java\nindex cd4f76113..896c64382 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/AbstractSparkConnectorTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/AbstractSparkConnectorTest.java\n\n@@ -159,24 +157,4 @@ public abstract class AbstractSparkConnectorTest extends AbstractHubTest {\n         }\n         return new GenericInternalRow(rowValues);\n     }\n-\n-    /**\n-     * @param options\n-     * @return\n-     * */\n-    protected DataWriter<InternalRow> buildStreamWriter(Options options) {\n-        HubDataSource dataSource = new HubDataSource();\n-        final String writeUUID = \"doesntMatter\";\n-        final OutputMode outputModeDoesntMatter = OutputMode.Append();\n-\n-        // Get the set of DHF properties used to connect to ML as a map, and then add connector-specific params\n-\n-        StreamWriter streamWriter = dataSource.createStreamWriter(writeUUID, FRUIT_SCHEMA, outputModeDoesntMatter, options.toDataSourceOptions());\n-        DataWriterFactory<InternalRow> dataWriterFactory = streamWriter.createWriterFactory();\n-\n-        final int partitionIdDoesntMatter = 0;\n-        final long taskId = 2;\n-        final int epochIdDoesntMatter = 0;\n-        return dataWriterFactory.createDataWriter(partitionIdDoesntMatter, taskId, epochIdDoesntMatter);\n-    }\n }\n"}}, {"oid": "ed5e1332921ed89a900e2a327c8dbf57f9834920", "url": "https://github.com/marklogic/marklogic-data-hub/commit/ed5e1332921ed89a900e2a327c8dbf57f9834920", "message": "DHFPROD-6056 - Structured Streaming in Spark", "committedDate": "2020-10-06T21:44:44Z", "type": "commit"}, {"oid": "ed5e1332921ed89a900e2a327c8dbf57f9834920", "url": "https://github.com/marklogic/marklogic-data-hub/commit/ed5e1332921ed89a900e2a327c8dbf57f9834920", "message": "DHFPROD-6056 - Structured Streaming in Spark", "committedDate": "2020-10-06T21:44:44Z", "type": "forcePushed"}]}