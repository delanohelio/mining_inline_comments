{"pr_number": 4916, "pr_title": "DHFPROD-6172: Customize Spark connector endpoints for reading rows", "pr_createdAt": "2020-11-30T06:00:33Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4916", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MzYxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532643617", "bodyText": "I don't think most of these should be class fields, which complicates the class. See next note below.", "author": "rjrudin", "createdAt": "2020-11-30T14:38:23Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -29,6 +33,11 @@\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n+    private final HubClient hubClient;", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -33,11 +33,8 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n-    private final HubClient hubClient;\n-    private final HubClientConfig hubClientConfig;\n-    private final ObjectMapper objectMapper;\n-    private CustomReadApiDefinitions customReadApiDefinitions;\n-    private final JsonNode endpointParams;\n+    private InputStreamHandle initializeDefinition;\n+    private JsonNode endpointParams;\n \n     /**\n      * The current default for partition count is based on the active SparkSession. The PartitionCountProvider is used\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NDg2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532644865", "bodyText": "I think this is all work to be done in initializeRead, which is already instantiating a HubClient. That can become a local variable in initializeRead, which can then say - if you've specified an initializereadapipath option, then I'll read that using the HubClient and pass it in as the second arg to the SparkService.on call.", "author": "rjrudin", "createdAt": "2020-11-30T14:40:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -41,7 +50,12 @@ public HubDataSourceReader(DataSourceOptions dataSourceOptions) {\n \n         this.options = dataSourceOptions.asMap();\n         validateOptions(this.options);\n+        this.hubClientConfig = Util.buildHubClientConfig(options);\n+        this.hubClient = HubClient.withHubClientConfig(hubClientConfig);\n+        this.objectMapper = new ObjectMapper();\n+        this.customReadApiDefinitions = readCustomJobApiDefinitions(options);\n \n+        this.endpointParams = determineReadRecordsEndpointParams(options);", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -50,12 +47,7 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n \n         this.options = dataSourceOptions.asMap();\n         validateOptions(this.options);\n-        this.hubClientConfig = Util.buildHubClientConfig(options);\n-        this.hubClient = HubClient.withHubClientConfig(hubClientConfig);\n-        this.objectMapper = new ObjectMapper();\n-        this.customReadApiDefinitions = readCustomJobApiDefinitions(options);\n \n-        this.endpointParams = determineReadRecordsEndpointParams(options);\n         this.initializationResponse = initializeRead(this.options);\n         this.sparkSchema = (StructType) StructType.fromJson(initializationResponse.get(\"sparkSchema\").toString());\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NzY2Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532647666", "bodyText": "Because there's only one API endpoint, there's no need for a CustomReadApiDefinition class. CustomWriteApiDefinitions was created as a holder for two API definitions. We don't have that need here, so this method should just return a single InputStreamHandle.\nAlso, should name this method \"readCustomInitializeApiDefinition\" - \"Job\" is not an appropriate name here.", "author": "rjrudin", "createdAt": "2020-11-30T14:43:42Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -126,6 +145,20 @@ private int determineNumPartitions(Map<String, String> options) {\n         return numPartitions;\n     }\n \n+    private CustomReadApiDefinitions readCustomJobApiDefinitions(Map<String, String> options) {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -145,7 +142,7 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n         return numPartitions;\n     }\n \n-    private CustomReadApiDefinitions readCustomJobApiDefinitions(Map<String, String> options) {\n+    private InputStreamHandle readCustomInitializeApiDefinition(Map<String, String> options, HubClient hubClient) {\n         InputStreamHandle initializeDefinition = null;\n         String key = \"initializereadapipath\";\n         if (options.containsKey(key)) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0NzkyMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532647920", "bodyText": "Should name this \"ReadRows\" instead of \"ReadRecords\" for consistency.", "author": "rjrudin", "createdAt": "2020-11-30T14:44:02Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -175,8 +172,9 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n         return sparkSchema;\n     }\n \n-    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+    private JsonNode determineReadRowsEndpointParams(Map<String, String> options) {\n         ObjectNode endpointParams;\n+        ObjectMapper objectMapper = new ObjectMapper();\n         if (options.containsKey(\"readrowsendpointparams\")) {\n             try {\n                 endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1MDE0OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532650149", "bodyText": "Let's give the same context as the error message above this, and I think some guidance is useful here - e.g. \"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState\".", "author": "rjrudin", "createdAt": "2020-11-30T14:46:57Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+        ObjectNode endpointParams;\n+        if (options.containsKey(\"readrowsendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n+            } catch (IOException e) {\n+                throw new IllegalArgumentException(\"Unable to parse readrowsendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasEndpointConstants = endpointParams.hasNonNull(\"endpointConstants\") && !StringUtils.isEmpty(endpointParams.get(\"endpointConstants\").asText());\n+        boolean hasEndpointState = endpointParams.hasNonNull(\"endpointState\") && !StringUtils.isEmpty(endpointParams.get(\"endpointState\").asText());\n+        if (doesNotHaveApiPath && hasEndpointState) {\n+            throw new IllegalArgumentException(\"Cannot set endpointState in readrowsendpointparams unless apiPath is defined as well.\");\n+        }\n+        if(hasEndpointConstants)\n+            throw new IllegalArgumentException(\"Cannot override endpointConstants.\");", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -175,8 +172,9 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n         return sparkSchema;\n     }\n \n-    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+    private JsonNode determineReadRowsEndpointParams(Map<String, String> options) {\n         ObjectNode endpointParams;\n+        ObjectMapper objectMapper = new ObjectMapper();\n         if (options.containsKey(\"readrowsendpointparams\")) {\n             try {\n                 endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1MDkyNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532650924", "bodyText": "Good error here, but add \"option\" after \"readrowsendpointparams\" to give it a little more context", "author": "rjrudin", "createdAt": "2020-11-30T14:47:56Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -141,5 +174,49 @@ private int determineNumPartitions(Map<String, String> options) {\n     public StructType readSchema() {\n         return sparkSchema;\n     }\n+\n+    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+        ObjectNode endpointParams;\n+        if (options.containsKey(\"readrowsendpointparams\")) {\n+            try {\n+                endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n+            } catch (IOException e) {\n+                throw new IllegalArgumentException(\"Unable to parse readrowsendpointparams, cause: \" + e.getMessage(), e);\n+            }\n+        } else {\n+            endpointParams = objectMapper.createObjectNode();\n+        }\n+\n+        boolean doesNotHaveApiPath = (!endpointParams.hasNonNull(\"apiPath\") || StringUtils.isEmpty(endpointParams.get(\"apiPath\").asText()));\n+        boolean hasEndpointConstants = endpointParams.hasNonNull(\"endpointConstants\") && !StringUtils.isEmpty(endpointParams.get(\"endpointConstants\").asText());\n+        boolean hasEndpointState = endpointParams.hasNonNull(\"endpointState\") && !StringUtils.isEmpty(endpointParams.get(\"endpointState\").asText());\n+        if (doesNotHaveApiPath && hasEndpointState) {\n+            throw new IllegalArgumentException(\"Cannot set endpointState in readrowsendpointparams unless apiPath is defined as well.\");", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 85dbc8bb2..68a563a9f 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -175,8 +172,9 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n         return sparkSchema;\n     }\n \n-    private JsonNode determineReadRecordsEndpointParams(Map<String, String> options) {\n+    private JsonNode determineReadRowsEndpointParams(Map<String, String> options) {\n         ObjectNode endpointParams;\n+        ObjectMapper objectMapper = new ObjectMapper();\n         if (options.containsKey(\"readrowsendpointparams\")) {\n             try {\n                 endpointParams = (ObjectNode) objectMapper.readTree(options.get(\"readrowsendpointparams\"));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1Mzg5Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532653893", "bodyText": "\"readRowsEndpointParams\" should be here as well, and it needs to be tested too. A good way to test that, along with the custom initialize endpoint, is the following:\n\nDon't load any customers, nor the TDE\nHave the initialize endpoint return a single static partition, regardless of what the other inputs are\nHave the initialize endpoint return a static Spark schema as well, regardless of what the other inputs are\nFor readrowsendpointparams/endpointState, define a single JSON row that can be returned by the readRows endpoint\nThen have the readRows endpoint return whatever value is in endpointState (it'll also need to use something in the endpointState object to know whether it's been called, since you only want to call it once)\n\nThat will allow you to verify that both your custom initialize and read endpoints are being used.", "author": "rjrudin", "createdAt": "2020-11-30T14:51:48Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java", "diffHunk": "@@ -27,6 +27,7 @@\n     private String initializeWriteApiPath;\n     private String finalizeWriteApiPath;\n     private JsonNode additionalExternalMetadata;\n+    private String initializeReadApiPath;", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java\nindex 91c4a93af..7b9b61b70 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/Options.java\n\n@@ -17,11 +17,11 @@ public class Options {\n     // All connector-specific options\n     private String uriTemplate;\n     private String uriPrefix;\n-    private String writeRecordsApiPath;\n     private String collections;\n     private String permissions;\n     private String sourceName;\n     private String sourceType;\n+    private String writeRecordsApiPath;\n     private JsonNode writeRecordsEndpointConstants;\n     private JsonNode writeRecordsEndpointState;\n     private String initializeWriteApiPath;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDE5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532654192", "bodyText": "Replace \"Job\" with \"Read\"", "author": "rjrudin", "createdAt": "2020-11-30T14:52:13Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void installCustomJobEndpoints() {", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\nindex 80fb4144f..d7f465dc5 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\n\n@@ -1,46 +1,78 @@\n package com.marklogic.hub.spark.sql.sources.v2.reader;\n \n-import com.marklogic.client.document.GenericDocumentManager;\n-import com.marklogic.client.io.DocumentMetadataHandle;\n-import com.marklogic.client.io.Format;\n-import com.marklogic.client.io.InputStreamHandle;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.types.StructType;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n-import static org.junit.jupiter.api.Assertions.assertTrue;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.*;\n \n public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n \n     @BeforeEach\n-    void installCustomJobEndpoints() {\n-        GenericDocumentManager mgr = getHubClient().getModulesClient().newDocumentManager();\n-        DocumentMetadataHandle metadata = new DocumentMetadataHandle()\n-            .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE, DocumentMetadataHandle.Capability.EXECUTE);\n-\n-        String apiPath = \"/custom-job-endpoints/initializeRead.api\";\n-        String scriptPath = \"/custom-job-endpoints/initializeRead.sjs\";\n-        mgr.write(apiPath, metadata, new InputStreamHandle(readInputStreamFromClasspath(apiPath)).withFormat(Format.JSON));\n-        mgr.write(scriptPath, metadata, new InputStreamHandle(readInputStreamFromClasspath(scriptPath)).withFormat(Format.TEXT));\n+    void setUp() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n     }\n \n     @Test\n     public void testCustomSchema() {\n-        runAsDataHubDeveloper();\n-        loadSimpleCustomerTDE();\n-        runAsDataHubOperator();\n-        loadTenSimpleCustomers();\n-        Options options = newOptions().withView(\"Customer\").withInitializeReadApiPath(\"/custom-job-endpoints/initializeRead.api\");\n+        setupCustomers();\n+        Options options = newOptions().withView(\"Customer\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n         HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n         StructType testStructType = dataSourceReader.readSchema();\n \n-        assertTrue(testStructType.toList().length() == 2, \"Splice filter did not take effect. \");\n+        assertTrue(testStructType.toList().length() == 2, \"Custom schema not as expected.\");\n         for(int i=0; i<testStructType.toList().length(); i++) {\n             String name = testStructType.toList().apply(0).name();\n-            assertTrue(name.equals(\"customerId\") || name.equals(\"name\"));\n-            assertFalse(name.equals(\"customerSince\"), \"The last StructField was not dropped.\");\n+            assertTrue(name.equals(\"myName\") || name.equals(\"myId\"));\n         }\n     }\n+\n+    @Test\n+    public void testHasEndpointStateWithoutApiPath() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointState in readrowsendpointparams option unless apiPath is defined as well.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testHasEndpointConstants() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointConstants\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testReadRowsEndpointparams() {\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", objectMapper.createObjectNode().put(\"hasRunOnce\", false));\n+        readrowsendpointparams.put(\"apiPath\", \"/custom-read-endpoints/customReadRows.api\");\n+        Options options = newOptions().withView(\"test\").withReadRowsEndpointparams(readrowsendpointparams)\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n+        List<InternalRow> rows = readRows(new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertTrue(rows.isEmpty());\n+    }\n+\n+    private void setupCustomers() {\n+        runAsDataHubDeveloper();\n+        loadSimpleCustomerTDE();\n+        runAsDataHubOperator();\n+        loadTenSimpleCustomers();\n+    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDQ3OQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r532654479", "bodyText": "Let's put these in a folder called \"/custom-read-endpoints\" so it's clear that they're for testing the read endpoint.", "author": "rjrudin", "createdAt": "2020-11-30T14:52:33Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,46 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.document.GenericDocumentManager;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void installCustomJobEndpoints() {\n+        GenericDocumentManager mgr = getHubClient().getModulesClient().newDocumentManager();\n+        DocumentMetadataHandle metadata = new DocumentMetadataHandle()\n+            .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE, DocumentMetadataHandle.Capability.EXECUTE);\n+\n+        String apiPath = \"/custom-job-endpoints/initializeRead.api\";", "originalCommit": "b2af72bf64782998315b17456152959b3272b7f0", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\nindex 80fb4144f..d7f465dc5 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\n\n@@ -1,46 +1,78 @@\n package com.marklogic.hub.spark.sql.sources.v2.reader;\n \n-import com.marklogic.client.document.GenericDocumentManager;\n-import com.marklogic.client.io.DocumentMetadataHandle;\n-import com.marklogic.client.io.Format;\n-import com.marklogic.client.io.InputStreamHandle;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.types.StructType;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n-import static org.junit.jupiter.api.Assertions.assertTrue;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.*;\n \n public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n \n     @BeforeEach\n-    void installCustomJobEndpoints() {\n-        GenericDocumentManager mgr = getHubClient().getModulesClient().newDocumentManager();\n-        DocumentMetadataHandle metadata = new DocumentMetadataHandle()\n-            .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE, DocumentMetadataHandle.Capability.EXECUTE);\n-\n-        String apiPath = \"/custom-job-endpoints/initializeRead.api\";\n-        String scriptPath = \"/custom-job-endpoints/initializeRead.sjs\";\n-        mgr.write(apiPath, metadata, new InputStreamHandle(readInputStreamFromClasspath(apiPath)).withFormat(Format.JSON));\n-        mgr.write(scriptPath, metadata, new InputStreamHandle(readInputStreamFromClasspath(scriptPath)).withFormat(Format.TEXT));\n+    void setUp() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n     }\n \n     @Test\n     public void testCustomSchema() {\n-        runAsDataHubDeveloper();\n-        loadSimpleCustomerTDE();\n-        runAsDataHubOperator();\n-        loadTenSimpleCustomers();\n-        Options options = newOptions().withView(\"Customer\").withInitializeReadApiPath(\"/custom-job-endpoints/initializeRead.api\");\n+        setupCustomers();\n+        Options options = newOptions().withView(\"Customer\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n         HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n         StructType testStructType = dataSourceReader.readSchema();\n \n-        assertTrue(testStructType.toList().length() == 2, \"Splice filter did not take effect. \");\n+        assertTrue(testStructType.toList().length() == 2, \"Custom schema not as expected.\");\n         for(int i=0; i<testStructType.toList().length(); i++) {\n             String name = testStructType.toList().apply(0).name();\n-            assertTrue(name.equals(\"customerId\") || name.equals(\"name\"));\n-            assertFalse(name.equals(\"customerSince\"), \"The last StructField was not dropped.\");\n+            assertTrue(name.equals(\"myName\") || name.equals(\"myId\"));\n         }\n     }\n+\n+    @Test\n+    public void testHasEndpointStateWithoutApiPath() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointState in readrowsendpointparams option unless apiPath is defined as well.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testHasEndpointConstants() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointConstants\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testReadRowsEndpointparams() {\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", objectMapper.createObjectNode().put(\"hasRunOnce\", false));\n+        readrowsendpointparams.put(\"apiPath\", \"/custom-read-endpoints/customReadRows.api\");\n+        Options options = newOptions().withView(\"test\").withReadRowsEndpointparams(readrowsendpointparams)\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n+        List<InternalRow> rows = readRows(new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertTrue(rows.isEmpty());\n+    }\n+\n+    private void setupCustomers() {\n+        runAsDataHubDeveloper();\n+        loadSimpleCustomerTDE();\n+        runAsDataHubOperator();\n+        loadTenSimpleCustomers();\n+    }\n }\n"}}, {"oid": "b2af72bf64782998315b17456152959b3272b7f0", "url": "https://github.com/marklogic/marklogic-data-hub/commit/b2af72bf64782998315b17456152959b3272b7f0", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-11-30T05:59:09Z", "type": "forcePushed"}, {"oid": "3abd77b345a84efab105e403c0c2eb21c4165c96", "url": "https://github.com/marklogic/marklogic-data-hub/commit/3abd77b345a84efab105e403c0c2eb21c4165c96", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-01T01:07:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY5MDIzNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r533690237", "bodyText": "This test isn't verifying that the customReadRows.api endpoint is being used, because it's asserting that rows is empty. Rows is empty because no customers have been loaded.\nThe test class I added - ReadWithCustomEndpointsTest - is failing because HubInputPartitionReader needs to look at the options to determine if it should use a custom API path or not. That test should suffice too for the happy path (you'll still want test methods for when endpointState is provided by not apiPath, and also for when endpointConstants is erroneously provided; I recommend moving both of those into ReadWithCustomEndpointsTest).", "author": "rjrudin", "createdAt": "2020-12-01T20:09:29Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,78 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.*;\n+\n+public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void setUp() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n+    }\n+\n+    @Test\n+    public void testCustomSchema() {\n+        setupCustomers();\n+        Options options = newOptions().withView(\"Customer\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        StructType testStructType = dataSourceReader.readSchema();\n+\n+        assertTrue(testStructType.toList().length() == 2, \"Custom schema not as expected.\");\n+        for(int i=0; i<testStructType.toList().length(); i++) {\n+            String name = testStructType.toList().apply(0).name();\n+            assertTrue(name.equals(\"myName\") || name.equals(\"myId\"));\n+        }\n+    }\n+\n+    @Test\n+    public void testHasEndpointStateWithoutApiPath() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointState\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointState in readrowsendpointparams option unless apiPath is defined as well.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testHasEndpointConstants() {\n+        setupCustomers();\n+        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n+        readrowsendpointparams.put(\"endpointConstants\", \"testValue\");\n+        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n+        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n+            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n+        assertEquals(\"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState.\",\n+            ex.getMessage());\n+    }\n+\n+    @Test\n+    public void testReadRowsEndpointparams() {", "originalCommit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d62c66cb746c126345a5e401668fdb8d0181d372", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\ndeleted file mode 100644\nindex d7f465dc5..000000000\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadDataWithCustomEndpointsTest.java\n+++ /dev/null\n\n@@ -1,78 +0,0 @@\n-package com.marklogic.hub.spark.sql.sources.v2.reader;\n-\n-import com.fasterxml.jackson.databind.node.ObjectNode;\n-import com.marklogic.hub.spark.sql.sources.v2.Options;\n-import org.apache.spark.sql.catalyst.InternalRow;\n-import org.apache.spark.sql.types.StructType;\n-import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n-\n-import java.util.List;\n-\n-import static org.junit.jupiter.api.Assertions.*;\n-\n-public class ReadDataWithCustomEndpointsTest extends AbstractSparkReadTest {\n-\n-    @BeforeEach\n-    void setUp() {\n-        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n-        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n-    }\n-\n-    @Test\n-    public void testCustomSchema() {\n-        setupCustomers();\n-        Options options = newOptions().withView(\"Customer\")\n-            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n-        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n-        StructType testStructType = dataSourceReader.readSchema();\n-\n-        assertTrue(testStructType.toList().length() == 2, \"Custom schema not as expected.\");\n-        for(int i=0; i<testStructType.toList().length(); i++) {\n-            String name = testStructType.toList().apply(0).name();\n-            assertTrue(name.equals(\"myName\") || name.equals(\"myId\"));\n-        }\n-    }\n-\n-    @Test\n-    public void testHasEndpointStateWithoutApiPath() {\n-        setupCustomers();\n-        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n-        readrowsendpointparams.put(\"endpointState\", \"testValue\");\n-        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n-        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n-            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n-        assertEquals(\"Cannot set endpointState in readrowsendpointparams option unless apiPath is defined as well.\",\n-            ex.getMessage());\n-    }\n-\n-    @Test\n-    public void testHasEndpointConstants() {\n-        setupCustomers();\n-        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n-        readrowsendpointparams.put(\"endpointConstants\", \"testValue\");\n-        Options options = newOptions().withView(\"Customer\").withReadRowsEndpointparams(readrowsendpointparams);\n-        IllegalArgumentException ex = assertThrows(IllegalArgumentException.class,\n-            () -> new HubDataSourceReader(options.toDataSourceOptions()));\n-        assertEquals(\"Cannot set endpointConstants in readrowsendpointparams option; can only set apiPath and endpointState.\",\n-            ex.getMessage());\n-    }\n-\n-    @Test\n-    public void testReadRowsEndpointparams() {\n-        ObjectNode readrowsendpointparams = objectMapper.createObjectNode();\n-        readrowsendpointparams.put(\"endpointState\", objectMapper.createObjectNode().put(\"hasRunOnce\", false));\n-        readrowsendpointparams.put(\"apiPath\", \"/custom-read-endpoints/customReadRows.api\");\n-        Options options = newOptions().withView(\"test\").withReadRowsEndpointparams(readrowsendpointparams)\n-            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\");\n-        List<InternalRow> rows = readRows(new HubDataSourceReader(options.toDataSourceOptions()));\n-        assertTrue(rows.isEmpty());\n-    }\n-\n-    private void setupCustomers() {\n-        runAsDataHubDeveloper();\n-        loadSimpleCustomerTDE();\n-        runAsDataHubOperator();\n-        loadTenSimpleCustomers();\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY5MDQ4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r533690488", "bodyText": "Once HubInputPartitionReader is updated to use the custom endpoint, this test should pass.", "author": "rjrudin", "createdAt": "2020-12-01T20:09:51Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java", "diffHunk": "@@ -0,0 +1,41 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class ReadWithCustomEndpointsTest extends AbstractSparkReadTest {\n+\n+    @BeforeEach\n+    void setup() {\n+        installCustomEndpoint(\"custom-read-endpoints/initializeRead.api\", \"custom-read-endpoints/initializeRead.sjs\");\n+        installCustomEndpoint(\"custom-read-endpoints/customReadRows.api\", \"custom-read-endpoints/customReadRows.sjs\");\n+    }\n+\n+    @Test\n+    void test() {\n+        ObjectNode endpointState = objectMapper.createObjectNode();\n+        ObjectNode testRow = endpointState.putObject(\"testRow\");\n+        testRow.put(\"myId\", \"12345\");\n+        testRow.put(\"myName\", \"This is a test row\");\n+\n+        Options options = newOptions().withView(\"doesnt-matter\")\n+            .withInitializeReadApiPath(\"/custom-read-endpoints/initializeRead.api\")\n+            .withReadRowsApiPath(\"/custom-read-endpoints/customReadRows.api\")\n+            .withReadRowsEndpointState(endpointState);\n+\n+        List<InternalRow> rows = readRows(new HubDataSourceReader(options.toDataSourceOptions()));", "originalCommit": "3abd77b345a84efab105e403c0c2eb21c4165c96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d62c66cb746c126345a5e401668fdb8d0181d372", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java\nindex 5f758e86c..08333f433 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithCustomEndpointsTest.java\n\n@@ -9,6 +9,7 @@ import org.junit.jupiter.api.Test;\n import java.util.List;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertThrows;\n \n public class ReadWithCustomEndpointsTest extends AbstractSparkReadTest {\n \n"}}, {"oid": "d62c66cb746c126345a5e401668fdb8d0181d372", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d62c66cb746c126345a5e401668fdb8d0181d372", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-02T22:09:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MzIzNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r535373237", "bodyText": "I like to minimize the number of class fields as much as possible to simplify a class; this one can be a local variable in the initializeRead method, it doesn't need to be a class field.", "author": "rjrudin", "createdAt": "2020-12-03T16:12:55Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java", "diffHunk": "@@ -29,6 +33,9 @@\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n+    private InputStreamHandle initializeDefinition;", "originalCommit": "d62c66cb746c126345a5e401668fdb8d0181d372", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "732bbdd1049bf20b7849d1b2295f3531f13019ca", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\nindex 8c66e2f51..695fb8bf6 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubDataSourceReader.java\n\n@@ -33,7 +33,6 @@ public class HubDataSourceReader extends LoggingObject implements DataSourceRead\n     private final Map<String, String> options;\n     private final JsonNode initializationResponse;\n     private final StructType sparkSchema;\n-    private InputStreamHandle initializeDefinition;\n     private JsonNode endpointParams;\n     private boolean hasCustomApiPath;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MzU5OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4916#discussion_r535373598", "bodyText": "Let's make this consistent with the HubInputPartitionReader constructor, where endpointParams is the last argument out of 4.", "author": "rjrudin", "createdAt": "2020-12-03T16:13:22Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java", "diffHunk": "@@ -16,15 +16,17 @@\n     private Map<String, String> options;\n     private JsonNode initializeResponse;\n     private int partitionNumber;\n+    private JsonNode endpointParams;\n \n-    public HubInputPartition(Map<String, String> options, JsonNode initializeResponse, int partitionNumber) {\n+    public HubInputPartition(JsonNode endpointParams, Map<String, String> options, JsonNode initializeResponse, int partitionNumber) {", "originalCommit": "d62c66cb746c126345a5e401668fdb8d0181d372", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "732bbdd1049bf20b7849d1b2295f3531f13019ca", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java\nindex 52e72c0a3..29450ccc6 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartition.java\n\n@@ -18,7 +18,7 @@ public class HubInputPartition implements InputPartition<InternalRow> {\n     private int partitionNumber;\n     private JsonNode endpointParams;\n \n-    public HubInputPartition(JsonNode endpointParams, Map<String, String> options, JsonNode initializeResponse, int partitionNumber) {\n+    public HubInputPartition(Map<String, String> options, JsonNode initializeResponse, int partitionNumber, JsonNode endpointParams) {\n         this.options = options;\n         this.initializeResponse = initializeResponse;\n         this.partitionNumber = partitionNumber;\n"}}, {"oid": "732bbdd1049bf20b7849d1b2295f3531f13019ca", "url": "https://github.com/marklogic/marklogic-data-hub/commit/732bbdd1049bf20b7849d1b2295f3531f13019ca", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-03T19:53:04Z", "type": "forcePushed"}, {"oid": "c3314a133efccc235d871f5aa85d15303ef9b611", "url": "https://github.com/marklogic/marklogic-data-hub/commit/c3314a133efccc235d871f5aa85d15303ef9b611", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T18:23:41Z", "type": "forcePushed"}, {"oid": "675038c5ceff4eb1580c9fd26190f3b49f4b693d", "url": "https://github.com/marklogic/marklogic-data-hub/commit/675038c5ceff4eb1580c9fd26190f3b49f4b693d", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T21:12:25Z", "type": "commit"}, {"oid": "675038c5ceff4eb1580c9fd26190f3b49f4b693d", "url": "https://github.com/marklogic/marklogic-data-hub/commit/675038c5ceff4eb1580c9fd26190f3b49f4b693d", "message": "DHFPROD-6172 : Customize Spark connector endpoints for reading rows", "committedDate": "2020-12-04T21:12:25Z", "type": "forcePushed"}]}