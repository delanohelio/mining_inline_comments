{"pr_number": 4816, "pr_title": "DHFPROD-5912: Initial read capability for Spark connector", "pr_createdAt": "2020-11-03T13:50:35Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4816", "timeline": [{"oid": "e84038b95dc0e710b44e214657d4ec031034eb5c", "url": "https://github.com/marklogic/marklogic-data-hub/commit/e84038b95dc0e710b44e214657d4ec031034eb5c", "message": "DHFPROD-5912: Initial read capability for Spark connector\n\nAs covered by the JIRA ticket, this is intentionally constrained to the minimum needed to read Customer instances in the example project. The ReadCustomers program can be used for manual verification of that. \n\nMoved Customer/ReferenceModelProject into testFixtures in API project so they can be reused in Spark project.", "committedDate": "2020-11-04T02:42:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzA2NzU2Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4816#discussion_r517067563", "bodyText": "This is only supporting what's need for the Customer view in the reference-entity-model project. Will be expanded via a separate story.", "author": "rjrudin", "createdAt": "2020-11-04T02:44:09Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java", "diffHunk": "@@ -0,0 +1,174 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.marklogic.client.dataservices.OutputCaller;\n+import com.marklogic.client.ext.helper.LoggingObject;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Util;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.Arrays;\n+import java.util.Date;\n+import java.util.Map;\n+\n+/**\n+ * Does all the work of using a BulkOutputCaller to repeatedly fetch rows for a given partition.\n+ */\n+public class HubInputPartitionReader extends LoggingObject implements InputPartitionReader<InternalRow> {\n+\n+    private final HubClient hubClient;\n+    private final ObjectMapper objectMapper;\n+    private final OutputCaller.BulkOutputCaller<InputStream> bulkOutputCaller;\n+    private final StructType schema;\n+\n+    private InputStream[] rows;\n+    private int rowIndex;\n+    private JsonNode currentRow;\n+    private long numberOfRowsRead;\n+\n+    /**\n+     * @param options                options provided by the Spark user; needed to both connect to ML and to query for data\n+     * @param initializationResponse\n+     * @param partitionNumber\n+     */\n+    public HubInputPartitionReader(Map<String, String> options, JsonNode initializationResponse, int partitionNumber) {\n+        this.hubClient = HubClient.withHubClientConfig(Util.buildHubClientConfig(options));\n+        this.schema = (StructType) StructType.fromJson(initializationResponse.get(\"schema\").toString());\n+        this.objectMapper = new ObjectMapper();\n+\n+        ObjectNode endpointConstants = buildEndpointConstants(options, initializationResponse, partitionNumber);\n+        ObjectNode endpointState = objectMapper.createObjectNode();\n+        endpointState.put(\"batchNumber\", 1);\n+        this.bulkOutputCaller = buildOutputCaller(endpointConstants, endpointState);\n+    }\n+\n+    @Override\n+    public boolean next() throws IOException {\n+        if (rows == null || rowIndex >= rows.length) {\n+            readNextBatchOfRows();\n+        }\n+\n+        if (rows == null || rows.length == 0) {\n+            logger.info(\"Finished reading rows\");\n+            return false;\n+        }\n+\n+        this.currentRow = objectMapper.readTree(rows[rowIndex++]);\n+        return true;\n+    }\n+\n+    /**\n+     * Convert the most recently read JSON object into an InternalRow.\n+     *\n+     * @return\n+     */\n+    @Override\n+    public InternalRow get() {\n+        Object[] values = Arrays.stream(schema.fields()).map(field -> {\n+            String fieldName = field.name();\n+            if (currentRow.has(fieldName) && !\"null\".equals(currentRow.get(fieldName).asText())) {\n+                return readValue(field);\n+            }\n+            return null;\n+        }).toArray();\n+\n+        Row row = RowFactory.create(values);\n+        return RowEncoder.apply(this.schema).toRow(row);\n+    }\n+\n+    @Override\n+    public void close() {\n+        logger.debug(\"Closing\");\n+    }\n+\n+    private ObjectNode buildEndpointConstants(Map<String, String> options, JsonNode initializationResponse, int partitionNumber) {\n+        ObjectNode endpointConstants = objectMapper.createObjectNode();\n+        endpointConstants.set(\"initializationResponse\", initializationResponse);\n+        endpointConstants.put(\"partitionNumber\", partitionNumber);\n+        try {\n+            endpointConstants.set(\"sparkSchema\", objectMapper.readTree(schema.json()));\n+        } catch (Exception ex) {\n+            throw new RuntimeException(\"Unable to write StructType as JSON; cause: \" + ex.getMessage(), ex);\n+        }\n+        return endpointConstants;\n+    }\n+\n+    private OutputCaller.BulkOutputCaller<InputStream> buildOutputCaller(ObjectNode endpointConstants, ObjectNode endpointState) {\n+        InputStreamHandle defaultApi = hubClient.getModulesClient().newJSONDocumentManager()\n+            .read(\"/marklogic-data-hub-spark-connector/readRows.api\", new InputStreamHandle());\n+\n+        OutputCaller<InputStream> outputCaller = OutputCaller.on(hubClient.getFinalClient(), defaultApi, new InputStreamHandle());\n+\n+        return outputCaller.bulkCaller(outputCaller.newCallContext()\n+            .withEndpointConstants(new JacksonHandle(endpointConstants))\n+            .withEndpointState(new JacksonHandle(endpointState)));\n+    }\n+\n+    private void readNextBatchOfRows() {\n+        this.rows = bulkOutputCaller.next();\n+        if (rows.length > 0) {\n+            numberOfRowsRead += rows.length;\n+            if (logger.isDebugEnabled()) {\n+                logger.debug(\"Rows read so far: \" + numberOfRowsRead);\n+            }\n+            rowIndex = 0;\n+        }\n+    }\n+\n+    /**\n+     * Just doing the bare minimum here for now, will have lots more to support soon.\n+     *\n+     * @param field\n+     * @return\n+     */\n+    private Object readValue(StructField field) {\n+        final String fieldName = field.name();\n+        Object value;\n+        switch (field.dataType().typeName()) {\n+            case \"integer\":", "originalCommit": "e84038b95dc0e710b44e214657d4ec031034eb5c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "fc273ad5cf4f4e9f2609b151ca74f68af089d578", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java\nindex 1e2127272..6075b4b02 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/reader/HubInputPartitionReader.java\n\n@@ -56,6 +56,13 @@ public class HubInputPartitionReader extends LoggingObject implements InputParti\n         this.bulkOutputCaller = buildOutputCaller(endpointConstants, endpointState);\n     }\n \n+    /**\n+     * If this reader's rows array is null or empty, then call the endpoint to read the next batch of rows. If no\n+     * rows are returned, then this returns false, as the endpoint is indicating it has found no more matching rows.\n+     *\n+     * @return\n+     * @throws IOException\n+     */\n     @Override\n     public boolean next() throws IOException {\n         if (rows == null || rowIndex >= rows.length) {\n"}}, {"oid": "fc273ad5cf4f4e9f2609b151ca74f68af089d578", "url": "https://github.com/marklogic/marklogic-data-hub/commit/fc273ad5cf4f4e9f2609b151ca74f68af089d578", "message": "DHFPROD-5912: Initial read capability for Spark connector\n\nAs covered by the JIRA ticket, this is intentionally constrained to the minimum needed to read Customer instances in the example project. The ReadCustomers program can be used for manual verification of that. \n\nMoved Customer/ReferenceModelProject into testFixtures in API project so they can be reused in Spark project.\nasdf", "committedDate": "2020-11-05T14:34:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODA5ODA4OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4816#discussion_r518098088", "bodyText": "We'll have to keep an eye on this. Creating multiple HubDataSourceReader objects isn't good, but I'm worried there's some downside to caching this instance that I haven't found yet.", "author": "rjrudin", "createdAt": "2020-11-05T14:36:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java", "diffHunk": "@@ -16,28 +16,51 @@\n package com.marklogic.hub.spark.sql.sources.v2;\n \n import com.marklogic.client.ext.helper.LoggingObject;\n+import com.marklogic.hub.spark.sql.sources.v2.reader.HubDataSourceReader;\n import com.marklogic.hub.spark.sql.sources.v2.writer.HubDataSourceWriter;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.sources.v2.DataSourceOptions;\n+import org.apache.spark.sql.sources.v2.ReadSupport;\n import org.apache.spark.sql.sources.v2.StreamWriteSupport;\n import org.apache.spark.sql.sources.v2.WriteSupport;\n+import org.apache.spark.sql.sources.v2.reader.DataSourceReader;\n import org.apache.spark.sql.sources.v2.writer.DataSourceWriter;\n import org.apache.spark.sql.sources.v2.writer.streaming.StreamWriter;\n import org.apache.spark.sql.streaming.OutputMode;\n import org.apache.spark.sql.types.StructType;\n \n import java.util.Optional;\n \n-public class DefaultSource extends LoggingObject implements WriteSupport, StreamWriteSupport {\n+public class DefaultSource extends LoggingObject implements WriteSupport, StreamWriteSupport, ReadSupport {\n+\n+    private HubDataSourceReader hubDataSourceReader;\n+\n+    public DefaultSource() {\n+        logger.debug(\"Created: \" + toString());\n+    }\n \n     @Override\n     public Optional<DataSourceWriter> createWriter(String writeUUID, StructType schema, SaveMode mode, DataSourceOptions options) {\n-        return Optional.of(new HubDataSourceWriter(options.asMap(), schema, false) {\n-        });\n+        return Optional.of(new HubDataSourceWriter(options.asMap(), schema, false));\n     }\n \n     @Override\n     public StreamWriter createStreamWriter(String queryId, StructType schema, OutputMode mode, DataSourceOptions options) {\n         return new HubDataSourceWriter(options.asMap(), schema, true);\n     }\n+\n+    @Override\n+    public DataSourceReader createReader(DataSourceOptions options) {\n+        // Logging of the HubDataSourceReader's constructor indicates that in a simple Spark test program, this method\n+        // is called multiple times. On the first occasion, the getSchema method is called, and then instance is seemingly\n+        // discarded. On the second occasion, the planInputPartitions method is called, which allows for partition\n+        // readers to then be created. And then this is called on a third occasion for unknown reasons. For performance\n+        // reasons then, this class will only create one HubDataSourceReader. Testing has shown that if the Spark\n+        // program then calls \"format\" again on a SQLContext, a new instance of this class - DefaultSource - will be\n+        // created, thus ensuring that a new data source reader is created.\n+        if (hubDataSourceReader == null) {", "originalCommit": "fc273ad5cf4f4e9f2609b151ca74f68af089d578", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"oid": "28414e7fcb34c37ea0f8b363009699a065e69e33", "url": "https://github.com/marklogic/marklogic-data-hub/commit/28414e7fcb34c37ea0f8b363009699a065e69e33", "message": "DHFPROD-5912: Initial read capability for Spark connector\n\nAs covered by the JIRA ticket, this is intentionally constrained to the minimum needed to read Customer instances in the example project. The ReadCustomers program can be used for manual verification of that. \n\nMoved Customer/ReferenceModelProject into testFixtures in API project so they can be reused in Spark project.", "committedDate": "2020-11-05T14:50:22Z", "type": "commit"}, {"oid": "28414e7fcb34c37ea0f8b363009699a065e69e33", "url": "https://github.com/marklogic/marklogic-data-hub/commit/28414e7fcb34c37ea0f8b363009699a065e69e33", "message": "DHFPROD-5912: Initial read capability for Spark connector\n\nAs covered by the JIRA ticket, this is intentionally constrained to the minimum needed to read Customer instances in the example project. The ReadCustomers program can be used for manual verification of that. \n\nMoved Customer/ReferenceModelProject into testFixtures in API project so they can be reused in Spark project.", "committedDate": "2020-11-05T14:50:22Z", "type": "forcePushed"}]}