{"pr_number": 4778, "pr_title": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "pr_createdAt": "2020-10-27T20:50:03Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4778", "timeline": [{"oid": "65a4f6b30d3c8fe6fdd515dcb13eee10a5623cdb", "url": "https://github.com/marklogic/marklogic-data-hub/commit/65a4f6b30d3c8fe6fdd515dcb13eee10a5623cdb", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-27T20:52:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA0OTg2Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513049863", "bodyText": "why do we need to set this to null explicitly if user does not configure?", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-27T21:41:31Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java", "diffHunk": "@@ -237,14 +237,17 @@ private DocumentMetadataHandle buildDocumentMetadata() {\n     private String initializeJob(StructType schema) {\n         loadJobEndpoints();\n \n-        ObjectNode sparkMetadata = objectMapper.createObjectNode();\n+        ObjectNode externalMetadata = objectMapper.createObjectNode();\n         try {\n-            sparkMetadata.set(\"schema\", objectMapper.readTree(schema.json()));\n+            externalMetadata.set(\"additionalExternalMetadata\",", "originalCommit": "65a4f6b30d3c8fe6fdd515dcb13eee10a5623cdb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2MzEyNA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513063124", "bodyText": "I'll edit the JIRA ticket with my proposal - I don't think we should force an \"additionalExternalMetadata\" key to exist. Instead, each key from the additionalexternalmetadata object should be added to the externalMetadata object.", "author": "rjrudin", "createdAt": "2020-10-27T22:10:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA0OTg2Mw=="}], "type": "inlineReview", "revised_code": {"commit": "d0b6ff9f3fd834cea4649faeba58d2532abd8d66", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java\nindex 1615af1c8..b744240e3 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/DefaultSource.java\n\n@@ -239,8 +239,9 @@ class HubDataSourceWriter extends LoggingObject implements StreamWriter {\n \n         ObjectNode externalMetadata = objectMapper.createObjectNode();\n         try {\n-            externalMetadata.set(\"additionalExternalMetadata\",\n-                options.get(\"additionalexternalmetadata\")!=null ? objectMapper.readTree(options.get(\"additionalexternalmetadata\")):null);\n+            if(options.get(\"additionalexternalmetadata\")!=null) {\n+                externalMetadata = (ObjectNode) objectMapper.readTree(options.get(\"additionalexternalmetadata\"));\n+            }\n             externalMetadata.set(\"sparkSchema\", objectMapper.readTree(schema.json()));\n \n         } catch (Exception e) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA1MTc5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513051792", "bodyText": "you might need to regenerate the service class again.", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-27T21:45:30Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/dataservices/SparkService.java", "diffHunk": "@@ -103,7 +103,7 @@ private String initializeJob(BaseProxy.DBFunctionRequest request, com.fasterxml.\n               return BaseProxy.StringType.toString(\n                 request\n                       .withParams(\n-                          BaseProxy.documentParam(\"sparkMetadata\", true, BaseProxy.JsonDocumentType.fromJsonNode(sparkMetadata))", "originalCommit": "65a4f6b30d3c8fe6fdd515dcb13eee10a5623cdb", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d0b6ff9f3fd834cea4649faeba58d2532abd8d66", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/dataservices/SparkService.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/dataservices/SparkService.java\nindex 1aadb2eed..68a553296 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/dataservices/SparkService.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/dataservices/SparkService.java\n\n@@ -49,61 +49,61 @@ public interface SparkService {\n             private DatabaseClient dbClient;\n             private BaseProxy baseProxy;\n \n-            private BaseProxy.DBFunctionRequest req_finalizeJob;\n             private BaseProxy.DBFunctionRequest req_bulkIngest;\n+            private BaseProxy.DBFunctionRequest req_finalizeJob;\n             private BaseProxy.DBFunctionRequest req_initializeJob;\n \n             private SparkServiceImpl(DatabaseClient dbClient, JSONWriteHandle servDecl) {\n                 this.dbClient  = dbClient;\n                 this.baseProxy = new BaseProxy(\"/marklogic-data-hub-spark-connector/\", servDecl);\n \n-                this.req_finalizeJob = this.baseProxy.request(\n-                    \"finalizeJob.sjs\", BaseProxy.ParameterValuesKind.MULTIPLE_ATOMICS);\n                 this.req_bulkIngest = this.baseProxy.request(\n                     \"bulkIngester.sjs\", BaseProxy.ParameterValuesKind.MULTIPLE_NODES);\n+                this.req_finalizeJob = this.baseProxy.request(\n+                    \"finalizeJob.sjs\", BaseProxy.ParameterValuesKind.MULTIPLE_ATOMICS);\n                 this.req_initializeJob = this.baseProxy.request(\n                     \"initializeJob.sjs\", BaseProxy.ParameterValuesKind.SINGLE_NODE);\n             }\n \n             @Override\n-            public void finalizeJob(String jobId, String status) {\n-                finalizeJob(\n-                    this.req_finalizeJob.on(this.dbClient), jobId, status\n+            public void bulkIngest(Reader endpointConstants, Stream<Reader> input) {\n+                bulkIngest(\n+                    this.req_bulkIngest.on(this.dbClient), endpointConstants, input\n                     );\n             }\n-            private void finalizeJob(BaseProxy.DBFunctionRequest request, String jobId, String status) {\n+            private void bulkIngest(BaseProxy.DBFunctionRequest request, Reader endpointConstants, Stream<Reader> input) {\n               request\n                       .withParams(\n-                          BaseProxy.atomicParam(\"jobId\", false, BaseProxy.StringType.fromString(jobId)),\n-                          BaseProxy.atomicParam(\"status\", false, BaseProxy.StringType.fromString(status))\n+                          BaseProxy.documentParam(\"endpointConstants\", true, BaseProxy.JsonDocumentType.fromReader(endpointConstants)),\n+                          BaseProxy.documentParam(\"input\", true, BaseProxy.JsonDocumentType.fromReader(input))\n                           ).responseNone();\n             }\n \n             @Override\n-            public void bulkIngest(Reader endpointConstants, Stream<Reader> input) {\n-                bulkIngest(\n-                    this.req_bulkIngest.on(this.dbClient), endpointConstants, input\n+            public void finalizeJob(String jobId, String status) {\n+                finalizeJob(\n+                    this.req_finalizeJob.on(this.dbClient), jobId, status\n                     );\n             }\n-            private void bulkIngest(BaseProxy.DBFunctionRequest request, Reader endpointConstants, Stream<Reader> input) {\n+            private void finalizeJob(BaseProxy.DBFunctionRequest request, String jobId, String status) {\n               request\n                       .withParams(\n-                          BaseProxy.documentParam(\"endpointConstants\", true, BaseProxy.JsonDocumentType.fromReader(endpointConstants)),\n-                          BaseProxy.documentParam(\"input\", true, BaseProxy.JsonDocumentType.fromReader(input))\n+                          BaseProxy.atomicParam(\"jobId\", false, BaseProxy.StringType.fromString(jobId)),\n+                          BaseProxy.atomicParam(\"status\", false, BaseProxy.StringType.fromString(status))\n                           ).responseNone();\n             }\n \n             @Override\n-            public String initializeJob(com.fasterxml.jackson.databind.JsonNode sparkMetadata) {\n+            public String initializeJob(com.fasterxml.jackson.databind.JsonNode externalMetadata) {\n                 return initializeJob(\n-                    this.req_initializeJob.on(this.dbClient), sparkMetadata\n+                    this.req_initializeJob.on(this.dbClient), externalMetadata\n                     );\n             }\n-            private String initializeJob(BaseProxy.DBFunctionRequest request, com.fasterxml.jackson.databind.JsonNode sparkMetadata) {\n+            private String initializeJob(BaseProxy.DBFunctionRequest request, com.fasterxml.jackson.databind.JsonNode externalMetadata) {\n               return BaseProxy.StringType.toString(\n                 request\n                       .withParams(\n-                          BaseProxy.documentParam(\"externalMetadata\", true, BaseProxy.JsonDocumentType.fromJsonNode(sparkMetadata))\n+                          BaseProxy.documentParam(\"externalMetadata\", true, BaseProxy.JsonDocumentType.fromJsonNode(externalMetadata))\n                           ).responseSingle(false, null)\n                 );\n             }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2NTE1OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513065158", "bodyText": "The story isn't calling for this, so I don't think we should add more logging here. I don't think this logging is solving any problem either. Ernie already knows what the Run ID of the Glue job is; he can use that to find the DHF job document.", "author": "rjrudin", "createdAt": "2020-10-27T22:15:08Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -80,6 +82,8 @@ public WriterCommitMessage commit() {\n         bulkInputCaller.awaitCompletion();\n         if (writeException != null) {\n             logger.info(\"At least one write failed\");\n+            logger.info(\"Additional External Metadata - \"+ options.get(\"additionalexternalmetadata\"));", "originalCommit": "65a4f6b30d3c8fe6fdd515dcb13eee10a5623cdb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3NTY1OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513075658", "bodyText": "But if the option \"additionalexternalmetadata\" is not there in the script, there is no way Ernie will  know the Glue Run id. In that case we might want to add a logger saying what and where he needs to add/change his Glue script.", "author": "anu3990", "createdAt": "2020-10-27T22:41:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2NTE1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5ODM1NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4778#discussion_r513098355", "bodyText": "I'm not following - I can see logging the DHF job ID here, because if Ernie doesn't provide the Glue Run ID in the options, then the DHF job ID would be useful in the Glue logs as a way of finding the associated DHF job document - though it won't be that helpful, since it won't have any additional info in it.\nBut if Ernie doesn't provide additionalexternalmetadata - well that's his choice. Our documentation can make it clear that that's the way he can add additional data to the job document. Also, we wouldn't want Glue-specific logging in a Spark-generic connector.", "author": "rjrudin", "createdAt": "2020-10-27T23:50:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2NTE1OA=="}], "type": "inlineReview", "revised_code": {"commit": "d0b6ff9f3fd834cea4649faeba58d2532abd8d66", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex cb82ce8de..ffcd23c43 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -82,8 +80,6 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n         bulkInputCaller.awaitCompletion();\n         if (writeException != null) {\n             logger.info(\"At least one write failed\");\n-            logger.info(\"Additional External Metadata - \"+ options.get(\"additionalexternalmetadata\"));\n-            logger.info(\"If the Additional External Metadata is null please add json values to the option - additionalExternalMetadata in the script.\");\n             return new AtLeastOneWriteFailedMessage();\n         }\n         return null;\n"}}, {"oid": "d0b6ff9f3fd834cea4649faeba58d2532abd8d66", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d0b6ff9f3fd834cea4649faeba58d2532abd8d66", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-28T04:26:34Z", "type": "forcePushed"}, {"oid": "235c4243481bc9a36430672090efd9aee501f98b", "url": "https://github.com/marklogic/marklogic-data-hub/commit/235c4243481bc9a36430672090efd9aee501f98b", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-28T16:45:48Z", "type": "forcePushed"}, {"oid": "08c4b34fe6e25efc4ad36f280a6b951f88e89ff3", "url": "https://github.com/marklogic/marklogic-data-hub/commit/08c4b34fe6e25efc4ad36f280a6b951f88e89ff3", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-28T17:09:27Z", "type": "forcePushed"}, {"oid": "d7481f90414523b3c04310256acb94c08a67bbaa", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d7481f90414523b3c04310256acb94c08a67bbaa", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-28T18:07:42Z", "type": "commit"}, {"oid": "d7481f90414523b3c04310256acb94c08a67bbaa", "url": "https://github.com/marklogic/marklogic-data-hub/commit/d7481f90414523b3c04310256acb94c08a67bbaa", "message": "DHFPROD-6163:Add the AWS Glue job info in the job document.", "committedDate": "2020-10-28T18:07:42Z", "type": "forcePushed"}]}