{"pr_number": 4891, "pr_title": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "pr_createdAt": "2020-11-19T20:44:29Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4891", "timeline": [{"oid": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "url": "https://github.com/marklogic/marklogic-data-hub/commit/ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "committedDate": "2020-11-21T07:55:19Z", "type": "forcePushed"}, {"oid": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "url": "https://github.com/marklogic/marklogic-data-hub/commit/70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "committedDate": "2020-11-23T18:57:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNjYwNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528926605", "bodyText": "Instead of renaming this, which then changes all the current clients, just introduce this as a new method, and then modify loadSimpleCustomerTDE to call loadTDE(\"Customer\"). That retains a self-documenting method name and avoids changing all the clients.", "author": "rjrudin", "createdAt": "2020-11-23T18:55:48Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java", "diffHunk": "@@ -35,15 +35,15 @@ void verifyMarkLogicSupportsRowID() {\n     }\n \n     // TODO Will soon have a nice convenience method for doing this\n-    protected void loadSimpleCustomerTDE() {\n+    protected void loadTDE(String name) {", "originalCommit": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java\nindex 65e1b060a..9aaa6c3de 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/AbstractSparkReadTest.java\n\n@@ -49,6 +49,10 @@ abstract class AbstractSparkReadTest extends AbstractSparkConnectorTest {\n             .javascript(query).addVariable(\"template\", new StringHandle(template).withFormat(Format.XML)).evalAs(String.class);\n     }\n \n+    protected void loadSimpleCustomerTDE(){\n+        loadTDE(\"Customer\");\n+    }\n+\n     protected void loadTenSimpleCustomers() {\n         ReferenceModelProject project = new ReferenceModelProject(getHubClient());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNjkyMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528926920", "bodyText": "Is this meant to be here?", "author": "rjrudin", "createdAt": "2020-11-23T18:56:27Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java", "diffHunk": "@@ -27,6 +27,7 @@ void test() {\n     }\n \n     private void verifyTwoColumnsSelected() {\n+        readRows(\"customerId,customerSince\");", "originalCommit": "ebfc0b5cc47066c7b18a394ab45108c37fd86b89", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTAwNTcyMA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r529005720", "bodyText": "It's not meant to be there. I was doing some testing and forgot to remove it. Thanks for catching it.", "author": "srinathgit", "createdAt": "2020-11-23T21:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyNjkyMA=="}], "type": "inlineReview", "revised_code": {"commit": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java\nindex 1638874cd..b8bbbdd81 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadWithSelectedColumnsTest.java\n\n@@ -27,7 +27,6 @@ public class ReadWithSelectedColumnsTest extends AbstractSparkReadTest {\n     }\n \n     private void verifyTwoColumnsSelected() {\n-        readRows(\"customerId,customerSince\");\n         readRows(\"customerId,name\").forEach(row -> {\n             assertEquals(2, row.numFields());\n             int customerId = row.getInt(0);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMDgwMw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528930803", "bodyText": "For clarity, just do \"InternalRow row : rows.get(0)\" since we know there's only one.", "author": "rjrudin", "createdAt": "2020-11-23T19:03:26Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {", "originalCommit": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\nindex f14cd0f72..57875d6b1 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n\n@@ -18,7 +18,7 @@ import static org.junit.jupiter.api.Assertions.fail;\n public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n \n     @Test\n-    void readAllDataTypes() {\n+    void readAllDataTypes() throws Exception{\n         runAsDataHubDeveloper();\n         loadTDE(\"User\");\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMTQ2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528931465", "bodyText": "Both this and the date row should have assertion messages explaining why these are the expected values, as it's not clear to the casual reader", "author": "rjrudin", "createdAt": "2020-11-23T19:04:38Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {\n+            //The comment in the assertions will have the datatype of the property in TDE template\n+            assertEquals(1, row.getInt(0), \"integer\");\n+            assertEquals(1, row.getInt(1), \"int\");\n+            assertEquals(Byte.decode(\"1\"), row.getByte(2),\"byte\");\n+            assertEquals(1, row.getInt(3),\"positiveInteger\");\n+            assertEquals(1, row.getInt(4), \"nonNegativeInteger\");\n+            assertEquals(1L, row.getLong(5), \"long\");\n+            assertEquals(1L, row.getLong(6), \"unsignedLong\");\n+            assertEquals(1L, row.getLong(7), \"unsignedInt\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(8), \"short\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(9), \"unsignedByte\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(10), \"unsignedShort\");\n+            assertEquals(\"John\", row.getString(11), \"string\");\n+            assertEquals(6218, row.get(12, DataTypes.DateType), \"date\");\n+            assertEquals(537336758000000L, row.get(13, DataTypes.TimestampType), \"dateTime\");", "originalCommit": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\nindex f14cd0f72..57875d6b1 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n\n@@ -18,7 +18,7 @@ import static org.junit.jupiter.api.Assertions.fail;\n public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n \n     @Test\n-    void readAllDataTypes() {\n+    void readAllDataTypes() throws Exception{\n         runAsDataHubDeveloper();\n         loadTDE(\"User\");\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjEyMg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528932122", "bodyText": "For tests, just do throw new RuntimeException(e). \"fail\" is really for when you expect an exception to occur, but one doesn't - in which case, it's better to use assertThrows.", "author": "rjrudin", "createdAt": "2020-11-23T19:05:36Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java", "diffHunk": "@@ -0,0 +1,91 @@\n+package com.marklogic.hub.spark.sql.sources.v2.reader;\n+\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.Format;\n+import com.marklogic.client.io.InputStreamHandle;\n+import com.marklogic.hub.HubClient;\n+import com.marklogic.hub.spark.sql.sources.v2.Options;\n+import com.marklogic.io.Base64;\n+import org.apache.commons.codec.binary.Hex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.jupiter.api.Test;\n+import java.util.List;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n+\n+    @Test\n+    void readAllDataTypes() {\n+        runAsDataHubDeveloper();\n+        loadTDE(\"User\");\n+\n+        HubClient client = runAsDataHubOperator();\n+        InputStreamHandle handle = new InputStreamHandle(readInputStreamFromClasspath(\"entityInstances/User/user1.json\"));\n+        handle.withFormat(Format.JSON);\n+        client.getFinalClient().newJSONDocumentManager().write(\"/doc1.json\", new DocumentMetadataHandle()\n+                .withCollections(\"User\")\n+                .withPermission(\"data-hub-operator\", DocumentMetadataHandle.Capability.READ, DocumentMetadataHandle.Capability.UPDATE),\n+            handle);\n+\n+\n+        Options options = newOptions().withView(\"User\").withNumPartitions(\"2\");\n+        HubDataSourceReader dataSourceReader = new HubDataSourceReader(options.toDataSourceOptions());\n+        List<InternalRow> rows = readRows(dataSourceReader);\n+        assertEquals(1, rows.size());\n+        for (InternalRow row : rows) {\n+            //The comment in the assertions will have the datatype of the property in TDE template\n+            assertEquals(1, row.getInt(0), \"integer\");\n+            assertEquals(1, row.getInt(1), \"int\");\n+            assertEquals(Byte.decode(\"1\"), row.getByte(2),\"byte\");\n+            assertEquals(1, row.getInt(3),\"positiveInteger\");\n+            assertEquals(1, row.getInt(4), \"nonNegativeInteger\");\n+            assertEquals(1L, row.getLong(5), \"long\");\n+            assertEquals(1L, row.getLong(6), \"unsignedLong\");\n+            assertEquals(1L, row.getLong(7), \"unsignedInt\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(8), \"short\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(9), \"unsignedByte\");\n+            assertEquals(Short.decode(\"1\"), row.getShort(10), \"unsignedShort\");\n+            assertEquals(\"John\", row.getString(11), \"string\");\n+            assertEquals(6218, row.get(12, DataTypes.DateType), \"date\");\n+            assertEquals(537336758000000L, row.get(13, DataTypes.TimestampType), \"dateTime\");\n+            assertEquals(\"20:12:38\", row.getString(14), \"time\");\n+            assertEquals(\"/John/IRI\", row.getString(15), \"IRI\");\n+            assertEquals(\"/John/IRI\", row.getString(16), \"anyURI\");\n+            assertEquals(true, row.getBoolean(17), \"boolean\");\n+            assertEquals(-10, row.getInt(18), \"negativeInteger\");\n+            assertEquals(-10, row.getInt(19), \"nonPositiveInteger\");\n+            assertEquals(30.5f, row.getFloat(20), \"float\");\n+            assertEquals(30.5d, row.getDouble(21), \"double\");\n+            assertEquals(30.5d, row.getDouble(22), \"decimal\");\n+            assertEquals(\"1987-01\", row.getString(23), \"gYearMonth\");\n+            assertEquals(\"1987+02:00\", row.getString(24), \"gYear\");\n+            assertEquals(\"--01+02:00\", row.getString(25), \"gMonth\");\n+            assertEquals(\"---10\", row.getString(26), \"gDay\");\n+            assertEquals(\"--01-10\", row.getString(27), \"gMonthDay\");\n+            assertEquals(\"P1M\", row.getString(28), \"duration\");\n+            assertEquals(\"P1M\", row.getString(29), \"yearMonthDuration\");\n+            assertEquals(\"P30DT1H\", row.getString(30), \"dayTimeDuration\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(31), \"point\");\n+            assertEquals(\"37.389965,-122.07858\", row.getString(32), \"longLatPoint\");\n+\n+            try {\n+                String base64BinaryString = new String(row.getBinary(33), \"UTF-8\");\n+                String hexBinaryString = new String(row.getBinary(34), \"UTF-8\");\n+                assertEquals(\"RHVtIHNwaXJvIHNwZXJv\", base64BinaryString, \"base64Binary\");\n+                assertEquals(\"44756D20737069726F20737065726F\", hexBinaryString, \"hexBinary\");\n+                String s1 = new String(Base64.decode(base64BinaryString), \"UTF-8\");\n+                String s2 = new String(Hex.decodeHex(hexBinaryString.toCharArray()), \"UTF-8\");\n+                assertEquals(\"Dum spiro spero\", s1);\n+                assertEquals(s1, s2);\n+\n+            } catch (Exception e) {\n+                logger.error(\"Failed to decode binary string: cause; \" + e.getMessage());", "originalCommit": "70fb6bc2b4c5984b067a6c9d0410f181100d8db4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjM4Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4891#discussion_r528932383", "bodyText": "Better yet - don't catch anything, just add \"throws Exception\" to the test method", "author": "rjrudin", "createdAt": "2020-11-23T19:05:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMjEyMg=="}], "type": "inlineReview", "revised_code": {"commit": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\nindex f14cd0f72..57875d6b1 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/reader/ReadAllDataTypesTest.java\n\n@@ -18,7 +18,7 @@ import static org.junit.jupiter.api.Assertions.fail;\n public class ReadAllDataTypesTest extends AbstractSparkReadTest{\n \n     @Test\n-    void readAllDataTypes() {\n+    void readAllDataTypes() throws Exception{\n         runAsDataHubDeveloper();\n         loadTDE(\"User\");\n \n"}}, {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "url": "https://github.com/marklogic/marklogic-data-hub/commit/e776e5d2209fbe40fe2fb8721dfb471850c713cd", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "committedDate": "2020-11-23T21:22:33Z", "type": "commit"}, {"oid": "e776e5d2209fbe40fe2fb8721dfb471850c713cd", "url": "https://github.com/marklogic/marklogic-data-hub/commit/e776e5d2209fbe40fe2fb8721dfb471850c713cd", "message": "DHFPROD-6169:Export rows from TDE view that contains every type of TDE column", "committedDate": "2020-11-23T21:22:33Z", "type": "forcePushed"}]}