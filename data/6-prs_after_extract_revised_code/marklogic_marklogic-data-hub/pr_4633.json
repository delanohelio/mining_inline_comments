{"pr_number": 4633, "pr_title": "DHFPROD-5944: Configure Document Ingestion via connector", "pr_createdAt": "2020-09-28T20:10:33Z", "pr_url": "https://github.com/marklogic/marklogic-data-hub/pull/4633", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNTg2NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496205865", "bodyText": "I don't think it's safe to write all of the params as a JSON object, as that could contain the user password and other sensitive information.\nWe should be much more precise about what goes into workUnit. So we should have a method of e.g. \"ObjectNode buildWorkUnitFromParams(Map)\" that builds an ObjectNode based on the params we know may exist in the Map, and nothing more than that.", "author": "rjrudin", "createdAt": "2020-09-28T20:14:12Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -61,7 +62,15 @@ public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<St\n             hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n         ).bulkCaller();\n \n-        loader.setWorkUnit(new ByteArrayInputStream((\"{\\\"taskId\\\":\" + taskId + \"}\").getBytes()));\n+        String workUnit = \"{\\\"taskId\\\":\" + taskId + \"}\";\n+        ObjectMapper objectMapper = new ObjectMapper();\n+        try {\n+            workUnit = workUnit.concat(objectMapper.writeValueAsString(params));", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 55a57cef6..66db3def9 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -43,35 +47,29 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n     private InputEndpoint.BulkInputCaller loader;\n     private StructType schema;\n     private int batchSize;\n+    private ObjectNode defaultWorkUnit;\n \n     /**\n      * @param hubClient\n-     * @param taskId\n      * @param schema\n-     * @param params contains all the params provided by Spark, which will include all connector-specific properties\n+     * @param params    contains all the params provided by Spark, which will include all connector-specific properties\n      */\n-    public HubDataWriter(HubClient hubClient, long taskId, StructType schema, Map<String, String> params) {\n+    public HubDataWriter(HubClient hubClient, StructType schema, Map<String, String> params) {\n         this.records = new ArrayList<>();\n         this.schema = schema;\n         this.batchSize = params.containsKey(\"batchsize\") ? Integer.parseInt(params.get(\"batchsize\")) : 100;\n \n-        final String apiModulePath = params.containsKey(\"apipath\") ? params.get(\"apipath\") : \"/data-hub/5/data-services/ingestion/bulkIngester.api\";\n-        logger.info(\"Will write to endpoint defined by: \" + apiModulePath);\n+        JsonNode endpointParams = determineIngestionEndpointParams(params);\n+\n+        final String apiPath = endpointParams.get(\"apiPath\").asText();\n+        logger.info(\"Will write to endpoint defined by: \" + apiPath);\n         this.loader = InputEndpoint.on(\n             hubClient.getStagingClient(),\n-            hubClient.getModulesClient().newTextDocumentManager().read(apiModulePath, new StringHandle())\n+            hubClient.getModulesClient().newJSONDocumentManager().read(apiPath, new StringHandle())\n         ).bulkCaller();\n \n-        String workUnit = \"{\\\"taskId\\\":\" + taskId + \"}\";\n-        ObjectMapper objectMapper = new ObjectMapper();\n-        try {\n-            workUnit = workUnit.concat(objectMapper.writeValueAsString(params));\n-        } catch (JsonProcessingException e) {\n-            throw new RuntimeException(e);\n-        }\n-\n-        loader.setWorkUnit(new ByteArrayInputStream((workUnit).getBytes()));\n-        loader.setEndpointState(new ByteArrayInputStream((\"{\\\"next\\\":\" + 0 + \", \\\"uriprefix\\\":\\\"\" + params.get(\"uriprefix\") + \"\\\"}\").getBytes()));\n+        this.loader.setEndpointState(new JacksonHandle(endpointParams.get(\"endpointState\")));\n+        this.loader.setWorkUnit(new JacksonHandle(endpointParams.get(\"workUnit\")));\n     }\n \n     @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwNjM3OA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r496206378", "bodyText": "See my comment to @anu3990  about an Options class that will simplify writing tests, as opposed to building a Map. We will likely have many tests that populate different sets of options. The Options class will handle that for us.", "author": "rjrudin", "createdAt": "2020-09-28T20:15:14Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java", "diffHunk": "@@ -86,9 +86,13 @@ public void testBulkIngestWithoutUriPrefix() throws IOException {\n         Map<String, String> params = getHubPropertiesAsMap();\n         params.put(\"batchsize\", batchSize);\n \n-        if(uriPrefix!=null && uriPrefix.length()!=0) {\n+        if (uriPrefix != null && uriPrefix.length() != 0) {\n             params.put(\"uriprefix\", uriPrefix);\n         }\n+        params.put(\"collections\", \"fruits,gluefruits\");", "originalCommit": "1ab74fa3d1727d1de9bc63bb967cb483d696e959", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\nindex 7fd19c057..a749754c2 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataTest.java\n\n@@ -68,49 +52,50 @@ public class WriteDataTest extends AbstractSparkConnectorTest {\n         assertFalse(uriQueryResult.hasNext());\n     }\n \n-    /**\n-     * Spark will do all of this in the real world - i.e. a user will specify the entry class and the set of options.\n-     * But in a test, we need to do that ourselves. So we create the DataSource class, build up the params, and then\n-     * call the factory/writer methods ourselves.\n-     *\n-     * @param batchSize\n-     * @param uriPrefix\n-     * @return\n-     */\n-    private DataWriter<InternalRow> buildDataWriter(String batchSize, String uriPrefix) {\n-        HubDataSource dataSource = new HubDataSource();\n-        final String writeUUID = \"doesntMatter\";\n-        final SaveMode saveModeDoesntMatter = SaveMode.Overwrite;\n-\n-        // Get the set of DHF properties used to connect to ML as a map, and then add connector-specific params\n-        Map<String, String> params = getHubPropertiesAsMap();\n-        params.put(\"batchsize\", batchSize);\n-\n-        if (uriPrefix != null && uriPrefix.length() != 0) {\n-            params.put(\"uriprefix\", uriPrefix);\n-        }\n-        params.put(\"collections\", \"fruits,gluefruits\");\n-        params.put(\"sourceName\", \"glue\");\n-        params.put(\"sourceType\", \"xml\");\n-        params.put(\"permissions\", \"data-hub-common,read,data-hub-operator,update\");\n-\n-        DataSourceOptions options = new DataSourceOptions(params);\n+    @Test\n+    public void ingestWithoutCustomApiWithCustomWorkunit(){\n+        ObjectNode customWorkUnit = objectMapper.createObjectNode();\n+        customWorkUnit.put(\"userDefinedValue\", 0);\n+\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestWorkUnit(customWorkUnit)),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        assertEquals(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\", ex.getMessage());\n+    }\n \n-        Optional<DataSourceWriter> dataSourceWriter = dataSource.createWriter(writeUUID, SCHEMA, saveModeDoesntMatter, options);\n-        DataWriterFactory<InternalRow> dataWriterFactory = dataSourceWriter.get().createWriterFactory();\n+    @Test\n+    public void ingestWithIncorrectApi(){\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestApiPath(\"/incorrect.api\")),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        System.out.println(ex.getMessage());\n+        assertTrue( ex.getMessage().contains(\"Could not read non-existent document.\"));\n+    }\n \n-        final int partitionIdDoesntMatter = 0;\n-        final long taskId = 2;\n-        final int epochIdDoesntMatter = 0;\n-        return dataWriterFactory.createDataWriter(partitionIdDoesntMatter, taskId, epochIdDoesntMatter);\n+    @Test\n+    public void ingestWithEmptyApiWithCustomWorkUnit() {\n+        ObjectNode customWorkUnit = objectMapper.createObjectNode();\n+        customWorkUnit.put(\"userDefinedValue\", 0);\n+\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestApiPath(\"\").withIngestWorkUnit(customWorkUnit)),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        assertEquals(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\", ex.getMessage());\n     }\n \n-    private GenericInternalRow buildRow(String... values) {\n-        Object[] rowValues = new Object[values.length];\n-        for (int i = 0; i < values.length; i++) {\n-            rowValues[i] = UTF8String.fromString(values[i]);\n-        }\n-        return new GenericInternalRow(rowValues);\n+    @Test\n+    public void ingestWithEmptyApiWithCustomEndpointState() {\n+        ObjectNode  customEndpointState= objectMapper.createObjectNode();\n+        customEndpointState.put(\"userDefinedValue\", 0);\n+\n+        RuntimeException ex = assertThrows(RuntimeException.class,\n+            () -> buildDataWriter(new Options(getHubPropertiesAsMap()).withIngestApiPath(\"\").withIngestEndpointState(customEndpointState)),\n+            \"Expected an error because a custom work unit was provided without a custom API path\"\n+        );\n+        assertEquals(\"Cannot set workUnit or endpointState in ingestionendpointparams unless apiPath is defined as well.\", ex.getMessage());\n     }\n \n     private void verifyFruitCount(int expectedCount, String message) {\n"}}, {"oid": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "url": "https://github.com/marklogic/marklogic-data-hub/commit/6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T18:50:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016284", "bodyText": "uriprefix should go into buildWorkUnitFromParams. Also, I'd just call that \"buildDefaultWorkUnit\" - it's clear that it's built from params because that's the sole argument.", "author": "rjrudin", "createdAt": "2020-10-02T19:40:58Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMjk3Ng==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499032976", "bodyText": "Added uriprefix to buildDefaultWorkUnit", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjI4NA=="}], "type": "inlineReview", "revised_code": {"commit": "410088ee47dbd98710c389893448d1f1b40e33bb", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 66db3def9..33bf28be9 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -152,27 +152,21 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n             defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");\n-            buildWorkUnitFromParams(params);\n+            if (params.get(\"uriprefix\") == null) {\n+                params.put(\"uriprefix\", \"\");\n+            }\n+            buildDefaultWorkUnit(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n \n         return endpointParams;\n     }\n \n-    protected void buildWorkUnitFromParams(Map<String, String> params) {\n-        if (params.get(\"collections\") != null) {\n-            defaultWorkUnit.put(\"collections\", params.get(\"collections\"));\n-        }\n-        if (params.get(\"permissions\") != null) {\n-            defaultWorkUnit.put(\"permissions\", params.get(\"permissions\"));\n-        }\n-        if (params.get(\"sourcename\") != null) {\n-            defaultWorkUnit.put(\"sourceName\", params.get(\"sourcename\"));\n-        }\n-        if (params.get(\"sourcetype\") != null) {\n-            defaultWorkUnit.put(\"sourceType\", params.get(\"sourcetype\"));\n-        }\n-\n+    protected void buildDefaultWorkUnit(Map<String, String> params) {\n+        Stream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n+            if (params.containsKey(key)) {\n+                defaultWorkUnit.put(key, params.get(key));\n+            }\n+        });\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499016973", "bodyText": "A more succinct approach with Jackson is to use \"has\":\nif (params.has(\"collections\"))\n\nAlso, all 4 conditionals are the same, so you could simplify this like so:\nStream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n  if (params.containsKey(key)) {\n    defaultWorkUnit.put(key, params.get(key));\n  }\n}\n\nThe one downside is we have to use the lower-casing that Spark requires. I think we can live with that for now though.", "author": "rjrudin", "createdAt": "2020-10-02T19:42:41Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,28 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");\n+            buildWorkUnitFromParams(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n \n         return endpointParams;\n     }\n+\n+    protected void buildWorkUnitFromParams(Map<String, String> params) {\n+        if (params.get(\"collections\") != null) {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzA1NA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033054", "bodyText": "Updated the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:21:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxNjk3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "410088ee47dbd98710c389893448d1f1b40e33bb", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 66db3def9..33bf28be9 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -152,27 +152,21 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n             defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\") != null ? params.get(\"uriprefix\") : \"\");\n-            buildWorkUnitFromParams(params);\n+            if (params.get(\"uriprefix\") == null) {\n+                params.put(\"uriprefix\", \"\");\n+            }\n+            buildDefaultWorkUnit(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n \n         return endpointParams;\n     }\n \n-    protected void buildWorkUnitFromParams(Map<String, String> params) {\n-        if (params.get(\"collections\") != null) {\n-            defaultWorkUnit.put(\"collections\", params.get(\"collections\"));\n-        }\n-        if (params.get(\"permissions\") != null) {\n-            defaultWorkUnit.put(\"permissions\", params.get(\"permissions\"));\n-        }\n-        if (params.get(\"sourcename\") != null) {\n-            defaultWorkUnit.put(\"sourceName\", params.get(\"sourcename\"));\n-        }\n-        if (params.get(\"sourcetype\") != null) {\n-            defaultWorkUnit.put(\"sourceType\", params.get(\"sourcetype\"));\n-        }\n-\n+    protected void buildDefaultWorkUnit(Map<String, String> params) {\n+        Stream.of(\"collections\", \"permissions\", \"sourcename\", \"sourcetype\", \"uriprefix\").forEach(key -> {\n+            if (params.containsKey(key)) {\n+                defaultWorkUnit.put(key, params.get(key));\n+            }\n+        });\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499018126", "bodyText": "Put \"Test\" as a suffix on this class name, that's our convention for every test class.", "author": "rjrudin", "createdAt": "2020-10-02T19:45:31Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptions extends AbstractSparkConnectorTest {", "originalCommit": "6374bde59eb128b8dbdd80aa70068b0c0d6f98bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAzMzE3NQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499033175", "bodyText": "Renamed the class", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-02T20:22:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTAxODEyNg=="}], "type": "inlineReview", "revised_code": {"commit": "410088ee47dbd98710c389893448d1f1b40e33bb", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\nsimilarity index 98%\nrename from marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java\nrename to marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\nindex 3364fe42e..cfbdbe360 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptions.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n\n@@ -16,7 +16,7 @@ import java.io.IOException;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n \n-public class WriteDataWithOptions extends AbstractSparkConnectorTest {\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n \n     @Test\n     void ingestDocsWithCollection() throws IOException {\n"}}, {"oid": "410088ee47dbd98710c389893448d1f1b40e33bb", "url": "https://github.com/marklogic/marklogic-data-hub/commit/410088ee47dbd98710c389893448d1f1b40e33bb", "message": "DHFPROD-5944: Configure Document Ingestion via connector", "committedDate": "2020-10-02T20:19:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637417", "bodyText": "The endpoint should do this, not the connector. The endpoint should say - if uriprefix is in the workUnit, then I'll use that, even if it's null. It's up to the client to pass in the correct value for uriprefix - if the client passes in \"uriprefix\": null, then the endpoint says - Well I guess you want null as the prefix, so I'll use that.", "author": "rjrudin", "createdAt": "2020-10-05T14:24:00Z", "path": "marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java", "diffHunk": "@@ -151,11 +151,22 @@ protected JsonNode determineIngestionEndpointParams(Map<String, String> params)\n         }\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n-            ObjectNode defaultWorkUnit = objectMapper.createObjectNode();\n-            defaultWorkUnit.put(\"uriprefix\", params.get(\"uriprefix\")!=null ? params.get(\"uriprefix\"):\"\");\n+            defaultWorkUnit = objectMapper.createObjectNode();\n+            if (params.get(\"uriprefix\") == null) {", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDM5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804390", "bodyText": "Made the changes", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzQxNw=="}], "type": "inlineReview", "revised_code": {"commit": "8b72cb309d0c8138f8669d70f3328b8775453a26", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\nindex 33bf28be9..f5d1d0495 100644\n--- a/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n+++ b/marklogic-data-hub-spark-connector/src/main/java/com/marklogic/hub/spark/sql/sources/v2/writer/HubDataWriter.java\n\n@@ -152,9 +152,6 @@ public class HubDataWriter extends LoggingObject implements DataWriter<InternalR\n \n         if (!endpointParams.hasNonNull(\"workUnit\")) {\n             defaultWorkUnit = objectMapper.createObjectNode();\n-            if (params.get(\"uriprefix\") == null) {\n-                params.put(\"uriprefix\", \"\");\n-            }\n             buildDefaultWorkUnit(params);\n             endpointParams.set(\"workUnit\", defaultWorkUnit);\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499637835", "bodyText": "This test should verify that two collections work, so e.g. \"fruits,stuff\".", "author": "rjrudin", "createdAt": "2020-10-05T14:24:32Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDU5Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804592", "bodyText": "Addressed via https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzNzgzNQ=="}], "type": "inlineReview", "revised_code": {"commit": "8b72cb309d0c8138f8669d70f3328b8775453a26", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\nindex cfbdbe360..ce2079fe4 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n\n@@ -1,89 +1,72 @@\n package com.marklogic.hub.spark.sql.sources.v2;\n \n import com.fasterxml.jackson.databind.JsonNode;\n-import com.marklogic.client.document.JSONDocumentManager;\n-import com.marklogic.client.eval.EvalResultIterator;\n-import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n-import com.marklogic.client.ext.util.DocumentPermissionsParser;\n import com.marklogic.client.io.DocumentMetadataHandle;\n import com.marklogic.client.io.JacksonHandle;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.sources.v2.writer.DataWriter;\n-import org.junit.Assert;\n import org.junit.jupiter.api.Test;\n \n import java.io.IOException;\n+import java.util.Set;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n \n     @Test\n     void ingestDocsWithCollection() throws IOException {\n-        String collections = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        String collections = \"fruits,stuff\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withCollections(collections));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n+        DocumentMetadataHandle metadata = getFirstFruitMetadata();\n+        assertEquals(2, metadata.getCollections().size());\n+        assertTrue(metadata.getCollections().contains(\"fruits\"));\n+        assertTrue(metadata.getCollections().contains(\"stuff\"));\n \n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+        // Verify default permissions\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        Set<DocumentMetadataHandle.Capability> capabilities = perms.get(\"data-hub-common\");\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.READ));\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.UPDATE));\n     }\n \n     @Test\n     void ingestDocsWithSourceName() throws IOException {\n         String sourceName = \"spark\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceName(sourceName));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceName, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"name\").asText());\n     }\n \n     @Test\n     void ingestDocsWithSourceType() throws IOException {\n         String sourceType = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceType(sourceType));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceType(sourceType));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceType, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"datahubSourceType\").asText());\n     }\n \n     @Test\n     void ingestDocsWithPermissions() throws IOException {\n-        String permissions = \"data-hub-common,read,data-hub-operator,update\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withPermissions(permissions));\n+        String permissions = \"qconsole-user,read,manage-admin,update\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withPermissions(permissions));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        DocumentPermissionsParser documentPermissionParser = new DefaultDocumentPermissionsParser();\n-        DocumentMetadataHandle permissionMetadata = new DocumentMetadataHandle();\n-        documentPermissionParser.parsePermissions(permissions, permissionMetadata.getPermissions());\n-\n-        Assert.assertEquals(permissionMetadata.getPermissions(), docMetaData.getPermissions());\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        assertEquals(DocumentMetadataHandle.Capability.READ, perms.get(\"qconsole-user\").iterator().next());\n+        assertEquals(DocumentMetadataHandle.Capability.UPDATE, perms.get(\"manage-admin\").iterator().next());\n     }\n \n+    private DocumentMetadataHandle getFirstFruitMetadata() {\n+        String[] uris = getFruitUris();\n+        assertTrue(uris.length > 0, \"Expected at least one fruit URI, found zero\");\n+        return getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n+    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499643716", "bodyText": "Let's not specify a uriprefix here, as the scope of this test is to verify the collections.\nSo instead of doing a uriMatch query, do a cts.uris(null, null, cts.collectionQuery('fruits')). And I recommend getting the value back as a string, and then splitting that into an array based on the newline symbol:\nString[] uris = getHubClient().getStagingClient().newServerEval().javascript(\"cts.uris(null, null, cts.collectionQuery('fruits'))\").evalAs(String.class).split(\"\\n\");\n        assertEquals(1, uris.length);\n        DocumentMetadataHandle metadata = getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n        assertEquals(2, metadata.getCollections().size());\n        assertTrue(metadata.getCollections().contains(\"fruits\"));\n        assertTrue(metadata.getCollections().contains(\"stuff\"));", "author": "rjrudin", "createdAt": "2020-10-05T14:32:12Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgwNDY5MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499804690", "bodyText": "Same https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "SameeraPriyathamTadikonda", "createdAt": "2020-10-05T18:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0MzcxNg=="}], "type": "inlineReview", "revised_code": {"commit": "8b72cb309d0c8138f8669d70f3328b8775453a26", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\nindex cfbdbe360..ce2079fe4 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n\n@@ -1,89 +1,72 @@\n package com.marklogic.hub.spark.sql.sources.v2;\n \n import com.fasterxml.jackson.databind.JsonNode;\n-import com.marklogic.client.document.JSONDocumentManager;\n-import com.marklogic.client.eval.EvalResultIterator;\n-import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n-import com.marklogic.client.ext.util.DocumentPermissionsParser;\n import com.marklogic.client.io.DocumentMetadataHandle;\n import com.marklogic.client.io.JacksonHandle;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.sources.v2.writer.DataWriter;\n-import org.junit.Assert;\n import org.junit.jupiter.api.Test;\n \n import java.io.IOException;\n+import java.util.Set;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n \n     @Test\n     void ingestDocsWithCollection() throws IOException {\n-        String collections = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        String collections = \"fruits,stuff\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withCollections(collections));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n+        DocumentMetadataHandle metadata = getFirstFruitMetadata();\n+        assertEquals(2, metadata.getCollections().size());\n+        assertTrue(metadata.getCollections().contains(\"fruits\"));\n+        assertTrue(metadata.getCollections().contains(\"stuff\"));\n \n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+        // Verify default permissions\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        Set<DocumentMetadataHandle.Capability> capabilities = perms.get(\"data-hub-common\");\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.READ));\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.UPDATE));\n     }\n \n     @Test\n     void ingestDocsWithSourceName() throws IOException {\n         String sourceName = \"spark\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceName(sourceName));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceName, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"name\").asText());\n     }\n \n     @Test\n     void ingestDocsWithSourceType() throws IOException {\n         String sourceType = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceType(sourceType));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceType(sourceType));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceType, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"datahubSourceType\").asText());\n     }\n \n     @Test\n     void ingestDocsWithPermissions() throws IOException {\n-        String permissions = \"data-hub-common,read,data-hub-operator,update\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withPermissions(permissions));\n+        String permissions = \"qconsole-user,read,manage-admin,update\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withPermissions(permissions));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        DocumentPermissionsParser documentPermissionParser = new DefaultDocumentPermissionsParser();\n-        DocumentMetadataHandle permissionMetadata = new DocumentMetadataHandle();\n-        documentPermissionParser.parsePermissions(permissions, permissionMetadata.getPermissions());\n-\n-        Assert.assertEquals(permissionMetadata.getPermissions(), docMetaData.getPermissions());\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        assertEquals(DocumentMetadataHandle.Capability.READ, perms.get(\"qconsole-user\").iterator().next());\n+        assertEquals(DocumentMetadataHandle.Capability.UPDATE, perms.get(\"manage-admin\").iterator().next());\n     }\n \n+    private DocumentMetadataHandle getFirstFruitMetadata() {\n+        String[] uris = getFruitUris();\n+        assertTrue(uris.length > 0, \"Expected at least one fruit URI, found zero\");\n+        return getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n+    }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499654740", "bodyText": "There's a lot of duplication across these 4 tests - I'm going to submit a PR to yours to resolve that.", "author": "rjrudin", "createdAt": "2020-10-05T14:46:45Z", "path": "marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java", "diffHunk": "@@ -0,0 +1,89 @@\n+package com.marklogic.hub.spark.sql.sources.v2;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.marklogic.client.document.JSONDocumentManager;\n+import com.marklogic.client.eval.EvalResultIterator;\n+import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n+import com.marklogic.client.ext.util.DocumentPermissionsParser;\n+import com.marklogic.client.io.DocumentMetadataHandle;\n+import com.marklogic.client.io.JacksonHandle;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.junit.Assert;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n+\n+    @Test\n+    void ingestDocsWithCollection() throws IOException {\n+        String collections = \"fruits\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n+\n+        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n+        String uri = uriQueryResult.next().getString();\n+\n+        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n+        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n+\n+        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+    }\n+\n+    @Test\n+    void ingestDocsWithSourceName() throws IOException {\n+        String sourceName = \"spark\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        dataWriter.write(buildRow(\"pineapple\", \"green\"));\n+        String uriQuery = \"cts.uriMatch('/testFruit**')\";", "originalCommit": "410088ee47dbd98710c389893448d1f1b40e33bb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1OTI0Mg==", "url": "https://github.com/marklogic/marklogic-data-hub/pull/4633#discussion_r499659242", "bodyText": "See https://github.com/SameeraPriyathamTadikonda/marklogic-data-hub/pull/25/files", "author": "rjrudin", "createdAt": "2020-10-05T14:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NDc0MA=="}], "type": "inlineReview", "revised_code": {"commit": "8b72cb309d0c8138f8669d70f3328b8775453a26", "chunk": "diff --git a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\nindex cfbdbe360..ce2079fe4 100644\n--- a/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n+++ b/marklogic-data-hub-spark-connector/src/test/java/com/marklogic/hub/spark/sql/sources/v2/WriteDataWithOptionsTest.java\n\n@@ -1,89 +1,72 @@\n package com.marklogic.hub.spark.sql.sources.v2;\n \n import com.fasterxml.jackson.databind.JsonNode;\n-import com.marklogic.client.document.JSONDocumentManager;\n-import com.marklogic.client.eval.EvalResultIterator;\n-import com.marklogic.client.ext.util.DefaultDocumentPermissionsParser;\n-import com.marklogic.client.ext.util.DocumentPermissionsParser;\n import com.marklogic.client.io.DocumentMetadataHandle;\n import com.marklogic.client.io.JacksonHandle;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.sources.v2.writer.DataWriter;\n-import org.junit.Assert;\n import org.junit.jupiter.api.Test;\n \n import java.io.IOException;\n+import java.util.Set;\n \n import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n \n public class WriteDataWithOptionsTest extends AbstractSparkConnectorTest {\n \n     @Test\n     void ingestDocsWithCollection() throws IOException {\n-        String collections = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withCollections(collections));\n+        String collections = \"fruits,stuff\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withCollections(collections));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n+        DocumentMetadataHandle metadata = getFirstFruitMetadata();\n+        assertEquals(2, metadata.getCollections().size());\n+        assertTrue(metadata.getCollections().contains(\"fruits\"));\n+        assertTrue(metadata.getCollections().contains(\"stuff\"));\n \n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        assertEquals(collections, docMetaData.getCollections().toArray()[0].toString());\n+        // Verify default permissions\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        Set<DocumentMetadataHandle.Capability> capabilities = perms.get(\"data-hub-common\");\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.READ));\n+        assertTrue(capabilities.contains(DocumentMetadataHandle.Capability.UPDATE));\n     }\n \n     @Test\n     void ingestDocsWithSourceName() throws IOException {\n         String sourceName = \"spark\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceName(sourceName));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceName(sourceName));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceName, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"name\").asText());\n     }\n \n     @Test\n     void ingestDocsWithSourceType() throws IOException {\n         String sourceType = \"fruits\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withSourceType(sourceType));\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withSourceType(sourceType));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n-\n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        JsonNode doc = jd.read(uri, new JacksonHandle()).get();\n \n+        JsonNode doc = getHubClient().getStagingClient().newJSONDocumentManager().read(getFruitUris()[0], new JacksonHandle()).get();\n         assertEquals(sourceType, doc.get(\"envelope\").get(\"headers\").get(\"sources\").get(0).get(\"datahubSourceType\").asText());\n     }\n \n     @Test\n     void ingestDocsWithPermissions() throws IOException {\n-        String permissions = \"data-hub-common,read,data-hub-operator,update\";\n-        DataWriter<InternalRow> dataWriter = buildDataWriter(new Options(getHubPropertiesAsMap()).withBatchSize(1).withUriPrefix(\"/testFruit\").withPermissions(permissions));\n+        String permissions = \"qconsole-user,read,manage-admin,update\";\n+        DataWriter<InternalRow> dataWriter = buildDataWriter(newFruitOptions().withPermissions(permissions));\n         dataWriter.write(buildRow(\"pineapple\", \"green\"));\n-        String uriQuery = \"cts.uriMatch('/testFruit**')\";\n \n-        EvalResultIterator uriQueryResult = getHubClient().getStagingClient().newServerEval().javascript(uriQuery).eval();\n-        String uri = uriQueryResult.next().getString();\n-\n-        JSONDocumentManager jd = getHubClient().getStagingClient().newJSONDocumentManager();\n-        DocumentMetadataHandle docMetaData = jd.readMetadata(uri, new DocumentMetadataHandle());\n-\n-        DocumentPermissionsParser documentPermissionParser = new DefaultDocumentPermissionsParser();\n-        DocumentMetadataHandle permissionMetadata = new DocumentMetadataHandle();\n-        documentPermissionParser.parsePermissions(permissions, permissionMetadata.getPermissions());\n-\n-        Assert.assertEquals(permissionMetadata.getPermissions(), docMetaData.getPermissions());\n+        DocumentMetadataHandle.DocumentPermissions perms = getFirstFruitMetadata().getPermissions();\n+        assertEquals(DocumentMetadataHandle.Capability.READ, perms.get(\"qconsole-user\").iterator().next());\n+        assertEquals(DocumentMetadataHandle.Capability.UPDATE, perms.get(\"manage-admin\").iterator().next());\n     }\n \n+    private DocumentMetadataHandle getFirstFruitMetadata() {\n+        String[] uris = getFruitUris();\n+        assertTrue(uris.length > 0, \"Expected at least one fruit URI, found zero\");\n+        return getHubClient().getStagingClient().newDocumentManager().readMetadata(uris[0], new DocumentMetadataHandle());\n+    }\n }\n"}}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "commit"}, {"oid": "8b72cb309d0c8138f8669d70f3328b8775453a26", "url": "https://github.com/marklogic/marklogic-data-hub/commit/8b72cb309d0c8138f8669d70f3328b8775453a26", "message": "DHFPROD-5944: Configure Document Ingestion via connector\n\nDHFPROD-5944: Adding helper methods for test\n\nuriprefix", "committedDate": "2020-10-05T18:43:08Z", "type": "forcePushed"}]}