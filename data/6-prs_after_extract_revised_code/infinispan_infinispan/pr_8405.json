{"pr_number": 8405, "pr_title": "ISPN-11930 Rocks db converted to new SPI", "pr_createdAt": "2020-05-29T05:22:23Z", "pr_url": "https://github.com/infinispan/infinispan/pull/8405", "timeline": [{"oid": "be24dc2231fa8baed421fe4a622b5347481a67a9", "url": "https://github.com/infinispan/infinispan/commit/be24dc2231fa8baed421fe4a622b5347481a67a9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-05-29T18:17:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434614613", "bodyText": "ContextBuilder as PersistenceMockUtil is not exclusive to InvocationContext?", "author": "ryanemerson", "createdAt": "2020-06-03T14:33:34Z", "path": "core/src/test/java/org/infinispan/util/PersistenceMockUtil.java", "diffHunk": "@@ -41,6 +47,51 @@\n  */\n public class PersistenceMockUtil {\n \n+   public static class Builder {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MTE3MQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434841171", "bodyText": "Sorry, I am not sure what you mean here. Are you proposing I move the Builder class somewhere else?", "author": "wburns", "createdAt": "2020-06-03T20:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEwNTI3MA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435105270", "bodyText": "Sorry, my message isn't clear in hindsight. I meant shall we rename to class ContextBuilder as the PersistenceMockUtil class is not exclusively used for creating/modifying InvocationContext instances. The name Builder is fine as an inner class that creates an instance of the parent class, e.g. DataFormat.Builder, however that is not the case with  PersistenceMockUtil.Builder. So my thoughts were that renaming the class to PersistenceMockUtil.ContextBuilder or even PersistenceMockUtil.InvocationContextBuilder makes it's purpose more explicit.", "author": "ryanemerson", "createdAt": "2020-06-04T09:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI1MTE2Ng==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435251166", "bodyText": "Sounds good. I guess because the class only creates an InvocationContext currently I hadn't thought about the name, but I agree.", "author": "wburns", "createdAt": "2020-06-04T13:26:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNDYxMw=="}], "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/core/src/test/java/org/infinispan/util/PersistenceMockUtil.java b/core/src/test/java/org/infinispan/util/PersistenceMockUtil.java\nindex 42c03a44b8..99db5ff043 100644\n--- a/core/src/test/java/org/infinispan/util/PersistenceMockUtil.java\n+++ b/core/src/test/java/org/infinispan/util/PersistenceMockUtil.java\n\n@@ -47,7 +47,7 @@\n  */\n public class PersistenceMockUtil {\n \n-   public static class Builder {\n+   public static class InvocationContextBuilder {\n       private final Class<?> testClass;\n       private final Configuration configuration;\n       private final PersistenceMarshaller persistenceMarshaller;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434615733", "bodyText": "impl package?", "author": "ryanemerson", "createdAt": "2020-06-03T14:34:57Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/internal/RocksDBBlockHoundIntegration.java", "diffHunk": "@@ -0,0 +1,20 @@\n+package org.infinispan.persistence.rocksdb.internal;", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MjAxMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434842011", "bodyText": "I personally find it being in a package like internal a bit better as it really isn't an implementation class of the public interfaces. I have just repurposed impl in some modules since it is private. WDYT?", "author": "wburns", "createdAt": "2020-06-03T20:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTEwMjU4OA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435102588", "bodyText": "That's a good point. I hadn't realised that we already used .internal. packages in some places, so let's stick with that.", "author": "ryanemerson", "createdAt": "2020-06-04T09:03:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYxNTczMw=="}], "type": "inlineReview", "revised_code": {"commit": "4f2898b3fc33c76e1ce14577dd980a7335274f68", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/internal/RocksDBBlockHoundIntegration.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/internal/RocksDBBlockHoundIntegration.java\ndeleted file mode 100644\nindex 9762c51427..0000000000\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/internal/RocksDBBlockHoundIntegration.java\n+++ /dev/null\n\n@@ -1,20 +0,0 @@\n-package org.infinispan.persistence.rocksdb.internal;\n-\n-import org.kohsuke.MetaInfServices;\n-import org.rocksdb.RocksDB;\n-\n-import reactor.blockhound.BlockHound;\n-import reactor.blockhound.integration.BlockHoundIntegration;\n-\n-@MetaInfServices\n-public class RocksDBBlockHoundIntegration implements BlockHoundIntegration {\n-   @Override\n-   public void applyTo(BlockHound.Builder builder) {\n-      builder.markAsBlocking(RocksDB.class, \"get\", \"(Lorg/rocksdb/ColumnFamilyHandle;[B)[B\");\n-      builder.markAsBlocking(RocksDB.class, \"put\", \"(Lorg/rocksdb/ColumnFamilyHandle;[B[B)V\");\n-      builder.markAsBlocking(RocksDB.class, \"delete\", \"(Lorg/rocksdb/ColumnFamilyHandle;[B)V\");\n-      builder.markAsBlocking(RocksDB.class, \"write\", \"(Lorg/rocksdb/WriteOptions;Lorg/rocksdb/WriteBatch;)V\");\n-      builder.markAsBlocking(RocksDB.class, \"close\", \"()V\");\n-      builder.markAsBlocking(RocksDB.class, \"open\", \"(Lorg/rocksdb/DBOptions;Ljava/lang/String;Ljava/util/List;Ljava/util/List;)Lorg/rocksdb/RocksDB;\");\n-   }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDYyOTc1Ng==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434629756", "bodyText": "Missing @ConfiguredBy(RocksDBStoreConfiguration.class)", "author": "ryanemerson", "createdAt": "2020-06-03T14:54:02Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bd3d9b10de1991887422123439cfdb9bd29e3ea", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex e6b523ca97..501f819b14 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -144,6 +144,12 @@ private Path getExpirationLocation() {\n       return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n    }\n \n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n    protected DBOptions dataDbOptions() {\n       DBOptions dbOptions;\n       if (databaseProperties != null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY0OTU4OQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434649589", "bodyText": "Never called.", "author": "ryanemerson", "createdAt": "2020-06-03T15:20:41Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n+            }\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n+            }\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n             db = handler.open(getLocation(), dataDbOptions());\n             expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            stopped = false;\n-        } catch (Exception e) {\n+         } catch (Exception e) {\n             throw new CacheConfigurationException(\"Unable to open database\", e);\n-        }\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n-            handler.close();\n-            expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n-    }\n-\n-    @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n-    }\n-\n-    @Override\n-    public void clear() {\n-        handler.clear(null);\n-    }\n-\n-    @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n-    }\n-\n-    @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n-    }\n-\n-    @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n-    }\n-\n-    @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n-            }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public void purge(Executor executor, PurgeListener purgeListener) {\n-        try {\n-            semaphore.acquire();\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore: CacheStore is likely stopped.\", e);\n-        }\n-        try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-            if (stopped) {\n-                throw new PersistenceException(\"RocksDB is stopped\");\n-            }\n-            long now = ctx.getTimeService().wallClockTime();\n-            RocksIterator iterator = expiredDb.newIterator(readOptions);\n-            if (iterator != null) {\n-                try (RocksIterator it = iterator) {\n-                    List<Long> times = new ArrayList<>();\n-                    List<Object> keys = new ArrayList<>();\n-                    List<byte[]> marshalledKeys = new ArrayList<>();\n-\n-                    for (it.seekToFirst(); it.isValid(); it.next()) {\n-                        Long time = (Long) unmarshall(it.key());\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n+                        }\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n+            }\n+         } catch (RocksDBException e) {\n+            throw new PersistenceException(e);\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      return Flowable.defer(() -> {\n+         UnicastProcessor<MarshallableEntry<K, V>> processor = UnicastProcessor.create();\n+         blockingManager.runBlocking(() -> {\n+            try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+               long now = ctx.getTimeService().wallClockTime();\n+               RocksIterator iterator = expiredDb.newIterator(readOptions);\n+               if (iterator != null) {\n+                  try (RocksIterator it = iterator) {\n+                     List<Long> times = new ArrayList<>();\n+                     List<Object> keys = new ArrayList<>();\n+                     List<byte[]> marshalledKeys = new ArrayList<>();\n+\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        Long time = unmarshall(it.key());\n                         if (time > now)\n-                            break;\n+                           break;\n                         times.add(time);\n                         byte[] marshalledKey = it.value();\n                         Object key = unmarshall(marshalledKey);\n                         if (key instanceof ExpiryBucket) {\n-                            for (byte[] bytes : ((ExpiryBucket) key).entries) {\n-                                marshalledKeys.add(bytes);\n-                                keys.add(unmarshall(bytes));\n-                            }\n+                           for (byte[] bytes : ((ExpiryBucket) key).entries) {\n+                              marshalledKeys.add(bytes);\n+                              keys.add(unmarshall(bytes));\n+                           }\n                         } else {\n-                            keys.add(key);\n-                            marshalledKeys.add(marshalledKey);\n+                           keys.add(key);\n+                           marshalledKeys.add(marshalledKey);\n                         }\n-                    }\n+                     }\n \n-                    for (Long time : times) {\n+                     for (Long time : times) {\n                         expiredDb.delete(marshall(time));\n-                    }\n+                     }\n \n-                    if (!keys.isEmpty())\n+                     if (!keys.isEmpty())\n                         log.debugf(\"purge (up to) %d entries\", keys.size());\n-                    int count = 0;\n-                    for (int i = 0; i < keys.size(); i++) {\n+                     int count = 0;\n+                     for (int i = 0; i < keys.size(); i++) {\n                         Object key = keys.get(i);\n                         byte[] keyBytes = marshalledKeys.get(i);\n-                        int segment = handler.calculateSegment(key);\n \n-                        ColumnFamilyHandle handle = handler.getHandle(segment);\n+                        ColumnFamilyHandle handle = handler.getHandle(key);\n                         byte[] valueBytes = db.get(handle, keyBytes);\n                         if (valueBytes == null)\n-                            continue;\n+                           continue;\n \n-                        MarshalledValue mv = (MarshalledValue) unmarshall(valueBytes);\n+                        MarshalledValue mv = unmarshall(valueBytes);\n                         if (mv != null) {\n-                            // TODO race condition: the entry could be updated between the get and delete!\n-                            Metadata metadata = (Metadata) unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-                            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                                // somewhat inefficient to FIND then REMOVE...\n-                                db.delete(handle, keyBytes);\n-                                purgeListener.entryPurged(key);\n-                                count++;\n-                            }\n+                           // TODO race condition: the entry could be updated between the get and delete!\n+                           Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+                           if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+                              // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+                              db.delete(handle, keyBytes);\n+                              processor.onNext(entryFactory.create(key, mv));\n+                              count++;\n+                           }\n                         }\n-                    }\n-                    if (count != 0)\n+                     }\n+                     if (count != 0)\n                         log.debugf(\"purged %d entries\", count);\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                } finally {\n-                    readOptions.close();\n-                }\n-            }\n-        } catch (PersistenceException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new PersistenceException(e);\n-        } finally {\n-            semaphore.release();\n-        }\n-    }\n-\n-    @Override\n-    public void addSegments(IntSet segments) {\n-        handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public void removeSegments(IntSet segments) {\n-        handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) throws IOException, InterruptedException {\n-        return marshaller.objectToByteBuffer(entry);\n-    }\n-\n-    private Object unmarshall(byte[] bytes) throws IOException, ClassNotFoundException {\n-        if (bytes == null)\n-            return null;\n-\n-        return marshaller.objectFromByteBuffer(bytes);\n-    }\n-\n-    private MarshallableEntry<K, V> valueToMarshallableEntry(Object key, byte[] valueBytes, boolean fetchMeta) throws IOException, ClassNotFoundException {\n-        MarshalledValue value = (MarshalledValue) unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        ByteBuffer metadataBytes = fetchMeta ? value.getMetadataBytes() : null;\n-        return entryFactory.create(key, value.getValueBytes(), metadataBytes, value.getInternalMetadataBytes(), value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException, IOException, ClassNotFoundException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        try {\n-            byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-            putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt(); // Restore interruption status\n-        }\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksKeyIterator extends AbstractIterator<K> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-\n-        public RocksKeyIterator(RocksIterator it, Predicate<? super K> filter) {\n-            this.it = it;\n-            this.filter = filter;\n-        }\n-\n-        @Override\n-        protected K getNext() {\n-            K key = null;\n-            try {\n-                while (key == null && it.isValid()) {\n-                    K testKey = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(testKey)) {\n-                        key = testKey;\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return key;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final boolean fetchValue;\n-        private final boolean fetchMetadata;\n-        private final long now;\n-\n-        public RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, boolean fetchValue,\n-              boolean fetchMetadata, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.fetchValue = fetchValue;\n-            this.fetchMetadata = fetchMetadata;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            try {\n-                while (entry == null && it.isValid()) {\n-                    K key = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(key)) {\n-                        if (fetchValue || fetchMetadata) {\n-                            MarshallableEntry<K, V> me = valueToMarshallableEntry(key, it.value(), fetchMetadata);\n-                            if (me != null && !me.isExpired(now)) {\n-                                entry = me;\n-                            }\n-                        } else {\n-                            entry = entryFactory.create(key);\n-                        }\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return entry;\n-        }\n-    }\n-\n-    private abstract class RocksDBHandler {\n-\n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n-\n-        abstract void close();\n-\n-        abstract ColumnFamilyHandle getHandle(int segment);\n-\n-        final ColumnFamilyHandle getHandle(int segment, Object key) {\n-            if (segment < 0) {\n-                segment = calculateSegment(key);\n-            }\n-            return getHandle(segment);\n-        }\n-\n-        abstract int calculateSegment(Object key);\n-\n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n+                  } catch (Exception e) {\n+                     throw new PersistenceException(e);\n+                  } finally {\n+                     readOptions.close();\n+                  }\n+               }\n+            }\n+         }, \"rocksdb-purgeExpired\").whenComplete((ignore, t) -> {\n+            if (t != null) {\n+               processor.onError(t);\n             } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n-            }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n-\n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n-            }\n-            try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n-            try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n-                    }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n+               processor.onComplete();\n+            }\n+         });\n+         return processor;\n+      });\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   private abstract class RocksDBHandler {\n+\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+\n+      abstract void close();\n+\n+      abstract ColumnFamilyHandle getHandle(int segment);\n+\n+      abstract ColumnFamilyHandle getHandle(Object key);\n+\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n+            }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<Boolean> contains(int segment, Object key) {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2MTM1MQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434761351", "bodyText": "Fixed, moved this to the actual store method for better visibility.", "author": "wburns", "createdAt": "2020-06-03T18:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY0OTU4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "7bd3d9b10de1991887422123439cfdb9bd29e3ea", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex e6b523ca97..501f819b14 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -144,6 +144,12 @@ private Path getExpirationLocation() {\n       return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n    }\n \n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n    protected DBOptions dataDbOptions() {\n       DBOptions dbOptions;\n       if (databaseProperties != null) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY1MzgzNA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r434653834", "bodyText": "Never used.", "author": "ryanemerson", "createdAt": "2020-06-03T15:26:28Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -67,1091 +67,933 @@\n import org.rocksdb.WriteOptions;\n \n import io.reactivex.rxjava3.core.Flowable;\n-\n-@Store\n-@ConfiguredBy(RocksDBStoreConfiguration.class)\n-public class RocksDBStore<K,V> implements SegmentedAdvancedLoadWriteStore<K,V> {\n-    private static final Log log = LogFactory.getLog(RocksDBStore.class, Log.class);\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private Semaphore semaphore;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private volatile boolean stopped = true;\n-\n-    @Override\n-    public void init(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.semaphore = new Semaphore(Integer.MAX_VALUE, true);\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-    }\n-\n-    @Override\n-    public void start() {\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        KeyPartitioner keyPartitioner = cache.getComponentRegistry().getComponent(KeyPartitioner.class);\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments(),\n-                  keyPartitioner);\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            }\n-        }\n-\n-        try {\n+import io.reactivex.rxjava3.processors.UnicastProcessor;\n+\n+public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n+            }\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n+            }\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n             db = handler.open(getLocation(), dataDbOptions());\n             expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            stopped = false;\n-        } catch (Exception e) {\n+         } catch (Exception e) {\n             throw new CacheConfigurationException(\"Unable to open database\", e);\n-        }\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n-            handler.close();\n-            expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n-    }\n-\n-    @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n-    }\n-\n-    @Override\n-    public void clear() {\n-        handler.clear(null);\n-    }\n-\n-    @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n-    }\n-\n-    @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n-    }\n-\n-    @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n-    }\n-\n-    @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n-    }\n-\n-    @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n-            }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    @Override\n-    public void purge(Executor executor, PurgeListener purgeListener) {\n-        try {\n-            semaphore.acquire();\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore: CacheStore is likely stopped.\", e);\n-        }\n-        try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-            if (stopped) {\n-                throw new PersistenceException(\"RocksDB is stopped\");\n-            }\n-            long now = ctx.getTimeService().wallClockTime();\n-            RocksIterator iterator = expiredDb.newIterator(readOptions);\n-            if (iterator != null) {\n-                try (RocksIterator it = iterator) {\n-                    List<Long> times = new ArrayList<>();\n-                    List<Object> keys = new ArrayList<>();\n-                    List<byte[]> marshalledKeys = new ArrayList<>();\n-\n-                    for (it.seekToFirst(); it.isValid(); it.next()) {\n-                        Long time = (Long) unmarshall(it.key());\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n+                        }\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n+            }\n+         } catch (RocksDBException e) {\n+            throw new PersistenceException(e);\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      return Flowable.defer(() -> {\n+         UnicastProcessor<MarshallableEntry<K, V>> processor = UnicastProcessor.create();\n+         blockingManager.runBlocking(() -> {\n+            try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+               long now = ctx.getTimeService().wallClockTime();\n+               RocksIterator iterator = expiredDb.newIterator(readOptions);\n+               if (iterator != null) {\n+                  try (RocksIterator it = iterator) {\n+                     List<Long> times = new ArrayList<>();\n+                     List<Object> keys = new ArrayList<>();\n+                     List<byte[]> marshalledKeys = new ArrayList<>();\n+\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        Long time = unmarshall(it.key());\n                         if (time > now)\n-                            break;\n+                           break;\n                         times.add(time);\n                         byte[] marshalledKey = it.value();\n                         Object key = unmarshall(marshalledKey);\n                         if (key instanceof ExpiryBucket) {\n-                            for (byte[] bytes : ((ExpiryBucket) key).entries) {\n-                                marshalledKeys.add(bytes);\n-                                keys.add(unmarshall(bytes));\n-                            }\n+                           for (byte[] bytes : ((ExpiryBucket) key).entries) {\n+                              marshalledKeys.add(bytes);\n+                              keys.add(unmarshall(bytes));\n+                           }\n                         } else {\n-                            keys.add(key);\n-                            marshalledKeys.add(marshalledKey);\n+                           keys.add(key);\n+                           marshalledKeys.add(marshalledKey);\n                         }\n-                    }\n+                     }\n \n-                    for (Long time : times) {\n+                     for (Long time : times) {\n                         expiredDb.delete(marshall(time));\n-                    }\n+                     }\n \n-                    if (!keys.isEmpty())\n+                     if (!keys.isEmpty())\n                         log.debugf(\"purge (up to) %d entries\", keys.size());\n-                    int count = 0;\n-                    for (int i = 0; i < keys.size(); i++) {\n+                     int count = 0;\n+                     for (int i = 0; i < keys.size(); i++) {\n                         Object key = keys.get(i);\n                         byte[] keyBytes = marshalledKeys.get(i);\n-                        int segment = handler.calculateSegment(key);\n \n-                        ColumnFamilyHandle handle = handler.getHandle(segment);\n+                        ColumnFamilyHandle handle = handler.getHandle(key);\n                         byte[] valueBytes = db.get(handle, keyBytes);\n                         if (valueBytes == null)\n-                            continue;\n+                           continue;\n \n-                        MarshalledValue mv = (MarshalledValue) unmarshall(valueBytes);\n+                        MarshalledValue mv = unmarshall(valueBytes);\n                         if (mv != null) {\n-                            // TODO race condition: the entry could be updated between the get and delete!\n-                            Metadata metadata = (Metadata) unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-                            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                                // somewhat inefficient to FIND then REMOVE...\n-                                db.delete(handle, keyBytes);\n-                                purgeListener.entryPurged(key);\n-                                count++;\n-                            }\n+                           // TODO race condition: the entry could be updated between the get and delete!\n+                           Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+                           if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+                              // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+                              db.delete(handle, keyBytes);\n+                              processor.onNext(entryFactory.create(key, mv));\n+                              count++;\n+                           }\n                         }\n-                    }\n-                    if (count != 0)\n+                     }\n+                     if (count != 0)\n                         log.debugf(\"purged %d entries\", count);\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                } finally {\n-                    readOptions.close();\n-                }\n-            }\n-        } catch (PersistenceException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new PersistenceException(e);\n-        } finally {\n-            semaphore.release();\n-        }\n-    }\n-\n-    @Override\n-    public void addSegments(IntSet segments) {\n-        handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public void removeSegments(IntSet segments) {\n-        handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) throws IOException, InterruptedException {\n-        return marshaller.objectToByteBuffer(entry);\n-    }\n-\n-    private Object unmarshall(byte[] bytes) throws IOException, ClassNotFoundException {\n-        if (bytes == null)\n-            return null;\n-\n-        return marshaller.objectFromByteBuffer(bytes);\n-    }\n-\n-    private MarshallableEntry<K, V> valueToMarshallableEntry(Object key, byte[] valueBytes, boolean fetchMeta) throws IOException, ClassNotFoundException {\n-        MarshalledValue value = (MarshalledValue) unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        ByteBuffer metadataBytes = fetchMeta ? value.getMetadataBytes() : null;\n-        return entryFactory.create(key, value.getValueBytes(), metadataBytes, value.getInternalMetadataBytes(), value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException, IOException, ClassNotFoundException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        try {\n-            byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-            putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt(); // Restore interruption status\n-        }\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksKeyIterator extends AbstractIterator<K> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-\n-        public RocksKeyIterator(RocksIterator it, Predicate<? super K> filter) {\n-            this.it = it;\n-            this.filter = filter;\n-        }\n-\n-        @Override\n-        protected K getNext() {\n-            K key = null;\n-            try {\n-                while (key == null && it.isValid()) {\n-                    K testKey = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(testKey)) {\n-                        key = testKey;\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return key;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final boolean fetchValue;\n-        private final boolean fetchMetadata;\n-        private final long now;\n-\n-        public RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, boolean fetchValue,\n-              boolean fetchMetadata, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.fetchValue = fetchValue;\n-            this.fetchMetadata = fetchMetadata;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            try {\n-                while (entry == null && it.isValid()) {\n-                    K key = (K) unmarshall(it.key());\n-                    if (filter == null || filter.test(key)) {\n-                        if (fetchValue || fetchMetadata) {\n-                            MarshallableEntry<K, V> me = valueToMarshallableEntry(key, it.value(), fetchMetadata);\n-                            if (me != null && !me.isExpired(now)) {\n-                                entry = me;\n-                            }\n-                        } else {\n-                            entry = entryFactory.create(key);\n-                        }\n-                    }\n-                    it.next();\n-                }\n-            } catch (IOException | ClassNotFoundException e) {\n-                throw new CacheException(e);\n-            }\n-            return entry;\n-        }\n-    }\n-\n-    private abstract class RocksDBHandler {\n-\n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n-\n-        abstract void close();\n-\n-        abstract ColumnFamilyHandle getHandle(int segment);\n-\n-        final ColumnFamilyHandle getHandle(int segment, Object key) {\n-            if (segment < 0) {\n-                segment = calculateSegment(key);\n-            }\n-            return getHandle(segment);\n-        }\n-\n-        abstract int calculateSegment(Object key);\n-\n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n+                  } catch (Exception e) {\n+                     throw new PersistenceException(e);\n+                  } finally {\n+                     readOptions.close();\n+                  }\n+               }\n+            }\n+         }, \"rocksdb-purgeExpired\").whenComplete((ignore, t) -> {\n+            if (t != null) {\n+               processor.onError(t);\n             } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n-            }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n-\n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n-            }\n-            try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n-            try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n-                    }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n+               processor.onComplete();\n+            }\n+         });\n+         return processor;\n+      });\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   private abstract class RocksDBHandler {\n+\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+\n+      abstract void close();\n+\n+      abstract ColumnFamilyHandle getHandle(int segment);\n+\n+      abstract ColumnFamilyHandle getHandle(Object key);\n+\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n+            }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<Boolean> contains(int segment, Object key) {\n+         // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+         return load(segment, key)\n+               .thenApply(Objects::nonNull);\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n+\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n+\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        int size(IntSet segments) {\n-            CompletionStage<Long> stage = Flowable.fromPublisher(publishKeys(segments, null))\n-                  .count().toCompletionStage();\n+      abstract CompletionStage<Void> clear();\n \n-            long count = CompletionStages.join(stage);\n-            if (count > Integer.MAX_VALUE) {\n-                return Integer.MAX_VALUE;\n-            }\n-            return (int) count;\n-        }\n-\n-        <P> Flowable<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return Flowable.using(() -> {\n-                semaphore.acquire();\n-                if (stopped) {\n-                    throw new PersistenceException(\"RocksDB is stopped\");\n-                }\n-                return wrapIterator(db, readOptions, segment);\n-            }, iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-                semaphore.release();\n-            });\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n \n-        private void writeBatch(WriteBatch batch) throws InterruptedException, RocksDBException {\n-            semaphore.acquire();\n-            try {\n-                if (stopped)\n-                    throw new PersistenceException(\"RocksDB is stopped\");\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n \n-                db.write(dataWriteOptions(), batch);\n-            } finally {\n-                batch.close();\n-                semaphore.release();\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.<P>empty();\n             }\n-        }\n-\n-        abstract void addSegments(IntSet segments);\n-\n-        abstract void removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        public NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        int calculateSegment(Object key) {\n-            // Segment not used\n-            return 0;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                  Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                  handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        void clear(IntSet segments) {\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      private void writeBatch(WriteBatch batch) throws RocksDBException {", "originalCommit": "be24dc2231fa8baed421fe4a622b5347481a67a9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bd3d9b10de1991887422123439cfdb9bd29e3ea", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex e6b523ca97..501f819b14 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -144,6 +144,12 @@ private Path getExpirationLocation() {\n       return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n    }\n \n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n    protected DBOptions dataDbOptions() {\n       DBOptions dbOptions;\n       if (databaseProperties != null) {\n"}}, {"oid": "7bd3d9b10de1991887422123439cfdb9bd29e3ea", "url": "https://github.com/infinispan/infinispan/commit/7bd3d9b10de1991887422123439cfdb9bd29e3ea", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T18:05:53Z", "type": "forcePushed"}, {"oid": "5fbbd75ca94c8e4512a9a8bbe7f3b020bc90be03", "url": "https://github.com/infinispan/infinispan/commit/5fbbd75ca94c8e4512a9a8bbe7f3b020bc90be03", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T18:14:30Z", "type": "forcePushed"}, {"oid": "778113107c54287a4d8b96fbb31e08400de3304d", "url": "https://github.com/infinispan/infinispan/commit/778113107c54287a4d8b96fbb31e08400de3304d", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T20:38:24Z", "type": "forcePushed"}, {"oid": "0af866533d6dbaa15977c7cd57ed5e92e165865d", "url": "https://github.com/infinispan/infinispan/commit/0af866533d6dbaa15977c7cd57ed5e92e165865d", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-03T20:44:16Z", "type": "forcePushed"}, {"oid": "dfd1106c672cbba56b2c6b3ae7632a5624edec0c", "url": "https://github.com/infinispan/infinispan/commit/dfd1106c672cbba56b2c6b3ae7632a5624edec0c", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-04T02:15:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwMjM4Mw==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435802383", "bodyText": "Performing a full load seems wasteful here, as it means that the value has to be unmarshalled even though it is never used.", "author": "ryanemerson", "createdAt": "2020-06-05T09:29:00Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MTExNA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436141114", "bodyText": "This is the same as it was prior, but I can see if I can add it easily enough.", "author": "wburns", "createdAt": "2020-06-05T20:08:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwMjM4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex d6f1df4446..25ddd42963 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -75,962 +75,962 @@\n \n @ConfiguredBy(RocksDBStoreConfiguration.class)\n public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n-    private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n-    private static final boolean trace = log.isTraceEnabled();\n-\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private KeyPartitioner keyPartitioner;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private BlockingManager blockingManager;\n-\n-    @Override\n-    public CompletionStage<Void> start(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        this.blockingManager = ctx.getBlockingManager();\n-        this.keyPartitioner = ctx.getKeyPartitioner();\n-\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n             }\n-        }\n-\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db = handler.open(getLocation(), dataDbOptions());\n-                expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            } catch (Exception e) {\n-                throw new CacheConfigurationException(\"Unable to open database\", e);\n-            }\n-        }, \"rocksdb-open\");\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> stop() {\n-        return blockingManager.runBlocking(() -> {\n-            handler.close();\n-            expiredDb.close();\n-        }, \"rocksdb-stop\");\n-    }\n-\n-    @Override\n-    public Set<Characteristic> characteristics() {\n-        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> isAvailable() {\n-        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n-                \"rocksdb-available\");\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> clear() {\n-        return handler.clear();\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> approximateSize(IntSet segments) {\n-        return handler.approximateSize(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n-        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-        return load(segment, key)\n-                .thenApply(Objects::nonNull);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n-                .map(MarshallableEntry::getKey);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n-        return handler.publishEntries(segments, filter, includeValues);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        return handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n-            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n-        WriteBatch batch = new WriteBatch();\n-        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n-        Flowable.fromPublisher(removePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n-                });\n-        Flowable.fromPublisher(writePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(me -> {\n-                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n-                                if (me.expiryTime() > -1) {\n-                                    expirableEntries.add(me);\n-                                }\n-                            });\n-                });\n-        if (batch.count() <= 0) {\n-            batch.close();\n-            return CompletableFutures.completedNull();\n-        }\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db.write(dataWriteOptions(), batch);\n-                for (MarshallableEntry<K, V> me : expirableEntries) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (RocksDBException e) {\n-                throw new PersistenceException(e);\n-            }\n-        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n-        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n-            // We check expiration based on time of subscription only\n-            long now = timeService.wallClockTime();\n-            return actualPurgeExpired(now)\n-                    // We return a buffer of expired entries emitted to the non blocking thread\n-                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n-                    // consumed much faster than emission (since each emission performs a get and remove)\n-                    .buffer(16);\n-        }));\n-\n-        return Flowable.fromPublisher(purgedBatches)\n-                .concatMap(Flowable::fromIterable);\n-    }\n-\n-    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n-        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n-        // given entries\n-        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n-        }, entry -> {\n-            if (entry.getValue() == null) {\n-                return Flowable.empty();\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n             }\n-            RocksIterator iterator = entry.getValue();\n-            iterator.seekToFirst();\n-\n-            return Flowable.fromIterable(() ->\n-                    new AbstractIterator<byte[]>() {\n-                        @Override\n-                        protected byte[] getNext() {\n-                            if (iterator.isValid()) {\n-                                byte[] keyBytes = iterator.key();\n-                                Long time = unmarshall(keyBytes);\n-                                if (time > now)\n-                                    return null;\n-                                try {\n-                                    expiredDb.delete(keyBytes);\n-                                } catch (RocksDBException e) {\n-                                    throw new PersistenceException(e);\n-                                }\n-                                byte[] value = iterator.value();\n-                                iterator.next();\n-                                return value;\n-                            }\n-                            return null;\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db = handler.open(getLocation(), dataDbOptions());\n+            expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n+         } catch (Exception e) {\n+            throw new CacheConfigurationException(\"Unable to open database\", e);\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n                         }\n-                    });\n-        }, entry -> {\n-            entry.getKey().close();\n-            RocksIterator rocksIterator = entry.getValue();\n-            if (rocksIterator != null) {\n-                rocksIterator.close();\n-            }\n-        });\n-\n-        Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n-            Object bucketKey = unmarshall(expiredBytes);\n-            if (bucketKey instanceof ExpiryBucket) {\n-                return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n-                        .flatMapMaybe(marshalledKey -> {\n-                            ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n-                            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n-                            return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n-                        });\n-            } else {\n-                // The bucketKey is an actual key\n-                ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n-                MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n-                return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n-            }\n-        });\n-\n-        if (trace) {\n-            // Note this tracing only works properly for one subscriber\n-            FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n-            expiredEntryFlowable = expiredEntryFlowable\n-                    .doOnEach(mirrorEntries)\n-                    .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n-            mirrorEntries.count()\n-                    .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n-        }\n-\n-        return expiredEntryFlowable;\n-    }\n-\n-    private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n-            long now) throws RocksDBException {\n-        byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n-        if (valueBytes == null) {\n-            return null;\n-        }\n-        MarshalledValue mv = unmarshall(valueBytes);\n-        if (mv != null) {\n-            // TODO race condition: the entry could be updated between the get and delete!\n-            Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n-                db.delete(columnFamilyHandle, marshalledKey);\n-                return mv;\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n             }\n-        }\n-        return null;\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> addSegments(IntSet segments) {\n-        return handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> removeSegments(IntSet segments) {\n-        return handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) {\n-        try {\n-            return marshaller.objectToByteBuffer(entry);\n-        } catch (IOException e) {\n-            throw new PersistenceException(e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private <E> E unmarshall(byte[] bytes) {\n-        if (bytes == null)\n-            return null;\n-\n-        try {\n-            //noinspection unchecked\n-            return (E) marshaller.objectFromByteBuffer(bytes);\n-        } catch (IOException | ClassNotFoundException e) {\n+         } catch (RocksDBException e) {\n             throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n-        MarshalledValue value = unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n-                value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-        putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final long now;\n-\n-        RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            while (entry == null && it.isValid()) {\n-                K key = unmarshall(it.key());\n-                if (filter == null || filter.test(key)) {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n-                    if (me != null && !me.isExpired(now)) {\n-                        entry = me;\n-                    }\n-                }\n-                it.next();\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+         // We check expiration based on time of subscription only\n+         long now = timeService.wallClockTime();\n+         return actualPurgeExpired(now)\n+               // We return a buffer of expired entries emitted to the non blocking thread\n+               // This prevents waking up the non blocking thread for every entry as they will most likely be\n+               // consumed much faster than emission (since each emission performs a get and remove)\n+               .buffer(16);\n+      }));\n+\n+      return Flowable.fromPublisher(purgedBatches)\n+            .concatMap(Flowable::fromIterable);\n+   }\n+\n+   private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+      // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+      // given entries\n+      Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+      }, entry -> {\n+         if (entry.getValue() == null) {\n+            return Flowable.empty();\n+         }\n+         RocksIterator iterator = entry.getValue();\n+         iterator.seekToFirst();\n+\n+         return Flowable.fromIterable(() ->\n+               new AbstractIterator<byte[]>() {\n+                  @Override\n+                  protected byte[] getNext() {\n+                     if (!iterator.isValid()) {\n+                        return null;\n+                     }\n+                     byte[] keyBytes = iterator.key();\n+                     Long time = unmarshall(keyBytes);\n+                     if (time > now)\n+                        return null;\n+                     try {\n+                        expiredDb.delete(keyBytes);\n+                     } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n+                     }\n+                     byte[] value = iterator.value();\n+                     iterator.next();\n+                     return value;\n+                  }\n+               });\n+      }, entry -> {\n+         entry.getKey().close();\n+         RocksIterator rocksIterator = entry.getValue();\n+         if (rocksIterator != null) {\n+            rocksIterator.close();\n+         }\n+      });\n+\n+      Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n+         Object bucketKey = unmarshall(expiredBytes);\n+         if (bucketKey instanceof ExpiryBucket) {\n+            return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n+                  .flatMapMaybe(marshalledKey -> {\n+                     ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n+                     MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n+                     return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n+                  });\n+         } else {\n+            // The bucketKey is an actual key\n+            ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n+            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n+            return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n+         }\n+      });\n+\n+      if (trace) {\n+         // Note this tracing only works properly for one subscriber\n+         FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n+         expiredEntryFlowable = expiredEntryFlowable\n+               .doOnEach(mirrorEntries)\n+               .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n+         mirrorEntries.count()\n+               .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n+      }\n+\n+      return expiredEntryFlowable;\n+   }\n+\n+   private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n+         long now) throws RocksDBException {\n+      byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n+      if (valueBytes == null) {\n+         return null;\n+      }\n+      MarshalledValue mv = unmarshall(valueBytes);\n+      if (mv != null) {\n+         // TODO race condition: the entry could be updated between the get and delete!\n+         Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+         if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+            // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+            db.delete(columnFamilyHandle, marshalledKey);\n+            return mv;\n+         }\n+      }\n+      return null;\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   private byte[] marshall(Object entry) {\n+      try {\n+         return marshaller.objectToByteBuffer(entry);\n+      } catch (IOException e) {\n+         throw new PersistenceException(e);\n+      } catch (InterruptedException e) {\n+         Thread.currentThread().interrupt();\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private <E> E unmarshall(byte[] bytes) {\n+      if (bytes == null)\n+         return null;\n+\n+      try {\n+         //noinspection unchecked\n+         return (E) marshaller.objectFromByteBuffer(bytes);\n+      } catch (IOException | ClassNotFoundException e) {\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n+      MarshalledValue value = unmarshall(valueBytes);\n+      if (value == null) return null;\n+\n+      return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n+            value.getCreated(), value.getLastUsed());\n+   }\n+\n+   private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n+      long expiry = entry.expiryTime();\n+      long maxIdle = entry.getMetadata().maxIdle();\n+      if (maxIdle > 0) {\n+         // Coding getExpiryTime() for transient entries has the risk of being a moving target\n+         // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n+         expiry = maxIdle + ctx.getTimeService().wallClockTime();\n+      }\n+      byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n+      putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n+   }\n+\n+   @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n+   static final class ExpiryBucket {\n+      @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n+      List<byte[]> entries;\n+\n+      ExpiryBucket(){}\n+\n+      ExpiryBucket(byte[] existingKey, byte[] newKey) {\n+         entries = new ArrayList<>(2);\n+         entries.add(existingKey);\n+         entries.add(newKey);\n+      }\n+   }\n+\n+   private static final class ExpiryEntry {\n+\n+      final long expiry;\n+      final byte[] keyBytes;\n+\n+      ExpiryEntry(long expiry, byte[] keyBytes) {\n+         this.expiry = expiry;\n+         this.keyBytes = keyBytes;\n+      }\n+\n+      @Override\n+      public boolean equals(Object o) {\n+         if (this == o) return true;\n+         if (o == null || getClass() != o.getClass()) return false;\n+         ExpiryEntry that = (ExpiryEntry) o;\n+         return expiry == that.expiry &&\n+               Arrays.equals(keyBytes, that.keyBytes);\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+         int result = Objects.hash(expiry);\n+         result = 31 * result + Arrays.hashCode(keyBytes);\n+         return result;\n+      }\n+   }\n+\n+   private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n+      private final RocksIterator it;\n+      private final Predicate<? super K> filter;\n+      private final long now;\n+\n+      RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n+         this.it = it;\n+         this.filter = filter;\n+         this.now = now;\n+      }\n+\n+      @Override\n+      protected MarshallableEntry<K, V> getNext() {\n+         MarshallableEntry<K, V> entry = null;\n+         while (entry == null && it.isValid()) {\n+            K key = unmarshall(it.key());\n+            if (filter == null || filter.test(key)) {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n+               if (me != null && !me.isExpired(now)) {\n+                  entry = me;\n+               }\n             }\n-            return entry;\n-        }\n-    }\n+            it.next();\n+         }\n+         return entry;\n+      }\n+   }\n \n-    private abstract class RocksDBHandler {\n+   private abstract class RocksDBHandler {\n \n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n \n-        abstract void close();\n+      abstract void close();\n \n-        abstract ColumnFamilyHandle getHandle(int segment);\n+      abstract ColumnFamilyHandle getHandle(int segment);\n \n-        abstract ColumnFamilyHandle getHandle(Object key);\n+      abstract ColumnFamilyHandle getHandle(Object key);\n \n-        abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n+      abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n \n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n-            } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n             }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        return db.get(handle, marshall(key));\n-                    } catch (RocksDBException e) {\n-                        throw new CompletionException(e);\n-                    }\n-                }, \"rocksdb-load\");\n-                return entryByteStage.thenApply(entryBytes -> {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n-                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                        return null;\n-                    }\n-                    return me;\n-                });\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n             ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                return blockingManager.runBlocking(() -> {\n-                    try {\n-                        db.put(handle, marshalledKey, marshalledValue);\n-                        if (me.expiryTime() > -1) {\n-                            addNewExpiry(me);\n-                        }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-write\");\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      abstract CompletionStage<Void> clear();\n \n-        CompletionStage<Boolean> delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                return blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        if (db.get(handle, keyBytes) == null) {\n-                            return Boolean.FALSE;\n-                        }\n-                        db.delete(handle, keyBytes);\n-                        return Boolean.TRUE;\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-delete\");\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract CompletionStage<Void> clear();\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                boolean fetchValue);\n-\n-        CompletionStage<Long> size(IntSet segments) {\n-            return Flowable.fromPublisher(publishKeys(segments, null))\n-                    .count().toCompletionStage();\n-        }\n-\n-        abstract CompletionStage<Long> approximateSize(IntSet segments);\n-\n-        <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-            }));\n-        }\n-\n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n-\n-        abstract CompletionStage<Void> addSegments(IntSet segments);\n-\n-        abstract CompletionStage<Void> removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                    Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                    handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> clear() {\n-            return clear(null);\n-        }\n-\n-        CompletionStage<Void> clear(IntSet segments) {\n-            return blockingManager.runBlocking(() -> {\n-                long count = 0;\n-                boolean destroyDatabase = false;\n-                try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                    RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n-                    if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n-                        try (RocksIterator it = optionalIterator) {\n-                            for (it.seekToFirst(); it.isValid(); it.next()) {\n-                                byte[] keyBytes = it.key();\n-                                if (segments != null) {\n-                                    Object key = unmarshall(keyBytes);\n-                                    int segment = keyPartitioner.getSegment(key);\n-                                    if (segments.contains(segment)) {\n-                                        db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    }\n-                                } else {\n-                                    db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    count++;\n-\n-                                    if (count > configuration.clearThreshold()) {\n-                                        destroyDatabase = true;\n-                                        break;\n-                                    }\n-                                }\n-                            }\n-                        } catch (RocksDBException e) {\n-                            if (segments != null) {\n-                                // Have to propagate error to user\n-                                throw e;\n-                            }\n-                            // If was error and no segment specific just delete entire thing\n-                            destroyDatabase = true;\n-                        }\n-                    } else {\n-                        destroyDatabase = true;\n-                    }\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                }\n-\n-                if (destroyDatabase) {\n-                    try {\n-                        reinitAllDatabases();\n-                    } catch (Exception e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        @Override\n-        void close() {\n-            defaultColumnFamilyHandle.close();\n-\n-            db.close();\n-        }\n-\n-        protected void reinitAllDatabases() throws RocksDBException {\n-            db.close();\n-            expiredDb.close();\n-            if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n-                // Force a GC to ensure that open file handles are released in Windows.\n-                System.gc();\n-            }\n-            Path dataLocation = getLocation();\n-            Util.recursiveFileRemove(dataLocation.toFile());\n-            db = open(getLocation(), dataDbOptions());\n-\n-            Path expirationLocation = getExpirationLocation();\n-            Util.recursiveFileRemove(expirationLocation.toFile());\n-            expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n-        }\n-\n-        protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            // Some Cache Store tests use clear and in case of the Rocks DB implementation\n-            // this clears out internal references and results in throwing exceptions\n-            // when getting an iterator. Unfortunately there is no nice way to check that...\n-            return db.newIterator(defaultColumnFamilyHandle, readOptions);\n-        }\n-\n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n-            return publish(-1, it -> Flowable.fromIterable(() -> {\n-                // Make sure this is taken when the iterator is created\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, combinedFilter, now);\n-            }));\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return size(segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            // Do nothing\n-            return CompletableFutures.completedNull();\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n-            // segment check on every entry\n-            return clear(segments);\n-        }\n-    }\n-\n-    private class SegmentedRocksDBHandler extends RocksDBHandler {\n-        private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n-\n-        private SegmentedRocksDBHandler(int segmentCount) {\n-            this.handles = new AtomicReferenceArray<>(segmentCount);\n-        }\n-\n-        byte[] byteArrayFromInt(int val) {\n-            return new byte[] {\n-                  (byte) (val >>> 24),\n-                  (byte) (val >>> 16),\n-                  (byte) (val >>> 8),\n-                  (byte) (val)\n-            };\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return handles.get(segment);\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return handles.get(keyPartitioner.getSegment(key));\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return getHandle(unmarshall(marshalledKey));\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            int segmentCount = handles.length();\n-            List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n-            List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n-            // You have to open the default column family\n-            descriptors.add(new ColumnFamilyDescriptor(\n-                  RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n-            for (int i = 0; i < segmentCount; ++i) {\n-                descriptors.add(newDescriptor(byteArrayFromInt(i)));\n-            }\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n-            for (int i = 0; i < segmentCount; ++i) {\n-                handles.set(i, outHandles.get(i + 1));\n-            }\n-            return rocksDB;\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        @Override\n-        CompletionStage<Void> clear() {\n-            return blockingManager.runBlocking(() -> {\n-                for (int i = 0; i < handles.length(); ++i) {\n-                    if (!clearForSegment(i)) {\n-                        recreateColumnFamily(i);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        /**\n-         * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n-         * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n-         * the entries it will instead return true\n-         * @param segment the segment to clear out\n-         * @return whether it was able to clear all entries for the segment\n-         */\n-        private boolean clearForSegment(int segment) {\n-            int clearThreshold = configuration.clearThreshold();\n-            // If we always have to recreate don't even create iterator\n-            if (clearThreshold <= 0) {\n-                return false;\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n+\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n+\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.empty();\n             }\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      abstract CompletionStage<Void> addSegments(IntSet segments);\n+\n+      abstract CompletionStage<Void> removeSegments(IntSet segments);\n+   }\n+\n+   private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n+      private final KeyPartitioner keyPartitioner;\n+\n+      private ColumnFamilyHandle defaultColumnFamilyHandle;\n+\n+      private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n+         this.keyPartitioner = keyPartitioner;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(),\n+               Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n+               handles);\n+         defaultColumnFamilyHandle = handles.get(0);\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return clear(null);\n+      }\n+\n+      CompletionStage<Void> clear(IntSet segments) {\n+         return blockingManager.runBlocking(() -> {\n+            long count = 0;\n+            boolean destroyDatabase = false;\n             try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n-                if (optionalIterator != null) {\n-                    ColumnFamilyHandle handle = handles.get(segment);\n-                    try (RocksIterator it = optionalIterator) {\n-                        long count = 0;\n-                        for (it.seekToFirst(); it.isValid(); it.next()) {\n-                            byte[] keyBytes = it.key();\n-                            db.delete(handle, keyBytes);\n-\n-                            if (++count > configuration.clearThreshold()) {\n-                                return false;\n-                            }\n+               RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n+               if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n+                  try (RocksIterator it = optionalIterator) {\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        byte[] keyBytes = it.key();\n+                        if (segments != null) {\n+                           Object key = unmarshall(keyBytes);\n+                           int segment = keyPartitioner.getSegment(key);\n+                           if (segments.contains(segment)) {\n+                              db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           }\n+                        } else {\n+                           db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           count++;\n+\n+                           if (count > configuration.clearThreshold()) {\n+                              destroyDatabase = true;\n+                              break;\n+                           }\n                         }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                    return true;\n-                } else {\n-                    // If optional iterator was null that means either we don't own this segment or it was just\n-                    // recrated - in either case we can consider that cleared\n-                    return true;\n-                }\n+                     }\n+                  } catch (RocksDBException e) {\n+                     if (segments != null) {\n+                        // Have to propagate error to user\n+                        throw e;\n+                     }\n+                     // If was error and no segment specific just delete entire thing\n+                     destroyDatabase = true;\n+                  }\n+               } else {\n+                  destroyDatabase = true;\n+               }\n             } catch (Exception e) {\n-                throw new PersistenceException(e);\n+               throw new PersistenceException(e);\n             }\n-        }\n \n-        @Override\n-        void close() {\n+            if (destroyDatabase) {\n+               try {\n+                  reinitAllDatabases();\n+               } catch (Exception e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      @Override\n+      void close() {\n+         defaultColumnFamilyHandle.close();\n+\n+         db.close();\n+      }\n+\n+      protected void reinitAllDatabases() throws RocksDBException {\n+         db.close();\n+         expiredDb.close();\n+         if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+            // Force a GC to ensure that open file handles are released in Windows.\n+            System.gc();\n+         }\n+         Path dataLocation = getLocation();\n+         Util.recursiveFileRemove(dataLocation.toFile());\n+         db = open(getLocation(), dataDbOptions());\n+\n+         Path expirationLocation = getExpirationLocation();\n+         Util.recursiveFileRemove(expirationLocation.toFile());\n+         expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n+      }\n+\n+      protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         // Some Cache Store tests use clear and in case of the Rocks DB implementation\n+         // this clears out internal references and results in throwing exceptions\n+         // when getting an iterator. Unfortunately there is no nice way to check that...\n+         return db.newIterator(defaultColumnFamilyHandle, readOptions);\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n+         return publish(-1, it -> Flowable.fromIterable(() -> {\n+            // Make sure this is taken when the iterator is created\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, combinedFilter, now);\n+         }));\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return size(segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         // Do nothing\n+         return CompletableFutures.completedNull();\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n+         // segment check on every entry\n+         return clear(segments);\n+      }\n+   }\n+\n+   private class SegmentedRocksDBHandler extends RocksDBHandler {\n+      private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n+\n+      private SegmentedRocksDBHandler(int segmentCount) {\n+         this.handles = new AtomicReferenceArray<>(segmentCount);\n+      }\n+\n+      byte[] byteArrayFromInt(int val) {\n+         return new byte[] {\n+               (byte) (val >>> 24),\n+               (byte) (val >>> 16),\n+               (byte) (val >>> 8),\n+               (byte) (val)\n+         };\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return handles.get(segment);\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return handles.get(keyPartitioner.getSegment(key));\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return getHandle(unmarshall(marshalledKey));\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         int segmentCount = handles.length();\n+         List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n+         List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n+         // You have to open the default column family\n+         descriptors.add(new ColumnFamilyDescriptor(\n+               RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n+         for (int i = 0; i < segmentCount; ++i) {\n+            descriptors.add(newDescriptor(byteArrayFromInt(i)));\n+         }\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n+         for (int i = 0; i < segmentCount; ++i) {\n+            handles.set(i, outHandles.get(i + 1));\n+         }\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return blockingManager.runBlocking(() -> {\n             for (int i = 0; i < handles.length(); ++i) {\n-                ColumnFamilyHandle handle = handles.getAndSet(i, null);\n-                if (handle != null) {\n-                    handle.close();\n-                }\n+               if (!clearForSegment(i)) {\n+                  recreateColumnFamily(i);\n+               }\n             }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      /**\n+       * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n+       * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n+       * the entries it will instead return true\n+       * @param segment the segment to clear out\n+       * @return whether it was able to clear all entries for the segment\n+       */\n+      private boolean clearForSegment(int segment) {\n+         int clearThreshold = configuration.clearThreshold();\n+         // If we always have to recreate don't even create iterator\n+         if (clearThreshold <= 0) {\n+            return false;\n+         }\n+         try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+            RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n+            if (optionalIterator != null) {\n+               ColumnFamilyHandle handle = handles.get(segment);\n+               try (RocksIterator it = optionalIterator) {\n+                  long count = 0;\n+                  for (it.seekToFirst(); it.isValid(); it.next()) {\n+                     byte[] keyBytes = it.key();\n+                     db.delete(handle, keyBytes);\n+\n+                     if (++count > configuration.clearThreshold()) {\n+                        return false;\n+                     }\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+               return true;\n+            } else {\n+               // If optional iterator was null that means either we don't own this segment or it was just\n+               // recrated - in either case we can consider that cleared\n+               return true;\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            db.close();\n-        }\n-\n-        private void recreateColumnFamily(int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n+      @Override\n+      void close() {\n+         for (int i = 0; i < handles.length(); ++i) {\n+            ColumnFamilyHandle handle = handles.getAndSet(i, null);\n             if (handle != null) {\n-                try {\n-                    db.dropColumnFamily(handle);\n-                    handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n+               handle.close();\n             }\n-        }\n+         }\n \n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, filter, now);\n-            });\n-            return handleIteratorFunction(function, segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                try {\n-                    return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }), \"rocksdb-approximateSize\");\n-        }\n-\n-        <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n-            // Short circuit if only a single segment - assumed to be invoked from persistence thread\n-            if (segments != null && segments.size() == 1) {\n-                return publish(segments.iterator().nextInt(), function);\n+         db.close();\n+      }\n+\n+      private void recreateColumnFamily(int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            try {\n+               db.dropColumnFamily(handle);\n+               handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n-            return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n-                    .concatMap(RxJavaInterop.identityFunction());\n-        }\n-\n-        @Override\n-        RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n-            if (handle != null) {\n-                return db.newIterator(handle, readOptions);\n+         }\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, filter, now);\n+         });\n+         return handleIteratorFunction(function, segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            try {\n+               return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            return null;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n-                    .filter(segment -> handles.get(segment) == null);\n-\n-            return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n-                if (trace) {\n-                    log.tracef(\"Creating column family for segment %d\", segment);\n-                }\n-                byte[] cfName = byteArrayFromInt(segment);\n-                try {\n-                    ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }, \"testng-addSegments\");\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n-                    .map(segment -> {\n-                        ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n-                        return cf != null ? cf : this;\n-                    }).ofType(ColumnFamilyHandle.class);\n-\n-            return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n-                if (trace) {\n-                    log.tracef(\"Dropping column family %s\", handle);\n-                }\n-                try {\n-                    db.dropColumnFamily(handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-                handle.close();\n-            }, \"testng-removeSegments\");\n-        }\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n+         }), \"rocksdb-approximateSize\");\n+      }\n+\n+      <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n+         // Short circuit if only a single segment - assumed to be invoked from persistence thread\n+         if (segments != null && segments.size() == 1) {\n+            return publish(segments.iterator().nextInt(), function);\n+         }\n+         IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n+         return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n+               .concatMap(RxJavaInterop.identityFunction());\n+      }\n+\n+      @Override\n+      RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            return db.newIterator(handle, readOptions);\n+         }\n+         return null;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n+               .filter(segment -> handles.get(segment) == null);\n+\n+         return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n+            if (trace) {\n+               log.tracef(\"Creating column family for segment %d\", segment);\n+            }\n+            byte[] cfName = byteArrayFromInt(segment);\n+            try {\n+               ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n+            }\n+         }, \"testng-addSegments\");\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n+               .map(segment -> {\n+                  ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n+                  return cf != null ? cf : this;\n+               }).ofType(ColumnFamilyHandle.class);\n+\n+         return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n+            if (trace) {\n+               log.tracef(\"Dropping column family %s\", handle);\n+            }\n+            try {\n+               db.dropColumnFamily(handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n+            handle.close();\n+         }, \"testng-removeSegments\");\n+      }\n+   }\n+\n+   private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n+      final byte[] expiryBytes = marshall(entry.expiry);\n+      final byte[] existingBytes = expiredDb.get(expiryBytes);\n+\n+      if (existingBytes != null) {\n+         // in the case of collision make the value a List ...\n+         final Object existing = unmarshall(existingBytes);\n+         if (existing instanceof ExpiryBucket) {\n+            ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(existing));\n+         } else {\n+            ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(bucket));\n+         }\n+      } else {\n+         expiredDb.put(expiryBytes, entry.keyBytes);\n+      }\n+   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwOTAxMg==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435809012", "bodyText": "As publishEntries utilises the RocksEntryIterator we're also unnecessarily unmarshalling the value. In this case I think we can simply add a boolean to the RocksEntryIterator constructor and pass the fetchValue parameter to it.", "author": "ryanemerson", "createdAt": "2020-06-05T09:41:04Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MjMzMA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436142330", "bodyText": "I looked closely at this one, unfortunately due to the contract of keyPublisher with expiration and the way values are stored we have to always unmarshall the value to get the metadata, so I just consolidated it.", "author": "wburns", "createdAt": "2020-06-05T20:12:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgwOTAxMg=="}], "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex d6f1df4446..25ddd42963 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -75,962 +75,962 @@\n \n @ConfiguredBy(RocksDBStoreConfiguration.class)\n public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n-    private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n-    private static final boolean trace = log.isTraceEnabled();\n-\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private KeyPartitioner keyPartitioner;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private BlockingManager blockingManager;\n-\n-    @Override\n-    public CompletionStage<Void> start(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        this.blockingManager = ctx.getBlockingManager();\n-        this.keyPartitioner = ctx.getKeyPartitioner();\n-\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n             }\n-        }\n-\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db = handler.open(getLocation(), dataDbOptions());\n-                expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            } catch (Exception e) {\n-                throw new CacheConfigurationException(\"Unable to open database\", e);\n-            }\n-        }, \"rocksdb-open\");\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> stop() {\n-        return blockingManager.runBlocking(() -> {\n-            handler.close();\n-            expiredDb.close();\n-        }, \"rocksdb-stop\");\n-    }\n-\n-    @Override\n-    public Set<Characteristic> characteristics() {\n-        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> isAvailable() {\n-        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n-                \"rocksdb-available\");\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> clear() {\n-        return handler.clear();\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> approximateSize(IntSet segments) {\n-        return handler.approximateSize(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n-        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-        return load(segment, key)\n-                .thenApply(Objects::nonNull);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n-                .map(MarshallableEntry::getKey);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n-        return handler.publishEntries(segments, filter, includeValues);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        return handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n-            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n-        WriteBatch batch = new WriteBatch();\n-        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n-        Flowable.fromPublisher(removePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n-                });\n-        Flowable.fromPublisher(writePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(me -> {\n-                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n-                                if (me.expiryTime() > -1) {\n-                                    expirableEntries.add(me);\n-                                }\n-                            });\n-                });\n-        if (batch.count() <= 0) {\n-            batch.close();\n-            return CompletableFutures.completedNull();\n-        }\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db.write(dataWriteOptions(), batch);\n-                for (MarshallableEntry<K, V> me : expirableEntries) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (RocksDBException e) {\n-                throw new PersistenceException(e);\n-            }\n-        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n-        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n-            // We check expiration based on time of subscription only\n-            long now = timeService.wallClockTime();\n-            return actualPurgeExpired(now)\n-                    // We return a buffer of expired entries emitted to the non blocking thread\n-                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n-                    // consumed much faster than emission (since each emission performs a get and remove)\n-                    .buffer(16);\n-        }));\n-\n-        return Flowable.fromPublisher(purgedBatches)\n-                .concatMap(Flowable::fromIterable);\n-    }\n-\n-    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n-        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n-        // given entries\n-        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n-        }, entry -> {\n-            if (entry.getValue() == null) {\n-                return Flowable.empty();\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n             }\n-            RocksIterator iterator = entry.getValue();\n-            iterator.seekToFirst();\n-\n-            return Flowable.fromIterable(() ->\n-                    new AbstractIterator<byte[]>() {\n-                        @Override\n-                        protected byte[] getNext() {\n-                            if (iterator.isValid()) {\n-                                byte[] keyBytes = iterator.key();\n-                                Long time = unmarshall(keyBytes);\n-                                if (time > now)\n-                                    return null;\n-                                try {\n-                                    expiredDb.delete(keyBytes);\n-                                } catch (RocksDBException e) {\n-                                    throw new PersistenceException(e);\n-                                }\n-                                byte[] value = iterator.value();\n-                                iterator.next();\n-                                return value;\n-                            }\n-                            return null;\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db = handler.open(getLocation(), dataDbOptions());\n+            expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n+         } catch (Exception e) {\n+            throw new CacheConfigurationException(\"Unable to open database\", e);\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n                         }\n-                    });\n-        }, entry -> {\n-            entry.getKey().close();\n-            RocksIterator rocksIterator = entry.getValue();\n-            if (rocksIterator != null) {\n-                rocksIterator.close();\n-            }\n-        });\n-\n-        Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n-            Object bucketKey = unmarshall(expiredBytes);\n-            if (bucketKey instanceof ExpiryBucket) {\n-                return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n-                        .flatMapMaybe(marshalledKey -> {\n-                            ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n-                            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n-                            return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n-                        });\n-            } else {\n-                // The bucketKey is an actual key\n-                ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n-                MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n-                return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n-            }\n-        });\n-\n-        if (trace) {\n-            // Note this tracing only works properly for one subscriber\n-            FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n-            expiredEntryFlowable = expiredEntryFlowable\n-                    .doOnEach(mirrorEntries)\n-                    .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n-            mirrorEntries.count()\n-                    .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n-        }\n-\n-        return expiredEntryFlowable;\n-    }\n-\n-    private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n-            long now) throws RocksDBException {\n-        byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n-        if (valueBytes == null) {\n-            return null;\n-        }\n-        MarshalledValue mv = unmarshall(valueBytes);\n-        if (mv != null) {\n-            // TODO race condition: the entry could be updated between the get and delete!\n-            Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n-                db.delete(columnFamilyHandle, marshalledKey);\n-                return mv;\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n             }\n-        }\n-        return null;\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> addSegments(IntSet segments) {\n-        return handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> removeSegments(IntSet segments) {\n-        return handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) {\n-        try {\n-            return marshaller.objectToByteBuffer(entry);\n-        } catch (IOException e) {\n-            throw new PersistenceException(e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private <E> E unmarshall(byte[] bytes) {\n-        if (bytes == null)\n-            return null;\n-\n-        try {\n-            //noinspection unchecked\n-            return (E) marshaller.objectFromByteBuffer(bytes);\n-        } catch (IOException | ClassNotFoundException e) {\n+         } catch (RocksDBException e) {\n             throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n-        MarshalledValue value = unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n-                value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-        putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final long now;\n-\n-        RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            while (entry == null && it.isValid()) {\n-                K key = unmarshall(it.key());\n-                if (filter == null || filter.test(key)) {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n-                    if (me != null && !me.isExpired(now)) {\n-                        entry = me;\n-                    }\n-                }\n-                it.next();\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+         // We check expiration based on time of subscription only\n+         long now = timeService.wallClockTime();\n+         return actualPurgeExpired(now)\n+               // We return a buffer of expired entries emitted to the non blocking thread\n+               // This prevents waking up the non blocking thread for every entry as they will most likely be\n+               // consumed much faster than emission (since each emission performs a get and remove)\n+               .buffer(16);\n+      }));\n+\n+      return Flowable.fromPublisher(purgedBatches)\n+            .concatMap(Flowable::fromIterable);\n+   }\n+\n+   private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+      // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+      // given entries\n+      Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+      }, entry -> {\n+         if (entry.getValue() == null) {\n+            return Flowable.empty();\n+         }\n+         RocksIterator iterator = entry.getValue();\n+         iterator.seekToFirst();\n+\n+         return Flowable.fromIterable(() ->\n+               new AbstractIterator<byte[]>() {\n+                  @Override\n+                  protected byte[] getNext() {\n+                     if (!iterator.isValid()) {\n+                        return null;\n+                     }\n+                     byte[] keyBytes = iterator.key();\n+                     Long time = unmarshall(keyBytes);\n+                     if (time > now)\n+                        return null;\n+                     try {\n+                        expiredDb.delete(keyBytes);\n+                     } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n+                     }\n+                     byte[] value = iterator.value();\n+                     iterator.next();\n+                     return value;\n+                  }\n+               });\n+      }, entry -> {\n+         entry.getKey().close();\n+         RocksIterator rocksIterator = entry.getValue();\n+         if (rocksIterator != null) {\n+            rocksIterator.close();\n+         }\n+      });\n+\n+      Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n+         Object bucketKey = unmarshall(expiredBytes);\n+         if (bucketKey instanceof ExpiryBucket) {\n+            return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n+                  .flatMapMaybe(marshalledKey -> {\n+                     ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n+                     MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n+                     return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n+                  });\n+         } else {\n+            // The bucketKey is an actual key\n+            ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n+            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n+            return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n+         }\n+      });\n+\n+      if (trace) {\n+         // Note this tracing only works properly for one subscriber\n+         FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n+         expiredEntryFlowable = expiredEntryFlowable\n+               .doOnEach(mirrorEntries)\n+               .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n+         mirrorEntries.count()\n+               .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n+      }\n+\n+      return expiredEntryFlowable;\n+   }\n+\n+   private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n+         long now) throws RocksDBException {\n+      byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n+      if (valueBytes == null) {\n+         return null;\n+      }\n+      MarshalledValue mv = unmarshall(valueBytes);\n+      if (mv != null) {\n+         // TODO race condition: the entry could be updated between the get and delete!\n+         Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+         if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+            // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+            db.delete(columnFamilyHandle, marshalledKey);\n+            return mv;\n+         }\n+      }\n+      return null;\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   private byte[] marshall(Object entry) {\n+      try {\n+         return marshaller.objectToByteBuffer(entry);\n+      } catch (IOException e) {\n+         throw new PersistenceException(e);\n+      } catch (InterruptedException e) {\n+         Thread.currentThread().interrupt();\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private <E> E unmarshall(byte[] bytes) {\n+      if (bytes == null)\n+         return null;\n+\n+      try {\n+         //noinspection unchecked\n+         return (E) marshaller.objectFromByteBuffer(bytes);\n+      } catch (IOException | ClassNotFoundException e) {\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n+      MarshalledValue value = unmarshall(valueBytes);\n+      if (value == null) return null;\n+\n+      return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n+            value.getCreated(), value.getLastUsed());\n+   }\n+\n+   private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n+      long expiry = entry.expiryTime();\n+      long maxIdle = entry.getMetadata().maxIdle();\n+      if (maxIdle > 0) {\n+         // Coding getExpiryTime() for transient entries has the risk of being a moving target\n+         // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n+         expiry = maxIdle + ctx.getTimeService().wallClockTime();\n+      }\n+      byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n+      putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n+   }\n+\n+   @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n+   static final class ExpiryBucket {\n+      @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n+      List<byte[]> entries;\n+\n+      ExpiryBucket(){}\n+\n+      ExpiryBucket(byte[] existingKey, byte[] newKey) {\n+         entries = new ArrayList<>(2);\n+         entries.add(existingKey);\n+         entries.add(newKey);\n+      }\n+   }\n+\n+   private static final class ExpiryEntry {\n+\n+      final long expiry;\n+      final byte[] keyBytes;\n+\n+      ExpiryEntry(long expiry, byte[] keyBytes) {\n+         this.expiry = expiry;\n+         this.keyBytes = keyBytes;\n+      }\n+\n+      @Override\n+      public boolean equals(Object o) {\n+         if (this == o) return true;\n+         if (o == null || getClass() != o.getClass()) return false;\n+         ExpiryEntry that = (ExpiryEntry) o;\n+         return expiry == that.expiry &&\n+               Arrays.equals(keyBytes, that.keyBytes);\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+         int result = Objects.hash(expiry);\n+         result = 31 * result + Arrays.hashCode(keyBytes);\n+         return result;\n+      }\n+   }\n+\n+   private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n+      private final RocksIterator it;\n+      private final Predicate<? super K> filter;\n+      private final long now;\n+\n+      RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n+         this.it = it;\n+         this.filter = filter;\n+         this.now = now;\n+      }\n+\n+      @Override\n+      protected MarshallableEntry<K, V> getNext() {\n+         MarshallableEntry<K, V> entry = null;\n+         while (entry == null && it.isValid()) {\n+            K key = unmarshall(it.key());\n+            if (filter == null || filter.test(key)) {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n+               if (me != null && !me.isExpired(now)) {\n+                  entry = me;\n+               }\n             }\n-            return entry;\n-        }\n-    }\n+            it.next();\n+         }\n+         return entry;\n+      }\n+   }\n \n-    private abstract class RocksDBHandler {\n+   private abstract class RocksDBHandler {\n \n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n \n-        abstract void close();\n+      abstract void close();\n \n-        abstract ColumnFamilyHandle getHandle(int segment);\n+      abstract ColumnFamilyHandle getHandle(int segment);\n \n-        abstract ColumnFamilyHandle getHandle(Object key);\n+      abstract ColumnFamilyHandle getHandle(Object key);\n \n-        abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n+      abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n \n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n-            } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n             }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        return db.get(handle, marshall(key));\n-                    } catch (RocksDBException e) {\n-                        throw new CompletionException(e);\n-                    }\n-                }, \"rocksdb-load\");\n-                return entryByteStage.thenApply(entryBytes -> {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n-                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                        return null;\n-                    }\n-                    return me;\n-                });\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n             ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                return blockingManager.runBlocking(() -> {\n-                    try {\n-                        db.put(handle, marshalledKey, marshalledValue);\n-                        if (me.expiryTime() > -1) {\n-                            addNewExpiry(me);\n-                        }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-write\");\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      abstract CompletionStage<Void> clear();\n \n-        CompletionStage<Boolean> delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                return blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        if (db.get(handle, keyBytes) == null) {\n-                            return Boolean.FALSE;\n-                        }\n-                        db.delete(handle, keyBytes);\n-                        return Boolean.TRUE;\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-delete\");\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract CompletionStage<Void> clear();\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                boolean fetchValue);\n-\n-        CompletionStage<Long> size(IntSet segments) {\n-            return Flowable.fromPublisher(publishKeys(segments, null))\n-                    .count().toCompletionStage();\n-        }\n-\n-        abstract CompletionStage<Long> approximateSize(IntSet segments);\n-\n-        <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-            }));\n-        }\n-\n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n-\n-        abstract CompletionStage<Void> addSegments(IntSet segments);\n-\n-        abstract CompletionStage<Void> removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                    Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                    handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> clear() {\n-            return clear(null);\n-        }\n-\n-        CompletionStage<Void> clear(IntSet segments) {\n-            return blockingManager.runBlocking(() -> {\n-                long count = 0;\n-                boolean destroyDatabase = false;\n-                try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                    RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n-                    if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n-                        try (RocksIterator it = optionalIterator) {\n-                            for (it.seekToFirst(); it.isValid(); it.next()) {\n-                                byte[] keyBytes = it.key();\n-                                if (segments != null) {\n-                                    Object key = unmarshall(keyBytes);\n-                                    int segment = keyPartitioner.getSegment(key);\n-                                    if (segments.contains(segment)) {\n-                                        db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    }\n-                                } else {\n-                                    db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    count++;\n-\n-                                    if (count > configuration.clearThreshold()) {\n-                                        destroyDatabase = true;\n-                                        break;\n-                                    }\n-                                }\n-                            }\n-                        } catch (RocksDBException e) {\n-                            if (segments != null) {\n-                                // Have to propagate error to user\n-                                throw e;\n-                            }\n-                            // If was error and no segment specific just delete entire thing\n-                            destroyDatabase = true;\n-                        }\n-                    } else {\n-                        destroyDatabase = true;\n-                    }\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                }\n-\n-                if (destroyDatabase) {\n-                    try {\n-                        reinitAllDatabases();\n-                    } catch (Exception e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        @Override\n-        void close() {\n-            defaultColumnFamilyHandle.close();\n-\n-            db.close();\n-        }\n-\n-        protected void reinitAllDatabases() throws RocksDBException {\n-            db.close();\n-            expiredDb.close();\n-            if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n-                // Force a GC to ensure that open file handles are released in Windows.\n-                System.gc();\n-            }\n-            Path dataLocation = getLocation();\n-            Util.recursiveFileRemove(dataLocation.toFile());\n-            db = open(getLocation(), dataDbOptions());\n-\n-            Path expirationLocation = getExpirationLocation();\n-            Util.recursiveFileRemove(expirationLocation.toFile());\n-            expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n-        }\n-\n-        protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            // Some Cache Store tests use clear and in case of the Rocks DB implementation\n-            // this clears out internal references and results in throwing exceptions\n-            // when getting an iterator. Unfortunately there is no nice way to check that...\n-            return db.newIterator(defaultColumnFamilyHandle, readOptions);\n-        }\n-\n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n-            return publish(-1, it -> Flowable.fromIterable(() -> {\n-                // Make sure this is taken when the iterator is created\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, combinedFilter, now);\n-            }));\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return size(segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            // Do nothing\n-            return CompletableFutures.completedNull();\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n-            // segment check on every entry\n-            return clear(segments);\n-        }\n-    }\n-\n-    private class SegmentedRocksDBHandler extends RocksDBHandler {\n-        private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n-\n-        private SegmentedRocksDBHandler(int segmentCount) {\n-            this.handles = new AtomicReferenceArray<>(segmentCount);\n-        }\n-\n-        byte[] byteArrayFromInt(int val) {\n-            return new byte[] {\n-                  (byte) (val >>> 24),\n-                  (byte) (val >>> 16),\n-                  (byte) (val >>> 8),\n-                  (byte) (val)\n-            };\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return handles.get(segment);\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return handles.get(keyPartitioner.getSegment(key));\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return getHandle(unmarshall(marshalledKey));\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            int segmentCount = handles.length();\n-            List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n-            List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n-            // You have to open the default column family\n-            descriptors.add(new ColumnFamilyDescriptor(\n-                  RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n-            for (int i = 0; i < segmentCount; ++i) {\n-                descriptors.add(newDescriptor(byteArrayFromInt(i)));\n-            }\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n-            for (int i = 0; i < segmentCount; ++i) {\n-                handles.set(i, outHandles.get(i + 1));\n-            }\n-            return rocksDB;\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        @Override\n-        CompletionStage<Void> clear() {\n-            return blockingManager.runBlocking(() -> {\n-                for (int i = 0; i < handles.length(); ++i) {\n-                    if (!clearForSegment(i)) {\n-                        recreateColumnFamily(i);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        /**\n-         * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n-         * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n-         * the entries it will instead return true\n-         * @param segment the segment to clear out\n-         * @return whether it was able to clear all entries for the segment\n-         */\n-        private boolean clearForSegment(int segment) {\n-            int clearThreshold = configuration.clearThreshold();\n-            // If we always have to recreate don't even create iterator\n-            if (clearThreshold <= 0) {\n-                return false;\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n+\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n+\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.empty();\n             }\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      abstract CompletionStage<Void> addSegments(IntSet segments);\n+\n+      abstract CompletionStage<Void> removeSegments(IntSet segments);\n+   }\n+\n+   private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n+      private final KeyPartitioner keyPartitioner;\n+\n+      private ColumnFamilyHandle defaultColumnFamilyHandle;\n+\n+      private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n+         this.keyPartitioner = keyPartitioner;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(),\n+               Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n+               handles);\n+         defaultColumnFamilyHandle = handles.get(0);\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return clear(null);\n+      }\n+\n+      CompletionStage<Void> clear(IntSet segments) {\n+         return blockingManager.runBlocking(() -> {\n+            long count = 0;\n+            boolean destroyDatabase = false;\n             try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n-                if (optionalIterator != null) {\n-                    ColumnFamilyHandle handle = handles.get(segment);\n-                    try (RocksIterator it = optionalIterator) {\n-                        long count = 0;\n-                        for (it.seekToFirst(); it.isValid(); it.next()) {\n-                            byte[] keyBytes = it.key();\n-                            db.delete(handle, keyBytes);\n-\n-                            if (++count > configuration.clearThreshold()) {\n-                                return false;\n-                            }\n+               RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n+               if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n+                  try (RocksIterator it = optionalIterator) {\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        byte[] keyBytes = it.key();\n+                        if (segments != null) {\n+                           Object key = unmarshall(keyBytes);\n+                           int segment = keyPartitioner.getSegment(key);\n+                           if (segments.contains(segment)) {\n+                              db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           }\n+                        } else {\n+                           db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           count++;\n+\n+                           if (count > configuration.clearThreshold()) {\n+                              destroyDatabase = true;\n+                              break;\n+                           }\n                         }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                    return true;\n-                } else {\n-                    // If optional iterator was null that means either we don't own this segment or it was just\n-                    // recrated - in either case we can consider that cleared\n-                    return true;\n-                }\n+                     }\n+                  } catch (RocksDBException e) {\n+                     if (segments != null) {\n+                        // Have to propagate error to user\n+                        throw e;\n+                     }\n+                     // If was error and no segment specific just delete entire thing\n+                     destroyDatabase = true;\n+                  }\n+               } else {\n+                  destroyDatabase = true;\n+               }\n             } catch (Exception e) {\n-                throw new PersistenceException(e);\n+               throw new PersistenceException(e);\n             }\n-        }\n \n-        @Override\n-        void close() {\n+            if (destroyDatabase) {\n+               try {\n+                  reinitAllDatabases();\n+               } catch (Exception e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      @Override\n+      void close() {\n+         defaultColumnFamilyHandle.close();\n+\n+         db.close();\n+      }\n+\n+      protected void reinitAllDatabases() throws RocksDBException {\n+         db.close();\n+         expiredDb.close();\n+         if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+            // Force a GC to ensure that open file handles are released in Windows.\n+            System.gc();\n+         }\n+         Path dataLocation = getLocation();\n+         Util.recursiveFileRemove(dataLocation.toFile());\n+         db = open(getLocation(), dataDbOptions());\n+\n+         Path expirationLocation = getExpirationLocation();\n+         Util.recursiveFileRemove(expirationLocation.toFile());\n+         expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n+      }\n+\n+      protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         // Some Cache Store tests use clear and in case of the Rocks DB implementation\n+         // this clears out internal references and results in throwing exceptions\n+         // when getting an iterator. Unfortunately there is no nice way to check that...\n+         return db.newIterator(defaultColumnFamilyHandle, readOptions);\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n+         return publish(-1, it -> Flowable.fromIterable(() -> {\n+            // Make sure this is taken when the iterator is created\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, combinedFilter, now);\n+         }));\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return size(segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         // Do nothing\n+         return CompletableFutures.completedNull();\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n+         // segment check on every entry\n+         return clear(segments);\n+      }\n+   }\n+\n+   private class SegmentedRocksDBHandler extends RocksDBHandler {\n+      private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n+\n+      private SegmentedRocksDBHandler(int segmentCount) {\n+         this.handles = new AtomicReferenceArray<>(segmentCount);\n+      }\n+\n+      byte[] byteArrayFromInt(int val) {\n+         return new byte[] {\n+               (byte) (val >>> 24),\n+               (byte) (val >>> 16),\n+               (byte) (val >>> 8),\n+               (byte) (val)\n+         };\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return handles.get(segment);\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return handles.get(keyPartitioner.getSegment(key));\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return getHandle(unmarshall(marshalledKey));\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         int segmentCount = handles.length();\n+         List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n+         List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n+         // You have to open the default column family\n+         descriptors.add(new ColumnFamilyDescriptor(\n+               RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n+         for (int i = 0; i < segmentCount; ++i) {\n+            descriptors.add(newDescriptor(byteArrayFromInt(i)));\n+         }\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n+         for (int i = 0; i < segmentCount; ++i) {\n+            handles.set(i, outHandles.get(i + 1));\n+         }\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return blockingManager.runBlocking(() -> {\n             for (int i = 0; i < handles.length(); ++i) {\n-                ColumnFamilyHandle handle = handles.getAndSet(i, null);\n-                if (handle != null) {\n-                    handle.close();\n-                }\n+               if (!clearForSegment(i)) {\n+                  recreateColumnFamily(i);\n+               }\n             }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      /**\n+       * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n+       * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n+       * the entries it will instead return true\n+       * @param segment the segment to clear out\n+       * @return whether it was able to clear all entries for the segment\n+       */\n+      private boolean clearForSegment(int segment) {\n+         int clearThreshold = configuration.clearThreshold();\n+         // If we always have to recreate don't even create iterator\n+         if (clearThreshold <= 0) {\n+            return false;\n+         }\n+         try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+            RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n+            if (optionalIterator != null) {\n+               ColumnFamilyHandle handle = handles.get(segment);\n+               try (RocksIterator it = optionalIterator) {\n+                  long count = 0;\n+                  for (it.seekToFirst(); it.isValid(); it.next()) {\n+                     byte[] keyBytes = it.key();\n+                     db.delete(handle, keyBytes);\n+\n+                     if (++count > configuration.clearThreshold()) {\n+                        return false;\n+                     }\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+               return true;\n+            } else {\n+               // If optional iterator was null that means either we don't own this segment or it was just\n+               // recrated - in either case we can consider that cleared\n+               return true;\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            db.close();\n-        }\n-\n-        private void recreateColumnFamily(int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n+      @Override\n+      void close() {\n+         for (int i = 0; i < handles.length(); ++i) {\n+            ColumnFamilyHandle handle = handles.getAndSet(i, null);\n             if (handle != null) {\n-                try {\n-                    db.dropColumnFamily(handle);\n-                    handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n+               handle.close();\n             }\n-        }\n+         }\n \n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, filter, now);\n-            });\n-            return handleIteratorFunction(function, segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                try {\n-                    return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }), \"rocksdb-approximateSize\");\n-        }\n-\n-        <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n-            // Short circuit if only a single segment - assumed to be invoked from persistence thread\n-            if (segments != null && segments.size() == 1) {\n-                return publish(segments.iterator().nextInt(), function);\n+         db.close();\n+      }\n+\n+      private void recreateColumnFamily(int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            try {\n+               db.dropColumnFamily(handle);\n+               handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n-            return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n-                    .concatMap(RxJavaInterop.identityFunction());\n-        }\n-\n-        @Override\n-        RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n-            if (handle != null) {\n-                return db.newIterator(handle, readOptions);\n+         }\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, filter, now);\n+         });\n+         return handleIteratorFunction(function, segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            try {\n+               return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            return null;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n-                    .filter(segment -> handles.get(segment) == null);\n-\n-            return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n-                if (trace) {\n-                    log.tracef(\"Creating column family for segment %d\", segment);\n-                }\n-                byte[] cfName = byteArrayFromInt(segment);\n-                try {\n-                    ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }, \"testng-addSegments\");\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n-                    .map(segment -> {\n-                        ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n-                        return cf != null ? cf : this;\n-                    }).ofType(ColumnFamilyHandle.class);\n-\n-            return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n-                if (trace) {\n-                    log.tracef(\"Dropping column family %s\", handle);\n-                }\n-                try {\n-                    db.dropColumnFamily(handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-                handle.close();\n-            }, \"testng-removeSegments\");\n-        }\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n+         }), \"rocksdb-approximateSize\");\n+      }\n+\n+      <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n+         // Short circuit if only a single segment - assumed to be invoked from persistence thread\n+         if (segments != null && segments.size() == 1) {\n+            return publish(segments.iterator().nextInt(), function);\n+         }\n+         IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n+         return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n+               .concatMap(RxJavaInterop.identityFunction());\n+      }\n+\n+      @Override\n+      RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            return db.newIterator(handle, readOptions);\n+         }\n+         return null;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n+               .filter(segment -> handles.get(segment) == null);\n+\n+         return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n+            if (trace) {\n+               log.tracef(\"Creating column family for segment %d\", segment);\n+            }\n+            byte[] cfName = byteArrayFromInt(segment);\n+            try {\n+               ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n+            }\n+         }, \"testng-addSegments\");\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n+               .map(segment -> {\n+                  ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n+                  return cf != null ? cf : this;\n+               }).ofType(ColumnFamilyHandle.class);\n+\n+         return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n+            if (trace) {\n+               log.tracef(\"Dropping column family %s\", handle);\n+            }\n+            try {\n+               db.dropColumnFamily(handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n+            handle.close();\n+         }, \"testng-removeSegments\");\n+      }\n+   }\n+\n+   private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n+      final byte[] expiryBytes = marshall(entry.expiry);\n+      final byte[] existingBytes = expiredDb.get(expiryBytes);\n+\n+      if (existingBytes != null) {\n+         // in the case of collision make the value a List ...\n+         final Object existing = unmarshall(existingBytes);\n+         if (existing instanceof ExpiryBucket) {\n+            ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(existing));\n+         } else {\n+            ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(bucket));\n+         }\n+      } else {\n+         expiredDb.put(expiryBytes, entry.keyBytes);\n+      }\n+   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435810745", "bodyText": "I missed this before, but I don't think boolean includeValues should be part of the publishEntries parameters. If you don't require the value, then you should just utilise publishKeys.\nProbably irrelavant now, but boolean includeValues has no Javadocs in the interface.", "author": "ryanemerson", "createdAt": "2020-06-05T09:44:03Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);\n     }\n \n     @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n+    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MjYyMg==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436142622", "bodyText": "You can use publishEntries with includeValues as false to get the metadata still (although  most stores may still return the value).", "author": "wburns", "createdAt": "2020-06-05T20:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU3OTgxMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436579811", "bodyText": "Ok, in that case publishEntries is still missing the JavDoc for the includeValues parameter. As part of the main javadocs \"body\" I think you should include your explanation of why !includeValues is useful and that some stores may still return the value in MarshallableEntry.", "author": "ryanemerson", "createdAt": "2020-06-08T09:51:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxMDc0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex d6f1df4446..25ddd42963 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -75,962 +75,962 @@\n \n @ConfiguredBy(RocksDBStoreConfiguration.class)\n public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n-    private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n-    private static final boolean trace = log.isTraceEnabled();\n-\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private KeyPartitioner keyPartitioner;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private BlockingManager blockingManager;\n-\n-    @Override\n-    public CompletionStage<Void> start(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        this.blockingManager = ctx.getBlockingManager();\n-        this.keyPartitioner = ctx.getKeyPartitioner();\n-\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n             }\n-        }\n-\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db = handler.open(getLocation(), dataDbOptions());\n-                expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            } catch (Exception e) {\n-                throw new CacheConfigurationException(\"Unable to open database\", e);\n-            }\n-        }, \"rocksdb-open\");\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> stop() {\n-        return blockingManager.runBlocking(() -> {\n-            handler.close();\n-            expiredDb.close();\n-        }, \"rocksdb-stop\");\n-    }\n-\n-    @Override\n-    public Set<Characteristic> characteristics() {\n-        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> isAvailable() {\n-        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n-                \"rocksdb-available\");\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> clear() {\n-        return handler.clear();\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> approximateSize(IntSet segments) {\n-        return handler.approximateSize(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n-        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-        return load(segment, key)\n-                .thenApply(Objects::nonNull);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n-                .map(MarshallableEntry::getKey);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n-        return handler.publishEntries(segments, filter, includeValues);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        return handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n-            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n-        WriteBatch batch = new WriteBatch();\n-        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n-        Flowable.fromPublisher(removePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n-                });\n-        Flowable.fromPublisher(writePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(me -> {\n-                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n-                                if (me.expiryTime() > -1) {\n-                                    expirableEntries.add(me);\n-                                }\n-                            });\n-                });\n-        if (batch.count() <= 0) {\n-            batch.close();\n-            return CompletableFutures.completedNull();\n-        }\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db.write(dataWriteOptions(), batch);\n-                for (MarshallableEntry<K, V> me : expirableEntries) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (RocksDBException e) {\n-                throw new PersistenceException(e);\n-            }\n-        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n-        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n-            // We check expiration based on time of subscription only\n-            long now = timeService.wallClockTime();\n-            return actualPurgeExpired(now)\n-                    // We return a buffer of expired entries emitted to the non blocking thread\n-                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n-                    // consumed much faster than emission (since each emission performs a get and remove)\n-                    .buffer(16);\n-        }));\n-\n-        return Flowable.fromPublisher(purgedBatches)\n-                .concatMap(Flowable::fromIterable);\n-    }\n-\n-    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n-        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n-        // given entries\n-        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n-        }, entry -> {\n-            if (entry.getValue() == null) {\n-                return Flowable.empty();\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n             }\n-            RocksIterator iterator = entry.getValue();\n-            iterator.seekToFirst();\n-\n-            return Flowable.fromIterable(() ->\n-                    new AbstractIterator<byte[]>() {\n-                        @Override\n-                        protected byte[] getNext() {\n-                            if (iterator.isValid()) {\n-                                byte[] keyBytes = iterator.key();\n-                                Long time = unmarshall(keyBytes);\n-                                if (time > now)\n-                                    return null;\n-                                try {\n-                                    expiredDb.delete(keyBytes);\n-                                } catch (RocksDBException e) {\n-                                    throw new PersistenceException(e);\n-                                }\n-                                byte[] value = iterator.value();\n-                                iterator.next();\n-                                return value;\n-                            }\n-                            return null;\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db = handler.open(getLocation(), dataDbOptions());\n+            expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n+         } catch (Exception e) {\n+            throw new CacheConfigurationException(\"Unable to open database\", e);\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n                         }\n-                    });\n-        }, entry -> {\n-            entry.getKey().close();\n-            RocksIterator rocksIterator = entry.getValue();\n-            if (rocksIterator != null) {\n-                rocksIterator.close();\n-            }\n-        });\n-\n-        Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n-            Object bucketKey = unmarshall(expiredBytes);\n-            if (bucketKey instanceof ExpiryBucket) {\n-                return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n-                        .flatMapMaybe(marshalledKey -> {\n-                            ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n-                            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n-                            return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n-                        });\n-            } else {\n-                // The bucketKey is an actual key\n-                ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n-                MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n-                return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n-            }\n-        });\n-\n-        if (trace) {\n-            // Note this tracing only works properly for one subscriber\n-            FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n-            expiredEntryFlowable = expiredEntryFlowable\n-                    .doOnEach(mirrorEntries)\n-                    .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n-            mirrorEntries.count()\n-                    .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n-        }\n-\n-        return expiredEntryFlowable;\n-    }\n-\n-    private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n-            long now) throws RocksDBException {\n-        byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n-        if (valueBytes == null) {\n-            return null;\n-        }\n-        MarshalledValue mv = unmarshall(valueBytes);\n-        if (mv != null) {\n-            // TODO race condition: the entry could be updated between the get and delete!\n-            Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n-                db.delete(columnFamilyHandle, marshalledKey);\n-                return mv;\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n             }\n-        }\n-        return null;\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> addSegments(IntSet segments) {\n-        return handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> removeSegments(IntSet segments) {\n-        return handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) {\n-        try {\n-            return marshaller.objectToByteBuffer(entry);\n-        } catch (IOException e) {\n-            throw new PersistenceException(e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private <E> E unmarshall(byte[] bytes) {\n-        if (bytes == null)\n-            return null;\n-\n-        try {\n-            //noinspection unchecked\n-            return (E) marshaller.objectFromByteBuffer(bytes);\n-        } catch (IOException | ClassNotFoundException e) {\n+         } catch (RocksDBException e) {\n             throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n-        MarshalledValue value = unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n-                value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-        putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final long now;\n-\n-        RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            while (entry == null && it.isValid()) {\n-                K key = unmarshall(it.key());\n-                if (filter == null || filter.test(key)) {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n-                    if (me != null && !me.isExpired(now)) {\n-                        entry = me;\n-                    }\n-                }\n-                it.next();\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+         // We check expiration based on time of subscription only\n+         long now = timeService.wallClockTime();\n+         return actualPurgeExpired(now)\n+               // We return a buffer of expired entries emitted to the non blocking thread\n+               // This prevents waking up the non blocking thread for every entry as they will most likely be\n+               // consumed much faster than emission (since each emission performs a get and remove)\n+               .buffer(16);\n+      }));\n+\n+      return Flowable.fromPublisher(purgedBatches)\n+            .concatMap(Flowable::fromIterable);\n+   }\n+\n+   private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+      // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+      // given entries\n+      Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+      }, entry -> {\n+         if (entry.getValue() == null) {\n+            return Flowable.empty();\n+         }\n+         RocksIterator iterator = entry.getValue();\n+         iterator.seekToFirst();\n+\n+         return Flowable.fromIterable(() ->\n+               new AbstractIterator<byte[]>() {\n+                  @Override\n+                  protected byte[] getNext() {\n+                     if (!iterator.isValid()) {\n+                        return null;\n+                     }\n+                     byte[] keyBytes = iterator.key();\n+                     Long time = unmarshall(keyBytes);\n+                     if (time > now)\n+                        return null;\n+                     try {\n+                        expiredDb.delete(keyBytes);\n+                     } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n+                     }\n+                     byte[] value = iterator.value();\n+                     iterator.next();\n+                     return value;\n+                  }\n+               });\n+      }, entry -> {\n+         entry.getKey().close();\n+         RocksIterator rocksIterator = entry.getValue();\n+         if (rocksIterator != null) {\n+            rocksIterator.close();\n+         }\n+      });\n+\n+      Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n+         Object bucketKey = unmarshall(expiredBytes);\n+         if (bucketKey instanceof ExpiryBucket) {\n+            return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n+                  .flatMapMaybe(marshalledKey -> {\n+                     ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n+                     MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n+                     return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n+                  });\n+         } else {\n+            // The bucketKey is an actual key\n+            ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n+            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n+            return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n+         }\n+      });\n+\n+      if (trace) {\n+         // Note this tracing only works properly for one subscriber\n+         FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n+         expiredEntryFlowable = expiredEntryFlowable\n+               .doOnEach(mirrorEntries)\n+               .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n+         mirrorEntries.count()\n+               .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n+      }\n+\n+      return expiredEntryFlowable;\n+   }\n+\n+   private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n+         long now) throws RocksDBException {\n+      byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n+      if (valueBytes == null) {\n+         return null;\n+      }\n+      MarshalledValue mv = unmarshall(valueBytes);\n+      if (mv != null) {\n+         // TODO race condition: the entry could be updated between the get and delete!\n+         Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+         if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+            // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+            db.delete(columnFamilyHandle, marshalledKey);\n+            return mv;\n+         }\n+      }\n+      return null;\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   private byte[] marshall(Object entry) {\n+      try {\n+         return marshaller.objectToByteBuffer(entry);\n+      } catch (IOException e) {\n+         throw new PersistenceException(e);\n+      } catch (InterruptedException e) {\n+         Thread.currentThread().interrupt();\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private <E> E unmarshall(byte[] bytes) {\n+      if (bytes == null)\n+         return null;\n+\n+      try {\n+         //noinspection unchecked\n+         return (E) marshaller.objectFromByteBuffer(bytes);\n+      } catch (IOException | ClassNotFoundException e) {\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n+      MarshalledValue value = unmarshall(valueBytes);\n+      if (value == null) return null;\n+\n+      return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n+            value.getCreated(), value.getLastUsed());\n+   }\n+\n+   private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n+      long expiry = entry.expiryTime();\n+      long maxIdle = entry.getMetadata().maxIdle();\n+      if (maxIdle > 0) {\n+         // Coding getExpiryTime() for transient entries has the risk of being a moving target\n+         // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n+         expiry = maxIdle + ctx.getTimeService().wallClockTime();\n+      }\n+      byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n+      putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n+   }\n+\n+   @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n+   static final class ExpiryBucket {\n+      @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n+      List<byte[]> entries;\n+\n+      ExpiryBucket(){}\n+\n+      ExpiryBucket(byte[] existingKey, byte[] newKey) {\n+         entries = new ArrayList<>(2);\n+         entries.add(existingKey);\n+         entries.add(newKey);\n+      }\n+   }\n+\n+   private static final class ExpiryEntry {\n+\n+      final long expiry;\n+      final byte[] keyBytes;\n+\n+      ExpiryEntry(long expiry, byte[] keyBytes) {\n+         this.expiry = expiry;\n+         this.keyBytes = keyBytes;\n+      }\n+\n+      @Override\n+      public boolean equals(Object o) {\n+         if (this == o) return true;\n+         if (o == null || getClass() != o.getClass()) return false;\n+         ExpiryEntry that = (ExpiryEntry) o;\n+         return expiry == that.expiry &&\n+               Arrays.equals(keyBytes, that.keyBytes);\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+         int result = Objects.hash(expiry);\n+         result = 31 * result + Arrays.hashCode(keyBytes);\n+         return result;\n+      }\n+   }\n+\n+   private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n+      private final RocksIterator it;\n+      private final Predicate<? super K> filter;\n+      private final long now;\n+\n+      RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n+         this.it = it;\n+         this.filter = filter;\n+         this.now = now;\n+      }\n+\n+      @Override\n+      protected MarshallableEntry<K, V> getNext() {\n+         MarshallableEntry<K, V> entry = null;\n+         while (entry == null && it.isValid()) {\n+            K key = unmarshall(it.key());\n+            if (filter == null || filter.test(key)) {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n+               if (me != null && !me.isExpired(now)) {\n+                  entry = me;\n+               }\n             }\n-            return entry;\n-        }\n-    }\n+            it.next();\n+         }\n+         return entry;\n+      }\n+   }\n \n-    private abstract class RocksDBHandler {\n+   private abstract class RocksDBHandler {\n \n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n \n-        abstract void close();\n+      abstract void close();\n \n-        abstract ColumnFamilyHandle getHandle(int segment);\n+      abstract ColumnFamilyHandle getHandle(int segment);\n \n-        abstract ColumnFamilyHandle getHandle(Object key);\n+      abstract ColumnFamilyHandle getHandle(Object key);\n \n-        abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n+      abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n \n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n-            } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n             }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        return db.get(handle, marshall(key));\n-                    } catch (RocksDBException e) {\n-                        throw new CompletionException(e);\n-                    }\n-                }, \"rocksdb-load\");\n-                return entryByteStage.thenApply(entryBytes -> {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n-                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                        return null;\n-                    }\n-                    return me;\n-                });\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n             ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                return blockingManager.runBlocking(() -> {\n-                    try {\n-                        db.put(handle, marshalledKey, marshalledValue);\n-                        if (me.expiryTime() > -1) {\n-                            addNewExpiry(me);\n-                        }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-write\");\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      abstract CompletionStage<Void> clear();\n \n-        CompletionStage<Boolean> delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                return blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        if (db.get(handle, keyBytes) == null) {\n-                            return Boolean.FALSE;\n-                        }\n-                        db.delete(handle, keyBytes);\n-                        return Boolean.TRUE;\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-delete\");\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract CompletionStage<Void> clear();\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                boolean fetchValue);\n-\n-        CompletionStage<Long> size(IntSet segments) {\n-            return Flowable.fromPublisher(publishKeys(segments, null))\n-                    .count().toCompletionStage();\n-        }\n-\n-        abstract CompletionStage<Long> approximateSize(IntSet segments);\n-\n-        <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-            }));\n-        }\n-\n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n-\n-        abstract CompletionStage<Void> addSegments(IntSet segments);\n-\n-        abstract CompletionStage<Void> removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                    Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                    handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> clear() {\n-            return clear(null);\n-        }\n-\n-        CompletionStage<Void> clear(IntSet segments) {\n-            return blockingManager.runBlocking(() -> {\n-                long count = 0;\n-                boolean destroyDatabase = false;\n-                try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                    RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n-                    if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n-                        try (RocksIterator it = optionalIterator) {\n-                            for (it.seekToFirst(); it.isValid(); it.next()) {\n-                                byte[] keyBytes = it.key();\n-                                if (segments != null) {\n-                                    Object key = unmarshall(keyBytes);\n-                                    int segment = keyPartitioner.getSegment(key);\n-                                    if (segments.contains(segment)) {\n-                                        db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    }\n-                                } else {\n-                                    db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    count++;\n-\n-                                    if (count > configuration.clearThreshold()) {\n-                                        destroyDatabase = true;\n-                                        break;\n-                                    }\n-                                }\n-                            }\n-                        } catch (RocksDBException e) {\n-                            if (segments != null) {\n-                                // Have to propagate error to user\n-                                throw e;\n-                            }\n-                            // If was error and no segment specific just delete entire thing\n-                            destroyDatabase = true;\n-                        }\n-                    } else {\n-                        destroyDatabase = true;\n-                    }\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                }\n-\n-                if (destroyDatabase) {\n-                    try {\n-                        reinitAllDatabases();\n-                    } catch (Exception e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        @Override\n-        void close() {\n-            defaultColumnFamilyHandle.close();\n-\n-            db.close();\n-        }\n-\n-        protected void reinitAllDatabases() throws RocksDBException {\n-            db.close();\n-            expiredDb.close();\n-            if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n-                // Force a GC to ensure that open file handles are released in Windows.\n-                System.gc();\n-            }\n-            Path dataLocation = getLocation();\n-            Util.recursiveFileRemove(dataLocation.toFile());\n-            db = open(getLocation(), dataDbOptions());\n-\n-            Path expirationLocation = getExpirationLocation();\n-            Util.recursiveFileRemove(expirationLocation.toFile());\n-            expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n-        }\n-\n-        protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            // Some Cache Store tests use clear and in case of the Rocks DB implementation\n-            // this clears out internal references and results in throwing exceptions\n-            // when getting an iterator. Unfortunately there is no nice way to check that...\n-            return db.newIterator(defaultColumnFamilyHandle, readOptions);\n-        }\n-\n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n-            return publish(-1, it -> Flowable.fromIterable(() -> {\n-                // Make sure this is taken when the iterator is created\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, combinedFilter, now);\n-            }));\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return size(segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            // Do nothing\n-            return CompletableFutures.completedNull();\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n-            // segment check on every entry\n-            return clear(segments);\n-        }\n-    }\n-\n-    private class SegmentedRocksDBHandler extends RocksDBHandler {\n-        private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n-\n-        private SegmentedRocksDBHandler(int segmentCount) {\n-            this.handles = new AtomicReferenceArray<>(segmentCount);\n-        }\n-\n-        byte[] byteArrayFromInt(int val) {\n-            return new byte[] {\n-                  (byte) (val >>> 24),\n-                  (byte) (val >>> 16),\n-                  (byte) (val >>> 8),\n-                  (byte) (val)\n-            };\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return handles.get(segment);\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return handles.get(keyPartitioner.getSegment(key));\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return getHandle(unmarshall(marshalledKey));\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            int segmentCount = handles.length();\n-            List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n-            List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n-            // You have to open the default column family\n-            descriptors.add(new ColumnFamilyDescriptor(\n-                  RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n-            for (int i = 0; i < segmentCount; ++i) {\n-                descriptors.add(newDescriptor(byteArrayFromInt(i)));\n-            }\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n-            for (int i = 0; i < segmentCount; ++i) {\n-                handles.set(i, outHandles.get(i + 1));\n-            }\n-            return rocksDB;\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        @Override\n-        CompletionStage<Void> clear() {\n-            return blockingManager.runBlocking(() -> {\n-                for (int i = 0; i < handles.length(); ++i) {\n-                    if (!clearForSegment(i)) {\n-                        recreateColumnFamily(i);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        /**\n-         * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n-         * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n-         * the entries it will instead return true\n-         * @param segment the segment to clear out\n-         * @return whether it was able to clear all entries for the segment\n-         */\n-        private boolean clearForSegment(int segment) {\n-            int clearThreshold = configuration.clearThreshold();\n-            // If we always have to recreate don't even create iterator\n-            if (clearThreshold <= 0) {\n-                return false;\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n+\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n+\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.empty();\n             }\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      abstract CompletionStage<Void> addSegments(IntSet segments);\n+\n+      abstract CompletionStage<Void> removeSegments(IntSet segments);\n+   }\n+\n+   private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n+      private final KeyPartitioner keyPartitioner;\n+\n+      private ColumnFamilyHandle defaultColumnFamilyHandle;\n+\n+      private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n+         this.keyPartitioner = keyPartitioner;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(),\n+               Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n+               handles);\n+         defaultColumnFamilyHandle = handles.get(0);\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return clear(null);\n+      }\n+\n+      CompletionStage<Void> clear(IntSet segments) {\n+         return blockingManager.runBlocking(() -> {\n+            long count = 0;\n+            boolean destroyDatabase = false;\n             try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n-                if (optionalIterator != null) {\n-                    ColumnFamilyHandle handle = handles.get(segment);\n-                    try (RocksIterator it = optionalIterator) {\n-                        long count = 0;\n-                        for (it.seekToFirst(); it.isValid(); it.next()) {\n-                            byte[] keyBytes = it.key();\n-                            db.delete(handle, keyBytes);\n-\n-                            if (++count > configuration.clearThreshold()) {\n-                                return false;\n-                            }\n+               RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n+               if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n+                  try (RocksIterator it = optionalIterator) {\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        byte[] keyBytes = it.key();\n+                        if (segments != null) {\n+                           Object key = unmarshall(keyBytes);\n+                           int segment = keyPartitioner.getSegment(key);\n+                           if (segments.contains(segment)) {\n+                              db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           }\n+                        } else {\n+                           db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           count++;\n+\n+                           if (count > configuration.clearThreshold()) {\n+                              destroyDatabase = true;\n+                              break;\n+                           }\n                         }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                    return true;\n-                } else {\n-                    // If optional iterator was null that means either we don't own this segment or it was just\n-                    // recrated - in either case we can consider that cleared\n-                    return true;\n-                }\n+                     }\n+                  } catch (RocksDBException e) {\n+                     if (segments != null) {\n+                        // Have to propagate error to user\n+                        throw e;\n+                     }\n+                     // If was error and no segment specific just delete entire thing\n+                     destroyDatabase = true;\n+                  }\n+               } else {\n+                  destroyDatabase = true;\n+               }\n             } catch (Exception e) {\n-                throw new PersistenceException(e);\n+               throw new PersistenceException(e);\n             }\n-        }\n \n-        @Override\n-        void close() {\n+            if (destroyDatabase) {\n+               try {\n+                  reinitAllDatabases();\n+               } catch (Exception e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      @Override\n+      void close() {\n+         defaultColumnFamilyHandle.close();\n+\n+         db.close();\n+      }\n+\n+      protected void reinitAllDatabases() throws RocksDBException {\n+         db.close();\n+         expiredDb.close();\n+         if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+            // Force a GC to ensure that open file handles are released in Windows.\n+            System.gc();\n+         }\n+         Path dataLocation = getLocation();\n+         Util.recursiveFileRemove(dataLocation.toFile());\n+         db = open(getLocation(), dataDbOptions());\n+\n+         Path expirationLocation = getExpirationLocation();\n+         Util.recursiveFileRemove(expirationLocation.toFile());\n+         expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n+      }\n+\n+      protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         // Some Cache Store tests use clear and in case of the Rocks DB implementation\n+         // this clears out internal references and results in throwing exceptions\n+         // when getting an iterator. Unfortunately there is no nice way to check that...\n+         return db.newIterator(defaultColumnFamilyHandle, readOptions);\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n+         return publish(-1, it -> Flowable.fromIterable(() -> {\n+            // Make sure this is taken when the iterator is created\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, combinedFilter, now);\n+         }));\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return size(segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         // Do nothing\n+         return CompletableFutures.completedNull();\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n+         // segment check on every entry\n+         return clear(segments);\n+      }\n+   }\n+\n+   private class SegmentedRocksDBHandler extends RocksDBHandler {\n+      private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n+\n+      private SegmentedRocksDBHandler(int segmentCount) {\n+         this.handles = new AtomicReferenceArray<>(segmentCount);\n+      }\n+\n+      byte[] byteArrayFromInt(int val) {\n+         return new byte[] {\n+               (byte) (val >>> 24),\n+               (byte) (val >>> 16),\n+               (byte) (val >>> 8),\n+               (byte) (val)\n+         };\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return handles.get(segment);\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return handles.get(keyPartitioner.getSegment(key));\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return getHandle(unmarshall(marshalledKey));\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         int segmentCount = handles.length();\n+         List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n+         List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n+         // You have to open the default column family\n+         descriptors.add(new ColumnFamilyDescriptor(\n+               RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n+         for (int i = 0; i < segmentCount; ++i) {\n+            descriptors.add(newDescriptor(byteArrayFromInt(i)));\n+         }\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n+         for (int i = 0; i < segmentCount; ++i) {\n+            handles.set(i, outHandles.get(i + 1));\n+         }\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return blockingManager.runBlocking(() -> {\n             for (int i = 0; i < handles.length(); ++i) {\n-                ColumnFamilyHandle handle = handles.getAndSet(i, null);\n-                if (handle != null) {\n-                    handle.close();\n-                }\n+               if (!clearForSegment(i)) {\n+                  recreateColumnFamily(i);\n+               }\n             }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      /**\n+       * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n+       * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n+       * the entries it will instead return true\n+       * @param segment the segment to clear out\n+       * @return whether it was able to clear all entries for the segment\n+       */\n+      private boolean clearForSegment(int segment) {\n+         int clearThreshold = configuration.clearThreshold();\n+         // If we always have to recreate don't even create iterator\n+         if (clearThreshold <= 0) {\n+            return false;\n+         }\n+         try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+            RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n+            if (optionalIterator != null) {\n+               ColumnFamilyHandle handle = handles.get(segment);\n+               try (RocksIterator it = optionalIterator) {\n+                  long count = 0;\n+                  for (it.seekToFirst(); it.isValid(); it.next()) {\n+                     byte[] keyBytes = it.key();\n+                     db.delete(handle, keyBytes);\n+\n+                     if (++count > configuration.clearThreshold()) {\n+                        return false;\n+                     }\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+               return true;\n+            } else {\n+               // If optional iterator was null that means either we don't own this segment or it was just\n+               // recrated - in either case we can consider that cleared\n+               return true;\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            db.close();\n-        }\n-\n-        private void recreateColumnFamily(int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n+      @Override\n+      void close() {\n+         for (int i = 0; i < handles.length(); ++i) {\n+            ColumnFamilyHandle handle = handles.getAndSet(i, null);\n             if (handle != null) {\n-                try {\n-                    db.dropColumnFamily(handle);\n-                    handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n+               handle.close();\n             }\n-        }\n+         }\n \n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, filter, now);\n-            });\n-            return handleIteratorFunction(function, segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                try {\n-                    return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }), \"rocksdb-approximateSize\");\n-        }\n-\n-        <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n-            // Short circuit if only a single segment - assumed to be invoked from persistence thread\n-            if (segments != null && segments.size() == 1) {\n-                return publish(segments.iterator().nextInt(), function);\n+         db.close();\n+      }\n+\n+      private void recreateColumnFamily(int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            try {\n+               db.dropColumnFamily(handle);\n+               handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n-            return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n-                    .concatMap(RxJavaInterop.identityFunction());\n-        }\n-\n-        @Override\n-        RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n-            if (handle != null) {\n-                return db.newIterator(handle, readOptions);\n+         }\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, filter, now);\n+         });\n+         return handleIteratorFunction(function, segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            try {\n+               return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            return null;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n-                    .filter(segment -> handles.get(segment) == null);\n-\n-            return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n-                if (trace) {\n-                    log.tracef(\"Creating column family for segment %d\", segment);\n-                }\n-                byte[] cfName = byteArrayFromInt(segment);\n-                try {\n-                    ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }, \"testng-addSegments\");\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n-                    .map(segment -> {\n-                        ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n-                        return cf != null ? cf : this;\n-                    }).ofType(ColumnFamilyHandle.class);\n-\n-            return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n-                if (trace) {\n-                    log.tracef(\"Dropping column family %s\", handle);\n-                }\n-                try {\n-                    db.dropColumnFamily(handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-                handle.close();\n-            }, \"testng-removeSegments\");\n-        }\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n+         }), \"rocksdb-approximateSize\");\n+      }\n+\n+      <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n+         // Short circuit if only a single segment - assumed to be invoked from persistence thread\n+         if (segments != null && segments.size() == 1) {\n+            return publish(segments.iterator().nextInt(), function);\n+         }\n+         IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n+         return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n+               .concatMap(RxJavaInterop.identityFunction());\n+      }\n+\n+      @Override\n+      RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            return db.newIterator(handle, readOptions);\n+         }\n+         return null;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n+               .filter(segment -> handles.get(segment) == null);\n+\n+         return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n+            if (trace) {\n+               log.tracef(\"Creating column family for segment %d\", segment);\n+            }\n+            byte[] cfName = byteArrayFromInt(segment);\n+            try {\n+               ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n+            }\n+         }, \"testng-addSegments\");\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n+               .map(segment -> {\n+                  ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n+                  return cf != null ? cf : this;\n+               }).ofType(ColumnFamilyHandle.class);\n+\n+         return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n+            if (trace) {\n+               log.tracef(\"Dropping column family %s\", handle);\n+            }\n+            try {\n+               db.dropColumnFamily(handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n+            handle.close();\n+         }, \"testng-removeSegments\");\n+      }\n+   }\n+\n+   private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n+      final byte[] expiryBytes = marshall(entry.expiry);\n+      final byte[] existingBytes = expiredDb.get(expiryBytes);\n+\n+      if (existingBytes != null) {\n+         // in the case of collision make the value a List ...\n+         final Object existing = unmarshall(existingBytes);\n+         if (existing instanceof ExpiryBucket) {\n+            ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(existing));\n+         } else {\n+            ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(bucket));\n+         }\n+      } else {\n+         expiredDb.put(expiryBytes, entry.keyBytes);\n+      }\n+   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgxNTMwMQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435815301", "bodyText": "Nitpick, but IMO it's better to return early if it's not valid to reduce nesting.\n    if (!iterator.isValid())\n        return null;", "author": "ryanemerson", "createdAt": "2020-06-05T09:52:48Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -187,271 +190,268 @@ protected RocksDB openDatabase(Path location, Options options) throws RocksDBExc\n     }\n \n     @Override\n-    public void stop() {\n-        try {\n-            semaphore.acquire(Integer.MAX_VALUE);\n-        } catch (InterruptedException e) {\n-            throw new PersistenceException(\"Cannot acquire semaphore\", e);\n-        }\n-        try {\n+    public CompletionStage<Void> stop() {\n+        return blockingManager.runBlocking(() -> {\n             handler.close();\n             expiredDb.close();\n-        } finally {\n-            stopped = true;\n-            semaphore.release(Integer.MAX_VALUE);\n-        }\n-    }\n-\n-    @Override\n-    public void destroy() {\n-        stop();\n-        Util.recursiveFileRemove(getLocation().toFile());\n-        Util.recursiveFileRemove(getExpirationLocation().toFile());\n+        }, \"rocksdb-stop\");\n     }\n \n     @Override\n-    public boolean isAvailable() {\n-        return getLocation().toFile().exists() && getExpirationLocation().toFile().exists();\n+    public Set<Characteristic> characteristics() {\n+        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n     }\n \n     @Override\n-    public void clear() {\n-        handler.clear(null);\n+    public CompletionStage<Boolean> isAvailable() {\n+        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+                \"rocksdb-available\");\n     }\n \n     @Override\n-    public void clear(IntSet segments) {\n-        handler.clear(segments);\n+    public CompletionStage<Void> clear() {\n+        return handler.clear();\n     }\n \n     @Override\n-    public int size() {\n-        return handler.size(null);\n-    }\n-\n-    @Override\n-    public int size(IntSet segments) {\n+    public CompletionStage<Long> size(IntSet segments) {\n         return handler.size(segments);\n     }\n \n     @Override\n-    public boolean contains(Object key) {\n-        return handler.contains(-1, key);\n+    public CompletionStage<Long> approximateSize(IntSet segments) {\n+        return handler.approximateSize(segments);\n     }\n \n     @Override\n-    public boolean contains(int segment, Object key) {\n-        return handler.contains(segment, key);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(Predicate<? super K> filter) {\n-        return handler.publishKeys(null, filter);\n+    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+        return load(segment, key)\n+                .thenApply(Objects::nonNull);\n     }\n \n     @Override\n     public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return handler.publishKeys(segments, filter);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(Predicate<? super K> filter, boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(null, filter, fetchValue, fetchMetadata);\n+        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+                .map(MarshallableEntry::getKey);\n     }\n \n     @Override\n-    public Publisher<MarshallableEntry<K, V>> entryPublisher(IntSet segments, Predicate<? super K> filter,\n-                                                             boolean fetchValue, boolean fetchMetadata) {\n-        return handler.publishEntries(segments, filter, fetchValue, fetchMetadata);\n+    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+        return handler.publishEntries(segments, filter, includeValues);\n     }\n \n     @Override\n-    public boolean delete(Object key) {\n-        return handler.delete(-1, key);\n-    }\n-\n-    @Override\n-    public boolean delete(int segment, Object key) {\n+    public CompletionStage<Boolean> delete(int segment, Object key) {\n         return handler.delete(segment, key);\n     }\n \n     @Override\n-    public void write(MarshallableEntry entry) {\n-        handler.write(-1, entry);\n-    }\n-\n-    @Override\n-    public void write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        handler.write(segment, entry);\n+    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+        return handler.write(segment, entry);\n     }\n \n     @Override\n-    public MarshallableEntry loadEntry(Object key) {\n-        return handler.load(-1, key);\n-    }\n-\n-    @Override\n-    public MarshallableEntry<K, V> get(int segment, Object key) {\n+    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n         return handler.load(segment, key);\n     }\n \n     @Override\n-    public CompletionStage<Void> bulkUpdate(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-        return handler.writeBatch(publisher);\n+    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+        WriteBatch batch = new WriteBatch();\n+        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+        Flowable.fromPublisher(removePublisher)\n+                .subscribe(sp -> {\n+                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+                    Flowable.fromPublisher(sp)\n+                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+                });\n+        Flowable.fromPublisher(writePublisher)\n+                .subscribe(sp -> {\n+                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+                    Flowable.fromPublisher(sp)\n+                            .subscribe(me -> {\n+                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                                if (me.expiryTime() > -1) {\n+                                    expirableEntries.add(me);\n+                                }\n+                            });\n+                });\n+        if (batch.count() <= 0) {\n+            batch.close();\n+            return CompletableFutures.completedNull();\n+        }\n+        return blockingManager.runBlocking(() -> {\n+            try {\n+                db.write(dataWriteOptions(), batch);\n+                for (MarshallableEntry<K, V> me : expirableEntries) {\n+                    addNewExpiry(me);\n+                }\n+            } catch (RocksDBException e) {\n+                throw new PersistenceException(e);\n+            }\n+        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n     }\n \n     @Override\n-    public void deleteBatch(Iterable<Object> keys) {\n-        handler.deleteBatch(keys);\n+    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+            // We check expiration based on time of subscription only\n+            long now = timeService.wallClockTime();\n+            return actualPurgeExpired(now)\n+                    // We return a buffer of expired entries emitted to the non blocking thread\n+                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n+                    // consumed much faster than emission (since each emission performs a get and remove)\n+                    .buffer(16);\n+        }));\n+\n+        return Flowable.fromPublisher(purgedBatches)\n+                .concatMap(Flowable::fromIterable);\n     }\n \n-    private void putExpireDbData(ExpiryEntry entry) throws InterruptedException, RocksDBException, IOException,\n-       ClassNotFoundException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n+    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+        // given entries\n+        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+        }, entry -> {\n+            if (entry.getValue() == null) {\n+                return Flowable.empty();\n+            }\n+            RocksIterator iterator = entry.getValue();\n+            iterator.seekToFirst();\n+\n+            return Flowable.fromIterable(() ->\n+                    new AbstractIterator<byte[]>() {\n+                        @Override\n+                        protected byte[] getNext() {\n+                            if (iterator.isValid()) {", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex d6f1df4446..25ddd42963 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -75,962 +75,962 @@\n \n @ConfiguredBy(RocksDBStoreConfiguration.class)\n public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n-    private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n-    private static final boolean trace = log.isTraceEnabled();\n-\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private KeyPartitioner keyPartitioner;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private BlockingManager blockingManager;\n-\n-    @Override\n-    public CompletionStage<Void> start(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        this.blockingManager = ctx.getBlockingManager();\n-        this.keyPartitioner = ctx.getKeyPartitioner();\n-\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n             }\n-        }\n-\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db = handler.open(getLocation(), dataDbOptions());\n-                expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            } catch (Exception e) {\n-                throw new CacheConfigurationException(\"Unable to open database\", e);\n-            }\n-        }, \"rocksdb-open\");\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> stop() {\n-        return blockingManager.runBlocking(() -> {\n-            handler.close();\n-            expiredDb.close();\n-        }, \"rocksdb-stop\");\n-    }\n-\n-    @Override\n-    public Set<Characteristic> characteristics() {\n-        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> isAvailable() {\n-        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n-                \"rocksdb-available\");\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> clear() {\n-        return handler.clear();\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> approximateSize(IntSet segments) {\n-        return handler.approximateSize(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n-        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-        return load(segment, key)\n-                .thenApply(Objects::nonNull);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n-                .map(MarshallableEntry::getKey);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n-        return handler.publishEntries(segments, filter, includeValues);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        return handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n-            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n-        WriteBatch batch = new WriteBatch();\n-        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n-        Flowable.fromPublisher(removePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n-                });\n-        Flowable.fromPublisher(writePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(me -> {\n-                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n-                                if (me.expiryTime() > -1) {\n-                                    expirableEntries.add(me);\n-                                }\n-                            });\n-                });\n-        if (batch.count() <= 0) {\n-            batch.close();\n-            return CompletableFutures.completedNull();\n-        }\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db.write(dataWriteOptions(), batch);\n-                for (MarshallableEntry<K, V> me : expirableEntries) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (RocksDBException e) {\n-                throw new PersistenceException(e);\n-            }\n-        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n-        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n-            // We check expiration based on time of subscription only\n-            long now = timeService.wallClockTime();\n-            return actualPurgeExpired(now)\n-                    // We return a buffer of expired entries emitted to the non blocking thread\n-                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n-                    // consumed much faster than emission (since each emission performs a get and remove)\n-                    .buffer(16);\n-        }));\n-\n-        return Flowable.fromPublisher(purgedBatches)\n-                .concatMap(Flowable::fromIterable);\n-    }\n-\n-    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n-        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n-        // given entries\n-        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n-        }, entry -> {\n-            if (entry.getValue() == null) {\n-                return Flowable.empty();\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n             }\n-            RocksIterator iterator = entry.getValue();\n-            iterator.seekToFirst();\n-\n-            return Flowable.fromIterable(() ->\n-                    new AbstractIterator<byte[]>() {\n-                        @Override\n-                        protected byte[] getNext() {\n-                            if (iterator.isValid()) {\n-                                byte[] keyBytes = iterator.key();\n-                                Long time = unmarshall(keyBytes);\n-                                if (time > now)\n-                                    return null;\n-                                try {\n-                                    expiredDb.delete(keyBytes);\n-                                } catch (RocksDBException e) {\n-                                    throw new PersistenceException(e);\n-                                }\n-                                byte[] value = iterator.value();\n-                                iterator.next();\n-                                return value;\n-                            }\n-                            return null;\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db = handler.open(getLocation(), dataDbOptions());\n+            expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n+         } catch (Exception e) {\n+            throw new CacheConfigurationException(\"Unable to open database\", e);\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n                         }\n-                    });\n-        }, entry -> {\n-            entry.getKey().close();\n-            RocksIterator rocksIterator = entry.getValue();\n-            if (rocksIterator != null) {\n-                rocksIterator.close();\n-            }\n-        });\n-\n-        Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n-            Object bucketKey = unmarshall(expiredBytes);\n-            if (bucketKey instanceof ExpiryBucket) {\n-                return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n-                        .flatMapMaybe(marshalledKey -> {\n-                            ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n-                            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n-                            return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n-                        });\n-            } else {\n-                // The bucketKey is an actual key\n-                ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n-                MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n-                return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n-            }\n-        });\n-\n-        if (trace) {\n-            // Note this tracing only works properly for one subscriber\n-            FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n-            expiredEntryFlowable = expiredEntryFlowable\n-                    .doOnEach(mirrorEntries)\n-                    .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n-            mirrorEntries.count()\n-                    .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n-        }\n-\n-        return expiredEntryFlowable;\n-    }\n-\n-    private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n-            long now) throws RocksDBException {\n-        byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n-        if (valueBytes == null) {\n-            return null;\n-        }\n-        MarshalledValue mv = unmarshall(valueBytes);\n-        if (mv != null) {\n-            // TODO race condition: the entry could be updated between the get and delete!\n-            Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n-                db.delete(columnFamilyHandle, marshalledKey);\n-                return mv;\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n             }\n-        }\n-        return null;\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> addSegments(IntSet segments) {\n-        return handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> removeSegments(IntSet segments) {\n-        return handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) {\n-        try {\n-            return marshaller.objectToByteBuffer(entry);\n-        } catch (IOException e) {\n-            throw new PersistenceException(e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private <E> E unmarshall(byte[] bytes) {\n-        if (bytes == null)\n-            return null;\n-\n-        try {\n-            //noinspection unchecked\n-            return (E) marshaller.objectFromByteBuffer(bytes);\n-        } catch (IOException | ClassNotFoundException e) {\n+         } catch (RocksDBException e) {\n             throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n-        MarshalledValue value = unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n-                value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-        putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final long now;\n-\n-        RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            while (entry == null && it.isValid()) {\n-                K key = unmarshall(it.key());\n-                if (filter == null || filter.test(key)) {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n-                    if (me != null && !me.isExpired(now)) {\n-                        entry = me;\n-                    }\n-                }\n-                it.next();\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+         // We check expiration based on time of subscription only\n+         long now = timeService.wallClockTime();\n+         return actualPurgeExpired(now)\n+               // We return a buffer of expired entries emitted to the non blocking thread\n+               // This prevents waking up the non blocking thread for every entry as they will most likely be\n+               // consumed much faster than emission (since each emission performs a get and remove)\n+               .buffer(16);\n+      }));\n+\n+      return Flowable.fromPublisher(purgedBatches)\n+            .concatMap(Flowable::fromIterable);\n+   }\n+\n+   private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+      // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+      // given entries\n+      Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+      }, entry -> {\n+         if (entry.getValue() == null) {\n+            return Flowable.empty();\n+         }\n+         RocksIterator iterator = entry.getValue();\n+         iterator.seekToFirst();\n+\n+         return Flowable.fromIterable(() ->\n+               new AbstractIterator<byte[]>() {\n+                  @Override\n+                  protected byte[] getNext() {\n+                     if (!iterator.isValid()) {\n+                        return null;\n+                     }\n+                     byte[] keyBytes = iterator.key();\n+                     Long time = unmarshall(keyBytes);\n+                     if (time > now)\n+                        return null;\n+                     try {\n+                        expiredDb.delete(keyBytes);\n+                     } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n+                     }\n+                     byte[] value = iterator.value();\n+                     iterator.next();\n+                     return value;\n+                  }\n+               });\n+      }, entry -> {\n+         entry.getKey().close();\n+         RocksIterator rocksIterator = entry.getValue();\n+         if (rocksIterator != null) {\n+            rocksIterator.close();\n+         }\n+      });\n+\n+      Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n+         Object bucketKey = unmarshall(expiredBytes);\n+         if (bucketKey instanceof ExpiryBucket) {\n+            return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n+                  .flatMapMaybe(marshalledKey -> {\n+                     ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n+                     MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n+                     return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n+                  });\n+         } else {\n+            // The bucketKey is an actual key\n+            ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n+            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n+            return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n+         }\n+      });\n+\n+      if (trace) {\n+         // Note this tracing only works properly for one subscriber\n+         FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n+         expiredEntryFlowable = expiredEntryFlowable\n+               .doOnEach(mirrorEntries)\n+               .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n+         mirrorEntries.count()\n+               .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n+      }\n+\n+      return expiredEntryFlowable;\n+   }\n+\n+   private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n+         long now) throws RocksDBException {\n+      byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n+      if (valueBytes == null) {\n+         return null;\n+      }\n+      MarshalledValue mv = unmarshall(valueBytes);\n+      if (mv != null) {\n+         // TODO race condition: the entry could be updated between the get and delete!\n+         Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+         if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+            // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+            db.delete(columnFamilyHandle, marshalledKey);\n+            return mv;\n+         }\n+      }\n+      return null;\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   private byte[] marshall(Object entry) {\n+      try {\n+         return marshaller.objectToByteBuffer(entry);\n+      } catch (IOException e) {\n+         throw new PersistenceException(e);\n+      } catch (InterruptedException e) {\n+         Thread.currentThread().interrupt();\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private <E> E unmarshall(byte[] bytes) {\n+      if (bytes == null)\n+         return null;\n+\n+      try {\n+         //noinspection unchecked\n+         return (E) marshaller.objectFromByteBuffer(bytes);\n+      } catch (IOException | ClassNotFoundException e) {\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n+      MarshalledValue value = unmarshall(valueBytes);\n+      if (value == null) return null;\n+\n+      return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n+            value.getCreated(), value.getLastUsed());\n+   }\n+\n+   private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n+      long expiry = entry.expiryTime();\n+      long maxIdle = entry.getMetadata().maxIdle();\n+      if (maxIdle > 0) {\n+         // Coding getExpiryTime() for transient entries has the risk of being a moving target\n+         // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n+         expiry = maxIdle + ctx.getTimeService().wallClockTime();\n+      }\n+      byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n+      putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n+   }\n+\n+   @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n+   static final class ExpiryBucket {\n+      @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n+      List<byte[]> entries;\n+\n+      ExpiryBucket(){}\n+\n+      ExpiryBucket(byte[] existingKey, byte[] newKey) {\n+         entries = new ArrayList<>(2);\n+         entries.add(existingKey);\n+         entries.add(newKey);\n+      }\n+   }\n+\n+   private static final class ExpiryEntry {\n+\n+      final long expiry;\n+      final byte[] keyBytes;\n+\n+      ExpiryEntry(long expiry, byte[] keyBytes) {\n+         this.expiry = expiry;\n+         this.keyBytes = keyBytes;\n+      }\n+\n+      @Override\n+      public boolean equals(Object o) {\n+         if (this == o) return true;\n+         if (o == null || getClass() != o.getClass()) return false;\n+         ExpiryEntry that = (ExpiryEntry) o;\n+         return expiry == that.expiry &&\n+               Arrays.equals(keyBytes, that.keyBytes);\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+         int result = Objects.hash(expiry);\n+         result = 31 * result + Arrays.hashCode(keyBytes);\n+         return result;\n+      }\n+   }\n+\n+   private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n+      private final RocksIterator it;\n+      private final Predicate<? super K> filter;\n+      private final long now;\n+\n+      RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n+         this.it = it;\n+         this.filter = filter;\n+         this.now = now;\n+      }\n+\n+      @Override\n+      protected MarshallableEntry<K, V> getNext() {\n+         MarshallableEntry<K, V> entry = null;\n+         while (entry == null && it.isValid()) {\n+            K key = unmarshall(it.key());\n+            if (filter == null || filter.test(key)) {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n+               if (me != null && !me.isExpired(now)) {\n+                  entry = me;\n+               }\n             }\n-            return entry;\n-        }\n-    }\n+            it.next();\n+         }\n+         return entry;\n+      }\n+   }\n \n-    private abstract class RocksDBHandler {\n+   private abstract class RocksDBHandler {\n \n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n \n-        abstract void close();\n+      abstract void close();\n \n-        abstract ColumnFamilyHandle getHandle(int segment);\n+      abstract ColumnFamilyHandle getHandle(int segment);\n \n-        abstract ColumnFamilyHandle getHandle(Object key);\n+      abstract ColumnFamilyHandle getHandle(Object key);\n \n-        abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n+      abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n \n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n-            } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n             }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        return db.get(handle, marshall(key));\n-                    } catch (RocksDBException e) {\n-                        throw new CompletionException(e);\n-                    }\n-                }, \"rocksdb-load\");\n-                return entryByteStage.thenApply(entryBytes -> {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n-                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                        return null;\n-                    }\n-                    return me;\n-                });\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n             ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                return blockingManager.runBlocking(() -> {\n-                    try {\n-                        db.put(handle, marshalledKey, marshalledValue);\n-                        if (me.expiryTime() > -1) {\n-                            addNewExpiry(me);\n-                        }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-write\");\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      abstract CompletionStage<Void> clear();\n \n-        CompletionStage<Boolean> delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                return blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        if (db.get(handle, keyBytes) == null) {\n-                            return Boolean.FALSE;\n-                        }\n-                        db.delete(handle, keyBytes);\n-                        return Boolean.TRUE;\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-delete\");\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract CompletionStage<Void> clear();\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                boolean fetchValue);\n-\n-        CompletionStage<Long> size(IntSet segments) {\n-            return Flowable.fromPublisher(publishKeys(segments, null))\n-                    .count().toCompletionStage();\n-        }\n-\n-        abstract CompletionStage<Long> approximateSize(IntSet segments);\n-\n-        <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-            }));\n-        }\n-\n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n-\n-        abstract CompletionStage<Void> addSegments(IntSet segments);\n-\n-        abstract CompletionStage<Void> removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                    Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                    handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> clear() {\n-            return clear(null);\n-        }\n-\n-        CompletionStage<Void> clear(IntSet segments) {\n-            return blockingManager.runBlocking(() -> {\n-                long count = 0;\n-                boolean destroyDatabase = false;\n-                try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                    RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n-                    if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n-                        try (RocksIterator it = optionalIterator) {\n-                            for (it.seekToFirst(); it.isValid(); it.next()) {\n-                                byte[] keyBytes = it.key();\n-                                if (segments != null) {\n-                                    Object key = unmarshall(keyBytes);\n-                                    int segment = keyPartitioner.getSegment(key);\n-                                    if (segments.contains(segment)) {\n-                                        db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    }\n-                                } else {\n-                                    db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    count++;\n-\n-                                    if (count > configuration.clearThreshold()) {\n-                                        destroyDatabase = true;\n-                                        break;\n-                                    }\n-                                }\n-                            }\n-                        } catch (RocksDBException e) {\n-                            if (segments != null) {\n-                                // Have to propagate error to user\n-                                throw e;\n-                            }\n-                            // If was error and no segment specific just delete entire thing\n-                            destroyDatabase = true;\n-                        }\n-                    } else {\n-                        destroyDatabase = true;\n-                    }\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                }\n-\n-                if (destroyDatabase) {\n-                    try {\n-                        reinitAllDatabases();\n-                    } catch (Exception e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        @Override\n-        void close() {\n-            defaultColumnFamilyHandle.close();\n-\n-            db.close();\n-        }\n-\n-        protected void reinitAllDatabases() throws RocksDBException {\n-            db.close();\n-            expiredDb.close();\n-            if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n-                // Force a GC to ensure that open file handles are released in Windows.\n-                System.gc();\n-            }\n-            Path dataLocation = getLocation();\n-            Util.recursiveFileRemove(dataLocation.toFile());\n-            db = open(getLocation(), dataDbOptions());\n-\n-            Path expirationLocation = getExpirationLocation();\n-            Util.recursiveFileRemove(expirationLocation.toFile());\n-            expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n-        }\n-\n-        protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            // Some Cache Store tests use clear and in case of the Rocks DB implementation\n-            // this clears out internal references and results in throwing exceptions\n-            // when getting an iterator. Unfortunately there is no nice way to check that...\n-            return db.newIterator(defaultColumnFamilyHandle, readOptions);\n-        }\n-\n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n-            return publish(-1, it -> Flowable.fromIterable(() -> {\n-                // Make sure this is taken when the iterator is created\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, combinedFilter, now);\n-            }));\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return size(segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            // Do nothing\n-            return CompletableFutures.completedNull();\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n-            // segment check on every entry\n-            return clear(segments);\n-        }\n-    }\n-\n-    private class SegmentedRocksDBHandler extends RocksDBHandler {\n-        private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n-\n-        private SegmentedRocksDBHandler(int segmentCount) {\n-            this.handles = new AtomicReferenceArray<>(segmentCount);\n-        }\n-\n-        byte[] byteArrayFromInt(int val) {\n-            return new byte[] {\n-                  (byte) (val >>> 24),\n-                  (byte) (val >>> 16),\n-                  (byte) (val >>> 8),\n-                  (byte) (val)\n-            };\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return handles.get(segment);\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return handles.get(keyPartitioner.getSegment(key));\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return getHandle(unmarshall(marshalledKey));\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            int segmentCount = handles.length();\n-            List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n-            List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n-            // You have to open the default column family\n-            descriptors.add(new ColumnFamilyDescriptor(\n-                  RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n-            for (int i = 0; i < segmentCount; ++i) {\n-                descriptors.add(newDescriptor(byteArrayFromInt(i)));\n-            }\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n-            for (int i = 0; i < segmentCount; ++i) {\n-                handles.set(i, outHandles.get(i + 1));\n-            }\n-            return rocksDB;\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        @Override\n-        CompletionStage<Void> clear() {\n-            return blockingManager.runBlocking(() -> {\n-                for (int i = 0; i < handles.length(); ++i) {\n-                    if (!clearForSegment(i)) {\n-                        recreateColumnFamily(i);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        /**\n-         * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n-         * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n-         * the entries it will instead return true\n-         * @param segment the segment to clear out\n-         * @return whether it was able to clear all entries for the segment\n-         */\n-        private boolean clearForSegment(int segment) {\n-            int clearThreshold = configuration.clearThreshold();\n-            // If we always have to recreate don't even create iterator\n-            if (clearThreshold <= 0) {\n-                return false;\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n+\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n+\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.empty();\n             }\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      abstract CompletionStage<Void> addSegments(IntSet segments);\n+\n+      abstract CompletionStage<Void> removeSegments(IntSet segments);\n+   }\n+\n+   private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n+      private final KeyPartitioner keyPartitioner;\n+\n+      private ColumnFamilyHandle defaultColumnFamilyHandle;\n+\n+      private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n+         this.keyPartitioner = keyPartitioner;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(),\n+               Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n+               handles);\n+         defaultColumnFamilyHandle = handles.get(0);\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return clear(null);\n+      }\n+\n+      CompletionStage<Void> clear(IntSet segments) {\n+         return blockingManager.runBlocking(() -> {\n+            long count = 0;\n+            boolean destroyDatabase = false;\n             try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n-                if (optionalIterator != null) {\n-                    ColumnFamilyHandle handle = handles.get(segment);\n-                    try (RocksIterator it = optionalIterator) {\n-                        long count = 0;\n-                        for (it.seekToFirst(); it.isValid(); it.next()) {\n-                            byte[] keyBytes = it.key();\n-                            db.delete(handle, keyBytes);\n-\n-                            if (++count > configuration.clearThreshold()) {\n-                                return false;\n-                            }\n+               RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n+               if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n+                  try (RocksIterator it = optionalIterator) {\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        byte[] keyBytes = it.key();\n+                        if (segments != null) {\n+                           Object key = unmarshall(keyBytes);\n+                           int segment = keyPartitioner.getSegment(key);\n+                           if (segments.contains(segment)) {\n+                              db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           }\n+                        } else {\n+                           db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           count++;\n+\n+                           if (count > configuration.clearThreshold()) {\n+                              destroyDatabase = true;\n+                              break;\n+                           }\n                         }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                    return true;\n-                } else {\n-                    // If optional iterator was null that means either we don't own this segment or it was just\n-                    // recrated - in either case we can consider that cleared\n-                    return true;\n-                }\n+                     }\n+                  } catch (RocksDBException e) {\n+                     if (segments != null) {\n+                        // Have to propagate error to user\n+                        throw e;\n+                     }\n+                     // If was error and no segment specific just delete entire thing\n+                     destroyDatabase = true;\n+                  }\n+               } else {\n+                  destroyDatabase = true;\n+               }\n             } catch (Exception e) {\n-                throw new PersistenceException(e);\n+               throw new PersistenceException(e);\n             }\n-        }\n \n-        @Override\n-        void close() {\n+            if (destroyDatabase) {\n+               try {\n+                  reinitAllDatabases();\n+               } catch (Exception e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      @Override\n+      void close() {\n+         defaultColumnFamilyHandle.close();\n+\n+         db.close();\n+      }\n+\n+      protected void reinitAllDatabases() throws RocksDBException {\n+         db.close();\n+         expiredDb.close();\n+         if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+            // Force a GC to ensure that open file handles are released in Windows.\n+            System.gc();\n+         }\n+         Path dataLocation = getLocation();\n+         Util.recursiveFileRemove(dataLocation.toFile());\n+         db = open(getLocation(), dataDbOptions());\n+\n+         Path expirationLocation = getExpirationLocation();\n+         Util.recursiveFileRemove(expirationLocation.toFile());\n+         expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n+      }\n+\n+      protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         // Some Cache Store tests use clear and in case of the Rocks DB implementation\n+         // this clears out internal references and results in throwing exceptions\n+         // when getting an iterator. Unfortunately there is no nice way to check that...\n+         return db.newIterator(defaultColumnFamilyHandle, readOptions);\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n+         return publish(-1, it -> Flowable.fromIterable(() -> {\n+            // Make sure this is taken when the iterator is created\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, combinedFilter, now);\n+         }));\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return size(segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         // Do nothing\n+         return CompletableFutures.completedNull();\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n+         // segment check on every entry\n+         return clear(segments);\n+      }\n+   }\n+\n+   private class SegmentedRocksDBHandler extends RocksDBHandler {\n+      private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n+\n+      private SegmentedRocksDBHandler(int segmentCount) {\n+         this.handles = new AtomicReferenceArray<>(segmentCount);\n+      }\n+\n+      byte[] byteArrayFromInt(int val) {\n+         return new byte[] {\n+               (byte) (val >>> 24),\n+               (byte) (val >>> 16),\n+               (byte) (val >>> 8),\n+               (byte) (val)\n+         };\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return handles.get(segment);\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return handles.get(keyPartitioner.getSegment(key));\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return getHandle(unmarshall(marshalledKey));\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         int segmentCount = handles.length();\n+         List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n+         List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n+         // You have to open the default column family\n+         descriptors.add(new ColumnFamilyDescriptor(\n+               RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n+         for (int i = 0; i < segmentCount; ++i) {\n+            descriptors.add(newDescriptor(byteArrayFromInt(i)));\n+         }\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n+         for (int i = 0; i < segmentCount; ++i) {\n+            handles.set(i, outHandles.get(i + 1));\n+         }\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return blockingManager.runBlocking(() -> {\n             for (int i = 0; i < handles.length(); ++i) {\n-                ColumnFamilyHandle handle = handles.getAndSet(i, null);\n-                if (handle != null) {\n-                    handle.close();\n-                }\n+               if (!clearForSegment(i)) {\n+                  recreateColumnFamily(i);\n+               }\n             }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      /**\n+       * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n+       * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n+       * the entries it will instead return true\n+       * @param segment the segment to clear out\n+       * @return whether it was able to clear all entries for the segment\n+       */\n+      private boolean clearForSegment(int segment) {\n+         int clearThreshold = configuration.clearThreshold();\n+         // If we always have to recreate don't even create iterator\n+         if (clearThreshold <= 0) {\n+            return false;\n+         }\n+         try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+            RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n+            if (optionalIterator != null) {\n+               ColumnFamilyHandle handle = handles.get(segment);\n+               try (RocksIterator it = optionalIterator) {\n+                  long count = 0;\n+                  for (it.seekToFirst(); it.isValid(); it.next()) {\n+                     byte[] keyBytes = it.key();\n+                     db.delete(handle, keyBytes);\n+\n+                     if (++count > configuration.clearThreshold()) {\n+                        return false;\n+                     }\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+               return true;\n+            } else {\n+               // If optional iterator was null that means either we don't own this segment or it was just\n+               // recrated - in either case we can consider that cleared\n+               return true;\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            db.close();\n-        }\n-\n-        private void recreateColumnFamily(int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n+      @Override\n+      void close() {\n+         for (int i = 0; i < handles.length(); ++i) {\n+            ColumnFamilyHandle handle = handles.getAndSet(i, null);\n             if (handle != null) {\n-                try {\n-                    db.dropColumnFamily(handle);\n-                    handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n+               handle.close();\n             }\n-        }\n+         }\n \n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, filter, now);\n-            });\n-            return handleIteratorFunction(function, segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                try {\n-                    return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }), \"rocksdb-approximateSize\");\n-        }\n-\n-        <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n-            // Short circuit if only a single segment - assumed to be invoked from persistence thread\n-            if (segments != null && segments.size() == 1) {\n-                return publish(segments.iterator().nextInt(), function);\n+         db.close();\n+      }\n+\n+      private void recreateColumnFamily(int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            try {\n+               db.dropColumnFamily(handle);\n+               handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n-            return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n-                    .concatMap(RxJavaInterop.identityFunction());\n-        }\n-\n-        @Override\n-        RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n-            if (handle != null) {\n-                return db.newIterator(handle, readOptions);\n+         }\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, filter, now);\n+         });\n+         return handleIteratorFunction(function, segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            try {\n+               return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            return null;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n-                    .filter(segment -> handles.get(segment) == null);\n-\n-            return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n-                if (trace) {\n-                    log.tracef(\"Creating column family for segment %d\", segment);\n-                }\n-                byte[] cfName = byteArrayFromInt(segment);\n-                try {\n-                    ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }, \"testng-addSegments\");\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n-                    .map(segment -> {\n-                        ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n-                        return cf != null ? cf : this;\n-                    }).ofType(ColumnFamilyHandle.class);\n-\n-            return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n-                if (trace) {\n-                    log.tracef(\"Dropping column family %s\", handle);\n-                }\n-                try {\n-                    db.dropColumnFamily(handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-                handle.close();\n-            }, \"testng-removeSegments\");\n-        }\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n+         }), \"rocksdb-approximateSize\");\n+      }\n+\n+      <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n+         // Short circuit if only a single segment - assumed to be invoked from persistence thread\n+         if (segments != null && segments.size() == 1) {\n+            return publish(segments.iterator().nextInt(), function);\n+         }\n+         IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n+         return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n+               .concatMap(RxJavaInterop.identityFunction());\n+      }\n+\n+      @Override\n+      RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            return db.newIterator(handle, readOptions);\n+         }\n+         return null;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n+               .filter(segment -> handles.get(segment) == null);\n+\n+         return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n+            if (trace) {\n+               log.tracef(\"Creating column family for segment %d\", segment);\n+            }\n+            byte[] cfName = byteArrayFromInt(segment);\n+            try {\n+               ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n+            }\n+         }, \"testng-addSegments\");\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n+               .map(segment -> {\n+                  ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n+                  return cf != null ? cf : this;\n+               }).ofType(ColumnFamilyHandle.class);\n+\n+         return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n+            if (trace) {\n+               log.tracef(\"Dropping column family %s\", handle);\n+            }\n+            try {\n+               db.dropColumnFamily(handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n+            handle.close();\n+         }, \"testng-removeSegments\");\n+      }\n+   }\n+\n+   private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n+      final byte[] expiryBytes = marshall(entry.expiry);\n+      final byte[] existingBytes = expiredDb.get(expiryBytes);\n+\n+      if (existingBytes != null) {\n+         // in the case of collision make the value a List ...\n+         final Object existing = unmarshall(existingBytes);\n+         if (existing instanceof ExpiryBucket) {\n+            ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(existing));\n+         } else {\n+            ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(bucket));\n+         }\n+      } else {\n+         expiredDb.put(expiryBytes, entry.keyBytes);\n+      }\n+   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyOTg5NA==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r435829894", "bodyText": "For a segmented store there is no need to unmarshall all of the keys during the size calculation. Instead we can just return a  \"empty\" MarshalledEntry instance from RocksEntryIterator in order for count() to still be utilised.", "author": "ryanemerson", "createdAt": "2020-06-05T10:21:21Z", "path": "persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java", "diffHunk": "@@ -594,161 +549,92 @@ ColumnFamilyDescriptor newDescriptor(byte[] name) {\n                   columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n         }\n \n-        boolean contains(int segment, Object key) {\n-            // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-            return load(segment, key) != null;\n-        }\n-\n-        MarshallableEntry<K, V> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n+        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+            ColumnFamilyHandle handle = getHandle(segment);\n             if (handle == null) {\n                 log.trace(\"Ignoring load as handle is not currently configured\");\n-                return null;\n+                return CompletableFutures.completedNull();\n             }\n             try {\n-                byte[] entryBytes;\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n+                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+                    try {\n+                        return db.get(handle, marshall(key));\n+                    } catch (RocksDBException e) {\n+                        throw new CompletionException(e);\n                     }\n-\n-                    entryBytes = db.get(handle, marshall(key));\n-                } finally {\n-                    semaphore.release();\n-                }\n-                MarshallableEntry<K, V> me = valueToMarshallableEntry(key, entryBytes, true);\n-                if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                    return null;\n-                }\n-                return me;\n+                }, \"rocksdb-load\");\n+                return entryByteStage.thenApply(entryBytes -> {\n+                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                        return null;\n+                    }\n+                    return me;\n+                });\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        void write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n-            Object key = me.getKey();\n-            ColumnFamilyHandle handle = getHandle(segment, key);\n+        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+            ColumnFamilyHandle handle = getHandle(segment);\n             if (handle == null) {\n                 log.trace(\"Ignoring write as handle is not currently configured\");\n-                return;\n+                return CompletableFutures.completedNull();\n             }\n             try {\n                 byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n                 byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n+                return blockingManager.runBlocking(() -> {\n+                    try {\n+                        db.put(handle, marshalledKey, marshalledValue);\n+                        if (me.expiryTime() > -1) {\n+                            addNewExpiry(me);\n+                        }\n+                    } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n                     }\n-                    db.put(handle, marshalledKey, marshalledValue);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                if (me.expiryTime() > -1) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+                }, \"rocksdb-write\");\n \n-        boolean delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                semaphore.acquire();\n-                try {\n-                    if (stopped) {\n-                        throw new PersistenceException(\"RocksDB is stopped\");\n-                    }\n-                    if (db.get(getHandle(segment, key), keyBytes) == null) {\n-                        return false;\n-                    }\n-                    db.delete(getHandle(segment, key), keyBytes);\n-                } finally {\n-                    semaphore.release();\n-                }\n-                return true;\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        CompletionStage<Void> writeBatch(Publisher<MarshallableEntry<? extends K, ? extends V>> publisher) {\n-            return Flowable.fromPublisher(publisher)\n-                  .buffer(configuration.maxBatchSize())\n-                  .doOnNext(entries -> {\n-                      WriteBatch batch = new WriteBatch();\n-                      for (MarshallableEntry<? extends K, ? extends V> entry : entries) {\n-                          int segment = calculateSegment(entry.getKey());\n-                          byte[] keyBytes = MarshallUtil.toByteArray(entry.getKeyBytes());\n-                          batch.put(getHandle(segment), keyBytes, marshall(entry.getMarshalledValue()));\n-                      }\n-                      writeBatch(batch);\n-\n-                      // Add metadata only after batch has been written\n-                      for (MarshallableEntry entry : entries) {\n-                          if (entry.expiryTime() > -1)\n-                              addNewExpiry(entry);\n-                      }\n-                  })\n-                  .doOnError(e -> {\n-                      throw new PersistenceException(e);\n-                  })\n-                  .ignoreElements()\n-                  .toCompletionStage(null);\n-        }\n-\n-        void deleteBatch(Iterable<Object> keys) {\n+        CompletionStage<Boolean> delete(int segment, Object key) {\n             try {\n-                int batchSize = 0;\n-                WriteBatch batch = new WriteBatch();\n-                for (Object key : keys) {\n-                    batch.remove(getHandle(calculateSegment(key)), marshall(key));\n-                    batchSize++;\n-\n-                    if (batchSize == configuration.maxBatchSize()) {\n-                        batchSize = 0;\n-                        writeBatch(batch);\n-                        batch = new WriteBatch();\n+                byte[] keyBytes = marshall(key);\n+                ColumnFamilyHandle handle = getHandle(segment);\n+                return blockingManager.supplyBlocking(() -> {\n+                    try {\n+                        if (db.get(handle, keyBytes) == null) {\n+                            return Boolean.FALSE;\n+                        }\n+                        db.delete(handle, keyBytes);\n+                        return Boolean.TRUE;\n+                    } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n                     }\n-                }\n-\n-                if (batchSize != 0)\n-                    writeBatch(batch);\n+                }, \"rocksdb-delete\");\n             } catch (Exception e) {\n                 throw new PersistenceException(e);\n             }\n         }\n \n-        abstract void clear(IntSet segments);\n-\n-        abstract Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter);\n+        abstract CompletionStage<Void> clear();\n \n         abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                                                                   boolean fetchValue, boolean fetchMetadata);\n-\n-        int size(IntSet segments) {\n-            CompletionStage<Long> stage = Flowable.fromPublisher(publishKeys(segments, null))\n-                  .count().toCompletionStage();\n+                boolean fetchValue);\n \n-            long count = CompletionStages.join(stage);\n-            if (count > Integer.MAX_VALUE) {\n-                return Integer.MAX_VALUE;\n-            }\n-            return (int) count;\n+        CompletionStage<Long> size(IntSet segments) {\n+            return Flowable.fromPublisher(publishKeys(segments, null))", "originalCommit": "e8e91c3ad807c6bdba1406c83b12f4bf79650577", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0NDIzOQ==", "url": "https://github.com/infinispan/infinispan/pull/8405#discussion_r436144239", "bodyText": "Unfortunately, expiration rears its ugly head again (we need to get at the metadata for expiration) - and at that point we require unmarshalling the value - so I don't think it is worth wasting code to not unmarshall the key.\nThis is one reason I was very glad to add in the approximateSize method (which won't work for non shared non segmented stores - shared or segmented should always work).", "author": "wburns", "createdAt": "2020-06-05T20:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTgyOTg5NA=="}], "type": "inlineReview", "revised_code": {"commit": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "chunk": "diff --git a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\nindex d6f1df4446..25ddd42963 100644\n--- a/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n+++ b/persistence/rocksdb/src/main/java/org/infinispan/persistence/rocksdb/RocksDBStore.java\n\n@@ -75,962 +75,962 @@\n \n @ConfiguredBy(RocksDBStoreConfiguration.class)\n public class RocksDBStore<K, V> implements NonBlockingStore<K, V> {\n-    private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n-    private static final boolean trace = log.isTraceEnabled();\n-\n-    static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n-    static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n-\n-    protected RocksDBStoreConfiguration configuration;\n-    private RocksDB db;\n-    private RocksDB expiredDb;\n-    private InitializationContext ctx;\n-    private TimeService timeService;\n-    private WriteOptions dataWriteOptions;\n-    private RocksDBHandler handler;\n-    private Properties databaseProperties;\n-    private Properties columnFamilyProperties;\n-    private Marshaller marshaller;\n-    private KeyPartitioner keyPartitioner;\n-    private MarshallableEntryFactory<K, V> entryFactory;\n-    private BlockingManager blockingManager;\n-\n-    @Override\n-    public CompletionStage<Void> start(InitializationContext ctx) {\n-        this.configuration = ctx.getConfiguration();\n-        this.ctx = ctx;\n-        this.timeService = ctx.getTimeService();\n-        this.marshaller = ctx.getPersistenceMarshaller();\n-        this.entryFactory = ctx.getMarshallableEntryFactory();\n-        this.blockingManager = ctx.getBlockingManager();\n-        this.keyPartitioner = ctx.getKeyPartitioner();\n-\n-        ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n-\n-        AdvancedCache cache = ctx.getCache().getAdvancedCache();\n-        if (configuration.segmented()) {\n-            handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n-        } else {\n-            handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n-        }\n-\n-        // Has to be done before we open the database, so we can pass the properties\n-        Properties allProperties = configuration.properties();\n-        for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n-            String key = entry.getKey().toString();\n-            if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (databaseProperties == null) {\n-                    databaseProperties = new Properties();\n-                }\n-                databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n-            } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n-                if (columnFamilyProperties == null) {\n-                    columnFamilyProperties = new Properties();\n-                }\n-                columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+   private static final Log log = LogFactory.getLog(MethodHandles.lookup().lookupClass(), Log.class);\n+   private static final boolean trace = log.isTraceEnabled();\n+\n+   static final String DATABASE_PROPERTY_NAME_WITH_SUFFIX = \"database.\";\n+   static final String COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX = \"data.\";\n+\n+   protected RocksDBStoreConfiguration configuration;\n+   private RocksDB db;\n+   private RocksDB expiredDb;\n+   private InitializationContext ctx;\n+   private TimeService timeService;\n+   private WriteOptions dataWriteOptions;\n+   private RocksDBHandler handler;\n+   private Properties databaseProperties;\n+   private Properties columnFamilyProperties;\n+   private Marshaller marshaller;\n+   private KeyPartitioner keyPartitioner;\n+   private MarshallableEntryFactory<K, V> entryFactory;\n+   private BlockingManager blockingManager;\n+\n+   @Override\n+   public CompletionStage<Void> start(InitializationContext ctx) {\n+      this.configuration = ctx.getConfiguration();\n+      this.ctx = ctx;\n+      this.timeService = ctx.getTimeService();\n+      this.marshaller = ctx.getPersistenceMarshaller();\n+      this.entryFactory = ctx.getMarshallableEntryFactory();\n+      this.blockingManager = ctx.getBlockingManager();\n+      this.keyPartitioner = ctx.getKeyPartitioner();\n+\n+      ctx.getPersistenceMarshaller().register(new PersistenceContextInitializerImpl());\n+\n+      AdvancedCache cache = ctx.getCache().getAdvancedCache();\n+      if (configuration.segmented()) {\n+         handler = new SegmentedRocksDBHandler(cache.getCacheConfiguration().clustering().hash().numSegments());\n+      } else {\n+         handler = new NonSegmentedRocksDBHandler(keyPartitioner);\n+      }\n+\n+      // Has to be done before we open the database, so we can pass the properties\n+      Properties allProperties = configuration.properties();\n+      for (Map.Entry<Object, Object> entry : allProperties.entrySet()) {\n+         String key = entry.getKey().toString();\n+         if (key.startsWith(DATABASE_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (databaseProperties == null) {\n+               databaseProperties = new Properties();\n             }\n-        }\n-\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db = handler.open(getLocation(), dataDbOptions());\n-                expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n-            } catch (Exception e) {\n-                throw new CacheConfigurationException(\"Unable to open database\", e);\n-            }\n-        }, \"rocksdb-open\");\n-    }\n-\n-    private Path getLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n-    }\n-\n-    private Path getExpirationLocation() {\n-        return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n-    }\n-\n-    private WriteOptions dataWriteOptions() {\n-        if (dataWriteOptions == null)\n-            dataWriteOptions = new WriteOptions().setDisableWAL(false);\n-        return dataWriteOptions;\n-    }\n-\n-    protected DBOptions dataDbOptions() {\n-        DBOptions dbOptions;\n-        if (databaseProperties != null) {\n-            dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n-            if (dbOptions == null) {\n-                throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n-            }\n-        } else {\n-            dbOptions = new DBOptions();\n-        }\n-        return dbOptions\n-              .setCreateIfMissing(true)\n-              // We have to create missing column families on open.\n-              // Otherwise when we start we won't know what column families this database had if any - thus\n-              // we must specify all of them and later remove them.\n-              .setCreateMissingColumnFamilies(true);\n-    }\n-\n-    protected Options expiredDbOptions() {\n-        return new Options()\n-              .setCreateIfMissing(true)\n-              // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n-              .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n-    }\n-\n-    /**\n-     * Creates database if it doesn't exist.\n-     */\n-    protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n-        File dir = location.toFile();\n-        dir.mkdirs();\n-        return RocksDB.open(options, location.toString());\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> stop() {\n-        return blockingManager.runBlocking(() -> {\n-            handler.close();\n-            expiredDb.close();\n-        }, \"rocksdb-stop\");\n-    }\n-\n-    @Override\n-    public Set<Characteristic> characteristics() {\n-        return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> isAvailable() {\n-        return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n-                \"rocksdb-available\");\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> clear() {\n-        return handler.clear();\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> size(IntSet segments) {\n-        return handler.size(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Long> approximateSize(IntSet segments) {\n-        return handler.approximateSize(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> containsKey(int segment, Object key) {\n-        // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n-        return load(segment, key)\n-                .thenApply(Objects::nonNull);\n-    }\n-\n-    @Override\n-    public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n-        return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n-                .map(MarshallableEntry::getKey);\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n-        return handler.publishEntries(segments, filter, includeValues);\n-    }\n-\n-    @Override\n-    public CompletionStage<Boolean> delete(int segment, Object key) {\n-        return handler.delete(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n-        return handler.write(segment, entry);\n-    }\n-\n-    @Override\n-    public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-        return handler.load(segment, key);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n-            Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n-        WriteBatch batch = new WriteBatch();\n-        Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n-        Flowable.fromPublisher(removePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(removed -> batch.delete(handle, marshall(removed)));\n-                });\n-        Flowable.fromPublisher(writePublisher)\n-                .subscribe(sp -> {\n-                    ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n-                    Flowable.fromPublisher(sp)\n-                            .subscribe(me -> {\n-                                batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n-                                if (me.expiryTime() > -1) {\n-                                    expirableEntries.add(me);\n-                                }\n-                            });\n-                });\n-        if (batch.count() <= 0) {\n-            batch.close();\n-            return CompletableFutures.completedNull();\n-        }\n-        return blockingManager.runBlocking(() -> {\n-            try {\n-                db.write(dataWriteOptions(), batch);\n-                for (MarshallableEntry<K, V> me : expirableEntries) {\n-                    addNewExpiry(me);\n-                }\n-            } catch (RocksDBException e) {\n-                throw new PersistenceException(e);\n-            }\n-        }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n-    }\n-\n-    @Override\n-    public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n-        Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n-            // We check expiration based on time of subscription only\n-            long now = timeService.wallClockTime();\n-            return actualPurgeExpired(now)\n-                    // We return a buffer of expired entries emitted to the non blocking thread\n-                    // This prevents waking up the non blocking thread for every entry as they will most likely be\n-                    // consumed much faster than emission (since each emission performs a get and remove)\n-                    .buffer(16);\n-        }));\n-\n-        return Flowable.fromPublisher(purgedBatches)\n-                .concatMap(Flowable::fromIterable);\n-    }\n-\n-    private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n-        // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n-        // given entries\n-        Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n-        }, entry -> {\n-            if (entry.getValue() == null) {\n-                return Flowable.empty();\n+            databaseProperties.setProperty(key.substring(DATABASE_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         } else if (key.startsWith(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX)) {\n+            if (columnFamilyProperties == null) {\n+               columnFamilyProperties = new Properties();\n             }\n-            RocksIterator iterator = entry.getValue();\n-            iterator.seekToFirst();\n-\n-            return Flowable.fromIterable(() ->\n-                    new AbstractIterator<byte[]>() {\n-                        @Override\n-                        protected byte[] getNext() {\n-                            if (iterator.isValid()) {\n-                                byte[] keyBytes = iterator.key();\n-                                Long time = unmarshall(keyBytes);\n-                                if (time > now)\n-                                    return null;\n-                                try {\n-                                    expiredDb.delete(keyBytes);\n-                                } catch (RocksDBException e) {\n-                                    throw new PersistenceException(e);\n-                                }\n-                                byte[] value = iterator.value();\n-                                iterator.next();\n-                                return value;\n-                            }\n-                            return null;\n+            columnFamilyProperties.setProperty(key.substring(COLUMN_FAMILY_PROPERTY_NAME_WITH_SUFFIX.length()), entry.getValue().toString());\n+         }\n+      }\n+\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db = handler.open(getLocation(), dataDbOptions());\n+            expiredDb = openDatabase(getExpirationLocation(), expiredDbOptions());\n+         } catch (Exception e) {\n+            throw new CacheConfigurationException(\"Unable to open database\", e);\n+         }\n+      }, \"rocksdb-open\");\n+   }\n+\n+   private Path getLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.location(), ctx.getCache().getName(), \"data\");\n+   }\n+\n+   private Path getExpirationLocation() {\n+      return getQualifiedLocation(ctx.getGlobalConfiguration(), configuration.expiredLocation(), ctx.getCache().getName(), \"expired\");\n+   }\n+\n+   private WriteOptions dataWriteOptions() {\n+      if (dataWriteOptions == null)\n+         dataWriteOptions = new WriteOptions().setDisableWAL(false);\n+      return dataWriteOptions;\n+   }\n+\n+   protected DBOptions dataDbOptions() {\n+      DBOptions dbOptions;\n+      if (databaseProperties != null) {\n+         dbOptions = DBOptions.getDBOptionsFromProps(databaseProperties);\n+         if (dbOptions == null) {\n+            throw log.rocksDBUnknownPropertiesSupplied(databaseProperties.toString());\n+         }\n+      } else {\n+         dbOptions = new DBOptions();\n+      }\n+      return dbOptions\n+            .setCreateIfMissing(true)\n+            // We have to create missing column families on open.\n+            // Otherwise when we start we won't know what column families this database had if any - thus\n+            // we must specify all of them and later remove them.\n+            .setCreateMissingColumnFamilies(true);\n+   }\n+\n+   protected Options expiredDbOptions() {\n+      return new Options()\n+            .setCreateIfMissing(true)\n+            // Make sure keys are sorted by bytes - we use this sorting to remove entries that have expired most recently\n+            .setComparator(BuiltinComparator.BYTEWISE_COMPARATOR);\n+   }\n+\n+   /**\n+    * Creates database if it doesn't exist.\n+    */\n+   protected RocksDB openDatabase(Path location, Options options) throws RocksDBException {\n+      File dir = location.toFile();\n+      dir.mkdirs();\n+      return RocksDB.open(options, location.toString());\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> stop() {\n+      return blockingManager.runBlocking(() -> {\n+         handler.close();\n+         expiredDb.close();\n+      }, \"rocksdb-stop\");\n+   }\n+\n+   @Override\n+   public Set<Characteristic> characteristics() {\n+      return EnumSet.of(Characteristic.BULK_READ, Characteristic.EXPIRATION, Characteristic.SEGMENTABLE);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> isAvailable() {\n+      return blockingManager.supplyBlocking(() -> getLocation().toFile().exists() && getExpirationLocation().toFile().exists(),\n+            \"rocksdb-available\");\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> clear() {\n+      return handler.clear();\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> size(IntSet segments) {\n+      return handler.size(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Long> approximateSize(IntSet segments) {\n+      return handler.approximateSize(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> containsKey(int segment, Object key) {\n+      // This might be able to use RocksDB#keyMayExist - but API is a bit flaky\n+      return load(segment, key)\n+            .thenApply(Objects::nonNull);\n+   }\n+\n+   @Override\n+   public Publisher<K> publishKeys(IntSet segments, Predicate<? super K> filter) {\n+      return Flowable.fromPublisher(handler.publishEntries(segments, filter, false))\n+            .map(MarshallableEntry::getKey);\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean includeValues) {\n+      return handler.publishEntries(segments, filter, includeValues);\n+   }\n+\n+   @Override\n+   public CompletionStage<Boolean> delete(int segment, Object key) {\n+      return handler.delete(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> entry) {\n+      return handler.write(segment, entry);\n+   }\n+\n+   @Override\n+   public CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+      return handler.load(segment, key);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> batch(int publisherCount, Publisher<SegmentedPublisher<Object>> removePublisher,\n+         Publisher<SegmentedPublisher<MarshallableEntry<K, V>>> writePublisher) {\n+      WriteBatch batch = new WriteBatch();\n+      Set<MarshallableEntry<K, V>> expirableEntries = new HashSet<>();\n+      Flowable.fromPublisher(removePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(removed -> batch.delete(handle, marshall(removed)));\n+            });\n+      Flowable.fromPublisher(writePublisher)\n+            .subscribe(sp -> {\n+               ColumnFamilyHandle handle = handler.getHandle(sp.getSegment());\n+               Flowable.fromPublisher(sp)\n+                     .subscribe(me -> {\n+                        batch.put(handle, marshall(me.getKey()), marshall(me.getMarshalledValue()));\n+                        if (me.expiryTime() > -1) {\n+                           expirableEntries.add(me);\n                         }\n-                    });\n-        }, entry -> {\n-            entry.getKey().close();\n-            RocksIterator rocksIterator = entry.getValue();\n-            if (rocksIterator != null) {\n-                rocksIterator.close();\n-            }\n-        });\n-\n-        Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n-            Object bucketKey = unmarshall(expiredBytes);\n-            if (bucketKey instanceof ExpiryBucket) {\n-                return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n-                        .flatMapMaybe(marshalledKey -> {\n-                            ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n-                            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n-                            return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n-                        });\n-            } else {\n-                // The bucketKey is an actual key\n-                ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n-                MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n-                return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n-            }\n-        });\n-\n-        if (trace) {\n-            // Note this tracing only works properly for one subscriber\n-            FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n-            expiredEntryFlowable = expiredEntryFlowable\n-                    .doOnEach(mirrorEntries)\n-                    .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n-            mirrorEntries.count()\n-                    .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n-        }\n-\n-        return expiredEntryFlowable;\n-    }\n-\n-    private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n-            long now) throws RocksDBException {\n-        byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n-        if (valueBytes == null) {\n-            return null;\n-        }\n-        MarshalledValue mv = unmarshall(valueBytes);\n-        if (mv != null) {\n-            // TODO race condition: the entry could be updated between the get and delete!\n-            Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n-            if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n-                // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n-                db.delete(columnFamilyHandle, marshalledKey);\n-                return mv;\n+                     });\n+            });\n+      if (batch.count() <= 0) {\n+         batch.close();\n+         return CompletableFutures.completedNull();\n+      }\n+      return blockingManager.runBlocking(() -> {\n+         try {\n+            db.write(dataWriteOptions(), batch);\n+            for (MarshallableEntry<K, V> me : expirableEntries) {\n+               addNewExpiry(me);\n             }\n-        }\n-        return null;\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> addSegments(IntSet segments) {\n-        return handler.addSegments(segments);\n-    }\n-\n-    @Override\n-    public CompletionStage<Void> removeSegments(IntSet segments) {\n-        return handler.removeSegments(segments);\n-    }\n-\n-    private byte[] marshall(Object entry) {\n-        try {\n-            return marshaller.objectToByteBuffer(entry);\n-        } catch (IOException e) {\n-            throw new PersistenceException(e);\n-        } catch (InterruptedException e) {\n-            Thread.currentThread().interrupt();\n-            throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private <E> E unmarshall(byte[] bytes) {\n-        if (bytes == null)\n-            return null;\n-\n-        try {\n-            //noinspection unchecked\n-            return (E) marshaller.objectFromByteBuffer(bytes);\n-        } catch (IOException | ClassNotFoundException e) {\n+         } catch (RocksDBException e) {\n             throw new PersistenceException(e);\n-        }\n-    }\n-\n-    private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n-        MarshalledValue value = unmarshall(valueBytes);\n-        if (value == null) return null;\n-\n-        return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n-                value.getCreated(), value.getLastUsed());\n-    }\n-\n-    private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n-        long expiry = entry.expiryTime();\n-        long maxIdle = entry.getMetadata().maxIdle();\n-        if (maxIdle > 0) {\n-            // Coding getExpiryTime() for transient entries has the risk of being a moving target\n-            // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n-            expiry = maxIdle + ctx.getTimeService().wallClockTime();\n-        }\n-        byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n-        putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n-    }\n-\n-    @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n-    static final class ExpiryBucket {\n-        @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n-        List<byte[]> entries;\n-\n-        ExpiryBucket(){}\n-\n-        ExpiryBucket(byte[] existingKey, byte[] newKey) {\n-            entries = new ArrayList<>(2);\n-            entries.add(existingKey);\n-            entries.add(newKey);\n-        }\n-    }\n-\n-    private static final class ExpiryEntry {\n-\n-        final long expiry;\n-        final byte[] keyBytes;\n-\n-        ExpiryEntry(long expiry, byte[] keyBytes) {\n-            this.expiry = expiry;\n-            this.keyBytes = keyBytes;\n-        }\n-\n-        @Override\n-        public boolean equals(Object o) {\n-            if (this == o) return true;\n-            if (o == null || getClass() != o.getClass()) return false;\n-            ExpiryEntry that = (ExpiryEntry) o;\n-            return expiry == that.expiry &&\n-                  Arrays.equals(keyBytes, that.keyBytes);\n-        }\n-\n-        @Override\n-        public int hashCode() {\n-            int result = Objects.hash(expiry);\n-            result = 31 * result + Arrays.hashCode(keyBytes);\n-            return result;\n-        }\n-    }\n-\n-    private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n-        private final RocksIterator it;\n-        private final Predicate<? super K> filter;\n-        private final long now;\n-\n-        RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n-            this.it = it;\n-            this.filter = filter;\n-            this.now = now;\n-        }\n-\n-        @Override\n-        protected MarshallableEntry<K, V> getNext() {\n-            MarshallableEntry<K, V> entry = null;\n-            while (entry == null && it.isValid()) {\n-                K key = unmarshall(it.key());\n-                if (filter == null || filter.test(key)) {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n-                    if (me != null && !me.isExpired(now)) {\n-                        entry = me;\n-                    }\n-                }\n-                it.next();\n+         }\n+      }, \"rocksdb-batch\").whenComplete((ignore, t) -> batch.close());\n+   }\n+\n+   @Override\n+   public Publisher<MarshallableEntry<K, V>> purgeExpired() {\n+      Publisher<List<MarshallableEntry<K, V>>> purgedBatches = blockingManager.blockingPublisher(Flowable.defer(() -> {\n+         // We check expiration based on time of subscription only\n+         long now = timeService.wallClockTime();\n+         return actualPurgeExpired(now)\n+               // We return a buffer of expired entries emitted to the non blocking thread\n+               // This prevents waking up the non blocking thread for every entry as they will most likely be\n+               // consumed much faster than emission (since each emission performs a get and remove)\n+               .buffer(16);\n+      }));\n+\n+      return Flowable.fromPublisher(purgedBatches)\n+            .concatMap(Flowable::fromIterable);\n+   }\n+\n+   private Flowable<MarshallableEntry<K, V>> actualPurgeExpired(long now) {\n+      // The following flowable is responsible for emitting entries that have expired from expiredDb and removing the\n+      // given entries\n+      Flowable<byte[]> expiredFlowable = Flowable.using(() -> {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return new AbstractMap.SimpleImmutableEntry<>(readOptions, expiredDb.newIterator(readOptions));\n+      }, entry -> {\n+         if (entry.getValue() == null) {\n+            return Flowable.empty();\n+         }\n+         RocksIterator iterator = entry.getValue();\n+         iterator.seekToFirst();\n+\n+         return Flowable.fromIterable(() ->\n+               new AbstractIterator<byte[]>() {\n+                  @Override\n+                  protected byte[] getNext() {\n+                     if (!iterator.isValid()) {\n+                        return null;\n+                     }\n+                     byte[] keyBytes = iterator.key();\n+                     Long time = unmarshall(keyBytes);\n+                     if (time > now)\n+                        return null;\n+                     try {\n+                        expiredDb.delete(keyBytes);\n+                     } catch (RocksDBException e) {\n+                        throw new PersistenceException(e);\n+                     }\n+                     byte[] value = iterator.value();\n+                     iterator.next();\n+                     return value;\n+                  }\n+               });\n+      }, entry -> {\n+         entry.getKey().close();\n+         RocksIterator rocksIterator = entry.getValue();\n+         if (rocksIterator != null) {\n+            rocksIterator.close();\n+         }\n+      });\n+\n+      Flowable<MarshallableEntry<K, V>> expiredEntryFlowable = expiredFlowable.flatMap(expiredBytes -> {\n+         Object bucketKey = unmarshall(expiredBytes);\n+         if (bucketKey instanceof ExpiryBucket) {\n+            return Flowable.fromIterable(((ExpiryBucket) bucketKey).entries)\n+                  .flatMapMaybe(marshalledKey -> {\n+                     ColumnFamilyHandle columnFamilyHandle = handler.getHandleForMarshalledKey(marshalledKey);\n+                     MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshalledKey, now);\n+                     return mv == null ? Maybe.empty() : Maybe.just(entryFactory.create(unmarshall(marshalledKey), mv));\n+                  });\n+         } else {\n+            // The bucketKey is an actual key\n+            ColumnFamilyHandle columnFamilyHandle = handler.getHandle(bucketKey);\n+            MarshalledValue mv = handlePossiblyExpiredKey(columnFamilyHandle, marshall(bucketKey), now);\n+            return mv == null ? Flowable.empty() : Flowable.just(entryFactory.create(bucketKey, mv));\n+         }\n+      });\n+\n+      if (trace) {\n+         // Note this tracing only works properly for one subscriber\n+         FlowableProcessor<MarshallableEntry<K, V>> mirrorEntries = UnicastProcessor.create();\n+         expiredEntryFlowable = expiredEntryFlowable\n+               .doOnEach(mirrorEntries)\n+               .doOnSubscribe(subscription -> log.tracef(\"Purging entries from RocksDBStore\"));\n+         mirrorEntries.count()\n+               .subscribe(count -> log.tracef(\"Purged %d entries from RocksDBStore\"));\n+      }\n+\n+      return expiredEntryFlowable;\n+   }\n+\n+   private MarshalledValue handlePossiblyExpiredKey(ColumnFamilyHandle columnFamilyHandle, byte[] marshalledKey,\n+         long now) throws RocksDBException {\n+      byte[] valueBytes = db.get(columnFamilyHandle, marshalledKey);\n+      if (valueBytes == null) {\n+         return null;\n+      }\n+      MarshalledValue mv = unmarshall(valueBytes);\n+      if (mv != null) {\n+         // TODO race condition: the entry could be updated between the get and delete!\n+         Metadata metadata = unmarshall(MarshallUtil.toByteArray(mv.getMetadataBytes()));\n+         if (MarshallableEntryImpl.isExpired(metadata, now, mv.getCreated(), mv.getLastUsed())) {\n+            // somewhat inefficient to FIND then REMOVE... but required if the value is updated\n+            db.delete(columnFamilyHandle, marshalledKey);\n+            return mv;\n+         }\n+      }\n+      return null;\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> addSegments(IntSet segments) {\n+      return handler.addSegments(segments);\n+   }\n+\n+   @Override\n+   public CompletionStage<Void> removeSegments(IntSet segments) {\n+      return handler.removeSegments(segments);\n+   }\n+\n+   private byte[] marshall(Object entry) {\n+      try {\n+         return marshaller.objectToByteBuffer(entry);\n+      } catch (IOException e) {\n+         throw new PersistenceException(e);\n+      } catch (InterruptedException e) {\n+         Thread.currentThread().interrupt();\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private <E> E unmarshall(byte[] bytes) {\n+      if (bytes == null)\n+         return null;\n+\n+      try {\n+         //noinspection unchecked\n+         return (E) marshaller.objectFromByteBuffer(bytes);\n+      } catch (IOException | ClassNotFoundException e) {\n+         throw new PersistenceException(e);\n+      }\n+   }\n+\n+   private MarshallableEntry<K, V> unmarshallEntry(Object key, byte[] valueBytes) {\n+      MarshalledValue value = unmarshall(valueBytes);\n+      if (value == null) return null;\n+\n+      return entryFactory.create(key, value.getValueBytes(), value.getMetadataBytes(), value.getInternalMetadataBytes(),\n+            value.getCreated(), value.getLastUsed());\n+   }\n+\n+   private void addNewExpiry(MarshallableEntry entry) throws RocksDBException {\n+      long expiry = entry.expiryTime();\n+      long maxIdle = entry.getMetadata().maxIdle();\n+      if (maxIdle > 0) {\n+         // Coding getExpiryTime() for transient entries has the risk of being a moving target\n+         // which could lead to unexpected results, hence, InternalCacheEntry calls are required\n+         expiry = maxIdle + ctx.getTimeService().wallClockTime();\n+      }\n+      byte[] keyBytes = entry.getKeyBytes().copy().getBuf();\n+      putExpireDbData(new ExpiryEntry(expiry, keyBytes));\n+   }\n+\n+   @ProtoTypeId(ProtoStreamTypeIds.ROCKSDB_EXPIRY_BUCKET)\n+   static final class ExpiryBucket {\n+      @ProtoField(number = 1, collectionImplementation = ArrayList.class)\n+      List<byte[]> entries;\n+\n+      ExpiryBucket(){}\n+\n+      ExpiryBucket(byte[] existingKey, byte[] newKey) {\n+         entries = new ArrayList<>(2);\n+         entries.add(existingKey);\n+         entries.add(newKey);\n+      }\n+   }\n+\n+   private static final class ExpiryEntry {\n+\n+      final long expiry;\n+      final byte[] keyBytes;\n+\n+      ExpiryEntry(long expiry, byte[] keyBytes) {\n+         this.expiry = expiry;\n+         this.keyBytes = keyBytes;\n+      }\n+\n+      @Override\n+      public boolean equals(Object o) {\n+         if (this == o) return true;\n+         if (o == null || getClass() != o.getClass()) return false;\n+         ExpiryEntry that = (ExpiryEntry) o;\n+         return expiry == that.expiry &&\n+               Arrays.equals(keyBytes, that.keyBytes);\n+      }\n+\n+      @Override\n+      public int hashCode() {\n+         int result = Objects.hash(expiry);\n+         result = 31 * result + Arrays.hashCode(keyBytes);\n+         return result;\n+      }\n+   }\n+\n+   private class RocksEntryIterator extends AbstractIterator<MarshallableEntry<K, V>> {\n+      private final RocksIterator it;\n+      private final Predicate<? super K> filter;\n+      private final long now;\n+\n+      RocksEntryIterator(RocksIterator it, Predicate<? super K> filter, long now) {\n+         this.it = it;\n+         this.filter = filter;\n+         this.now = now;\n+      }\n+\n+      @Override\n+      protected MarshallableEntry<K, V> getNext() {\n+         MarshallableEntry<K, V> entry = null;\n+         while (entry == null && it.isValid()) {\n+            K key = unmarshall(it.key());\n+            if (filter == null || filter.test(key)) {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, it.value());\n+               if (me != null && !me.isExpired(now)) {\n+                  entry = me;\n+               }\n             }\n-            return entry;\n-        }\n-    }\n+            it.next();\n+         }\n+         return entry;\n+      }\n+   }\n \n-    private abstract class RocksDBHandler {\n+   private abstract class RocksDBHandler {\n \n-        abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n+      abstract RocksDB open(Path location, DBOptions options) throws RocksDBException;\n \n-        abstract void close();\n+      abstract void close();\n \n-        abstract ColumnFamilyHandle getHandle(int segment);\n+      abstract ColumnFamilyHandle getHandle(int segment);\n \n-        abstract ColumnFamilyHandle getHandle(Object key);\n+      abstract ColumnFamilyHandle getHandle(Object key);\n \n-        abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n+      abstract ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey);\n \n-        ColumnFamilyDescriptor newDescriptor(byte[] name) {\n-            ColumnFamilyOptions columnFamilyOptions;\n-            if (columnFamilyProperties != null) {\n-                columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n-                if (columnFamilyOptions == null) {\n-                    throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n-                }\n-            } else {\n-                columnFamilyOptions = new ColumnFamilyOptions();\n+      ColumnFamilyDescriptor newDescriptor(byte[] name) {\n+         ColumnFamilyOptions columnFamilyOptions;\n+         if (columnFamilyProperties != null) {\n+            columnFamilyOptions = ColumnFamilyOptions.getColumnFamilyOptionsFromProps(columnFamilyProperties);\n+            if (columnFamilyOptions == null) {\n+               throw log.rocksDBUnknownPropertiesSupplied(columnFamilyProperties.toString());\n             }\n-            return new ColumnFamilyDescriptor(name,\n-                  columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n-        }\n+         } else {\n+            columnFamilyOptions = new ColumnFamilyOptions();\n+         }\n+         return new ColumnFamilyDescriptor(name,\n+               columnFamilyOptions.setCompressionType(CompressionType.getCompressionType(configuration.compressionType().toString())));\n+      }\n+\n+      CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring load as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n+               try {\n+                  return db.get(handle, marshall(key));\n+               } catch (RocksDBException e) {\n+                  throw new CompletionException(e);\n+               }\n+            }, \"rocksdb-load\");\n+            return entryByteStage.thenApply(entryBytes -> {\n+               MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n+               if (me == null || me.isExpired(timeService.wallClockTime())) {\n+                  return null;\n+               }\n+               return me;\n+            });\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<MarshallableEntry<K, V>> load(int segment, Object key) {\n-            ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring load as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                CompletionStage<byte[]> entryByteStage = blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        return db.get(handle, marshall(key));\n-                    } catch (RocksDBException e) {\n-                        throw new CompletionException(e);\n-                    }\n-                }, \"rocksdb-load\");\n-                return entryByteStage.thenApply(entryBytes -> {\n-                    MarshallableEntry<K, V> me = unmarshallEntry(key, entryBytes);\n-                    if (me == null || me.isExpired(timeService.wallClockTime())) {\n-                        return null;\n-                    }\n-                    return me;\n-                });\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+         ColumnFamilyHandle handle = getHandle(segment);\n+         if (handle == null) {\n+            log.trace(\"Ignoring write as handle is not currently configured\");\n+            return CompletableFutures.completedNull();\n+         }\n+         try {\n+            byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n+            byte[] marshalledValue = marshall(me.getMarshalledValue());\n+            return blockingManager.runBlocking(() -> {\n+               try {\n+                  db.put(handle, marshalledKey, marshalledValue);\n+                  if (me.expiryTime() > -1) {\n+                     addNewExpiry(me);\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-write\");\n+\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-        CompletionStage<Void> write(int segment, MarshallableEntry<? extends K, ? extends V> me) {\n+      CompletionStage<Boolean> delete(int segment, Object key) {\n+         try {\n+            byte[] keyBytes = marshall(key);\n             ColumnFamilyHandle handle = getHandle(segment);\n-            if (handle == null) {\n-                log.trace(\"Ignoring write as handle is not currently configured\");\n-                return CompletableFutures.completedNull();\n-            }\n-            try {\n-                byte[] marshalledKey = MarshallUtil.toByteArray(me.getKeyBytes());\n-                byte[] marshalledValue = marshall(me.getMarshalledValue());\n-                return blockingManager.runBlocking(() -> {\n-                    try {\n-                        db.put(handle, marshalledKey, marshalledValue);\n-                        if (me.expiryTime() > -1) {\n-                            addNewExpiry(me);\n-                        }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-write\");\n+            return blockingManager.supplyBlocking(() -> {\n+               try {\n+                  if (db.get(handle, keyBytes) == null) {\n+                     return Boolean.FALSE;\n+                  }\n+                  db.delete(handle, keyBytes);\n+                  return Boolean.TRUE;\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }, \"rocksdb-delete\");\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n+      abstract CompletionStage<Void> clear();\n \n-        CompletionStage<Boolean> delete(int segment, Object key) {\n-            try {\n-                byte[] keyBytes = marshall(key);\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                return blockingManager.supplyBlocking(() -> {\n-                    try {\n-                        if (db.get(handle, keyBytes) == null) {\n-                            return Boolean.FALSE;\n-                        }\n-                        db.delete(handle, keyBytes);\n-                        return Boolean.TRUE;\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }, \"rocksdb-delete\");\n-            } catch (Exception e) {\n-                throw new PersistenceException(e);\n-            }\n-        }\n-\n-        abstract CompletionStage<Void> clear();\n-\n-        abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n-                boolean fetchValue);\n-\n-        CompletionStage<Long> size(IntSet segments) {\n-            return Flowable.fromPublisher(publishKeys(segments, null))\n-                    .count().toCompletionStage();\n-        }\n-\n-        abstract CompletionStage<Long> approximateSize(IntSet segments);\n-\n-        <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n-            ReadOptions readOptions = new ReadOptions().setFillCache(false);\n-            return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n-                if (iterator == null) {\n-                    return Flowable.empty();\n-                }\n-                iterator.seekToFirst();\n-                return function.apply(iterator);\n-            }, iterator -> {\n-                if (iterator != null) {\n-                    iterator.close();\n-                }\n-                readOptions.close();\n-            }));\n-        }\n-\n-        abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n-\n-        abstract CompletionStage<Void> addSegments(IntSet segments);\n-\n-        abstract CompletionStage<Void> removeSegments(IntSet segments);\n-    }\n-\n-    private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n-        private final KeyPartitioner keyPartitioner;\n-\n-        private ColumnFamilyHandle defaultColumnFamilyHandle;\n-\n-        private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n-            this.keyPartitioner = keyPartitioner;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return defaultColumnFamilyHandle;\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(),\n-                    Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n-                    handles);\n-            defaultColumnFamilyHandle = handles.get(0);\n-            return rocksDB;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> clear() {\n-            return clear(null);\n-        }\n-\n-        CompletionStage<Void> clear(IntSet segments) {\n-            return blockingManager.runBlocking(() -> {\n-                long count = 0;\n-                boolean destroyDatabase = false;\n-                try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                    RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n-                    if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n-                        try (RocksIterator it = optionalIterator) {\n-                            for (it.seekToFirst(); it.isValid(); it.next()) {\n-                                byte[] keyBytes = it.key();\n-                                if (segments != null) {\n-                                    Object key = unmarshall(keyBytes);\n-                                    int segment = keyPartitioner.getSegment(key);\n-                                    if (segments.contains(segment)) {\n-                                        db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    }\n-                                } else {\n-                                    db.delete(defaultColumnFamilyHandle, keyBytes);\n-                                    count++;\n-\n-                                    if (count > configuration.clearThreshold()) {\n-                                        destroyDatabase = true;\n-                                        break;\n-                                    }\n-                                }\n-                            }\n-                        } catch (RocksDBException e) {\n-                            if (segments != null) {\n-                                // Have to propagate error to user\n-                                throw e;\n-                            }\n-                            // If was error and no segment specific just delete entire thing\n-                            destroyDatabase = true;\n-                        }\n-                    } else {\n-                        destroyDatabase = true;\n-                    }\n-                } catch (Exception e) {\n-                    throw new PersistenceException(e);\n-                }\n-\n-                if (destroyDatabase) {\n-                    try {\n-                        reinitAllDatabases();\n-                    } catch (Exception e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        @Override\n-        void close() {\n-            defaultColumnFamilyHandle.close();\n-\n-            db.close();\n-        }\n-\n-        protected void reinitAllDatabases() throws RocksDBException {\n-            db.close();\n-            expiredDb.close();\n-            if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n-                // Force a GC to ensure that open file handles are released in Windows.\n-                System.gc();\n-            }\n-            Path dataLocation = getLocation();\n-            Util.recursiveFileRemove(dataLocation.toFile());\n-            db = open(getLocation(), dataDbOptions());\n-\n-            Path expirationLocation = getExpirationLocation();\n-            Util.recursiveFileRemove(expirationLocation.toFile());\n-            expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n-        }\n-\n-        protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            // Some Cache Store tests use clear and in case of the Rocks DB implementation\n-            // this clears out internal references and results in throwing exceptions\n-            // when getting an iterator. Unfortunately there is no nice way to check that...\n-            return db.newIterator(defaultColumnFamilyHandle, readOptions);\n-        }\n-\n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n-            return publish(-1, it -> Flowable.fromIterable(() -> {\n-                // Make sure this is taken when the iterator is created\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, combinedFilter, now);\n-            }));\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return size(segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            // Do nothing\n-            return CompletableFutures.completedNull();\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n-            // segment check on every entry\n-            return clear(segments);\n-        }\n-    }\n-\n-    private class SegmentedRocksDBHandler extends RocksDBHandler {\n-        private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n-\n-        private SegmentedRocksDBHandler(int segmentCount) {\n-            this.handles = new AtomicReferenceArray<>(segmentCount);\n-        }\n-\n-        byte[] byteArrayFromInt(int val) {\n-            return new byte[] {\n-                  (byte) (val >>> 24),\n-                  (byte) (val >>> 16),\n-                  (byte) (val >>> 8),\n-                  (byte) (val)\n-            };\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(int segment) {\n-            return handles.get(segment);\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandle(Object key) {\n-            return handles.get(keyPartitioner.getSegment(key));\n-        }\n-\n-        @Override\n-        ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n-            return getHandle(unmarshall(marshalledKey));\n-        }\n-\n-        @Override\n-        RocksDB open(Path location, DBOptions options) throws RocksDBException {\n-            File dir = location.toFile();\n-            dir.mkdirs();\n-            int segmentCount = handles.length();\n-            List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n-            List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n-            // You have to open the default column family\n-            descriptors.add(new ColumnFamilyDescriptor(\n-                  RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n-            for (int i = 0; i < segmentCount; ++i) {\n-                descriptors.add(newDescriptor(byteArrayFromInt(i)));\n-            }\n-            RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n-            for (int i = 0; i < segmentCount; ++i) {\n-                handles.set(i, outHandles.get(i + 1));\n-            }\n-            return rocksDB;\n-        }\n+      abstract Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter,\n+            boolean fetchValue);\n \n-        @Override\n-        CompletionStage<Void> clear() {\n-            return blockingManager.runBlocking(() -> {\n-                for (int i = 0; i < handles.length(); ++i) {\n-                    if (!clearForSegment(i)) {\n-                        recreateColumnFamily(i);\n-                    }\n-                }\n-            }, \"rocksdb-clear\");\n-        }\n-\n-        /**\n-         * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n-         * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n-         * the entries it will instead return true\n-         * @param segment the segment to clear out\n-         * @return whether it was able to clear all entries for the segment\n-         */\n-        private boolean clearForSegment(int segment) {\n-            int clearThreshold = configuration.clearThreshold();\n-            // If we always have to recreate don't even create iterator\n-            if (clearThreshold <= 0) {\n-                return false;\n+      CompletionStage<Long> size(IntSet segments) {\n+         return Flowable.fromPublisher(publishKeys(segments, null))\n+               .count().toCompletionStage();\n+      }\n+\n+      abstract CompletionStage<Long> approximateSize(IntSet segments);\n+\n+      <P> Publisher<P> publish(int segment, Function<RocksIterator, Flowable<P>> function) {\n+         ReadOptions readOptions = new ReadOptions().setFillCache(false);\n+         return blockingManager.blockingPublisher(Flowable.using(() -> wrapIterator(db, readOptions, segment), iterator -> {\n+            if (iterator == null) {\n+               return Flowable.empty();\n             }\n+            iterator.seekToFirst();\n+            return function.apply(iterator);\n+         }, iterator -> {\n+            if (iterator != null) {\n+               iterator.close();\n+            }\n+            readOptions.close();\n+         }));\n+      }\n+\n+      abstract RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment);\n+\n+      abstract CompletionStage<Void> addSegments(IntSet segments);\n+\n+      abstract CompletionStage<Void> removeSegments(IntSet segments);\n+   }\n+\n+   private final class NonSegmentedRocksDBHandler extends RocksDBHandler {\n+      private final KeyPartitioner keyPartitioner;\n+\n+      private ColumnFamilyHandle defaultColumnFamilyHandle;\n+\n+      private NonSegmentedRocksDBHandler(KeyPartitioner keyPartitioner) {\n+         this.keyPartitioner = keyPartitioner;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return defaultColumnFamilyHandle;\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         List<ColumnFamilyHandle> handles = new ArrayList<>(1);\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(),\n+               Collections.singletonList(newDescriptor(RocksDB.DEFAULT_COLUMN_FAMILY)),\n+               handles);\n+         defaultColumnFamilyHandle = handles.get(0);\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return clear(null);\n+      }\n+\n+      CompletionStage<Void> clear(IntSet segments) {\n+         return blockingManager.runBlocking(() -> {\n+            long count = 0;\n+            boolean destroyDatabase = false;\n             try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n-                RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n-                if (optionalIterator != null) {\n-                    ColumnFamilyHandle handle = handles.get(segment);\n-                    try (RocksIterator it = optionalIterator) {\n-                        long count = 0;\n-                        for (it.seekToFirst(); it.isValid(); it.next()) {\n-                            byte[] keyBytes = it.key();\n-                            db.delete(handle, keyBytes);\n-\n-                            if (++count > configuration.clearThreshold()) {\n-                                return false;\n-                            }\n+               RocksIterator optionalIterator = wrapIterator(db, readOptions, -1);\n+               if (optionalIterator != null && (configuration.clearThreshold() > 0 || segments == null)) {\n+                  try (RocksIterator it = optionalIterator) {\n+                     for (it.seekToFirst(); it.isValid(); it.next()) {\n+                        byte[] keyBytes = it.key();\n+                        if (segments != null) {\n+                           Object key = unmarshall(keyBytes);\n+                           int segment = keyPartitioner.getSegment(key);\n+                           if (segments.contains(segment)) {\n+                              db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           }\n+                        } else {\n+                           db.delete(defaultColumnFamilyHandle, keyBytes);\n+                           count++;\n+\n+                           if (count > configuration.clearThreshold()) {\n+                              destroyDatabase = true;\n+                              break;\n+                           }\n                         }\n-                    } catch (RocksDBException e) {\n-                        throw new PersistenceException(e);\n-                    }\n-                    return true;\n-                } else {\n-                    // If optional iterator was null that means either we don't own this segment or it was just\n-                    // recrated - in either case we can consider that cleared\n-                    return true;\n-                }\n+                     }\n+                  } catch (RocksDBException e) {\n+                     if (segments != null) {\n+                        // Have to propagate error to user\n+                        throw e;\n+                     }\n+                     // If was error and no segment specific just delete entire thing\n+                     destroyDatabase = true;\n+                  }\n+               } else {\n+                  destroyDatabase = true;\n+               }\n             } catch (Exception e) {\n-                throw new PersistenceException(e);\n+               throw new PersistenceException(e);\n             }\n-        }\n \n-        @Override\n-        void close() {\n+            if (destroyDatabase) {\n+               try {\n+                  reinitAllDatabases();\n+               } catch (Exception e) {\n+                  throw new PersistenceException(e);\n+               }\n+            }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      @Override\n+      void close() {\n+         defaultColumnFamilyHandle.close();\n+\n+         db.close();\n+      }\n+\n+      protected void reinitAllDatabases() throws RocksDBException {\n+         db.close();\n+         expiredDb.close();\n+         if (System.getProperty(\"os.name\").startsWith(\"Windows\")) {\n+            // Force a GC to ensure that open file handles are released in Windows.\n+            System.gc();\n+         }\n+         Path dataLocation = getLocation();\n+         Util.recursiveFileRemove(dataLocation.toFile());\n+         db = open(getLocation(), dataDbOptions());\n+\n+         Path expirationLocation = getExpirationLocation();\n+         Util.recursiveFileRemove(expirationLocation.toFile());\n+         expiredDb = openDatabase(expirationLocation, expiredDbOptions());\n+      }\n+\n+      protected RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         // Some Cache Store tests use clear and in case of the Rocks DB implementation\n+         // this clears out internal references and results in throwing exceptions\n+         // when getting an iterator. Unfortunately there is no nice way to check that...\n+         return db.newIterator(defaultColumnFamilyHandle, readOptions);\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Predicate<? super K> combinedFilter = PersistenceUtil.combinePredicate(segments, keyPartitioner, filter);\n+         return publish(-1, it -> Flowable.fromIterable(() -> {\n+            // Make sure this is taken when the iterator is created\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, combinedFilter, now);\n+         }));\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return size(segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         // Do nothing\n+         return CompletableFutures.completedNull();\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         // Unfortunately we have to clear all entries that map to each entry, which requires a full iteration and\n+         // segment check on every entry\n+         return clear(segments);\n+      }\n+   }\n+\n+   private class SegmentedRocksDBHandler extends RocksDBHandler {\n+      private final AtomicReferenceArray<ColumnFamilyHandle> handles;\n+\n+      private SegmentedRocksDBHandler(int segmentCount) {\n+         this.handles = new AtomicReferenceArray<>(segmentCount);\n+      }\n+\n+      byte[] byteArrayFromInt(int val) {\n+         return new byte[] {\n+               (byte) (val >>> 24),\n+               (byte) (val >>> 16),\n+               (byte) (val >>> 8),\n+               (byte) (val)\n+         };\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(int segment) {\n+         return handles.get(segment);\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandle(Object key) {\n+         return handles.get(keyPartitioner.getSegment(key));\n+      }\n+\n+      @Override\n+      ColumnFamilyHandle getHandleForMarshalledKey(byte[] marshalledKey) {\n+         return getHandle(unmarshall(marshalledKey));\n+      }\n+\n+      @Override\n+      RocksDB open(Path location, DBOptions options) throws RocksDBException {\n+         File dir = location.toFile();\n+         dir.mkdirs();\n+         int segmentCount = handles.length();\n+         List<ColumnFamilyDescriptor> descriptors = new ArrayList<>(segmentCount + 1);\n+         List<ColumnFamilyHandle> outHandles = new ArrayList<>(segmentCount + 1);\n+         // You have to open the default column family\n+         descriptors.add(new ColumnFamilyDescriptor(\n+               RocksDB.DEFAULT_COLUMN_FAMILY, new ColumnFamilyOptions()));\n+         for (int i = 0; i < segmentCount; ++i) {\n+            descriptors.add(newDescriptor(byteArrayFromInt(i)));\n+         }\n+         RocksDB rocksDB = RocksDB.open(options, location.toString(), descriptors, outHandles);\n+         for (int i = 0; i < segmentCount; ++i) {\n+            handles.set(i, outHandles.get(i + 1));\n+         }\n+         return rocksDB;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> clear() {\n+         return blockingManager.runBlocking(() -> {\n             for (int i = 0; i < handles.length(); ++i) {\n-                ColumnFamilyHandle handle = handles.getAndSet(i, null);\n-                if (handle != null) {\n-                    handle.close();\n-                }\n+               if (!clearForSegment(i)) {\n+                  recreateColumnFamily(i);\n+               }\n             }\n+         }, \"rocksdb-clear\");\n+      }\n+\n+      /**\n+       * Attempts to clear out the entries for a segment by using an iterator and deleting. If however an iterator\n+       * goes above the clear threshold it will immediately stop and return false. If it was able to remove all\n+       * the entries it will instead return true\n+       * @param segment the segment to clear out\n+       * @return whether it was able to clear all entries for the segment\n+       */\n+      private boolean clearForSegment(int segment) {\n+         int clearThreshold = configuration.clearThreshold();\n+         // If we always have to recreate don't even create iterator\n+         if (clearThreshold <= 0) {\n+            return false;\n+         }\n+         try (ReadOptions readOptions = new ReadOptions().setFillCache(false)) {\n+            RocksIterator optionalIterator = wrapIterator(db, readOptions, segment);\n+            if (optionalIterator != null) {\n+               ColumnFamilyHandle handle = handles.get(segment);\n+               try (RocksIterator it = optionalIterator) {\n+                  long count = 0;\n+                  for (it.seekToFirst(); it.isValid(); it.next()) {\n+                     byte[] keyBytes = it.key();\n+                     db.delete(handle, keyBytes);\n+\n+                     if (++count > configuration.clearThreshold()) {\n+                        return false;\n+                     }\n+                  }\n+               } catch (RocksDBException e) {\n+                  throw new PersistenceException(e);\n+               }\n+               return true;\n+            } else {\n+               // If optional iterator was null that means either we don't own this segment or it was just\n+               // recrated - in either case we can consider that cleared\n+               return true;\n+            }\n+         } catch (Exception e) {\n+            throw new PersistenceException(e);\n+         }\n+      }\n \n-            db.close();\n-        }\n-\n-        private void recreateColumnFamily(int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n+      @Override\n+      void close() {\n+         for (int i = 0; i < handles.length(); ++i) {\n+            ColumnFamilyHandle handle = handles.getAndSet(i, null);\n             if (handle != null) {\n-                try {\n-                    db.dropColumnFamily(handle);\n-                    handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n+               handle.close();\n             }\n-        }\n+         }\n \n-        @Override\n-        Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n-            Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n-                long now = timeService.wallClockTime();\n-                return new RocksEntryIterator(it, filter, now);\n-            });\n-            return handleIteratorFunction(function, segments);\n-        }\n-\n-        @Override\n-        CompletionStage<Long> approximateSize(IntSet segments) {\n-            return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n-                ColumnFamilyHandle handle = getHandle(segment);\n-                try {\n-                    return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }), \"rocksdb-approximateSize\");\n-        }\n-\n-        <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n-            // Short circuit if only a single segment - assumed to be invoked from persistence thread\n-            if (segments != null && segments.size() == 1) {\n-                return publish(segments.iterator().nextInt(), function);\n+         db.close();\n+      }\n+\n+      private void recreateColumnFamily(int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            try {\n+               db.dropColumnFamily(handle);\n+               handle = db.createColumnFamily(newDescriptor(byteArrayFromInt(segment)));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n-            return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n-                    .concatMap(RxJavaInterop.identityFunction());\n-        }\n-\n-        @Override\n-        RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n-            ColumnFamilyHandle handle = handles.get(segment);\n-            if (handle != null) {\n-                return db.newIterator(handle, readOptions);\n+         }\n+      }\n+\n+      @Override\n+      Publisher<MarshallableEntry<K, V>> publishEntries(IntSet segments, Predicate<? super K> filter, boolean fetchValue) {\n+         Function<RocksIterator, Flowable<MarshallableEntry<K, V>>> function = it -> Flowable.fromIterable(() -> {\n+            long now = timeService.wallClockTime();\n+            return new RocksEntryIterator(it, filter, now);\n+         });\n+         return handleIteratorFunction(function, segments);\n+      }\n+\n+      @Override\n+      CompletionStage<Long> approximateSize(IntSet segments) {\n+         return blockingManager.subscribeBlockingCollector(Flowable.fromIterable(segments), Collectors.summingLong(segment -> {\n+            ColumnFamilyHandle handle = getHandle(segment);\n+            try {\n+               return Long.parseLong(db.getProperty(handle, \"rocksdb.estimate-num-keys\"));\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-            return null;\n-        }\n-\n-        @Override\n-        CompletionStage<Void> addSegments(IntSet segments) {\n-            Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n-                    .filter(segment -> handles.get(segment) == null);\n-\n-            return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n-                if (trace) {\n-                    log.tracef(\"Creating column family for segment %d\", segment);\n-                }\n-                byte[] cfName = byteArrayFromInt(segment);\n-                try {\n-                    ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n-                    handles.set(segment, handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-            }, \"testng-addSegments\");\n-        }\n-\n-        @Override\n-        CompletionStage<Void> removeSegments(IntSet segments) {\n-            Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n-                    .map(segment -> {\n-                        ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n-                        return cf != null ? cf : this;\n-                    }).ofType(ColumnFamilyHandle.class);\n-\n-            return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n-                if (trace) {\n-                    log.tracef(\"Dropping column family %s\", handle);\n-                }\n-                try {\n-                    db.dropColumnFamily(handle);\n-                } catch (RocksDBException e) {\n-                    throw new PersistenceException(e);\n-                }\n-                handle.close();\n-            }, \"testng-removeSegments\");\n-        }\n-    }\n-\n-    private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n-        final byte[] expiryBytes = marshall(entry.expiry);\n-        final byte[] existingBytes = expiredDb.get(expiryBytes);\n-\n-        if (existingBytes != null) {\n-            // in the case of collision make the value a List ...\n-            final Object existing = unmarshall(existingBytes);\n-            if (existing instanceof ExpiryBucket) {\n-                ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(existing));\n-            } else {\n-                ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n-                expiredDb.put(expiryBytes, marshall(bucket));\n+         }), \"rocksdb-approximateSize\");\n+      }\n+\n+      <R> Publisher<R> handleIteratorFunction(Function<RocksIterator, Flowable<R>> function, IntSet segments) {\n+         // Short circuit if only a single segment - assumed to be invoked from persistence thread\n+         if (segments != null && segments.size() == 1) {\n+            return publish(segments.iterator().nextInt(), function);\n+         }\n+         IntSet segmentsToUse = segments == null ? IntSets.immutableRangeSet(handles.length()) : segments;\n+         return Flowable.fromStream(segmentsToUse.intStream().mapToObj(i -> publish(i, function)))\n+               .concatMap(RxJavaInterop.identityFunction());\n+      }\n+\n+      @Override\n+      RocksIterator wrapIterator(RocksDB db, ReadOptions readOptions, int segment) {\n+         ColumnFamilyHandle handle = handles.get(segment);\n+         if (handle != null) {\n+            return db.newIterator(handle, readOptions);\n+         }\n+         return null;\n+      }\n+\n+      @Override\n+      CompletionStage<Void> addSegments(IntSet segments) {\n+         Flowable<Integer> segmentFlowable = Flowable.fromIterable(segments)\n+               .filter(segment -> handles.get(segment) == null);\n+\n+         return blockingManager.subscribeBlockingConsumer(segmentFlowable, segment -> {\n+            if (trace) {\n+               log.tracef(\"Creating column family for segment %d\", segment);\n+            }\n+            byte[] cfName = byteArrayFromInt(segment);\n+            try {\n+               ColumnFamilyHandle handle = db.createColumnFamily(newDescriptor(cfName));\n+               handles.set(segment, handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n+            }\n+         }, \"testng-addSegments\");\n+      }\n+\n+      @Override\n+      CompletionStage<Void> removeSegments(IntSet segments) {\n+         Flowable<ColumnFamilyHandle> handleFlowable = Flowable.fromIterable(segments)\n+               .map(segment -> {\n+                  ColumnFamilyHandle cf = handles.getAndSet(segment, null);\n+                  return cf != null ? cf : this;\n+               }).ofType(ColumnFamilyHandle.class);\n+\n+         return blockingManager.subscribeBlockingConsumer(handleFlowable, handle -> {\n+            if (trace) {\n+               log.tracef(\"Dropping column family %s\", handle);\n+            }\n+            try {\n+               db.dropColumnFamily(handle);\n+            } catch (RocksDBException e) {\n+               throw new PersistenceException(e);\n             }\n-        } else {\n-            expiredDb.put(expiryBytes, entry.keyBytes);\n-        }\n-    }\n+            handle.close();\n+         }, \"testng-removeSegments\");\n+      }\n+   }\n+\n+   private void putExpireDbData(ExpiryEntry entry) throws RocksDBException {\n+      final byte[] expiryBytes = marshall(entry.expiry);\n+      final byte[] existingBytes = expiredDb.get(expiryBytes);\n+\n+      if (existingBytes != null) {\n+         // in the case of collision make the value a List ...\n+         final Object existing = unmarshall(existingBytes);\n+         if (existing instanceof ExpiryBucket) {\n+            ((ExpiryBucket) existing).entries.add(entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(existing));\n+         } else {\n+            ExpiryBucket bucket = new ExpiryBucket(existingBytes, entry.keyBytes);\n+            expiredDb.put(expiryBytes, marshall(bucket));\n+         }\n+      } else {\n+         expiredDb.put(expiryBytes, entry.keyBytes);\n+      }\n+   }\n }\n"}}, {"oid": "e8fa6bdc48daafc58e4263bc903bb422571478b4", "url": "https://github.com/infinispan/infinispan/commit/e8fa6bdc48daafc58e4263bc903bb422571478b4", "message": "fixup", "committedDate": "2020-06-05T20:21:18Z", "type": "forcePushed"}, {"oid": "bda05885fa9402829f90bb45dc8230ebe6f66057", "url": "https://github.com/infinispan/infinispan/commit/bda05885fa9402829f90bb45dc8230ebe6f66057", "message": "Iterator indentation", "committedDate": "2020-06-05T20:23:05Z", "type": "forcePushed"}, {"oid": "b49b25911980f3f11c9534e811567496fe192ea9", "url": "https://github.com/infinispan/infinispan/commit/b49b25911980f3f11c9534e811567496fe192ea9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-08T14:04:04Z", "type": "forcePushed"}, {"oid": "4f2898b3fc33c76e1ce14577dd980a7335274f68", "url": "https://github.com/infinispan/infinispan/commit/4f2898b3fc33c76e1ce14577dd980a7335274f68", "message": "ISPN-11930 Convert RocksDBStore to new Store SPI\n\n* Fix indentation of existing file (no actual code changes)", "committedDate": "2020-06-09T13:25:30Z", "type": "commit"}, {"oid": "4a22922d3209fb73a9e935dbff122953a63212a7", "url": "https://github.com/infinispan/infinispan/commit/4a22922d3209fb73a9e935dbff122953a63212a7", "message": "ISPN-11930 Convert RocksDBStore to new Store SPI", "committedDate": "2020-06-09T13:45:24Z", "type": "commit"}, {"oid": "efae2dba4fa4dfb9393e67e90706a2fc277521e9", "url": "https://github.com/infinispan/infinispan/commit/efae2dba4fa4dfb9393e67e90706a2fc277521e9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-09T13:45:24Z", "type": "commit"}, {"oid": "efae2dba4fa4dfb9393e67e90706a2fc277521e9", "url": "https://github.com/infinispan/infinispan/commit/efae2dba4fa4dfb9393e67e90706a2fc277521e9", "message": "ISPN-11931 Add blockhound to rocksdb module", "committedDate": "2020-06-09T13:45:24Z", "type": "forcePushed"}]}