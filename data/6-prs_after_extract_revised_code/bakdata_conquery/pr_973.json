{"pr_number": 973, "pr_title": "imports by column name", "pr_createdAt": "2020-01-02T15:50:30Z", "pr_url": "https://github.com/bakdata/conquery/pull/973", "timeline": [{"oid": "51b3039b65f1b69b073c51a62e7355bf17fea516", "url": "https://github.com/bakdata/conquery/commit/51b3039b65f1b69b073c51a62e7355bf17fea516", "message": "moved error processing of filter out of try-catch block in Preprocessor", "committedDate": "2020-05-12T07:03:38Z", "type": "commit"}, {"oid": "a565ea3e16c2b0805b44f5aefb50bd7115fa33ad", "url": "https://github.com/bakdata/conquery/commit/a565ea3e16c2b0805b44f5aefb50bd7115fa33ad", "message": "fix assertion handling of PreprocessedHeader to not go out of bounds and try checking as much cols as possible", "committedDate": "2020-05-13T07:28:31Z", "type": "commit"}, {"oid": "797b63a909135bf6b1e4034ead415c9d1ce78c35", "url": "https://github.com/bakdata/conquery/commit/797b63a909135bf6b1e4034ead415c9d1ce78c35", "message": "upgrade select if big", "committedDate": "2020-05-13T14:37:05Z", "type": "commit"}, {"oid": "1eb95d008f0266adefae918629d655179a49a09b", "url": "https://github.com/bakdata/conquery/commit/1eb95d008f0266adefae918629d655179a49a09b", "message": "add: still send options argument.", "committedDate": "2020-05-13T15:34:08Z", "type": "commit"}, {"oid": "ae45b18cc240bc18ee220ff0da780bb82818191f", "url": "https://github.com/bakdata/conquery/commit/ae45b18cc240bc18ee220ff0da780bb82818191f", "message": "Add better error logging", "committedDate": "2020-05-13T16:01:08Z", "type": "commit"}, {"oid": "faf6c1aff54bc019c90d034c3f9fb9a59e2656aa", "url": "https://github.com/bakdata/conquery/commit/faf6c1aff54bc019c90d034c3f9fb9a59e2656aa", "message": "set defaultReturnValue to  -1", "committedDate": "2020-05-13T16:07:14Z", "type": "commit"}, {"oid": "a4c29f6a67c18e522db0287aab5628a947df504b", "url": "https://github.com/bakdata/conquery/commit/a4c29f6a67c18e522db0287aab5628a947df504b", "message": "fixed always returning false :/", "committedDate": "2020-05-13T16:13:13Z", "type": "commit"}, {"oid": "516f0dc44111de268d0f4bbf4ebfdc18116796c4", "url": "https://github.com/bakdata/conquery/commit/516f0dc44111de268d0f4bbf4ebfdc18116796c4", "message": "Simplify TableInputDescriptor; logging and headers map", "committedDate": "2020-05-14T07:24:28Z", "type": "commit"}, {"oid": "d16d27b19c2754ee2e6b864e28781de9b604cba7", "url": "https://github.com/bakdata/conquery/commit/d16d27b19c2754ee2e6b864e28781de9b604cba7", "message": "new exception logging", "committedDate": "2020-05-14T14:30:45Z", "type": "commit"}, {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "url": "https://github.com/bakdata/conquery/commit/5c5a11a46f77cf954df5040bdf01f3a34388a20c", "message": "fix NullPointerException and remove redundant exception throws", "committedDate": "2020-05-14T14:52:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTAzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675038", "bodyText": "Dieses und das n\u00e4chste if k\u00f6nnen weg wenn  ein max-Value == Integer.MAX_VALUE dann kannst einfach Math#min drauf aufrufen. Das ist dann auch egal", "author": "thoniTUB", "createdAt": "2020-05-18T14:38:46Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\nindex bddaa6bf2..565e04143 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n\n@@ -195,74 +199,6 @@ public abstract class CDateRange implements IRange<LocalDate, CDateRange> {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n-\t/**\n-\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n-\t *\n-\t * @param other Date range to span over, may be open.\n-\t * @return A new closed span.\n-\t */\n-\tpublic CDateRange spanClosed(CDateRange other) {\n-\t\tif(other == null){\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tint min = Integer.MAX_VALUE;\n-\t\t{\n-\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmin = Math.min(min, getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmin = Math.min(min, other.getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmin = Math.min(min, getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmin = Math.min(min, other.getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (min == Integer.MAX_VALUE) {\n-\t\t\t\tmin = Integer.MIN_VALUE;\n-\t\t\t}\n-\t\t}\n-\n-\t\tint max = Integer.MIN_VALUE;\n-\t\t{\n-\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmax = Math.max(max, getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmax = Math.max(max, other.getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmax = Math.max(max, getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmax = Math.max(max, other.getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (max == Integer.MIN_VALUE) {\n-\t\t\t\tmax = Integer.MAX_VALUE;\n-\t\t\t}\n-\t\t}\n-\n-\t\tif(min == Integer.MIN_VALUE && max != Integer.MAX_VALUE){\n-\t\t\tmin = max;\n-\t\t}\n-\n-\t\tif(max == Integer.MAX_VALUE && min != Integer.MIN_VALUE){\n-\t\t\tmax = min;\n-\t\t}\n-\n-\t\treturn of(min, max);\n-\t}\n-\n \tpublic static CDateRange spanOf(CDateRange a, CDateRange b) {\n \t\tif (a == null) {\n \t\t\treturn b;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTQ5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675494", "bodyText": "Auch hier k\u00f6nnen dieses und das n\u00e4chste if  weg", "author": "thoniTUB", "createdAt": "2020-05-18T14:39:26Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (min == Integer.MAX_VALUE) {\n+\t\t\t\tmin = Integer.MIN_VALUE;\n+\t\t\t}\n+\t\t}\n+\n+\t\tint max = Integer.MIN_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\nindex bddaa6bf2..565e04143 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n\n@@ -195,74 +199,6 @@ public abstract class CDateRange implements IRange<LocalDate, CDateRange> {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n-\t/**\n-\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n-\t *\n-\t * @param other Date range to span over, may be open.\n-\t * @return A new closed span.\n-\t */\n-\tpublic CDateRange spanClosed(CDateRange other) {\n-\t\tif(other == null){\n-\t\t\treturn this;\n-\t\t}\n-\n-\t\tint min = Integer.MAX_VALUE;\n-\t\t{\n-\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmin = Math.min(min, getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmin = Math.min(min, other.getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmin = Math.min(min, getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmin = Math.min(min, other.getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (min == Integer.MAX_VALUE) {\n-\t\t\t\tmin = Integer.MIN_VALUE;\n-\t\t\t}\n-\t\t}\n-\n-\t\tint max = Integer.MIN_VALUE;\n-\t\t{\n-\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmax = Math.max(max, getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n-\t\t\t\tmax = Math.max(max, other.getMinValue());\n-\t\t\t}\n-\n-\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmax = Math.max(max, getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n-\t\t\t\tmax = Math.max(max, other.getMaxValue());\n-\t\t\t}\n-\n-\t\t\tif (max == Integer.MIN_VALUE) {\n-\t\t\t\tmax = Integer.MAX_VALUE;\n-\t\t\t}\n-\t\t}\n-\n-\t\tif(min == Integer.MIN_VALUE && max != Integer.MAX_VALUE){\n-\t\t\tmin = max;\n-\t\t}\n-\n-\t\tif(max == Integer.MAX_VALUE && min != Integer.MIN_VALUE){\n-\t\t\tmax = min;\n-\t\t}\n-\n-\t\treturn of(min, max);\n-\t}\n-\n \tpublic static CDateRange spanOf(CDateRange a, CDateRange b) {\n \t\tif (a == null) {\n \t\t\treturn b;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODY4Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426698683", "bodyText": "Error hat mich etwas verwundert:\n\nAn Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch.", "author": "thoniTUB", "createdAt": "2020-05-18T15:11:20Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);\n+\n+\t\t\tfor (int col = 0; col < headers.length; col++) {\n+\t\t\t\tgroovy.setVariable(headers[col], col);\n+\t\t\t}\n+\n+\t\t\treturn  (GroovyPredicate) groovy.parse(filter);\n+\t\t} catch (Exception | Error e) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\ndeleted file mode 100644\nindex dd06416c3..000000000\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n+++ /dev/null\n\n@@ -1,141 +0,0 @@\n-package com.bakdata.conquery.models.preproc;\n-\n-import java.io.File;\n-import java.io.Serializable;\n-import java.time.LocalDate;\n-import java.util.Arrays;\n-import java.util.stream.Stream;\n-\n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n-import com.bakdata.conquery.models.common.Range;\n-import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n-import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n-import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.fasterxml.jackson.annotation.JsonIgnore;\n-import groovy.lang.GroovyShell;\n-import io.dropwizard.validation.ValidationMethod;\n-import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import it.unimi.dsi.fastutil.objects.Object2IntMap;\n-import lombok.Data;\n-import lombok.extern.slf4j.Slf4j;\n-import org.codehaus.groovy.control.CompilerConfiguration;\n-import org.codehaus.groovy.control.customizers.ImportCustomizer;\n-import org.hibernate.validator.constraints.NotEmpty;\n-\n-/**\n- * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n- *\n- * It requires a primary Output and at least one normal output.\n- *\n- * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n- */\n-@Data\n-@Slf4j\n-public class TableInputDescriptor implements Serializable {\n-\n-\tprivate static final long serialVersionUID = 1L;\n-\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n-\t\t\tLocalDate.class,\n-\t\t\tRange.class\n-\t).map(Class::getName).toArray(String[]::new);\n-\n-\t@NotNull\n-\tprivate File sourceFile;\n-\n-\tprivate String filter;\n-\n-\t/**\n-\t * Output producing the primary column. This should be the primary key across all tables.\n-\t * Default is `COPY(\"pid\", STRING)`\n-\t */\n-\t@NotNull\n-\t@Valid\n-\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n-\t@Valid @NotEmpty\n-\tprivate OutputDescription[] output;\n-\n-\t/**\n-\t * Empty array to be used only for validation of groovy script.\n-\t */\n-\tpublic static final String[] FAKE_HEADERS = new String[50];\n-\n-\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n-\tpublic boolean isValidGroovyScript(){\n-\t\ttry{\n-\t\t\tcreateFilter(FAKE_HEADERS);\n-\t\t}\n-\t\tcatch (Exception ex) {\n-\t\t\tlog.error(\"Groovy script is not valid\",ex);\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\n-\t@JsonIgnore\n-\t@ValidationMethod(message = \"Each column requires a unique name\")\n-\tpublic boolean isEachNameUnique() {\n-\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n-\t\tnames.defaultReturnValue(-1);\n-\n-\t\tfor (int index = 0; index < output.length; index++) {\n-\t\t\tint prev = names.put(output[index].getName(), index);\n-\t\t\tif(prev != -1){\n-\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n-\t\t\t\treturn false;\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\t@JsonIgnore\n-\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n-\tpublic boolean isPrimaryString() {\n-\t\treturn primary.getResultType() == MajorTypeId.STRING;\n-\t}\n-\n-\tpublic GroovyPredicate createFilter(String[] headers){\n-\t\tif(filter == null) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\ttry {\n-\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n-\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n-\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n-\n-\t\t\tGroovyShell groovy = new GroovyShell(config);\n-\n-\t\t\tfor (int col = 0; col < headers.length; col++) {\n-\t\t\t\tgroovy.setVariable(headers[col], col);\n-\t\t\t}\n-\n-\t\t\treturn  (GroovyPredicate) groovy.parse(filter);\n-\t\t} catch (Exception | Error e) {\n-\t\t\tthrow new RuntimeException(\"Failed to compile filter `\" + filter + \"`\", e);\n-\t\t}\n-\t}\n-\n-\t@JsonIgnore\n-\tpublic int getWidth() {\n-\t\treturn getOutput().length;\n-\t}\n-\n-\tpublic ColumnDescription getColumnDescription(int i) {\n-\t\treturn output[i].getColumnDescription();\n-\t}\n-\n-\n-\t/**\n-\t * Create a mapping from a header to it's column position.\n-\t */\n-\tpublic static Object2IntArrayMap<String> buildHeaderMap(String[] headers) {\n-\t\tfinal int[] indices = new int[headers.length];\n-\t\tArrays.setAll(indices, i -> i);\n-\t\treturn new Object2IntArrayMap<>(headers, indices);\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426701091", "bodyText": "Das sieht so aus, als k\u00f6nnte es auch statisch sein.", "author": "thoniTUB", "createdAt": "2020-05-18T15:14:39Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NjQ5MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429856490", "bodyText": "Wir setzen \u00fcber die Skripte Properties der Shell, dann w\u00fcrden die sich gegenseitig reingr\u00e4tschen.", "author": "awildturtok", "createdAt": "2020-05-25T10:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\ndeleted file mode 100644\nindex dd06416c3..000000000\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n+++ /dev/null\n\n@@ -1,141 +0,0 @@\n-package com.bakdata.conquery.models.preproc;\n-\n-import java.io.File;\n-import java.io.Serializable;\n-import java.time.LocalDate;\n-import java.util.Arrays;\n-import java.util.stream.Stream;\n-\n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n-import com.bakdata.conquery.models.common.Range;\n-import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n-import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n-import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.fasterxml.jackson.annotation.JsonIgnore;\n-import groovy.lang.GroovyShell;\n-import io.dropwizard.validation.ValidationMethod;\n-import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import it.unimi.dsi.fastutil.objects.Object2IntMap;\n-import lombok.Data;\n-import lombok.extern.slf4j.Slf4j;\n-import org.codehaus.groovy.control.CompilerConfiguration;\n-import org.codehaus.groovy.control.customizers.ImportCustomizer;\n-import org.hibernate.validator.constraints.NotEmpty;\n-\n-/**\n- * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n- *\n- * It requires a primary Output and at least one normal output.\n- *\n- * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n- */\n-@Data\n-@Slf4j\n-public class TableInputDescriptor implements Serializable {\n-\n-\tprivate static final long serialVersionUID = 1L;\n-\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n-\t\t\tLocalDate.class,\n-\t\t\tRange.class\n-\t).map(Class::getName).toArray(String[]::new);\n-\n-\t@NotNull\n-\tprivate File sourceFile;\n-\n-\tprivate String filter;\n-\n-\t/**\n-\t * Output producing the primary column. This should be the primary key across all tables.\n-\t * Default is `COPY(\"pid\", STRING)`\n-\t */\n-\t@NotNull\n-\t@Valid\n-\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n-\t@Valid @NotEmpty\n-\tprivate OutputDescription[] output;\n-\n-\t/**\n-\t * Empty array to be used only for validation of groovy script.\n-\t */\n-\tpublic static final String[] FAKE_HEADERS = new String[50];\n-\n-\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n-\tpublic boolean isValidGroovyScript(){\n-\t\ttry{\n-\t\t\tcreateFilter(FAKE_HEADERS);\n-\t\t}\n-\t\tcatch (Exception ex) {\n-\t\t\tlog.error(\"Groovy script is not valid\",ex);\n-\t\t\treturn false;\n-\t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\n-\t@JsonIgnore\n-\t@ValidationMethod(message = \"Each column requires a unique name\")\n-\tpublic boolean isEachNameUnique() {\n-\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n-\t\tnames.defaultReturnValue(-1);\n-\n-\t\tfor (int index = 0; index < output.length; index++) {\n-\t\t\tint prev = names.put(output[index].getName(), index);\n-\t\t\tif(prev != -1){\n-\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n-\t\t\t\treturn false;\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\t@JsonIgnore\n-\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n-\tpublic boolean isPrimaryString() {\n-\t\treturn primary.getResultType() == MajorTypeId.STRING;\n-\t}\n-\n-\tpublic GroovyPredicate createFilter(String[] headers){\n-\t\tif(filter == null) {\n-\t\t\treturn null;\n-\t\t}\n-\n-\t\ttry {\n-\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n-\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n-\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n-\n-\t\t\tGroovyShell groovy = new GroovyShell(config);\n-\n-\t\t\tfor (int col = 0; col < headers.length; col++) {\n-\t\t\t\tgroovy.setVariable(headers[col], col);\n-\t\t\t}\n-\n-\t\t\treturn  (GroovyPredicate) groovy.parse(filter);\n-\t\t} catch (Exception | Error e) {\n-\t\t\tthrow new RuntimeException(\"Failed to compile filter `\" + filter + \"`\", e);\n-\t\t}\n-\t}\n-\n-\t@JsonIgnore\n-\tpublic int getWidth() {\n-\t\treturn getOutput().length;\n-\t}\n-\n-\tpublic ColumnDescription getColumnDescription(int i) {\n-\t\treturn output[i].getColumnDescription();\n-\t}\n-\n-\n-\t/**\n-\t * Create a mapping from a header to it's column position.\n-\t */\n-\tpublic static Object2IntArrayMap<String> buildHeaderMap(String[] headers) {\n-\t\tfinal int[] indices = new int[headers.length];\n-\t\tArrays.setAll(indices, i -> i);\n-\t\treturn new Object2IntArrayMap<>(headers, indices);\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTAyOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705028", "bodyText": "Ich sehe hier das Upgrade zum BigMultiSelect irgendwie nicht", "author": "thoniTUB", "createdAt": "2020-05-18T15:20:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java", "diffHunk": "@@ -58,20 +60,22 @@\n \tpublic void configureFrontend(FEFilter f) throws ConceptConfigurationException {\n \t\tf.setTemplate(getTemplate());\n \t\tf.setType(filterType);\n-\t\t// TODO: 20.11.2019 Upgrade to BigMultiSelect if more than maximumSize values are found.\n-\t\tif (values != null) {\n-\t\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n-\t\t\t\tthrow new ConceptConfigurationException(getConnector(),\n-\t\t\t\t\tString.format(\"Too many possible values (%d of %d in filter %s).\", values.size(), maximumSize, this.getId()));\n-\t\t\t}\n-\t\t\tif(this.filterType != FEFilterType.BIG_MULTI_SELECT) {\n-\t\t\t\tf.setOptions(\n-\t\t\t\t\tvalues\n-\t\t\t\t\t\t.stream()\n-\t\t\t\t\t\t.map(v->new FEValue(getLabelFor(v), v))\n-\t\t\t\t\t\t.collect(Collectors.toList())\n-\t\t\t\t);\n-\t\t\t}\n+\n+\t\tif (values == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n+\t\t\tlog.warn(\"Too many possible values ({} of {} in Filter[{}]). Upgrading to BigMultiSelect\", values.size(), maximumSize, getId());", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java b/backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java\nindex 2f6366b5d..67efd62c3 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java\n\n@@ -60,22 +58,20 @@ public abstract class AbstractSelectFilter<FE_TYPE> extends SingleColumnFilter<F\n \tpublic void configureFrontend(FEFilter f) throws ConceptConfigurationException {\n \t\tf.setTemplate(getTemplate());\n \t\tf.setType(filterType);\n-\n-\t\tif (values == null) {\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n-\t\t\tlog.warn(\"Too many possible values ({} of {} in Filter[{}]). Upgrading to BigMultiSelect\", values.size(), maximumSize, getId());\n-\t\t}\n-\n-\t\tif(this.filterType != FEFilterType.BIG_MULTI_SELECT) {\n-\t\t\tf.setOptions(\n-\t\t\t\tvalues\n-\t\t\t\t\t.stream()\n-\t\t\t\t\t.map(v->new FEValue(getLabelFor(v), v))\n-\t\t\t\t\t.collect(Collectors.toList())\n-\t\t\t);\n+\t\t// TODO: 20.11.2019 Upgrade to BigMultiSelect if more than maximumSize values are found.\n+\t\tif (values != null) {\n+\t\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n+\t\t\t\tthrow new ConceptConfigurationException(getConnector(),\n+\t\t\t\t\tString.format(\"Too many possible values (%d of %d in filter %s).\", values.size(), maximumSize, this.getId()));\n+\t\t\t}\n+\t\t\tif(this.filterType != FEFilterType.BIG_MULTI_SELECT) {\n+\t\t\t\tf.setOptions(\n+\t\t\t\t\tvalues\n+\t\t\t\t\t\t.stream()\n+\t\t\t\t\t\t.map(v->new FEValue(getLabelFor(v), v))\n+\t\t\t\t\t\t.collect(Collectors.toList())\n+\t\t\t\t);\n+\t\t\t}\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTk5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705994", "bodyText": "Das war die Syntax die nicht funktioniert", "author": "thoniTUB", "createdAt": "2020-05-18T15:21:51Z", "path": "backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java", "diffHunk": "@@ -78,7 +77,7 @@ public void execute() throws Exception {\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (Exception e) {\n-\t\t\t\tlog.error(\"Failed to collect the matching stats for CBlock \" + cBlock.getId(), e);\n+\t\t\t\tlog.error(\"Failed to collect the matching stats for {}\", cBlock, e);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java b/backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java\nindex 1c64bb80d..092ab9348 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java\n\n@@ -77,7 +78,7 @@ public class UpdateMatchingStats extends Job {\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (Exception e) {\n-\t\t\t\tlog.error(\"Failed to collect the matching stats for {}\", cBlock, e);\n+\t\t\t\tlog.error(\"Failed to collect the matching stats for CBlock \" + cBlock.getId(), e);\n \t\t\t}\n \n \t\t\tprogressReporter.report(1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzYwMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426707602", "bodyText": "Nice", "author": "thoniTUB", "createdAt": "2020-05-18T15:24:01Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "diffHunk": "@@ -1,41 +1,50 @@\n package com.bakdata.conquery.models.preproc;\n \n-import com.bakdata.conquery.models.common.daterange.CDateRange;\n+import java.util.StringJoiner;\n+\n import com.bakdata.conquery.models.datasets.Table;\n import lombok.AllArgsConstructor;\n-import lombok.Builder;\n import lombok.Data;\n import lombok.NoArgsConstructor;\n+import lombok.extern.slf4j.Slf4j;\n \n-@Data @Builder @NoArgsConstructor @AllArgsConstructor\n+@Data @NoArgsConstructor @AllArgsConstructor @Slf4j\n public class PreprocessedHeader {\n-\tprivate int validityHash;\n \tprivate String name;\n \tprivate String table;\n+\tprivate String suffix;\n+\n \tprivate long rows;\n \tprivate long groups;\n-\tprivate CDateRange eventRange;\n \tprivate PPColumn primaryColumn;\n \tprivate PPColumn[] columns;\n-\tprivate String suffix;\n+\n+\tprivate int validityHash;\n+\n \n \t/**\n \t * Verify that the supplied table matches the preprocessed' data in shape.\n \t */\n-\tpublic boolean matches(Table table) {\n-\t\tif(!table.getPrimaryColumn().matches(getPrimaryColumn())) {\n-\t\t\treturn false;\n+\tpublic void assertMatch(Table table) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java\nindex 9e5d03701..0de19dac6 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java\n\n@@ -1,50 +1,41 @@\n package com.bakdata.conquery.models.preproc;\n \n-import java.util.StringJoiner;\n-\n+import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.datasets.Table;\n import lombok.AllArgsConstructor;\n+import lombok.Builder;\n import lombok.Data;\n import lombok.NoArgsConstructor;\n-import lombok.extern.slf4j.Slf4j;\n \n-@Data @NoArgsConstructor @AllArgsConstructor @Slf4j\n+@Data @Builder @NoArgsConstructor @AllArgsConstructor\n public class PreprocessedHeader {\n+\tprivate int validityHash;\n \tprivate String name;\n \tprivate String table;\n-\tprivate String suffix;\n-\n \tprivate long rows;\n \tprivate long groups;\n+\tprivate CDateRange eventRange;\n \tprivate PPColumn primaryColumn;\n \tprivate PPColumn[] columns;\n-\n-\tprivate int validityHash;\n-\n+\tprivate String suffix;\n \n \t/**\n \t * Verify that the supplied table matches the preprocessed' data in shape.\n \t */\n-\tpublic void assertMatch(Table table) {\n-\t\tStringJoiner errors = new StringJoiner(\"\\n\");\n-\n-\t\tif (!table.getPrimaryColumn().matches(getPrimaryColumn())) {\n-\t\t\terrors.add(String.format(\"PrimaryColumn[%s] does not match table PrimaryColumn[%s]\", getPrimaryColumn(), table.getPrimaryColumn()));\n+\tpublic boolean matches(Table table) {\n+\t\tif(!table.getPrimaryColumn().matches(getPrimaryColumn())) {\n+\t\t\treturn false;\n \t\t}\n \n-\t\tif (table.getColumns().length != getColumns().length) {\n-\t\t\terrors.add(String.format(\"Length=`%d` does not match table Length=`%d`\", getColumns().length, table.getColumns().length));\n+\t\tif(table.getColumns().length != getColumns().length) {\n+\t\t\treturn false;\n \t\t}\n \n-\t\tfor (int i = 0; i < Math.min(table.getColumns().length, getColumns().length); i++) {\n-\t\t\tif (!table.getColumns()[i].matches(getColumns()[i])) {\n-\t\t\t\terrors.add(String.format(\"Column[%s] does not match table Column[%s]`\", getColumns()[i], table.getColumns()[i]));\n+\t\tfor(int i = 0; i < table.getColumns().length; i++) {\n+\t\t\tif(!table.getColumns()[i].matches(getColumns()[i])) {\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n-\n-\t\tif (errors.length() != 0) {\n-\t\t\tlog.error(errors.toString());\n-\t\t\tthrow new IllegalArgumentException(String.format(\"Headers[%s.%s.%s] do not match Table[%s]\", getTable(), getName(), getSuffix(), table.getId()));\n-\t\t}\n+\t\treturn true;\n \t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwODg1Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426708853", "bodyText": "Den Reader k\u00f6nntest du schon mal statisch vorbereiten mit Jackson.BINARY_MAPPER.readFor(PreprocessedHeader.class)", "author": "thoniTUB", "createdAt": "2020-05-18T15:25:48Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\nindex 7e0dfe57b..419c8057d 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n\n@@ -29,298 +33,266 @@ import com.bakdata.conquery.util.io.ConqueryFileUtil;\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n-import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import it.unimi.dsi.fastutil.objects.Object2IntMap;\n-import lombok.experimental.UtilityClass;\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@UtilityClass\n+@RequiredArgsConstructor\n+@Getter\n public class Preprocessor {\n \n-\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n-\t\tlong totalCsvSize = 0;\n-\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\n-\t\treturn totalCsvSize;\n-\t}\n+\tprivate final ConqueryConfig config;\n+\tprivate final ImportDescriptor descriptor;\n+\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n+\tprivate long totalCsvSize;\n \n-\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n+\tpublic boolean requiresProcessing() {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n \t\t\tlog.info(\"EXISTS ALREADY\");\n-\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n-\t\t\t\t InputStream is = outFile.readHeader()) {\n-\n-\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\n-\t\t\t\tif (header.getValidityHash() == currentHash) {\n-\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n+\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n+\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n+\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\t\treturn false;\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t} catch (Exception e) {\n-\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n-\t\t\t\treturn false;\n+\t\t\t}\n+\t\t\tcatch(Exception e) {\n+\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\t/**\n-\t * Create version of file-name with tag.\n-\t */\n-\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n-\t\tif(Strings.isNullOrEmpty(tag)) {\n-\t\t\treturn file;\n+\t\t\n+\t\tfor(Input input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n \t\t}\n-\n-\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t\t\n+\t\treturn true;\n \t}\n-\n-\n-\t/**\n-\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n-\t *\n-\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n-\t */\n-\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n-\n+\t\n+\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\t\tConqueryMDC.setLocation(descriptor.toString());\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\n-\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\n-\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n+\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n \t\t}\n-\n \t\t//delete target file if it exists\n-\t\tif (preprocessedFile.exists()) {\n-\t\t\tFileUtils.forceDelete(preprocessedFile);\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n \t\t}\n \n-\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\n-\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n-\n-\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n-\n-\t\tlong lineId = 0;\n-\n-\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n-\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n-\t\texceptions.defaultReturnValue(0);\n \n+\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n+\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n-\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal File sourceFile = input.getSourceFile();\n-\n-\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n+\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n+\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\tCsvParser parser = null;\n-\n-\n-\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n+\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n-\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n-\n-\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n-\n-\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n-\n-\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n-\n-\t\t\t\t\t// Compile filter.\n-\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n-\n-\n-\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n-\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n-\n-\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n-\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n-\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tString[] row;\n-\n-\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n-\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n-\n-\t\t\t\t\t\t// Check if row shall be evaluated\n-\t\t\t\t\t\t// This is explicitly NOT in a try-catch block as scripts may not fail and we should not recover from faulty scripts.\n-\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\ttry {\n-\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n-\n-\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n-\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n-\n-\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n-\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n-\t\t\t\t\t\t\texceptions.put(e.getCause().getClass(), exceptions.getInt(e.getCause().getClass()) + 1);\n+\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n+\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n+\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n+\t\t\t\t\t\t\t1_000\n+\t\t\t\t\t);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\t\t\twhile (it.hasNext()) {\n \n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}\", e.getSource(), lineId, row, e.getCause());\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n+\t\t\t\t\t\tString[] row = it.next();\n \n+\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n+\t\t\t\t\t\tif (primary != null) {\n+\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n+\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (Exception e) {\n-\t\t\t\t\t\t\texceptions.put(e.getClass(), exceptions.getInt(e.getClass()) + 1);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n-\n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse line: {} content: {}\", lineId, row, e);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tfinally {\n-\t\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\t\tlineId++;\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\t//report progress\n+\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n+\t\t\t\t\t\tprogress = countingIn.getCount();\n+\t\t\t\t\t\tlineId++;\n \t\t\t\t\t}\n \n-\t\t\t\t}finally {\n-\t\t\t\t\tif(parser != null) {\n-\t\t\t\t\t\tparser.stopParsing();\n+\t\t\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n+\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tif (errorCounter.get() > 0) {\n-\t\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t\t//find the optimal subtypes\n+\t\t\tlog.info(\"finding optimal column types\");\n+\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), result.getPrimaryColumn().getName(), result.getPrimaryColumn().getParser(), result.getPrimaryColumn().getType());\n+\t\t\t\n+\t\t\tStringParser parser = (StringParser)result.getPrimaryColumn().getParser();\n+\t\t\tparser.setEncoding(Encoding.UTF8);\n+\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\t\tfor(PPColumn c:result.getColumns()) {\n+\t\t\t\tc.findBestType();\n+\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n+\t\t\t}\n+\t\t\t//estimate memory weight\n+\t\t\tlog.info(\"estimated total memory consumption: {} + n*{}\", \n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n+\t\t\t\t),\n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n+\t\t\t\t)\n+\t\t\t);\n+\n+\t\t\tfor(PPColumn c:ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n+\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n+\t\t\t\tlog.info(\"\\t{}.{}: {}{}\",\n+\t\t\t\t\tresult.getName(),\n+\t\t\t\t\tc.getName(),\n+\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n+\t\t\t\t\ttypeConsumption==0?\"\":(\" + n*\"+BinaryByteUnit.format(typeConsumption))\n+\t\t\t\t);\n \t\t\t}\n \n-\t\t\tif (log.isWarnEnabled()) {\n-\t\t\t\texceptions.forEach((clazz, count) -> log.warn(\"Got {} `{}`\", count, clazz.getSimpleName()));\n+\t\t\ttry (com.esotericsoftware.kryo.io.Output out = new com.esotericsoftware.kryo.io.Output(outFile.writeContent())) {\n+\t\t\t\tresult.writeToFile(out);\n \t\t\t}\n \n-\t\t\t//find the optimal subtypes\n-\t\t\t{\n-\t\t\t\tlog.info(\"finding optimal column types\");\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"\\t{}.{}: {} -> {}\",\n-\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getParser(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getType()\n-\t\t\t\t);\n+\t\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\t\tresult.writeHeader(out);\n+\t\t\t}\n+\t\t}\n \n-\t\t\t\tStringParser parser = (StringParser) result.getPrimaryColumn().getParser();\n-\t\t\t\tparser.setEncoding(Encoding.UTF8);\n-\t\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\tif(errorCounter.get() > 0 && log.isWarnEnabled()) {\n+\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t}\n \n-\t\t\t\tfor (PPColumn c : result.getColumns()) {\n-\t\t\t\t\tc.findBestType();\n-\t\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n-\t\t\t\t}\n+\t\t//if successful move the tmp file to the target location\n+\t\tFileUtils.moveFile(tmp, descriptor.getInputFile().getPreprocessedFile());\n+\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t}\n \n-\t\t\t\t//estimate memory weight\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"estimated total memory consumption: {} + n*{}\",\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n-\t\t\t\t\t\t),\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n-\t\t\t\t\t\t)\n-\t\t\t\t);\n+\tprivate void parseRow(int primaryId, PPColumn[] columns, String[] row, Input input, long lineId, Preprocessed result, int inputSource) {\n+\t\ttry {\n \n-\t\t\t\tfor (PPColumn c : ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n-\t\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n-\t\t\t\t\tlog.info(\n-\t\t\t\t\t\t\t\"\\t{}.{}: {}{}\",\n-\t\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\t\tc.getName(),\n-\t\t\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n-\t\t\t\t\t\t\ttypeConsumption == 0 ? \"\" : (\" + n*\" + BinaryByteUnit.format(typeConsumption))\n-\t\t\t\t\t);\n+\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().createOutput(primaryId, row, columns, inputSource, lineId);\n+\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\tresult.addRow(primaryId, columns, outRow.getData());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif (input.filter(row)) {\n+\t\t\t\t\tfor (Object[] outRow : generateOutput(input, columns, row, inputSource, lineId)) {\n+\t\t\t\t\t\tresult.addRow(primaryId, columns, outRow);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tresult.write(outFile);\n \t\t}\n+\t\tcatch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif (errors < config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if (errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \" + config.getPreprocessor().getMaximumPrintedErrors() + \" were printed.\");\n+\t\t\t}\n \n-\t\t//if successful move the tmp file to the target location\n-\t\tFileUtils.moveFile(tmp, preprocessedFile);\n-\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tthrow new IllegalStateException(\"failed while processing line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t}\n \t}\n \n-\t/**\n-\t * Apply each output for a single row. Returning all resulting values.\n-\t */\n-\tprivate static Object[] applyOutputs(List<OutputDescription.Output> outputs, PPColumn[] columns, String[] row, long lineId)\n-\t\t\tthrows ParsingException, OutputDescription.OutputException {\n-\t\tObject[] outRow = new Object[outputs.size()];\n-\n-\t\tfor (int index = 0; index < outputs.size(); index++) {\n-\t\t\tfinal OutputDescription.Output out = outputs.get(index);\n+\tprivate Integer getPrimary(StringParser primaryType, String[] row, long lineId, int source, Output primaryOutput) {\n+\t\ttry {\n+\t\t\tList<Object> primary = primaryOutput.createOutput(primaryType, row, source, lineId);\n+\t\t\tif(primary.size()!=1 || !(primary.get(0) instanceof Integer)) {\n+\t\t\t\tthrow new IllegalStateException(\"The returned primary was the illegal value \"+primary+\" in \"+Arrays.toString(row));\n+\t\t\t}\n+\t\t\treturn (int)primary.get(0);\n+\t\t} catch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif(errors<config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse primary from line:\"+lineId+\" content:\"+Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if(errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"+config.getPreprocessor().getMaximumPrintedErrors()+\" were printed.\");\n+\t\t\t}\n+\t\t\treturn null;\n+\t\t}\n+\t}\n \n-\t\t\ttry {\n-\t\t\t\tfinal Parser<?> parser = columns[index].getParser();\n+\tprivate static List<Object[]> generateOutput(Input input, PPColumn[] columns, String[] row, int source, long lineId) throws ParsingException {\n+\t\tList<Object[]> resultRows = new ArrayList<>();\n+\t\tint oid = 0;\n+\t\tfor(int c = 0; c<input.getOutput().length; c++) {\n+\t\t\tOutput out = input.getOutput()[c];\n+\t\t\tParser<?> parser = columns[c].getParser();\n+\n+\t\t\tList<Object> result;\n+\t\t\tresult = out.createOutput(parser, row, source, lineId);\n+\t\t\tif(result==null) {\n+\t\t\t\tthrow new IllegalStateException(out+\" returned null result for \"+Arrays.toString(row));\n+\t\t\t}\n \n-\t\t\t\tfinal Object result = out.createOutput(row, parser, lineId);\n \n-\t\t\t\tif (result == null) {\n-\t\t\t\t\tcontinue;\n+\t\t\t//if the result is a single NULL and we don't want to include such rows\n+\t\t\tif(result.size()==1 && result.get(0)==null && out.isRequired()) {\n+\t\t\t\treturn Collections.emptyList();\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif(resultRows.isEmpty()) {\n+\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\tObject[] newRow = new Object[input.getOutput().length];\n+\t\t\t\t\t\tnewRow[oid]=v;\n+\t\t\t\t\t\tresultRows.add(newRow);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\tif(result.size()==1) {\n+\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\tresultRow[oid]=result.get(0);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tList<Object[]> newResultRows = new ArrayList<>(resultRows.size()*result.size());\n+\t\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\t\tObject[] newResultRow = Arrays.copyOf(resultRow,resultRow.length);\n+\t\t\t\t\t\t\t\tnewResultRow[oid]=v;\n+\t\t\t\t\t\t\t\tnewResultRows.add(newResultRow);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tresultRows = newResultRows;\n+\t\t\t\t\t}\n \t\t\t\t}\n-\n-\t\t\t\toutRow[index] = result;\n-\t\t\t}catch (Exception e){\n-\t\t\t\tthrow new OutputDescription.OutputException(out.getDescription(), e);\n \t\t\t}\n+\t\t\toid++;\n \t\t}\n-\t\treturn outRow;\n+\t\treturn resultRows;\n \t}\n }\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426713146", "bodyText": "Hier w\u00fcrde doch auch ein int reichen", "author": "thoniTUB", "createdAt": "2020-05-18T15:31:59Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\nindex 7e0dfe57b..419c8057d 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n\n@@ -29,298 +33,266 @@ import com.bakdata.conquery.util.io.ConqueryFileUtil;\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n-import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import it.unimi.dsi.fastutil.objects.Object2IntMap;\n-import lombok.experimental.UtilityClass;\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@UtilityClass\n+@RequiredArgsConstructor\n+@Getter\n public class Preprocessor {\n \n-\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n-\t\tlong totalCsvSize = 0;\n-\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\n-\t\treturn totalCsvSize;\n-\t}\n+\tprivate final ConqueryConfig config;\n+\tprivate final ImportDescriptor descriptor;\n+\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n+\tprivate long totalCsvSize;\n \n-\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n+\tpublic boolean requiresProcessing() {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n \t\t\tlog.info(\"EXISTS ALREADY\");\n-\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n-\t\t\t\t InputStream is = outFile.readHeader()) {\n-\n-\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\n-\t\t\t\tif (header.getValidityHash() == currentHash) {\n-\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n+\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n+\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n+\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\t\treturn false;\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t} catch (Exception e) {\n-\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n-\t\t\t\treturn false;\n+\t\t\t}\n+\t\t\tcatch(Exception e) {\n+\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\t/**\n-\t * Create version of file-name with tag.\n-\t */\n-\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n-\t\tif(Strings.isNullOrEmpty(tag)) {\n-\t\t\treturn file;\n+\t\t\n+\t\tfor(Input input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n \t\t}\n-\n-\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t\t\n+\t\treturn true;\n \t}\n-\n-\n-\t/**\n-\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n-\t *\n-\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n-\t */\n-\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n-\n+\t\n+\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\t\tConqueryMDC.setLocation(descriptor.toString());\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\n-\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\n-\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n+\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n \t\t}\n-\n \t\t//delete target file if it exists\n-\t\tif (preprocessedFile.exists()) {\n-\t\t\tFileUtils.forceDelete(preprocessedFile);\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n \t\t}\n \n-\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\n-\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n-\n-\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n-\n-\t\tlong lineId = 0;\n-\n-\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n-\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n-\t\texceptions.defaultReturnValue(0);\n \n+\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n+\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n-\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal File sourceFile = input.getSourceFile();\n-\n-\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n+\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n+\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\tCsvParser parser = null;\n-\n-\n-\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n+\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n-\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n-\n-\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n-\n-\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n-\n-\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n-\n-\t\t\t\t\t// Compile filter.\n-\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n-\n-\n-\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n-\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n-\n-\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n-\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n-\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tString[] row;\n-\n-\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n-\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n-\n-\t\t\t\t\t\t// Check if row shall be evaluated\n-\t\t\t\t\t\t// This is explicitly NOT in a try-catch block as scripts may not fail and we should not recover from faulty scripts.\n-\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\ttry {\n-\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n-\n-\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n-\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n-\n-\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n-\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n-\t\t\t\t\t\t\texceptions.put(e.getCause().getClass(), exceptions.getInt(e.getCause().getClass()) + 1);\n+\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n+\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n+\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n+\t\t\t\t\t\t\t1_000\n+\t\t\t\t\t);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\t\t\twhile (it.hasNext()) {\n \n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}\", e.getSource(), lineId, row, e.getCause());\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n+\t\t\t\t\t\tString[] row = it.next();\n \n+\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n+\t\t\t\t\t\tif (primary != null) {\n+\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n+\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (Exception e) {\n-\t\t\t\t\t\t\texceptions.put(e.getClass(), exceptions.getInt(e.getClass()) + 1);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n-\n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse line: {} content: {}\", lineId, row, e);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tfinally {\n-\t\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\t\tlineId++;\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\t//report progress\n+\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n+\t\t\t\t\t\tprogress = countingIn.getCount();\n+\t\t\t\t\t\tlineId++;\n \t\t\t\t\t}\n \n-\t\t\t\t}finally {\n-\t\t\t\t\tif(parser != null) {\n-\t\t\t\t\t\tparser.stopParsing();\n+\t\t\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n+\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tif (errorCounter.get() > 0) {\n-\t\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t\t//find the optimal subtypes\n+\t\t\tlog.info(\"finding optimal column types\");\n+\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), result.getPrimaryColumn().getName(), result.getPrimaryColumn().getParser(), result.getPrimaryColumn().getType());\n+\t\t\t\n+\t\t\tStringParser parser = (StringParser)result.getPrimaryColumn().getParser();\n+\t\t\tparser.setEncoding(Encoding.UTF8);\n+\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\t\tfor(PPColumn c:result.getColumns()) {\n+\t\t\t\tc.findBestType();\n+\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n+\t\t\t}\n+\t\t\t//estimate memory weight\n+\t\t\tlog.info(\"estimated total memory consumption: {} + n*{}\", \n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n+\t\t\t\t),\n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n+\t\t\t\t)\n+\t\t\t);\n+\n+\t\t\tfor(PPColumn c:ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n+\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n+\t\t\t\tlog.info(\"\\t{}.{}: {}{}\",\n+\t\t\t\t\tresult.getName(),\n+\t\t\t\t\tc.getName(),\n+\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n+\t\t\t\t\ttypeConsumption==0?\"\":(\" + n*\"+BinaryByteUnit.format(typeConsumption))\n+\t\t\t\t);\n \t\t\t}\n \n-\t\t\tif (log.isWarnEnabled()) {\n-\t\t\t\texceptions.forEach((clazz, count) -> log.warn(\"Got {} `{}`\", count, clazz.getSimpleName()));\n+\t\t\ttry (com.esotericsoftware.kryo.io.Output out = new com.esotericsoftware.kryo.io.Output(outFile.writeContent())) {\n+\t\t\t\tresult.writeToFile(out);\n \t\t\t}\n \n-\t\t\t//find the optimal subtypes\n-\t\t\t{\n-\t\t\t\tlog.info(\"finding optimal column types\");\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"\\t{}.{}: {} -> {}\",\n-\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getParser(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getType()\n-\t\t\t\t);\n+\t\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\t\tresult.writeHeader(out);\n+\t\t\t}\n+\t\t}\n \n-\t\t\t\tStringParser parser = (StringParser) result.getPrimaryColumn().getParser();\n-\t\t\t\tparser.setEncoding(Encoding.UTF8);\n-\t\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\tif(errorCounter.get() > 0 && log.isWarnEnabled()) {\n+\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t}\n \n-\t\t\t\tfor (PPColumn c : result.getColumns()) {\n-\t\t\t\t\tc.findBestType();\n-\t\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n-\t\t\t\t}\n+\t\t//if successful move the tmp file to the target location\n+\t\tFileUtils.moveFile(tmp, descriptor.getInputFile().getPreprocessedFile());\n+\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t}\n \n-\t\t\t\t//estimate memory weight\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"estimated total memory consumption: {} + n*{}\",\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n-\t\t\t\t\t\t),\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n-\t\t\t\t\t\t)\n-\t\t\t\t);\n+\tprivate void parseRow(int primaryId, PPColumn[] columns, String[] row, Input input, long lineId, Preprocessed result, int inputSource) {\n+\t\ttry {\n \n-\t\t\t\tfor (PPColumn c : ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n-\t\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n-\t\t\t\t\tlog.info(\n-\t\t\t\t\t\t\t\"\\t{}.{}: {}{}\",\n-\t\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\t\tc.getName(),\n-\t\t\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n-\t\t\t\t\t\t\ttypeConsumption == 0 ? \"\" : (\" + n*\" + BinaryByteUnit.format(typeConsumption))\n-\t\t\t\t\t);\n+\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().createOutput(primaryId, row, columns, inputSource, lineId);\n+\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\tresult.addRow(primaryId, columns, outRow.getData());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif (input.filter(row)) {\n+\t\t\t\t\tfor (Object[] outRow : generateOutput(input, columns, row, inputSource, lineId)) {\n+\t\t\t\t\t\tresult.addRow(primaryId, columns, outRow);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tresult.write(outFile);\n \t\t}\n+\t\tcatch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif (errors < config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if (errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \" + config.getPreprocessor().getMaximumPrintedErrors() + \" were printed.\");\n+\t\t\t}\n \n-\t\t//if successful move the tmp file to the target location\n-\t\tFileUtils.moveFile(tmp, preprocessedFile);\n-\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tthrow new IllegalStateException(\"failed while processing line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t}\n \t}\n \n-\t/**\n-\t * Apply each output for a single row. Returning all resulting values.\n-\t */\n-\tprivate static Object[] applyOutputs(List<OutputDescription.Output> outputs, PPColumn[] columns, String[] row, long lineId)\n-\t\t\tthrows ParsingException, OutputDescription.OutputException {\n-\t\tObject[] outRow = new Object[outputs.size()];\n-\n-\t\tfor (int index = 0; index < outputs.size(); index++) {\n-\t\t\tfinal OutputDescription.Output out = outputs.get(index);\n+\tprivate Integer getPrimary(StringParser primaryType, String[] row, long lineId, int source, Output primaryOutput) {\n+\t\ttry {\n+\t\t\tList<Object> primary = primaryOutput.createOutput(primaryType, row, source, lineId);\n+\t\t\tif(primary.size()!=1 || !(primary.get(0) instanceof Integer)) {\n+\t\t\t\tthrow new IllegalStateException(\"The returned primary was the illegal value \"+primary+\" in \"+Arrays.toString(row));\n+\t\t\t}\n+\t\t\treturn (int)primary.get(0);\n+\t\t} catch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif(errors<config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse primary from line:\"+lineId+\" content:\"+Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if(errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"+config.getPreprocessor().getMaximumPrintedErrors()+\" were printed.\");\n+\t\t\t}\n+\t\t\treturn null;\n+\t\t}\n+\t}\n \n-\t\t\ttry {\n-\t\t\t\tfinal Parser<?> parser = columns[index].getParser();\n+\tprivate static List<Object[]> generateOutput(Input input, PPColumn[] columns, String[] row, int source, long lineId) throws ParsingException {\n+\t\tList<Object[]> resultRows = new ArrayList<>();\n+\t\tint oid = 0;\n+\t\tfor(int c = 0; c<input.getOutput().length; c++) {\n+\t\t\tOutput out = input.getOutput()[c];\n+\t\t\tParser<?> parser = columns[c].getParser();\n+\n+\t\t\tList<Object> result;\n+\t\t\tresult = out.createOutput(parser, row, source, lineId);\n+\t\t\tif(result==null) {\n+\t\t\t\tthrow new IllegalStateException(out+\" returned null result for \"+Arrays.toString(row));\n+\t\t\t}\n \n-\t\t\t\tfinal Object result = out.createOutput(row, parser, lineId);\n \n-\t\t\t\tif (result == null) {\n-\t\t\t\t\tcontinue;\n+\t\t\t//if the result is a single NULL and we don't want to include such rows\n+\t\t\tif(result.size()==1 && result.get(0)==null && out.isRequired()) {\n+\t\t\t\treturn Collections.emptyList();\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif(resultRows.isEmpty()) {\n+\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\tObject[] newRow = new Object[input.getOutput().length];\n+\t\t\t\t\t\tnewRow[oid]=v;\n+\t\t\t\t\t\tresultRows.add(newRow);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\tif(result.size()==1) {\n+\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\tresultRow[oid]=result.get(0);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tList<Object[]> newResultRows = new ArrayList<>(resultRows.size()*result.size());\n+\t\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\t\tObject[] newResultRow = Arrays.copyOf(resultRow,resultRow.length);\n+\t\t\t\t\t\t\t\tnewResultRow[oid]=v;\n+\t\t\t\t\t\t\t\tnewResultRows.add(newResultRow);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tresultRows = newResultRows;\n+\t\t\t\t\t}\n \t\t\t\t}\n-\n-\t\t\t\toutRow[index] = result;\n-\t\t\t}catch (Exception e){\n-\t\t\t\tthrow new OutputDescription.OutputException(out.getDescription(), e);\n \t\t\t}\n+\t\t\toid++;\n \t\t}\n-\t\treturn outRow;\n+\t\treturn resultRows;\n \t}\n }\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426714717", "bodyText": "Du z\u00e4hlst hier keine Errors vom GroovyScript mit, nur die von den Outputs", "author": "thoniTUB", "createdAt": "2020-05-18T15:34:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n+\n+\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n+\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n+\t\texceptions.defaultReturnValue(0);\n+\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NzYyMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429857621", "bodyText": "Ja, weil die skripte nicht abst\u00fcrzen d\u00fcrfen, und wenn sie es doch tun den ganzen preprocess der datei killen sollen", "author": "awildturtok", "createdAt": "2020-05-25T10:19:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw=="}], "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\nindex 7e0dfe57b..419c8057d 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n\n@@ -29,298 +33,266 @@ import com.bakdata.conquery.util.io.ConqueryFileUtil;\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n-import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import it.unimi.dsi.fastutil.objects.Object2IntMap;\n-import lombok.experimental.UtilityClass;\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@UtilityClass\n+@RequiredArgsConstructor\n+@Getter\n public class Preprocessor {\n \n-\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n-\t\tlong totalCsvSize = 0;\n-\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\n-\t\treturn totalCsvSize;\n-\t}\n+\tprivate final ConqueryConfig config;\n+\tprivate final ImportDescriptor descriptor;\n+\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n+\tprivate long totalCsvSize;\n \n-\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n+\tpublic boolean requiresProcessing() {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n \t\t\tlog.info(\"EXISTS ALREADY\");\n-\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n-\t\t\t\t InputStream is = outFile.readHeader()) {\n-\n-\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\n-\t\t\t\tif (header.getValidityHash() == currentHash) {\n-\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\treturn false;\n-\t\t\t\t}\n-\t\t\t\telse {\n-\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n+\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n+\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n+\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\t\treturn false;\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t\t}\n \t\t\t\t}\n-\t\t\t} catch (Exception e) {\n-\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n-\t\t\t\treturn false;\n+\t\t\t}\n+\t\t\tcatch(Exception e) {\n+\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\n-\t\treturn true;\n-\t}\n-\n-\t/**\n-\t * Create version of file-name with tag.\n-\t */\n-\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n-\t\tif(Strings.isNullOrEmpty(tag)) {\n-\t\t\treturn file;\n+\t\t\n+\t\tfor(Input input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n \t\t}\n-\n-\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t\t\n+\t\treturn true;\n \t}\n-\n-\n-\t/**\n-\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n-\t *\n-\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n-\t */\n-\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n-\n+\t\n+\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\t\tConqueryMDC.setLocation(descriptor.toString());\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\n-\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\n-\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n+\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n \t\t}\n-\n \t\t//delete target file if it exists\n-\t\tif (preprocessedFile.exists()) {\n-\t\t\tFileUtils.forceDelete(preprocessedFile);\n+\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n \t\t}\n \n-\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\n-\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n-\n-\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n-\n-\t\tlong lineId = 0;\n-\n-\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n-\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n-\t\texceptions.defaultReturnValue(0);\n \n+\t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n+\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n-\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal File sourceFile = input.getSourceFile();\n-\n-\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n+\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n+\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\tCsvParser parser = null;\n-\n-\n-\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n+\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n-\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n-\n-\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n-\n-\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n-\n-\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n-\n-\t\t\t\t\t// Compile filter.\n-\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n-\n-\n-\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n-\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n-\n-\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n-\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n-\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tString[] row;\n-\n-\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n-\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n-\n-\t\t\t\t\t\t// Check if row shall be evaluated\n-\t\t\t\t\t\t// This is explicitly NOT in a try-catch block as scripts may not fail and we should not recover from faulty scripts.\n-\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n-\t\t\t\t\t\t\tcontinue;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\ttry {\n-\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n-\n-\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n-\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n-\n-\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n-\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n-\t\t\t\t\t\t\texceptions.put(e.getCause().getClass(), exceptions.getInt(e.getCause().getClass()) + 1);\n+\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n+\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n+\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n+\t\t\t\t\t\t\t1_000\n+\t\t\t\t\t);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\t\t\twhile (it.hasNext()) {\n \n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}\", e.getSource(), lineId, row, e.getCause());\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n+\t\t\t\t\t\tString[] row = it.next();\n \n+\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n+\t\t\t\t\t\tif (primary != null) {\n+\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n+\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n \t\t\t\t\t\t}\n-\t\t\t\t\t\tcatch (Exception e) {\n-\t\t\t\t\t\t\texceptions.put(e.getClass(), exceptions.getInt(e.getClass()) + 1);\n \n-\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n-\n-\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"Failed to parse line: {} content: {}\", lineId, row, e);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\telse if (errors == ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {\n-\t\t\t\t\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"\n-\t\t\t\t\t\t\t\t\t\t + ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()\n-\t\t\t\t\t\t\t\t\t\t + \" were printed.\");\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tfinally {\n-\t\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\t\tlineId++;\n-\t\t\t\t\t\t}\n+\t\t\t\t\t\t//report progress\n+\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n+\t\t\t\t\t\tprogress = countingIn.getCount();\n+\t\t\t\t\t\tlineId++;\n \t\t\t\t\t}\n \n-\t\t\t\t}finally {\n-\t\t\t\t\tif(parser != null) {\n-\t\t\t\t\t\tparser.stopParsing();\n+\t\t\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n+\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tif (errorCounter.get() > 0) {\n-\t\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t\t//find the optimal subtypes\n+\t\t\tlog.info(\"finding optimal column types\");\n+\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), result.getPrimaryColumn().getName(), result.getPrimaryColumn().getParser(), result.getPrimaryColumn().getType());\n+\t\t\t\n+\t\t\tStringParser parser = (StringParser)result.getPrimaryColumn().getParser();\n+\t\t\tparser.setEncoding(Encoding.UTF8);\n+\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\t\tfor(PPColumn c:result.getColumns()) {\n+\t\t\t\tc.findBestType();\n+\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n+\t\t\t}\n+\t\t\t//estimate memory weight\n+\t\t\tlog.info(\"estimated total memory consumption: {} + n*{}\", \n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n+\t\t\t\t),\n+\t\t\t\tBinaryByteUnit.format(\n+\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n+\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n+\t\t\t\t)\n+\t\t\t);\n+\n+\t\t\tfor(PPColumn c:ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n+\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n+\t\t\t\tlog.info(\"\\t{}.{}: {}{}\",\n+\t\t\t\t\tresult.getName(),\n+\t\t\t\t\tc.getName(),\n+\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n+\t\t\t\t\ttypeConsumption==0?\"\":(\" + n*\"+BinaryByteUnit.format(typeConsumption))\n+\t\t\t\t);\n \t\t\t}\n \n-\t\t\tif (log.isWarnEnabled()) {\n-\t\t\t\texceptions.forEach((clazz, count) -> log.warn(\"Got {} `{}`\", count, clazz.getSimpleName()));\n+\t\t\ttry (com.esotericsoftware.kryo.io.Output out = new com.esotericsoftware.kryo.io.Output(outFile.writeContent())) {\n+\t\t\t\tresult.writeToFile(out);\n \t\t\t}\n \n-\t\t\t//find the optimal subtypes\n-\t\t\t{\n-\t\t\t\tlog.info(\"finding optimal column types\");\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"\\t{}.{}: {} -> {}\",\n-\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getName(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getParser(),\n-\t\t\t\t\t\tresult.getPrimaryColumn().getType()\n-\t\t\t\t);\n+\t\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\t\tresult.writeHeader(out);\n+\t\t\t}\n+\t\t}\n \n-\t\t\t\tStringParser parser = (StringParser) result.getPrimaryColumn().getParser();\n-\t\t\t\tparser.setEncoding(Encoding.UTF8);\n-\t\t\t\tresult.getPrimaryColumn().setType(new MapTypeGuesser(parser).createGuess().getType());\n+\t\tif(errorCounter.get() > 0 && log.isWarnEnabled()) {\n+\t\t\tlog.warn(\"File `{}` contained {} faulty lines of ~{} total.\", descriptor.getInputFile().getDescriptionFile(), errorCounter.get(), lineId);\n+\t\t}\n \n-\t\t\t\tfor (PPColumn c : result.getColumns()) {\n-\t\t\t\t\tc.findBestType();\n-\t\t\t\t\tlog.info(\"\\t{}.{}: {} -> {}\", result.getName(), c.getName(), c.getParser(), c.getType());\n-\t\t\t\t}\n+\t\t//if successful move the tmp file to the target location\n+\t\tFileUtils.moveFile(tmp, descriptor.getInputFile().getPreprocessedFile());\n+\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t}\n \n-\t\t\t\t//estimate memory weight\n-\t\t\t\tlog.info(\n-\t\t\t\t\t\t\"estimated total memory consumption: {} + n*{}\",\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateMemoryConsumption).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateMemoryConsumption()\n-\t\t\t\t\t\t),\n-\t\t\t\t\t\tBinaryByteUnit.format(\n-\t\t\t\t\t\t\t\tArrays.stream(result.getColumns()).map(PPColumn::getType).mapToLong(CType::estimateTypeSize).sum()\n-\t\t\t\t\t\t\t\t+ result.getPrimaryColumn().getType().estimateTypeSize()\n-\t\t\t\t\t\t)\n-\t\t\t\t);\n+\tprivate void parseRow(int primaryId, PPColumn[] columns, String[] row, Input input, long lineId, Preprocessed result, int inputSource) {\n+\t\ttry {\n \n-\t\t\t\tfor (PPColumn c : ArrayUtils.add(result.getColumns(), result.getPrimaryColumn())) {\n-\t\t\t\t\tlong typeConsumption = c.getType().estimateTypeSize();\n-\t\t\t\t\tlog.info(\n-\t\t\t\t\t\t\t\"\\t{}.{}: {}{}\",\n-\t\t\t\t\t\t\tresult.getName(),\n-\t\t\t\t\t\t\tc.getName(),\n-\t\t\t\t\t\t\tBinaryByteUnit.format(c.getType().estimateMemoryConsumption()),\n-\t\t\t\t\t\t\ttypeConsumption == 0 ? \"\" : (\" + n*\" + BinaryByteUnit.format(typeConsumption))\n-\t\t\t\t\t);\n+\t\t\tif (input.checkAutoOutput()) {\n+\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().createOutput(primaryId, row, columns, inputSource, lineId);\n+\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n+\t\t\t\t\tresult.addRow(primaryId, columns, outRow.getData());\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif (input.filter(row)) {\n+\t\t\t\t\tfor (Object[] outRow : generateOutput(input, columns, row, inputSource, lineId)) {\n+\t\t\t\t\t\tresult.addRow(primaryId, columns, outRow);\n+\t\t\t\t\t}\n \t\t\t\t}\n \t\t\t}\n-\n-\t\t\tresult.write(outFile);\n \t\t}\n+\t\tcatch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif (errors < config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if (errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \" + config.getPreprocessor().getMaximumPrintedErrors() + \" were printed.\");\n+\t\t\t}\n \n-\t\t//if successful move the tmp file to the target location\n-\t\tFileUtils.moveFile(tmp, preprocessedFile);\n-\t\tlog.info(\"PREPROCESSING DONE in {}\", descriptor.getInputFile().getDescriptionFile());\n+\t\t}\n+\t\tcatch (Exception e) {\n+\t\t\tthrow new IllegalStateException(\"failed while processing line:\" + lineId + \" content:\" + Arrays.toString(row), e);\n+\t\t}\n \t}\n \n-\t/**\n-\t * Apply each output for a single row. Returning all resulting values.\n-\t */\n-\tprivate static Object[] applyOutputs(List<OutputDescription.Output> outputs, PPColumn[] columns, String[] row, long lineId)\n-\t\t\tthrows ParsingException, OutputDescription.OutputException {\n-\t\tObject[] outRow = new Object[outputs.size()];\n-\n-\t\tfor (int index = 0; index < outputs.size(); index++) {\n-\t\t\tfinal OutputDescription.Output out = outputs.get(index);\n+\tprivate Integer getPrimary(StringParser primaryType, String[] row, long lineId, int source, Output primaryOutput) {\n+\t\ttry {\n+\t\t\tList<Object> primary = primaryOutput.createOutput(primaryType, row, source, lineId);\n+\t\t\tif(primary.size()!=1 || !(primary.get(0) instanceof Integer)) {\n+\t\t\t\tthrow new IllegalStateException(\"The returned primary was the illegal value \"+primary+\" in \"+Arrays.toString(row));\n+\t\t\t}\n+\t\t\treturn (int)primary.get(0);\n+\t\t} catch (ParsingException e) {\n+\t\t\tlong errors = errorCounter.getAndIncrement();\n+\t\t\tif(errors<config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"Failed to parse primary from line:\"+lineId+\" content:\"+Arrays.toString(row), e);\n+\t\t\t}\n+\t\t\telse if(errors == config.getPreprocessor().getMaximumPrintedErrors()) {\n+\t\t\t\tlog.warn(\"More erroneous lines occurred. Only the first \"+config.getPreprocessor().getMaximumPrintedErrors()+\" were printed.\");\n+\t\t\t}\n+\t\t\treturn null;\n+\t\t}\n+\t}\n \n-\t\t\ttry {\n-\t\t\t\tfinal Parser<?> parser = columns[index].getParser();\n+\tprivate static List<Object[]> generateOutput(Input input, PPColumn[] columns, String[] row, int source, long lineId) throws ParsingException {\n+\t\tList<Object[]> resultRows = new ArrayList<>();\n+\t\tint oid = 0;\n+\t\tfor(int c = 0; c<input.getOutput().length; c++) {\n+\t\t\tOutput out = input.getOutput()[c];\n+\t\t\tParser<?> parser = columns[c].getParser();\n+\n+\t\t\tList<Object> result;\n+\t\t\tresult = out.createOutput(parser, row, source, lineId);\n+\t\t\tif(result==null) {\n+\t\t\t\tthrow new IllegalStateException(out+\" returned null result for \"+Arrays.toString(row));\n+\t\t\t}\n \n-\t\t\t\tfinal Object result = out.createOutput(row, parser, lineId);\n \n-\t\t\t\tif (result == null) {\n-\t\t\t\t\tcontinue;\n+\t\t\t//if the result is a single NULL and we don't want to include such rows\n+\t\t\tif(result.size()==1 && result.get(0)==null && out.isRequired()) {\n+\t\t\t\treturn Collections.emptyList();\n+\t\t\t}\n+\t\t\telse {\n+\t\t\t\tif(resultRows.isEmpty()) {\n+\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\tObject[] newRow = new Object[input.getOutput().length];\n+\t\t\t\t\t\tnewRow[oid]=v;\n+\t\t\t\t\t\tresultRows.add(newRow);\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\telse {\n+\t\t\t\t\tif(result.size()==1) {\n+\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\tresultRow[oid]=result.get(0);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t}\n+\t\t\t\t\telse {\n+\t\t\t\t\t\tList<Object[]> newResultRows = new ArrayList<>(resultRows.size()*result.size());\n+\t\t\t\t\t\tfor(Object v:result) {\n+\t\t\t\t\t\t\tfor(Object[] resultRow:resultRows) {\n+\t\t\t\t\t\t\t\tObject[] newResultRow = Arrays.copyOf(resultRow,resultRow.length);\n+\t\t\t\t\t\t\t\tnewResultRow[oid]=v;\n+\t\t\t\t\t\t\t\tnewResultRows.add(newResultRow);\n+\t\t\t\t\t\t\t}\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\tresultRows = newResultRows;\n+\t\t\t\t\t}\n \t\t\t\t}\n-\n-\t\t\t\toutRow[index] = result;\n-\t\t\t}catch (Exception e){\n-\t\t\t\tthrow new OutputDescription.OutputException(out.getDescription(), e);\n \t\t\t}\n+\t\t\toid++;\n \t\t}\n-\t\treturn outRow;\n+\t\treturn resultRows;\n \t}\n }\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDMxOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724319", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMaxValue() > maxValue) {\n          \n          \n            \n            \t\t\tmaxValue = v.getMaxValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t  maxValue = Math.max(maxValue,v.getMaxValue());", "author": "thoniTUB", "createdAt": "2020-05-18T15:48:16Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex bc9f3c1f8..c372ee96f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -32,19 +30,12 @@ public class DateRangeParser extends Parser<CDateRange> {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n+\t\tif(!v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n-\n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n-\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}\n-\n \t\tif(v.getMinValue() < minValue) {\n \t\t\tminValue = v.getMinValue();\n \t\t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDgxMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724811", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMinValue() < minValue) {\n          \n          \n            \n            \t\t\tminValue = v.getMinValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n                minValue = Math.min(minValue,v.getMinValue());", "author": "thoniTUB", "createdAt": "2020-05-18T15:48:58Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}\n+\n \t\tif(v.getMinValue() < minValue) {\n \t\t\tminValue = v.getMinValue();\n \t\t}", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex bc9f3c1f8..c372ee96f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -32,19 +30,12 @@ public class DateRangeParser extends Parser<CDateRange> {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n+\t\tif(!v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n-\n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n-\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}\n-\n \t\tif(v.getMinValue() < minValue) {\n \t\t\tminValue = v.getMinValue();\n \t\t}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczMjQ0OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426732448", "bodyText": "Ich bin mir nicht ganz sicher warum die Map jetzt synchronisiert ist, aber m\u00fcsstest du  dann hier nicht explizit synchronisieren? Du \u00e4nderst du die Referenz von string.", "author": "thoniTUB", "createdAt": "2020-05-18T15:59:12Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java", "diffHunk": "@@ -94,8 +94,8 @@ public Boolean transform(@NonNull Integer value) {\n \t\t//remove prefix and suffix\n \t\tif(!StringUtils.isEmpty(prefix) || !StringUtils.isEmpty(suffix)) {\n \t\t\tlog.debug(\"Reduced strings by the '{}' prefix and '{}' suffix\", prefix, suffix);\n-\t\t\tLinkedHashMap<String, Integer> oldStrings = strings;\n-\t\t\tstrings = new LinkedHashMap<>(oldStrings.size());\n+\t\t\tMap<String, Integer> oldStrings = strings;\n+\t\t\tstrings = Collections.synchronizedMap(new LinkedHashMap<>(oldStrings.size()));", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java\nindex 4336873f2..c59b2bedc 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java\n\n@@ -94,8 +94,8 @@ public class StringParser extends Parser<Integer> {\n \t\t//remove prefix and suffix\n \t\tif(!StringUtils.isEmpty(prefix) || !StringUtils.isEmpty(suffix)) {\n \t\t\tlog.debug(\"Reduced strings by the '{}' prefix and '{}' suffix\", prefix, suffix);\n-\t\t\tMap<String, Integer> oldStrings = strings;\n-\t\t\tstrings = Collections.synchronizedMap(new LinkedHashMap<>(oldStrings.size()));\n+\t\t\tLinkedHashMap<String, Integer> oldStrings = strings;\n+\t\t\tstrings = new LinkedHashMap<>(oldStrings.size());\n \t\t\tfor(Entry<String, Integer> e : oldStrings.entrySet()) {\n \t\t\t\tstrings.put(\n \t\t\t\t\te.getKey().substring(\n"}}, {"oid": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "url": "https://github.com/bakdata/conquery/commit/d1a3baba9a4e77fa65b38030bd7aee14bb739564", "message": "removes duplicates and adapts tests", "committedDate": "2020-05-20T12:48:24Z", "type": "commit"}, {"oid": "cb24d9914900201ea1cc0958fd15bd53ba53d9a5", "url": "https://github.com/bakdata/conquery/commit/cb24d9914900201ea1cc0958fd15bd53ba53d9a5", "message": "fix: resolves external queries prior to execution", "committedDate": "2020-05-20T12:59:43Z", "type": "commit"}, {"oid": "9bbf3c4dba3eea008544134385bf254dd0b4b4c6", "url": "https://github.com/bakdata/conquery/commit/9bbf3c4dba3eea008544134385bf254dd0b4b4c6", "message": "Temporary revert \"remove unused eventrange\"\n\nThis reverts commit 27edc0503ae00e70a8e133c620cc4a92f77fb746.\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "committedDate": "2020-05-20T14:25:43Z", "type": "commit"}, {"oid": "d72bb14732ee48ee034a070ce4e9871188402558", "url": "https://github.com/bakdata/conquery/commit/d72bb14732ee48ee034a070ce4e9871188402558", "message": "review changes", "committedDate": "2020-05-25T12:47:50Z", "type": "commit"}, {"oid": "d586a356f123f11c49c47ceb59d89a320922719a", "url": "https://github.com/bakdata/conquery/commit/d586a356f123f11c49c47ceb59d89a320922719a", "message": "Merge branch 'develop' into feature/clean-up-duplicate-test-methods", "committedDate": "2020-05-25T12:49:47Z", "type": "commit"}, {"oid": "f3b67a64738c3d513fbe19977b7431f9c1700830", "url": "https://github.com/bakdata/conquery/commit/f3b67a64738c3d513fbe19977b7431f9c1700830", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ManagedExecution.java\n#\tbackend/src/main/java/com/bakdata/conquery/resources/api/ResultCSVResource.java\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING_NO_RESTRICTION/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING_NO_RESTRICTION/NUMBER2.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_NEGATION/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/SUM_EMPTY_DATE_CONCEPT_QUERY/SUM_EMPTY_DATE_CONCEPT_QUERY.test.json\n#\tdocs/Config JSON.md", "committedDate": "2020-05-25T13:27:47Z", "type": "commit"}, {"oid": "4c7b2561cc629064508c72322ecea18302f53f6e", "url": "https://github.com/bakdata/conquery/commit/4c7b2561cc629064508c72322ecea18302f53f6e", "message": "Merge f3b67a64738c3d513fbe19977b7431f9c1700830 into 4b9eaa6c72030991fea3d667e185aa14896ae906", "committedDate": "2020-05-25T13:33:22Z", "type": "commit"}, {"oid": "4286eb9d22a1fd0a08d76de0fdb06b79cb9fedb1", "url": "https://github.com/bakdata/conquery/commit/4286eb9d22a1fd0a08d76de0fdb06b79cb9fedb1", "message": "automatic update to docs", "committedDate": "2020-05-25T13:35:12Z", "type": "commit"}, {"oid": "c681575242f11596cea919ee061dd2683aecd607", "url": "https://github.com/bakdata/conquery/commit/c681575242f11596cea919ee061dd2683aecd607", "message": "Fail Preprocessing when too many faulty lines", "committedDate": "2020-05-27T09:29:43Z", "type": "commit"}, {"oid": "cd509a27e028eb6397401ede75bd083eedf63cf4", "url": "https://github.com/bakdata/conquery/commit/cd509a27e028eb6397401ede75bd083eedf63cf4", "message": "Merge c681575242f11596cea919ee061dd2683aecd607 into ba996c3747ed3bf0e4854acde49a6ac81a366eb5", "committedDate": "2020-05-27T09:29:53Z", "type": "commit"}, {"oid": "fcb9796773f792166b38080374221725657ab7f1", "url": "https://github.com/bakdata/conquery/commit/fcb9796773f792166b38080374221725657ab7f1", "message": "automatic update to docs", "committedDate": "2020-05-27T09:31:54Z", "type": "commit"}, {"oid": "e830ad2ac457b6c5ace1604ec31f3b19b5e02bbd", "url": "https://github.com/bakdata/conquery/commit/e830ad2ac457b6c5ace1604ec31f3b19b5e02bbd", "message": "adds login for parser type", "committedDate": "2020-05-28T08:59:53Z", "type": "commit"}, {"oid": "a1e25840a474553edcd1b70b9de32a31a98692be", "url": "https://github.com/bakdata/conquery/commit/a1e25840a474553edcd1b70b9de32a31a98692be", "message": "remove to string method from Stringparser", "committedDate": "2020-05-28T09:11:29Z", "type": "commit"}, {"oid": "f1a25206648978ad900c6473f1f3f1b09a9c48a9", "url": "https://github.com/bakdata/conquery/commit/f1a25206648978ad900c6473f1f3f1b09a9c48a9", "message": "revert wrongly commited changes to FormConfigProcessor", "committedDate": "2020-05-28T09:18:15Z", "type": "commit"}, {"oid": "7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e", "url": "https://github.com/bakdata/conquery/commit/7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e", "message": "very aggressive tracing for values throughout Preprocessor", "committedDate": "2020-05-28T09:30:13Z", "type": "commit"}, {"oid": "d74da46abf66d1262bf497b8f7268cd05716a1ef", "url": "https://github.com/bakdata/conquery/commit/d74da46abf66d1262bf497b8f7268cd05716a1ef", "message": "Merge 7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e into ba996c3747ed3bf0e4854acde49a6ac81a366eb5", "committedDate": "2020-05-28T09:31:22Z", "type": "commit"}, {"oid": "4bb6e10e0a82057caf193ad7480590b8907d1f04", "url": "https://github.com/bakdata/conquery/commit/4bb6e10e0a82057caf193ad7480590b8907d1f04", "message": "automatic update to docs", "committedDate": "2020-05-28T09:34:02Z", "type": "commit"}, {"oid": "ff0cffa41bc8cbb3094e3c327755fa59b20d4769", "url": "https://github.com/bakdata/conquery/commit/ff0cffa41bc8cbb3094e3c327755fa59b20d4769", "message": "fix trying to apply logic when no max was present in DecimalParser::decideType", "committedDate": "2020-05-28T12:12:58Z", "type": "commit"}, {"oid": "ccecaf4fa615bfdac801fac25f9ffe2afdc3df74", "url": "https://github.com/bakdata/conquery/commit/ccecaf4fa615bfdac801fac25f9ffe2afdc3df74", "message": "Fix percentage formatting", "committedDate": "2020-05-28T12:13:16Z", "type": "commit"}, {"oid": "762619079483413c915ae0f0ca8139292615685d", "url": "https://github.com/bakdata/conquery/commit/762619079483413c915ae0f0ca8139292615685d", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-05-28T12:13:47Z", "type": "commit"}, {"oid": "77e41f9384297e943c872c66386cda3fc38e70e6", "url": "https://github.com/bakdata/conquery/commit/77e41f9384297e943c872c66386cda3fc38e70e6", "message": "Merge remote-tracking branch 'origin/feature/clean-up-duplicate-test-methods' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/test/java/com/bakdata/conquery/integration/common/IntegrationUtils.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/common/LoadingUtil.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/ConceptPermissionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ConceptUpdateAndDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/DatasetDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/TableDeletionTest.java", "committedDate": "2020-05-28T13:26:43Z", "type": "commit"}, {"oid": "6b35000ad99188054a8449c7b47b3e9d7d004ce0", "url": "https://github.com/bakdata/conquery/commit/6b35000ad99188054a8449c7b47b3e9d7d004ce0", "message": "make tests pass again: Fix headers/tables and fix *Descriptor usage", "committedDate": "2020-05-28T14:29:54Z", "type": "commit"}, {"oid": "41cdf2dbaae4c6250d45354929cb3551ed873477", "url": "https://github.com/bakdata/conquery/commit/41cdf2dbaae4c6250d45354929cb3551ed873477", "message": "Add better error/success logging", "committedDate": "2020-05-29T10:02:10Z", "type": "commit"}, {"oid": "46ec8ec578fd70849a0725d832e9aecde16cbd36", "url": "https://github.com/bakdata/conquery/commit/46ec8ec578fd70849a0725d832e9aecde16cbd36", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-05-29T10:07:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553000", "bodyText": "onlyQuarters = onlyQuarters && v.isSingleQuarter()", "author": "thoniTUB", "createdAt": "2020-06-03T13:08:56Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +34,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}", "originalCommit": "46ec8ec578fd70849a0725d832e9aecde16cbd36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1Mzk5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553994", "bodyText": "Ich wei\u00df nicht ob das assignment hier overhead machen w\u00fcrde", "author": "thoniTUB", "createdAt": "2020-06-03T13:10:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU2MzU5Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434563597", "bodyText": "branching ist teurer als assignment vermutlich", "author": "awildturtok", "createdAt": "2020-06-03T13:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}], "type": "inlineReview", "revised_code": {"commit": "b93bb899bb7b43dbe52b783770468e0f6e33dab4", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex 3e2699b3d..91cf9dac1 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -35,21 +35,12 @@ public class DateRangeParser extends Parser<CDateRange> {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyQuarters = onlyClosed && v.isOpen();\n \n-\t\tif(v.getMaxValue() > maxValue) {\n-\t\t\tmaxValue = v.getMaxValue();\n-\t\t}\n-\n-\t\tif(v.getMinValue() < minValue) {\n-\t\t\tminValue = v.getMinValue();\n-\t\t}\n+\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\tminValue = Math.min(minValue, v.getMinValue());\n \t}\n \n \tpublic static CDateRange parseISORange(String value) throws ParsingException {\n"}}, {"oid": "258986dd7b129bd64aa06d1b9edc653f383d9565", "url": "https://github.com/bakdata/conquery/commit/258986dd7b129bd64aa06d1b9edc653f383d9565", "message": "process multiple tags at once", "committedDate": "2020-06-11T11:05:26Z", "type": "commit"}, {"oid": "ab31e6a098ddd9e959ee5a73559d6820007c971b", "url": "https://github.com/bakdata/conquery/commit/ab31e6a098ddd9e959ee5a73559d6820007c971b", "message": "fix using char instead of string", "committedDate": "2020-06-11T11:23:08Z", "type": "commit"}, {"oid": "0fcd7802a66998e96fa7783a5b8c2bc8eeac68a8", "url": "https://github.com/bakdata/conquery/commit/0fcd7802a66998e96fa7783a5b8c2bc8eeac68a8", "message": "collect missing jobs separately", "committedDate": "2020-06-11T11:54:50Z", "type": "commit"}, {"oid": "d14c730811590fc98d11f52f411ffb9ec912de42", "url": "https://github.com/bakdata/conquery/commit/d14c730811590fc98d11f52f411ffb9ec912de42", "message": "rework logging to be less verbose by default", "committedDate": "2020-06-11T11:57:05Z", "type": "commit"}, {"oid": "2af20e77c97ef6da68a55ac435ad2f1c33ab04d7", "url": "https://github.com/bakdata/conquery/commit/2af20e77c97ef6da68a55ac435ad2f1c33ab04d7", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-16T12:05:09Z", "type": "commit"}, {"oid": "a3f153ca43a9461869afd9ca92a149df12193046", "url": "https://github.com/bakdata/conquery/commit/a3f153ca43a9461869afd9ca92a149df12193046", "message": "handle no tags as well", "committedDate": "2020-06-16T12:25:11Z", "type": "commit"}, {"oid": "648519966f13f1eb3441e5a6e27ed6e719fa730d", "url": "https://github.com/bakdata/conquery/commit/648519966f13f1eb3441e5a6e27ed6e719fa730d", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-06-16T12:25:48Z", "type": "commit"}, {"oid": "05a9da70f9349fa9f7f3ff232f3880c7c735e755", "url": "https://github.com/bakdata/conquery/commit/05a9da70f9349fa9f7f3ff232f3880c7c735e755", "message": "fixed not Preprocessing", "committedDate": "2020-06-16T13:35:45Z", "type": "commit"}, {"oid": "b93bb899bb7b43dbe52b783770468e0f6e33dab4", "url": "https://github.com/bakdata/conquery/commit/b93bb899bb7b43dbe52b783770468e0f6e33dab4", "message": "cleanup of code, simplifying branching", "committedDate": "2020-06-17T13:31:38Z", "type": "commit"}, {"oid": "16b001d7dc638113768c3151094eb81631e001c8", "url": "https://github.com/bakdata/conquery/commit/16b001d7dc638113768c3151094eb81631e001c8", "message": "cleanup of code, simplifying branching", "committedDate": "2020-06-17T13:32:11Z", "type": "commit"}, {"oid": "3b136800091730f996af907ed8803f7cb5af4757", "url": "https://github.com/bakdata/conquery/commit/3b136800091730f996af907ed8803f7cb5af4757", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "committedDate": "2020-06-17T13:32:25Z", "type": "commit"}, {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "url": "https://github.com/bakdata/conquery/commit/43af35c470e844bb2e08e9c44f149e6cee56be4c", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-17T13:57:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYyOTg1NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441629854", "bodyText": "Autoformat?", "author": "thoniTUB", "createdAt": "2020-06-17T15:21:04Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -56,7 +57,10 @@ public PreprocessorCommand(ExecutorService pool) {\n \tpublic void configure(Subparser subparser) {\n \t\tsuper.configure(subparser);\n \n-\t\tfinal ArgumentGroup group = subparser.addArgumentGroup(\"Preprocessing CLI Config\").description(\"Optional arguments to do a single import step by hand. Overrides json configuration.\");\n+\t\tfinal ArgumentGroup\n+\t\t\t\tgroup =", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNTAyNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441635027", "bodyText": "Ist das ! richtig? mein Brain macht gerade nicht mehr so richtig mit", "author": "thoniTUB", "createdAt": "2020-06-17T15:28:13Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b753a8030b5154d6f083a70fd3d930b923a58c39", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex acabcd2e5..54d5364df 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -36,11 +36,16 @@ public class DateRangeParser extends Parser<CDateRange> {\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n \n-\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n-\t\tonlyClosed = onlyClosed && v.isOpen();\n+\t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n+\t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\tif(!v.isAtLeast()) {\n+\t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\t}\n+\n+\t\tif(!v.isAtMost()) {\n+\t\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\t}\n \t}\n \n \tpublic static CDateRange parseISORange(String value) throws ParsingException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636232", "bodyText": "Und hier h\u00e4tte ich ein !v.isOpen() erwartet", "author": "thoniTUB", "createdAt": "2020-06-17T15:29:59Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyClosed = onlyClosed && v.isOpen();", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjcwNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636707", "bodyText": "Hast du einen Test hier f\u00fcr?", "author": "thoniTUB", "createdAt": "2020-06-17T15:30:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA0MjY3Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444042672", "bodyText": "Gute Idee!", "author": "awildturtok", "createdAt": "2020-06-23T08:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}], "type": "inlineReview", "revised_code": {"commit": "b753a8030b5154d6f083a70fd3d930b923a58c39", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex acabcd2e5..54d5364df 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -36,11 +36,16 @@ public class DateRangeParser extends Parser<CDateRange> {\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n \n-\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n-\t\tonlyClosed = onlyClosed && v.isOpen();\n+\t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n+\t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\tif(!v.isAtLeast()) {\n+\t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\t}\n+\n+\t\tif(!v.isAtMost()) {\n+\t\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\t}\n \t}\n \n \tpublic static CDateRange parseISORange(String value) throws ParsingException {\n"}}, {"oid": "b753a8030b5154d6f083a70fd3d930b923a58c39", "url": "https://github.com/bakdata/conquery/commit/b753a8030b5154d6f083a70fd3d930b923a58c39", "message": "Tests for DateRangeParser verifying handling of  open ranges and packing", "committedDate": "2020-06-23T14:00:57Z", "type": "commit"}, {"oid": "dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "url": "https://github.com/bakdata/conquery/commit/dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-23T14:02:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2OTYxNg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444269616", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(!v.isAtLeast()) {\n          \n          \n            \n            \t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \n          \n          \n            \n            \t\tif(!v.isAtMost()) {\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t\tif(!anyOpen) {\n          \n          \n            \n            \t\t        maxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}", "author": "thoniTUB", "createdAt": "2020-06-23T14:30:28Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -36,11 +36,16 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n \n-\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n-\t\tonlyClosed = onlyClosed && v.isOpen();\n+\t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n+\t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\tif(!v.isAtLeast()) {\n+\t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\t}\n+\n+\t\tif(!v.isAtMost()) {\n+\t\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\t}", "originalCommit": "dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b3895c68c9b581e00eae1fdb24581acfe77c5228", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex 54d5364df..cb39b6aff 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -26,34 +26,29 @@ public class DateRangeParser extends Parser<CDateRange> {\n \tprivate boolean anyOpen = false;\n \tprivate int maxValue = Integer.MIN_VALUE;\n \tprivate int minValue = Integer.MAX_VALUE;\n-\t\n+\n \t@Override\n \tprotected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\treturn DateRangeParser.parseISORange(value);\n \t}\n-\t\n+\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\t// test if value is already set to avoid expensive computation.\n-\n \t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n \t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tif(!v.isAtLeast()) {\n+\t\tif (!anyOpen) {\n \t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\t}\n-\n-\t\tif(!v.isAtMost()) {\n \t\t\tminValue = Math.min(minValue, v.getMinValue());\n \t\t}\n \t}\n \n \tpublic static CDateRange parseISORange(String value) throws ParsingException {\n-\t\tif(value==null) {\n+\t\tif (value == null) {\n \t\t\treturn null;\n \t\t}\n \t\tString[] parts = StringUtils.split(value, '/');\n-\t\tif(parts.length!=2) {\n+\t\tif (parts.length != 2) {\n \t\t\tthrow ParsingException.of(value, \"daterange\");\n \t\t}\n \n"}}, {"oid": "b3895c68c9b581e00eae1fdb24581acfe77c5228", "url": "https://github.com/bakdata/conquery/commit/b3895c68c9b581e00eae1fdb24581acfe77c5228", "message": "cleanup", "committedDate": "2020-06-23T14:34:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5MzU0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363193546", "bodyText": "K\u00f6nntest du java importe nach vor javax machen?", "author": "thoniTUB", "createdAt": "2020-01-06T08:15:30Z", "path": "autodoc/src/main/java/com/bakdata/conquery/Constants.java", "diffHunk": "@@ -16,6 +8,14 @@\n import javax.ws.rs.core.Context;\n import javax.ws.rs.core.Response;\n \n+import java.net.InetAddress;", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/autodoc/src/main/java/com/bakdata/conquery/Constants.java b/autodoc/src/main/java/com/bakdata/conquery/Constants.java\nindex bd4506ead..442aad300 100644\n--- a/autodoc/src/main/java/com/bakdata/conquery/Constants.java\n+++ b/autodoc/src/main/java/com/bakdata/conquery/Constants.java\n\n@@ -1,13 +1,5 @@\n package com.bakdata.conquery;\n \n-import javax.ws.rs.DELETE;\n-import javax.ws.rs.GET;\n-import javax.ws.rs.POST;\n-import javax.ws.rs.PUT;\n-import javax.ws.rs.PathParam;\n-import javax.ws.rs.core.Context;\n-import javax.ws.rs.core.Response;\n-\n import java.net.InetAddress;\n import java.nio.charset.Charset;\n import java.nio.file.Path;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5NDgyOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363194829", "bodyText": "umbenennen Jobs->Descriptiors", "author": "thoniTUB", "createdAt": "2020-01-06T08:20:48Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,8 +111,8 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<ImportDescriptor> findPreprocessingJobs(Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex 61eddb7f7..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -96,40 +106,56 @@ public class PreprocessorCommand extends ConqueryCommand {\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (ImportDescriptor descriptor : descriptors) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n \t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tPreprocessor.preprocess(totalProgress, descriptor);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n+\t\t}\n \t}\n \n-\tpublic static List<ImportDescriptor> findPreprocessingJobs(Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<ImportDescriptor> l = new ArrayList<>();\n+\tpublic List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\tFile inDir = description.getDescriptionsDir().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles(((dir, name) -> name.endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)))) {\n+\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(descr);\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);\n+\t\t\t\t\tfailed.add(file.getDescriptionFile().toString());\n \t\t\t\t}\n \t\t\t}\n \t\t}\n-\t\treturn l;\n+\t\treturn out;\n \t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363198412", "bodyText": "Muss die Klasse ein Threadlocal sein?\nKann man\nprivate final Set<DateTimeFormatter> formats = new HashSet<>();\nprivate DateTimeFormatter lastFormat;\t\tprivate DateTimeFormatter lastFormat;\n\nnicht auch static und threadsafe machen und dann ein Singleton machen?", "author": "thoniTUB", "createdAt": "2020-01-06T08:34:03Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java", "diffHunk": "@@ -11,27 +11,19 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.bakdata.conquery.models.config.ConqueryConfig;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n \n public class DateFormats {\n \n-\tprivate static ThreadLocal<DateFormats> INSTANCE;\n-\tprivate static String[] ADDITIONAL_FORMATS;\n+\tprivate static ThreadLocal<DateFormats> INSTANCE = ThreadLocal.withInitial(() -> new DateFormats(ConqueryConfig.getInstance().getAdditionalFormats()));", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzY5OTMwMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363699302", "bodyText": "Ich habs \u00fcberarbeitet. Das Threadlocal ist schon sinnvoll, dass es keine Contention darauf gibt. Synchronisierung darauf w\u00e4re nicht gut, weil das extra daf\u00fcr da ist schnell zu sein und nicht pr\u00e4zise", "author": "awildturtok", "createdAt": "2020-01-07T11:08:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java\ndeleted file mode 100644\nindex 36628ccaf..000000000\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java\n+++ /dev/null\n\n@@ -1,101 +0,0 @@\n-package com.bakdata.conquery.models.preproc;\n-\n-import java.time.LocalDate;\n-import java.time.format.DateTimeFormatter;\n-import java.time.format.DateTimeFormatterBuilder;\n-import java.time.format.DateTimeParseException;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.Locale;\n-import java.util.Map.Entry;\n-import java.util.Set;\n-import java.util.concurrent.ConcurrentHashMap;\n-\n-import com.bakdata.conquery.models.config.ConqueryConfig;\n-import com.bakdata.conquery.models.exceptions.ParsingException;\n-\n-public class DateFormats {\n-\n-\tprivate static ThreadLocal<DateFormats> INSTANCE = ThreadLocal.withInitial(() -> new DateFormats(ConqueryConfig.getInstance().getAdditionalFormats()));\n-\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n-\tprivate static final ConcurrentHashMap<String, LocalDate> DATE_CACHE = new ConcurrentHashMap<>(64000, 0.75f, 10);\n-\n-\tpublic static DateFormats instance() {\n-\t\treturn INSTANCE.get();\n-\t}\n-\n-\tprivate final Set<DateTimeFormatter> formats = new HashSet<>();\n-\tprivate DateTimeFormatter lastFormat;\n-\n-\tpublic DateFormats(String[] additionalFormats) {\n-\t\tformats.add(toFormat(\"yyyy-MM-dd\"));\n-\t\tformats.add(toFormat(\"ddMMMyyyy\"));\n-\t\tformats.add(toFormat(\"yyyyMMdd\"));\n-\t\tfor (String p : additionalFormats) {\n-\t\t\tformats.add(toFormat(p));\n-\t\t}\n-\t}\n-\n-\tprivate DateTimeFormatter toFormat(String pattern) {\n-\t\treturn new DateTimeFormatterBuilder().parseCaseInsensitive().appendPattern(pattern).toFormatter(Locale.US);\n-\t}\n-\n-\tprivate LocalDate tryParse(String value) {\n-\t\tif (lastFormat != null) {\n-\t\t\ttry {\n-\t\t\t\treturn LocalDate.parse(value, lastFormat);\n-\t\t\t}\n-\t\t\tcatch (DateTimeParseException e) {\n-\t\t\t\t//intentionally left blank\n-\t\t\t}\n-\t\t}\n-\t\tfor (DateTimeFormatter format : formats) {\n-\t\t\tif (lastFormat != format) {\n-\t\t\t\ttry {\n-\t\t\t\t\tLocalDate res = LocalDate.parse(value, format);\n-\t\t\t\t\tlastFormat = format;\n-\t\t\t\t\treturn res;\n-\t\t\t\t}\n-\t\t\t\tcatch (DateTimeParseException e) {\n-\t\t\t\t\t//intentionally left blank\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\treturn ERROR_DATE;\n-\t}\n-\n-\tpublic boolean isValidDate(String value) {\n-\t\ttry {\n-\t\t\tparseToLocalDate(value);\n-\t\t\treturn true;\n-\t\t} catch (ParsingException e) {\n-\t\t\treturn false;\n-\t\t}\n-\t}\n-\n-\tpublic LocalDate parseToLocalDate(String value) throws ParsingException {\n-\t\ttry {\n-\t\t\tLocalDate d = DATE_CACHE.computeIfAbsent(value, this::tryParse);\n-\t\t\t\n-\t\t\tif(DATE_CACHE.size()>64000) {\n-\t\t\t\tIterator<Entry<String, LocalDate>> it = DATE_CACHE.entrySet().iterator();\n-\t\t\t\tit.next();\n-\t\t\t\tit.remove();\n-\t\t\t}\n-\t\t\t\n-\t\t\tif(d!=ERROR_DATE) {\n-\t\t\t\treturn d;\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthrow ParsingException.of(value, \"date\");\n-\t\t\t}\n-\t\t} catch(Exception e) {\n-\t\t\tif(e instanceof ParsingException) {\n-\t\t\t\tthrow e;\n-\t\t\t}\n-\t\t\telse {\n-\t\t\t\tthrow ParsingException.of(value, \"date\", e);\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5OTg5OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363199898", "bodyText": "Doku und im namen verdeutlichen, dass die Klasse auf eine Column abzielt.", "author": "thoniTUB", "createdAt": "2020-01-06T08:39:05Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java", "diffHunk": "@@ -0,0 +1,56 @@\n+package com.bakdata.conquery.models.preproc.outputs;\n+\n+import java.io.Serializable;\n+import java.util.InputMismatchException;\n+import java.util.StringJoiner;\n+\n+import com.bakdata.conquery.io.cps.CPSBase;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.bakdata.conquery.models.preproc.ColumnDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.bakdata.conquery.models.types.parser.Parser;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonTypeInfo;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+@Data", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java\nindex a810598eb..bd120849a 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java\n\n@@ -2,6 +2,7 @@ package com.bakdata.conquery.models.preproc.outputs;\n \n import java.io.Serializable;\n import java.util.InputMismatchException;\n+import java.util.Objects;\n import java.util.StringJoiner;\n \n import com.bakdata.conquery.io.cps.CPSBase;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDM1OQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200359", "bodyText": "Klammern", "author": "thoniTUB", "createdAt": "2020-01-06T08:40:42Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -69,39 +68,49 @@ public boolean checkAutoOutput() {\n \t}\n \n \t@JsonIgnore\n-\t@ValidationMethod(message=\"The primary column must be of type STRING\")\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n \tpublic boolean isPrimaryString() {\n-\t\treturn primary.getResultType()==MajorTypeId.STRING;\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n \t}\n-\t\n-\tpublic boolean filter(String[] row) {\n-\t\tif(filter == null) {\n-\t\t\treturn true;\n-\t\t}\n-\t\telse {\n-\t\t\tif(script==null) {\n-\t\t\t\ttry {\n-\t\t\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n-\t\t\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n-\t\t\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n-\t\t\t\t\tGroovyShell groovy = new GroovyShell(config);\n-\t\t\t\t\t\n-\t\t\t\t\tscript = (GroovyPredicate) groovy.parse(filter);\n-\t\t\t\t} catch(Exception|Error e) {\n-\t\t\t\t\tthrow new RuntimeException(\"Failed to compile filter '\" + filter + \"'\", e);\n-\t\t\t\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null)", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\nsimilarity index 70%\nrename from backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java\nrename to backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\nindex 5e4ca5b16..58124fd6e 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n\n@@ -59,12 +85,7 @@ public class Input implements Serializable {\n \t@JsonIgnore\n \t@ValidationMethod(message = \"Outputs must not be empty\")\n \tpublic boolean isOutputsNotEmpty() {\n-\t\treturn checkAutoOutput() || (output != null && output.length > 0);\n-\t}\n-\n-\t@JsonIgnore\n-\tpublic boolean checkAutoOutput() {\n-\t\treturn autoOutput != null;\n+\t\treturn output != null && output.length > 0;\n \t}\n \n \t@JsonIgnore\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDY3MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200671", "bodyText": "InputTableDescriptor?", "author": "thoniTUB", "createdAt": "2020-01-06T08:41:46Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -1,60 +1,59 @@\n package com.bakdata.conquery.models.preproc;\n \n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n import java.io.File;\n import java.io.Serializable;\n import java.time.LocalDate;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n-import org.codehaus.groovy.control.CompilerConfiguration;\n-import org.codehaus.groovy.control.customizers.ImportCustomizer;\n-\n import com.bakdata.conquery.models.common.Range;\n import com.bakdata.conquery.models.exceptions.validators.ExistingFile;\n import com.bakdata.conquery.models.preproc.outputs.AutoOutput;\n-import com.bakdata.conquery.models.preproc.outputs.Output;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n import com.bakdata.conquery.models.types.MajorTypeId;\n import com.fasterxml.jackson.annotation.JsonIgnore;\n-\n import groovy.lang.GroovyShell;\n import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n \n @Data\n public class Input implements Serializable {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\nsimilarity index 70%\nrename from backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java\nrename to backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\nindex 5e4ca5b16..58124fd6e 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n\n@@ -1,17 +1,16 @@\n package com.bakdata.conquery.models.preproc;\n \n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n import java.io.File;\n import java.io.Serializable;\n import java.time.LocalDate;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n import com.bakdata.conquery.models.common.Range;\n-import com.bakdata.conquery.models.exceptions.validators.ExistingFile;\n-import com.bakdata.conquery.models.preproc.outputs.AutoOutput;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n import com.bakdata.conquery.models.types.MajorTypeId;\n import com.fasterxml.jackson.annotation.JsonIgnore;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDAzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210039", "bodyText": "Schau mal ob die Checks nicht schon von der CDateRange Klasse gemacht werden", "author": "thoniTUB", "createdAt": "2020-01-06T09:13:23Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -1,59 +1,58 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"EPOCH_DATE_RANGE\", base=Output.class)\n-public class EpochDateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"EPOCH_DATE_RANGE\", base = OutputDescription.class)\n+public class EpochDateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\nindex b0712596f..2e9bdf76a 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\n\n@@ -2,20 +2,21 @@ package com.bakdata.conquery.models.preproc.outputs;\n \n import javax.validation.constraints.NotNull;\n \n-import java.time.LocalDate;\n-\n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.bakdata.conquery.models.types.parser.Parser;\n+import com.google.common.base.Strings;\n import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import lombok.Getter;\n-import lombok.Setter;\n-import lombok.extern.slf4j.Slf4j;\n+import lombok.Data;\n+import lombok.ToString;\n \n-@Slf4j\n-@Getter\n-@Setter\n+/**\n+ * Parse input columns as {@link CDateRange}. Input values must be {@link com.bakdata.conquery.models.common.CDate} based ints.\n+ */\n+@Data\n+@ToString(of = {\"startColumn\", \"endColumn\"})\n @CPSType(id = \"EPOCH_DATE_RANGE\", base = OutputDescription.class)\n public class EpochDateRangeOutput extends OutputDescription {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210493", "bodyText": "Wie werden offene ranges dargestellt?", "author": "thoniTUB", "createdAt": "2020-01-06T09:14:45Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java", "diffHunk": "@@ -1,60 +1,60 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.preproc.DateFormats;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"DATE_RANGE\", base=Output.class)\n-public class DateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"DATE_RANGE\", base = OutputDescription.class)\n+public class DateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc3MTM1NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363771354", "bodyText": "bisher gar nicht", "author": "awildturtok", "createdAt": "2020-01-07T14:26:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java\nindex 078cb9914..79edb5a94 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java\n\n@@ -1,22 +1,25 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import javax.validation.constraints.NotNull;\n-\n import java.time.LocalDate;\n \n+import javax.validation.constraints.NotNull;\n+\n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n-import com.bakdata.conquery.models.preproc.DateFormats;\n import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.bakdata.conquery.models.types.parser.Parser;\n+import com.bakdata.conquery.util.DateFormats;\n+import com.google.common.base.Strings;\n import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n-import lombok.Getter;\n-import lombok.Setter;\n-import lombok.extern.slf4j.Slf4j;\n-\n-@Slf4j\n-@Getter\n-@Setter\n+import lombok.Data;\n+import lombok.ToString;\n+\n+/**\n+ * Parse input columns as {@link CDateRange}. Input values must be {@link com.bakdata.conquery.models.common.CDate} based ints.\n+ */\n+@Data\n+@ToString(of = {\"startColumn\", \"endColumn\"})\n @CPSType(id = \"DATE_RANGE\", base = OutputDescription.class)\n public class DateRangeOutput extends OutputDescription {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0MzEzMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363343132", "bodyText": "copyOutput von QueryTest.java wiederverwenden", "author": "thoniTUB", "createdAt": "2020-01-06T15:31:10Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -142,9 +142,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\t}\n \t}\n \n-\tprivate Output copyOutput(int columnPosition, RequiredColumn column) {\n+\tprivate OutputDescription copyOutput(RequiredColumn column) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\nindex acdf08fb7..c2b9e94f1 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n\n@@ -90,65 +77,16 @@ public class FilterTest extends AbstractQueryEngineTest {\n \t\timportTables(support);\n \t\tsupport.waitUntilWorkDone();\n \n+\n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\t\n \t\tquery = parseQuery(support);\n \n-\t\timportTableContents(support);\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t}\n \n-\tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n \n-\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);\n-\n-\t\tList<File> preprocessedFiles = new ArrayList<>();\n-\n-\t\tfor (RequiredTable rTable : content.getTables()) {\n-\t\t\t//copy csv to tmp folder\n-\t\t\tString name = rTable.getCsv().getName().substring(0, rTable.getCsv().getName().lastIndexOf('.'));\n-\t\t\tFileUtils.copyInputStreamToFile(rTable.getCsv().stream(), new File(support.getTmpDir(), rTable.getCsv().getName()));\n-\n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(support.getConfig().getPreprocessor().getDirectories()[0], name);\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(rTable.getName() + \"_import\");\n-\t\t\tdesc.setTable(rTable.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(copyOutput(rTable.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), rTable.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new OutputDescription[rTable.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < rTable.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = copyOutput(rTable.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n-\t\t\tpreprocessedFiles.add(inputFile.getPreprocessedFile());\n-\t\t}\n-\t\t//preprocess\n-\t\tsupport.preprocessTmp();\n-\n-\t\t//import preprocessedFiles\n-\t\tfor (File file : preprocessedFiles) {\n-\t\t\tsupport.getDatasetsProcessor().addImport(support.getDataset(), file);\n-\t\t}\n-\t}\n-\n-\tprivate OutputDescription copyOutput(RequiredColumn column) {\n-\t\tCopyOutput out = new CopyOutput();\n-\t\tout.setInputColumn(column.getName());\n-\t\tout.setInputType(column.getType());\n-\t\tout.setName(column.getName());\n-\t\treturn out;\n-\t}\n \n \tprivate void importConcepts(StandaloneSupport support) throws JSONException, IOException, ConfigurationException {\n \t\tDataset dataset = support.getDataset();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDUzMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344530", "bodyText": "Werden die settings gebraucht?", "author": "thoniTUB", "createdAt": "2020-01-06T15:34:04Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -135,7 +135,7 @@ public void importTableContents(StandaloneSupport support, Collection<RequiredTa\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\nindex 1e648f129..aa2a01e8d 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n\n@@ -73,141 +42,23 @@ public class QueryTest extends AbstractQueryEngineTest {\n \t@JsonIgnore\n \tprivate IQuery query;\n \n+\t@Override\n+\tpublic IQuery getQuery() {\n+\t\treturn query;\n+\t}\n+\n \t@Override\n \tpublic void importRequiredData(StandaloneSupport support) throws IOException, JSONException, ConfigurationException {\n-\t\timportTables(support);\n+\t\tIntegrationUtils.importTables(support, content);\n \t\tsupport.waitUntilWorkDone();\n \n-\t\timportConcepts(support);\n+\t\tIntegrationUtils.importConcepts(support, rawConcepts);\n \t\tsupport.waitUntilWorkDone();\n-\t\tquery = parseQuery(support);\n+\t\tquery = IntegrationUtils.parseQuery(support, rawQuery);\n \n-\t\timportTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n-\t\timportIdMapping(support);\n-\t\timportPreviousQueries(support);\n-\t}\n-\n-\tpublic void importIdMapping(StandaloneSupport support) throws JSONException, IOException {\n-\t\tif(content.getIdMapping() == null) {\n-\t\t\treturn;\n-\t\t}\n-\t\ttry(InputStream in = content.getIdMapping().stream()) {\n-\t\t\tsupport.getDatasetsProcessor().setIdMapping(in, support.getNamespace());\n-\t\t}\n-\t}\n-\tpublic void importPreviousQueries(StandaloneSupport support) throws JSONException, IOException {\n-\t\t// Load previous query results if available\n-\t\tint id = 1;\n-\t\tfor(ResourceFile queryResults : content.getPreviousQueryResults()) {\n-\t\t\tUUID queryId = new UUID(0L, id++);\n-\n-\t\t\t//Just read the file without parsing headers etc.\n-\t\t\tCsvParserSettings parserSettings = support.getConfig().getCsv()\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .withParseHeaders(false)\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .withSkipHeader(false)\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .createCsvParserSettings();\n-\n-\t\t\tCsvParser parser = new CsvParser(parserSettings);\n-\n-\t\t\tString[][] data = parser.parseAll(queryResults.stream()).toArray(String[][]::new);\n-\n-\t\t\tConceptQuery q = new ConceptQuery();\n-\t\t\tq.setRoot(new CQExternal(Arrays.asList(FormatColumn.ID, FormatColumn.DATE_SET), data));\n-\t\t\t\n-\t\t\tManagedExecution managed = support.getNamespace().getQueryManager().runQuery(q, queryId, DevAuthConfig.USER);\n-\t\t\tmanaged.awaitDone(1, TimeUnit.DAYS);\n-\n-\t\t\tif (managed.getState() == ExecutionState.FAILED) {\n-\t\t\t\tfail(\"Query failed\");\n-\t\t\t}\n-\t\t}\n-\n-\t\t//wait only if we actually did anything\n-\t\tif(!content.getPreviousQueryResults().isEmpty()) {\n-\t\t\tsupport.waitUntilWorkDone();\n-\t\t}\n-\t}\n-\n-\tpublic void importTableContents(StandaloneSupport support, Collection<RequiredTable> tables, Dataset dataset) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n-\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);\n-\t\tList<File> preprocessedFiles = new ArrayList<>();\n-\n-\t\tfor (RequiredTable rTable : tables) {\n-\t\t\t//copy csv to tmp folder\n-\t\t\tString name = rTable.getCsv().getName().substring(0, rTable.getCsv().getName().lastIndexOf('.'));\n-\t\t\tFileUtils.copyInputStreamToFile(rTable.getCsv().stream(), new File(support.getTmpDir(), rTable.getCsv().getName()));\n-\n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(support.getConfig().getPreprocessor().getDirectories()[0], name);\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(rTable.getName() + \"_import\");\n-\t\t\tdesc.setTable(rTable.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(copyOutput(rTable.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), rTable.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new OutputDescription[rTable.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < rTable.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = copyOutput(rTable.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n-\t\t\tpreprocessedFiles.add(inputFile.getPreprocessedFile());\n-\t\t}\n-\t\t//preprocess\n-\t\tsupport.preprocessTmp();\n-\n-\t\t//import preprocessedFiles\n-\t\tfor (File file : preprocessedFiles) {\n-\t\t\tsupport.getDatasetsProcessor().addImport(dataset, file);\n-\t\t}\n-\t}\n-\n-\tpublic static OutputDescription copyOutput(RequiredColumn column) {\n-\t\tCopyOutput out = new CopyOutput();\n-\t\tout.setInputColumn(column.getName());\n-\t\tout.setInputType(column.getType());\n-\t\tout.setName(column.getName());\n-\t\treturn out;\n-\t}\n-\n-\tpublic void importConcepts(StandaloneSupport support) throws JSONException, IOException, ConfigurationException {\n-\t\tDataset dataset = support.getDataset();\n-\n-\t\tList<Concept<?>> concepts = parseSubTree(\n-\t\t\t\tsupport,\n-\t\t\t\trawConcepts,\n-\t\t\t\tJackson.MAPPER.getTypeFactory().constructParametricType(List.class, Concept.class),\n-\t\t\t\tlist -> list.forEach(c -> c.setDataset(support.getDataset().getId()))\n-\t\t);\n-\n-\t\tfor (Concept<?> concept : concepts) {\n-\t\t\tsupport.getDatasetsProcessor().addConcept(dataset, concept);\n-\t\t}\n-\t}\n-\n-\tpublic IQuery parseQuery(StandaloneSupport support) throws JSONException, IOException {\n-\t\treturn parseSubTree(support, rawQuery, IQuery.class);\n-\t}\n-\n-\t@Override\n-\tpublic IQuery getQuery() {\n-\t\treturn query;\n-\t}\n-\n-\tpublic void importTables(StandaloneSupport support) throws JSONException {\n-\t\tDataset dataset = support.getDataset();\n-\n-\t\tfor (RequiredTable rTable : content.getTables()) {\n-\t\t\tsupport.getDatasetsProcessor().addTable(dataset, rTable.toTable());\n-\t\t}\n+\t\tIntegrationUtils.importIdMapping(support, content.getIdMapping());\n+\t\tIntegrationUtils.importPreviousQueries(support, content);\n \t}\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDcxMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344712", "bodyText": "Again: Werden die settings gebraucht?", "author": "thoniTUB", "createdAt": "2020-01-06T15:34:28Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -106,7 +104,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\nindex acdf08fb7..c2b9e94f1 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n\n@@ -90,65 +77,16 @@ public class FilterTest extends AbstractQueryEngineTest {\n \t\timportTables(support);\n \t\tsupport.waitUntilWorkDone();\n \n+\n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\t\n \t\tquery = parseQuery(support);\n \n-\t\timportTableContents(support);\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t}\n \n-\tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n \n-\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);\n-\n-\t\tList<File> preprocessedFiles = new ArrayList<>();\n-\n-\t\tfor (RequiredTable rTable : content.getTables()) {\n-\t\t\t//copy csv to tmp folder\n-\t\t\tString name = rTable.getCsv().getName().substring(0, rTable.getCsv().getName().lastIndexOf('.'));\n-\t\t\tFileUtils.copyInputStreamToFile(rTable.getCsv().stream(), new File(support.getTmpDir(), rTable.getCsv().getName()));\n-\n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(support.getConfig().getPreprocessor().getDirectories()[0], name);\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(rTable.getName() + \"_import\");\n-\t\t\tdesc.setTable(rTable.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(copyOutput(rTable.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), rTable.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new OutputDescription[rTable.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < rTable.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = copyOutput(rTable.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n-\t\t\tpreprocessedFiles.add(inputFile.getPreprocessedFile());\n-\t\t}\n-\t\t//preprocess\n-\t\tsupport.preprocessTmp();\n-\n-\t\t//import preprocessedFiles\n-\t\tfor (File file : preprocessedFiles) {\n-\t\t\tsupport.getDatasetsProcessor().addImport(support.getDataset(), file);\n-\t\t}\n-\t}\n-\n-\tprivate OutputDescription copyOutput(RequiredColumn column) {\n-\t\tCopyOutput out = new CopyOutput();\n-\t\tout.setInputColumn(column.getName());\n-\t\tout.setInputType(column.getType());\n-\t\tout.setName(column.getName());\n-\t\treturn out;\n-\t}\n \n \tprivate void importConcepts(StandaloneSupport support) throws JSONException, IOException, ConfigurationException {\n \t\tDataset dataset = support.getDataset();\n"}}, {"oid": "39d30b1d83a2323daf59774326ed09b684420983", "url": "https://github.com/bakdata/conquery/commit/39d30b1d83a2323daf59774326ed09b684420983", "message": "header based imports", "committedDate": "2020-01-27T13:42:00Z", "type": "commit"}, {"oid": "77ffaa048d4bb250c1ecb9ed057c777d91adce4c", "url": "https://github.com/bakdata/conquery/commit/77ffaa048d4bb250c1ecb9ed057c777d91adce4c", "message": "forced line endings to linux style", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "3614694bfa2ba1986834ab0481189e0441fc4052", "url": "https://github.com/bakdata/conquery/commit/3614694bfa2ba1986834ab0481189e0441fc4052", "message": "fixed headers for CQEXTERNAL test.", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "a5df13441070cee39dab3ea5a45dfafe74a67a6a", "url": "https://github.com/bakdata/conquery/commit/a5df13441070cee39dab3ea5a45dfafe74a67a6a", "message": "code cleanup", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "bfad8cb99ea1e75c4ee0f53efc766e802bf7f605", "url": "https://github.com/bakdata/conquery/commit/bfad8cb99ea1e75c4ee0f53efc766e802bf7f605", "message": "made script column name based", "committedDate": "2020-01-27T13:42:03Z", "type": "commit"}, {"oid": "ce47c5fa49093522d1d068383567178c5f85946d", "url": "https://github.com/bakdata/conquery/commit/ce47c5fa49093522d1d068383567178c5f85946d", "message": "cleanup and decoupling of code.", "committedDate": "2020-01-27T13:42:39Z", "type": "commit"}, {"oid": "ee461aebe00bada7a7a3d683b4b753152e74134e", "url": "https://github.com/bakdata/conquery/commit/ee461aebe00bada7a7a3d683b4b753152e74134e", "message": "refactoring away from Preprocessor class", "committedDate": "2020-01-27T13:42:41Z", "type": "commit"}, {"oid": "33cc55cceed5ed1e54e312e3152f14d12ed158cf", "url": "https://github.com/bakdata/conquery/commit/33cc55cceed5ed1e54e312e3152f14d12ed158cf", "message": "fixed creation of InputFile", "committedDate": "2020-01-27T13:42:42Z", "type": "commit"}, {"oid": "66c643afddfa54fa8ba2117e1121c4990ae7f2ea", "url": "https://github.com/bakdata/conquery/commit/66c643afddfa54fa8ba2117e1121c4990ae7f2ea", "message": "refactoring of Output into OutputDescription (as API-layer class) and OutputDescription.Output as transformation.", "committedDate": "2020-01-27T13:42:57Z", "type": "commit"}, {"oid": "cc6f08647e13d7544296207b54ae91eada8cf942", "url": "https://github.com/bakdata/conquery/commit/cc6f08647e13d7544296207b54ae91eada8cf942", "message": "null tests for groovy predicate filter", "committedDate": "2020-01-27T13:42:59Z", "type": "commit"}, {"oid": "5a350e05beecbc9a7684e87a4b0dcf9c0bffeeb3", "url": "https://github.com/bakdata/conquery/commit/5a350e05beecbc9a7684e87a4b0dcf9c0bffeeb3", "message": "refactoring away from static initializer of DebugMode.java", "committedDate": "2020-01-27T13:43:17Z", "type": "commit"}, {"oid": "22bad57274ccce24730bf08a9514d05b3b8802e8", "url": "https://github.com/bakdata/conquery/commit/22bad57274ccce24730bf08a9514d05b3b8802e8", "message": "remove ExecutorService from Preprocessor and undo importing by upload", "committedDate": "2020-01-27T13:43:19Z", "type": "commit"}, {"oid": "ee9231e573f9ca58a2ca392403e3ab4476309d1e", "url": "https://github.com/bakdata/conquery/commit/ee9231e573f9ca58a2ca392403e3ab4476309d1e", "message": "automatic update to docs", "committedDate": "2020-01-27T13:43:19Z", "type": "commit"}, {"oid": "37863080692e07c0998b6f75187d9feb690aae0a", "url": "https://github.com/bakdata/conquery/commit/37863080692e07c0998b6f75187d9feb690aae0a", "message": "removed outputting of multiple lines as that is no longer used", "committedDate": "2020-01-27T13:43:20Z", "type": "commit"}, {"oid": "4252e36a083bbf94b7220574a93beff0312e0807", "url": "https://github.com/bakdata/conquery/commit/4252e36a083bbf94b7220574a93beff0312e0807", "message": "refactored DateFormats to be more atomic and concise", "committedDate": "2020-01-27T13:43:20Z", "type": "commit"}, {"oid": "f3e8bb7f79ac831afc37ad1501dc96128ab04984", "url": "https://github.com/bakdata/conquery/commit/f3e8bb7f79ac831afc37ad1501dc96128ab04984", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:24Z", "type": "commit"}, {"oid": "ff667a8e156aca392d61a921c3722167000de11c", "url": "https://github.com/bakdata/conquery/commit/ff667a8e156aca392d61a921c3722167000de11c", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:31Z", "type": "commit"}, {"oid": "069459f3e601cb7449b2840bec3ee7bac022ef70", "url": "https://github.com/bakdata/conquery/commit/069459f3e601cb7449b2840bec3ee7bac022ef70", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:31Z", "type": "commit"}, {"oid": "bc97453e6b3ac9f8fc6e4585aef4b69a581487d6", "url": "https://github.com/bakdata/conquery/commit/bc97453e6b3ac9f8fc6e4585aef4b69a581487d6", "message": "automatic update to docs", "committedDate": "2020-01-27T13:44:32Z", "type": "commit"}, {"oid": "401473392cd2052325606e77cb79bf44698c4c8d", "url": "https://github.com/bakdata/conquery/commit/401473392cd2052325606e77cb79bf44698c4c8d", "message": "removed unused settings", "committedDate": "2020-01-27T13:45:05Z", "type": "commit"}, {"oid": "d33c8227974e89420ad64e24c0a67e03a63a9b65", "url": "https://github.com/bakdata/conquery/commit/d33c8227974e89420ad64e24c0a67e03a63a9b65", "message": "automatic update to docs", "committedDate": "2020-01-27T13:45:08Z", "type": "commit"}, {"oid": "d4d973c49db79813d39f471ac9956e1d26460b9f", "url": "https://github.com/bakdata/conquery/commit/d4d973c49db79813d39f471ac9956e1d26460b9f", "message": "post rebase fixes", "committedDate": "2020-01-27T13:56:46Z", "type": "commit"}, {"oid": "d4d973c49db79813d39f471ac9956e1d26460b9f", "url": "https://github.com/bakdata/conquery/commit/d4d973c49db79813d39f471ac9956e1d26460b9f", "message": "post rebase fixes", "committedDate": "2020-01-27T13:56:46Z", "type": "forcePushed"}, {"oid": "67d14fd4cd27b777a5f42fa03e02eda5b1343d93", "url": "https://github.com/bakdata/conquery/commit/67d14fd4cd27b777a5f42fa03e02eda5b1343d93", "message": "dont track xodus files", "committedDate": "2020-01-27T17:01:33Z", "type": "commit"}, {"oid": "302066ec07be14a40eb16ad5eef5e7c4080f2aaa", "url": "https://github.com/bakdata/conquery/commit/302066ec07be14a40eb16ad5eef5e7c4080f2aaa", "message": "added removed required field", "committedDate": "2020-01-29T12:01:34Z", "type": "commit"}, {"oid": "336059e079bb7b9e808e9921b05b64644778024c", "url": "https://github.com/bakdata/conquery/commit/336059e079bb7b9e808e9921b05b64644778024c", "message": "handle required", "committedDate": "2020-01-29T13:52:53Z", "type": "commit"}, {"oid": "b60eb068989746776e1ef3d75290fd002c4a8c92", "url": "https://github.com/bakdata/conquery/commit/b60eb068989746776e1ef3d75290fd002c4a8c92", "message": "properly closing the CSV-Parser and better defaults", "committedDate": "2020-02-11T08:51:44Z", "type": "commit"}, {"oid": "4f5830654dac0305e11c7d769092be2cc3bb85db", "url": "https://github.com/bakdata/conquery/commit/4f5830654dac0305e11c7d769092be2cc3bb85db", "message": "Merge remote-tracking branch 'remotes/origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/apiv1/ResultCSVResource.java", "committedDate": "2020-02-11T08:53:23Z", "type": "commit"}, {"oid": "a4232cc294da2ae97b7b720c10003ffc1a6b7869", "url": "https://github.com/bakdata/conquery/commit/a4232cc294da2ae97b7b720c10003ffc1a6b7869", "message": "fixed constructors", "committedDate": "2020-02-11T09:12:21Z", "type": "commit"}, {"oid": "f182da3a6e35b983113639586c9417ec9a76d998", "url": "https://github.com/bakdata/conquery/commit/f182da3a6e35b983113639586c9417ec9a76d998", "message": "fixed name of column to inputColumn", "committedDate": "2020-02-11T10:57:19Z", "type": "commit"}, {"oid": "787c534a006b94e228e5a7db2a04f2a37880bea2", "url": "https://github.com/bakdata/conquery/commit/787c534a006b94e228e5a7db2a04f2a37880bea2", "message": "improved error logging for Preprocessor", "committedDate": "2020-02-24T15:53:20Z", "type": "commit"}, {"oid": "1463c3847447b8e9add88065548c0d4abfa28496", "url": "https://github.com/bakdata/conquery/commit/1463c3847447b8e9add88065548c0d4abfa28496", "message": "Added ToString annotations", "committedDate": "2020-02-24T16:12:56Z", "type": "commit"}, {"oid": "fca7139ec5deaefcfd527bd8f9fa389a6776fd5c", "url": "https://github.com/bakdata/conquery/commit/fca7139ec5deaefcfd527bd8f9fa389a6776fd5c", "message": "force error output", "committedDate": "2020-02-24T16:20:36Z", "type": "commit"}, {"oid": "9aae94c4ac29e0d43b8a5010522126181081793e", "url": "https://github.com/bakdata/conquery/commit/9aae94c4ac29e0d43b8a5010522126181081793e", "message": "fixed not giving out proper source", "committedDate": "2020-02-24T16:25:23Z", "type": "commit"}, {"oid": "54046deecbf1eccd4181a6adf726f5578422fbc2", "url": "https://github.com/bakdata/conquery/commit/54046deecbf1eccd4181a6adf726f5578422fbc2", "message": "allow open EpochDateRangeOutput", "committedDate": "2020-03-02T12:24:43Z", "type": "commit"}, {"oid": "d9994993f932a9aa1bb151813afd353989dda3e5", "url": "https://github.com/bakdata/conquery/commit/d9994993f932a9aa1bb151813afd353989dda3e5", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\t.gitignore\n#\tbackend/src/main/java/com/bakdata/conquery/models/config/ConqueryConfig.java\n#\tbackend/src/main/java/com/bakdata/conquery/models/query/concept/specific/CQExternal.java", "committedDate": "2020-03-02T12:33:57Z", "type": "commit"}, {"oid": "fa5d42a320ddda0b973ffafe5373cbfb298f4a8d", "url": "https://github.com/bakdata/conquery/commit/fa5d42a320ddda0b973ffafe5373cbfb298f4a8d", "message": "fixed merge", "committedDate": "2020-03-02T12:41:36Z", "type": "commit"}, {"oid": "4c27ba7933fec89b8c7910bdadbdf3ddd4215582", "url": "https://github.com/bakdata/conquery/commit/4c27ba7933fec89b8c7910bdadbdf3ddd4215582", "message": "added more info when failing", "committedDate": "2020-03-02T13:55:26Z", "type": "commit"}, {"oid": "079dd8d74aa8575c2b76ad3ac14ac524c3db8256", "url": "https://github.com/bakdata/conquery/commit/079dd8d74aa8575c2b76ad3ac14ac524c3db8256", "message": "delete now unused DateRangeOutput.java", "committedDate": "2020-03-02T14:01:41Z", "type": "commit"}, {"oid": "1aa304a6ef8a18e01af0cd0a1d88ec315a73b1a9", "url": "https://github.com/bakdata/conquery/commit/1aa304a6ef8a18e01af0cd0a1d88ec315a73b1a9", "message": "introduction of tags for imports as CLI param", "committedDate": "2020-03-02T15:19:43Z", "type": "commit"}, {"oid": "78aeb4800307d9bf533b831631c428d1bc05103d", "url": "https://github.com/bakdata/conquery/commit/78aeb4800307d9bf533b831631c428d1bc05103d", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/config/ConqueryConfig.java", "committedDate": "2020-03-02T15:39:34Z", "type": "commit"}, {"oid": "6f924a958583af2253f6bd34602563139eb0443d", "url": "https://github.com/bakdata/conquery/commit/6f924a958583af2253f6bd34602563139eb0443d", "message": "Merge 78aeb4800307d9bf533b831631c428d1bc05103d into 242513d2a0a3f57200b7c97de7ea35866b6ae12f", "committedDate": "2020-03-02T15:39:50Z", "type": "commit"}, {"oid": "47260d5177b4a83f0461c3af23ad3ecdfb8c6011", "url": "https://github.com/bakdata/conquery/commit/47260d5177b4a83f0461c3af23ad3ecdfb8c6011", "message": "automatic update to docs", "committedDate": "2020-03-02T15:41:52Z", "type": "commit"}, {"oid": "56dc230c243df657a0273d83ecdb4f2bd6d12eda", "url": "https://github.com/bakdata/conquery/commit/56dc230c243df657a0273d83ecdb4f2bd6d12eda", "message": "changed mdc to contain csv file name instead of index.", "committedDate": "2020-03-05T14:53:14Z", "type": "commit"}, {"oid": "d1dcd98f15e3eec4e8198be7e28479667cf561f9", "url": "https://github.com/bakdata/conquery/commit/d1dcd98f15e3eec4e8198be7e28479667cf561f9", "message": "changed mdc to contain csv file name instead of index.", "committedDate": "2020-03-05T14:53:42Z", "type": "commit"}, {"oid": "1745a5e36cbd3cdb4b832c72ebae9d06c7fa3b62", "url": "https://github.com/bakdata/conquery/commit/1745a5e36cbd3cdb4b832c72ebae9d06c7fa3b62", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "committedDate": "2020-03-05T14:54:35Z", "type": "commit"}, {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "url": "https://github.com/bakdata/conquery/commit/a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "message": "only file name not full path", "committedDate": "2020-03-05T16:53:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394912673", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen.\nOder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.", "author": "thoniTUB", "createdAt": "2020-03-19T10:03:43Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(new Preprocessor(config, descr));\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDk1MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395000950", "bodyText": "Oder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.\n\nGef\u00e4llt mir!", "author": "awildturtok", "createdAt": "2020-03-19T12:50:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTUwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395001500", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen\n\nFrage ist wie man das kommuniziert, wir kriegen ja selber auch nicht so die guten Exceptions. Ich k\u00f6nnte eine Map machen, die einfach z\u00e4hlt wie of welche Exception kam und dann das loggen?", "author": "awildturtok", "createdAt": "2020-03-19T12:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex a8ac057ee..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -112,24 +113,30 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n+\t\t}\n \t}\n \n-\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\tpublic List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {\n \t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n \n-\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\t\t\tFile inDir = description.getDescriptionsDir().getAbsoluteFile();\n \n-\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n+\t\t\tfor (File descriptionFile : inDir.listFiles(((dir, name) -> name.endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)))) {\n \n \t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394914288", "bodyText": "Dieses stille \u00dcberspringen kann zu sneaky Fehlern f\u00fchren.", "author": "thoniTUB", "createdAt": "2020-03-19T10:06:29Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMjQzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395002439", "bodyText": "Ja, das Problem ist so ein bisschen wie wir weiter gehen wollen. An und f\u00fcr sich ist das okay, weil das einfach ein contract ist. Aber ich w\u00fcnsche mir f\u00fcr diese stelle auch sowas wie globbing (wie oben angemerkt) dann k\u00f6nnen wir n\u00e4mlich in zukunft deutlich selektiver preprocessen ohne immer den ganzen stand ordentlich vorhalten zu m\u00fcssen etc. Bin mir noch nicht sicher ob das ne gute idee ist aber sieht man dann ja.", "author": "awildturtok", "createdAt": "2020-03-19T12:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex a8ac057ee..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -112,24 +113,30 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n+\t\t}\n \t}\n \n-\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\tpublic List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {\n \t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n \n-\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\t\t\tFile inDir = description.getDescriptionsDir().getAbsoluteFile();\n \n-\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n+\t\t\tfor (File descriptionFile : inDir.listFiles(((dir, name) -> name.endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)))) {\n \n \t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394969993", "bodyText": "Das ist etwas gef\u00e4hrlich davon auszugehen, dass nicht schon vorher punkte drinne sind", "author": "thoniTUB", "createdAt": "2020-03-19T11:50:40Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMzU0NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395003544", "bodyText": "hm, was w\u00e4re dein vorschlag? Eigentlich sollte es das vorletzte sein\nalso table.tag.csv.gz oder table.dead.tag.csv.gz aber nicht so einfach.", "author": "awildturtok", "createdAt": "2020-03-19T12:55:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\nindex a73f93e48..7e0dfe57b 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n\n@@ -34,6 +34,7 @@ import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3MzkyNg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394973926", "bodyText": "Macht ein Builder hier Sinn? Funktioniert das auch wenn man einen Member nicht setzen w\u00fcrde?", "author": "thoniTUB", "createdAt": "2020-03-19T11:58:45Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {\n+\t\t// Write content to file\n+\t\tImport imp = Import.createForPreprocessing(descriptor.getTable(), descriptor.getName(), columns);\n+\n+\t\ttry (Output out = new Output(outFile.writeContent())) {\n+\t\t\tfor(int entityId = 0; entityId < entries.size(); entityId++) {\n+\t\t\t\tList<Object[]> events = (List<Object[]>) entries.get(entityId);\n+\n+\t\t\t\tif(!events.isEmpty()) {\n+\t\t\t\t\twriteRowsToFile(out, imp, entityId, events);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Then write headers.\n+\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\tint hash = descriptor.calculateValidityHash();\n+\n+\t\t\tPreprocessedHeader header = PreprocessedHeader.builder()", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\nindex 29e5b9711..12fb957af 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n\n@@ -59,12 +57,16 @@ public class Preprocessed {\n \t}\n \n \tpublic void write(HCFile outFile) throws IOException {\n+\t\tif(!outFile.isWrite()){\n+\t\t\tthrow new IllegalArgumentException(\"outfile was opened in read-only mode.\");\n+\t\t}\n+\n \t\t// Write content to file\n \t\tImport imp = Import.createForPreprocessing(descriptor.getTable(), descriptor.getName(), columns);\n \n \t\ttry (Output out = new Output(outFile.writeContent())) {\n \t\t\tfor(int entityId = 0; entityId < entries.size(); entityId++) {\n-\t\t\t\tList<Object[]> events = (List<Object[]>) entries.get(entityId);\n+\t\t\t\tList<Object[]> events = entries.getOrDefault(entityId, Collections.emptyList());\n \n \t\t\t\tif(!events.isEmpty()) {\n \t\t\t\t\twriteRowsToFile(out, imp, entityId, events);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3ODYzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394978639", "bodyText": "Wird das noch gebraucht?", "author": "thoniTUB", "createdAt": "2020-03-19T12:08:26Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n+\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\tlineId++;\n+\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n+\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n+\n+\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n+\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n+\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n \t\t\t\t\t}\n \n-\t\t\t\t\tif (input.checkAutoOutput()) {\n-\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n-\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n-\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\n+\t\t\t\t\tString[] row;\n+\n+\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n+\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n+\t\t\t\t\t\ttry {\n+\n+\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n+\n+\t\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n+\t\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n+\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n+\n+\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n+\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n+\n+\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\n+\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}. Errors={}\", e.getSource().getDescription(), lineId, Arrays.toString(row), errors, e.getCause());\n+\n+//\n+//\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\nindex a73f93e48..7e0dfe57b 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java\n\n@@ -34,6 +34,7 @@ import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3OTY5NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394979695", "bodyText": "Cool!", "author": "thoniTUB", "createdAt": "2020-03-19T12:10:37Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"pid\", MajorTypeId.STRING);\n+\t@Valid\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ae45b18cc240bc18ee220ff0da780bb82818191f", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\nindex 58124fd6e..f28ece61d 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java\n\n@@ -3,7 +3,6 @@ package com.bakdata.conquery.models.preproc;\n import java.io.File;\n import java.io.Serializable;\n import java.time.LocalDate;\n-import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n import javax.validation.Valid;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394992077", "bodyText": "Nach Au\u00dfen sieht es aus als w\u00fcrde die Funktion immer erfolgreich sein. Ist das nicht ein Problem?", "author": "thoniTUB", "createdAt": "2020-03-19T12:34:14Z", "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.bakdata.conquery.util;\n+\n+import java.time.LocalDate;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.format.DateTimeParseException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Locale;\n+import java.util.Set;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.google.common.base.Strings;\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import lombok.experimental.UtilityClass;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Utility class for parsing multiple dateformats. Parsing is cached in two ways: First parsed values are cached. Second, the last used parser is cached since it's likely that it will be used again, we therefore try to use it first, then try all others.\n+ */\n+@UtilityClass\n+@Slf4j\n+public class DateFormats {\n+\n+\t/**\n+\t * All available formats for parsing.\n+\t */\n+\tprivate static Set<DateTimeFormatter> formats;\n+\n+\t/**\n+\t * Last successfully parsed dateformat.\n+\t */\n+\tprivate static ThreadLocal<DateTimeFormatter> lastFormat = new ThreadLocal<>();\n+\n+\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n+\n+\t/**\n+\t * Parsed values cache.\n+\t */\n+\tprivate static final LoadingCache<String, LocalDate> DATE_CACHE = CacheBuilder.newBuilder()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .weakKeys().weakValues()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  // TODO: 07.01.2020 fk: Tweak this number?\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .concurrencyLevel(10)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .initialCapacity(64000)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .build(CacheLoader.from(DateFormats::tryParse));\n+\n+\t/**\n+\t * Try parsing the String value to a LocalDate.\n+\t */\n+\tpublic static LocalDate parseToLocalDate(String value) throws ParsingException {\n+\t\tif(Strings.isNullOrEmpty(value)) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\treturn DATE_CACHE.getUnchecked(value);\n+\t}\n+\n+\t/**\n+\t * Try and parse with the last successful parser. If not successful try and parse with other parsers and update the last successful parser.\n+\t *\n+\t * Method is private as it is only directly accessed via the Cache.\n+\t */\n+\tprivate static LocalDate tryParse(String value) {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNTExNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412225117", "bodyText": "Kein Kommentar von dir \ud83d\ude22", "author": "thoniTUB", "createdAt": "2020-04-21T14:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNjgyMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412226820", "bodyText": "Was macht das ErrorDate in den Daten? Taucht das Event dann nur auf wenn man quasi keine Datumsbeschr\u00e4nkung gesetzt hat?", "author": "thoniTUB", "createdAt": "2020-04-21T14:19:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/util/DateFormats.java b/backend/src/main/java/com/bakdata/conquery/util/DateFormats.java\nindex 98244764f..c1d2f7160 100644\n--- a/backend/src/main/java/com/bakdata/conquery/util/DateFormats.java\n+++ b/backend/src/main/java/com/bakdata/conquery/util/DateFormats.java\n\n@@ -25,6 +25,7 @@ import lombok.extern.slf4j.Slf4j;\n @Slf4j\n public class DateFormats {\n \n+\n \t/**\n \t * All available formats for parsing.\n \t */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MzY3NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394993675", "bodyText": "Die Funktionen sehen aus wie Util methoden. K\u00f6nnen die nicht in die LoadingUtil?", "author": "thoniTUB", "createdAt": "2020-03-19T12:37:10Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -59,8 +86,97 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \n \t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n+\t\timportIdMapping(support);\n+\t\timportPreviousQueries(support);\n+\t}\n+\n+\tpublic void importIdMapping(StandaloneSupport support) throws JSONException, IOException {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\nindex 6bf41b60e..aa2a01e8d 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n\n@@ -84,99 +56,9 @@ public class QueryTest extends AbstractQueryEngineTest {\n \t\tsupport.waitUntilWorkDone();\n \t\tquery = IntegrationUtils.parseQuery(support, rawQuery);\n \n-\t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n-\t\timportIdMapping(support);\n-\t\timportPreviousQueries(support);\n-\t}\n-\n-\tpublic void importIdMapping(StandaloneSupport support) throws JSONException, IOException {\n-\t\tif(content.getIdMapping() == null) {\n-\t\t\treturn;\n-\t\t}\n-\t\ttry(InputStream in = content.getIdMapping().stream()) {\n-\t\t\tsupport.getDatasetsProcessor().setIdMapping(in, support.getNamespace());\n-\t\t}\n-\t}\n-\tpublic void importPreviousQueries(StandaloneSupport support) throws JSONException, IOException {\n-\t\t// Load previous query results if available\n-\t\tint id = 1;\n-\t\tfor(ResourceFile queryResults : content.getPreviousQueryResults()) {\n-\t\t\tUUID queryId = new UUID(0L, id++);\n-\n-\t\t\t//Just read the file without parsing headers etc.\n-\t\t\tCsvParserSettings parserSettings = support.getConfig().getCsv()\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .withParseHeaders(false)\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .withSkipHeader(false)\n-\t\t\t\t\t\t\t\t\t\t\t\t\t  .createCsvParserSettings();\n-\n-\t\t\tCsvParser parser = new CsvParser(parserSettings);\n-\n-\t\t\tString[][] data = parser.parseAll(queryResults.stream()).toArray(String[][]::new);\n-\n-\t\t\tConceptQuery q = new ConceptQuery(new CQExternal(Arrays.asList(CQExternal.FormatColumn.ID, CQExternal.FormatColumn.DATE_SET), data));\n-\n-\t\t\tManagedExecution managed = support.getNamespace().getQueryManager().runQuery(q, queryId, IntegrationUtils.getDefaultUser());\n-\t\t\tmanaged.awaitDone(1, TimeUnit.DAYS);\n-\n-\t\t\tif (managed.getState() == ExecutionState.FAILED) {\n-\t\t\t\tfail(\"Query failed\");\n-\t\t\t}\n-\t\t}\n-\n-\t\t//wait only if we actually did anything\n-\t\tif(!content.getPreviousQueryResults().isEmpty()) {\n-\t\t\tsupport.waitUntilWorkDone();\n-\t\t}\n-\t}\n-\n-\tpublic void importTableContents(StandaloneSupport support, Collection<RequiredTable> tables, Dataset dataset) throws IOException, JSONException {\n-\n-\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);\n-\t\tList<File> preprocessedFiles = new ArrayList<>();\n-\n-\t\tfor (RequiredTable rTable : tables) {\n-\t\t\t//copy csv to tmp folder\n-\t\t\tString name = rTable.getCsv().getName().substring(0, rTable.getCsv().getName().lastIndexOf('.'));\n-\t\t\tFileUtils.copyInputStreamToFile(rTable.getCsv().stream(), new File(support.getTmpDir(), rTable.getCsv().getName()));\n-\n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(support.getConfig().getPreprocessor().getDirectories()[0], name, null);\n-\t\t\tTableImportDescriptor desc = new TableImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(rTable.getName() + \"_import\");\n-\t\t\tdesc.setTable(rTable.getName());\n-\t\t\tTableInputDescriptor input = new TableInputDescriptor();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(copyOutput(rTable.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), rTable.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new OutputDescription[rTable.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < rTable.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = copyOutput(rTable.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new TableInputDescriptor[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n-\t\t\tpreprocessedFiles.add(inputFile.getPreprocessedFile());\n-\t\t}\n-\t\t//preprocess\n-\t\tsupport.preprocessTmp();\n-\n-\t\t//import preprocessedFiles\n-\t\tfor (File file : preprocessedFiles) {\n-\t\t\tsupport.getDatasetsProcessor().addImport(dataset, file);\n-\t\t}\n-\n-\t\tIntegrationUtils.importIdMapping(support, content);\n+\t\tIntegrationUtils.importIdMapping(support, content.getIdMapping());\n \t\tIntegrationUtils.importPreviousQueries(support, content);\n \t}\n-\n-\tpublic static OutputDescription copyOutput(RequiredColumn column) {\n-\t\tCopyOutput out = new CopyOutput();\n-\t\tout.setInputColumn(column.getName());\n-\t\tout.setInputType(column.getType());\n-\t\tout.setName(column.getName());\n-\t\treturn out;\n-\t}\n-\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDE5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394994194", "bodyText": "Warum wird das gemacht?", "author": "thoniTUB", "createdAt": "2020-03-19T12:38:04Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -99,12 +96,9 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \t}\n \n \tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n-\t\tDateFormats.initialize(ArrayUtils.EMPTY_STRING_ARRAY);\n+\n+\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\nindex 3c42cd86c..c2b9e94f1 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java\n\n@@ -87,52 +77,16 @@ public class FilterTest extends AbstractQueryEngineTest {\n \t\timportTables(support);\n \t\tsupport.waitUntilWorkDone();\n \n+\n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\t\n \t\tquery = parseQuery(support);\n \n-\t\timportTableContents(support);\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t}\n \n-\tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\n-\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);\n-\n-\t\tList<File> preprocessedFiles = new ArrayList<>();\n-\n-\t\tfor (RequiredTable rTable : content.getTables()) {\n-\t\t\t//copy csv to tmp folder\n-\t\t\tString name = rTable.getCsv().getName().substring(0, rTable.getCsv().getName().lastIndexOf('.'));\n-\t\t\tFileUtils.copyInputStreamToFile(rTable.getCsv().stream(), new File(support.getTmpDir(), rTable.getCsv().getName()));\n-\n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(support.getConfig().getPreprocessor().getDirectories()[0], name, null);\n-\t\t\tTableImportDescriptor desc = new TableImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(rTable.getName() + \"_import\");\n-\t\t\tdesc.setTable(rTable.getName());\n-\t\t\tTableInputDescriptor input = new TableInputDescriptor();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(QueryTest.copyOutput(rTable.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), rTable.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new OutputDescription[rTable.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < rTable.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = QueryTest.copyOutput(rTable.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new TableInputDescriptor[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n-\t\t\tpreprocessedFiles.add(inputFile.getPreprocessedFile());\n-\t\t}\n-\t\t//preprocess\n-\t\tsupport.preprocessTmp();\n \n-\t\t//import preprocessedFiles\n-\t\tfor (File file : preprocessedFiles) {\n-\t\t\tsupport.getDatasetsProcessor().addImport(support.getDataset(), file);\n-\t\t}\n-\t}\n \n \tprivate void importConcepts(StandaloneSupport support) throws JSONException, IOException, ConfigurationException {\n \t\tDataset dataset = support.getDataset();\n"}}, {"oid": "900c08629b3b0b56429980d5eda7ad080494f4ee", "url": "https://github.com/bakdata/conquery/commit/900c08629b3b0b56429980d5eda7ad080494f4ee", "message": "Gather exception classes into multiset", "committedDate": "2020-04-14T13:57:43Z", "type": "commit"}, {"oid": "06018a531e13216cc5d1031f0db48fe0f6c73402", "url": "https://github.com/bakdata/conquery/commit/06018a531e13216cc5d1031f0db48fe0f6c73402", "message": "refactor unused builder into AllArgsCtor", "committedDate": "2020-04-14T13:58:05Z", "type": "commit"}, {"oid": "0804ec1b82d1fc7a595e29cfc8c3a9667603a13a", "url": "https://github.com/bakdata/conquery/commit/0804ec1b82d1fc7a595e29cfc8c3a9667603a13a", "message": "Check for Error date", "committedDate": "2020-04-14T14:05:18Z", "type": "commit"}, {"oid": "cb17d3d551f3f7b6b5e022277586ff4e49d5c693", "url": "https://github.com/bakdata/conquery/commit/cb17d3d551f3f7b6b5e022277586ff4e49d5c693", "message": "refactor as static", "committedDate": "2020-04-14T14:23:16Z", "type": "commit"}, {"oid": "42f3deba7ca312dbe280dd8232a6710f1774be07", "url": "https://github.com/bakdata/conquery/commit/42f3deba7ca312dbe280dd8232a6710f1774be07", "message": "post merge fixes", "committedDate": "2020-04-16T09:00:02Z", "type": "commit"}, {"oid": "c6c360e678f33a51ce60ecc9b9eec2eee4907c5a", "url": "https://github.com/bakdata/conquery/commit/c6c360e678f33a51ce60ecc9b9eec2eee4907c5a", "message": "fix case sensitivity in headers.", "committedDate": "2020-04-20T14:15:36Z", "type": "commit"}, {"oid": "105fdd94585200e10fd4d48a5a0786dfda0705fb", "url": "https://github.com/bakdata/conquery/commit/105fdd94585200e10fd4d48a5a0786dfda0705fb", "message": "fix injection of content2.2.csv", "committedDate": "2020-04-20T14:17:28Z", "type": "commit"}, {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "url": "https://github.com/bakdata/conquery/commit/81ccda1cd3154f135b7f37dccfea58634c5138fc", "message": "fix usage of previousQuery Import", "committedDate": "2020-04-20T14:26:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ0NTk5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411445993", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n          \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {", "author": "thoniTUB", "createdAt": "2020-04-20T14:55:58Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex b30d1ad55..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -112,19 +113,28 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n+\t\t}\n \t}\n \n-\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\tpublic List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {\n \t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n \n-\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\t\t\tFile inDir = description.getDescriptionsDir().getAbsoluteFile();\n \n \t\t\tfor (File descriptionFile : inDir.listFiles(((dir, name) -> name.endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)))) {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411452497", "bodyText": "Hier w\u00fcrde ich den Member von description umbenennen.  Sodass man im code ein Hint welcher Dateityp dahinter steht: import/table ...", "author": "thoniTUB", "createdAt": "2020-04-20T15:04:01Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1NDYxOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411454618", "bodyText": "Oder in dem fall, dass es ein Pfad ist", "author": "thoniTUB", "createdAt": "2020-04-20T15:06:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex b30d1ad55..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -112,19 +113,28 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n+\t\t}\n \t}\n \n-\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\tpublic List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {\n \t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n \n-\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\t\t\tFile inDir = description.getDescriptionsDir().getAbsoluteFile();\n \n \t\t\tfor (File descriptionFile : inDir.listFiles(((dir, name) -> name.endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)))) {\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412212100", "bodyText": "Die Methoden Signatur ist hier unsauber, weil man der Methode auch ein HCFile im READ mode \u00fcbergeben kann.", "author": "thoniTUB", "createdAt": "2020-04-21T14:02:30Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcxMDE1MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415710151", "bodyText": "Ich hab hier einen Test eingebaut ob es in read ge\u00f6ffnet ist. Die alte variante war mMn noch bl\u00f6der aber ich sehe hier keinen trivialen Weg das zu garantieren.  Ich hab auch ein todo in die HCFile klasse geschrieben aber so wie sie aktuell ist gef\u00e4llt sie mir eigentlich nicht", "author": "awildturtok", "createdAt": "2020-04-27T10:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\nindex 0158b0faa..12fb957af 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n\n@@ -59,12 +57,16 @@ public class Preprocessed {\n \t}\n \n \tpublic void write(HCFile outFile) throws IOException {\n+\t\tif(!outFile.isWrite()){\n+\t\t\tthrow new IllegalArgumentException(\"outfile was opened in read-only mode.\");\n+\t\t}\n+\n \t\t// Write content to file\n \t\tImport imp = Import.createForPreprocessing(descriptor.getTable(), descriptor.getName(), columns);\n \n \t\ttry (Output out = new Output(outFile.writeContent())) {\n \t\t\tfor(int entityId = 0; entityId < entries.size(); entityId++) {\n-\t\t\t\tList<Object[]> events = (List<Object[]>) entries.get(entityId);\n+\t\t\t\tList<Object[]> events = entries.getOrDefault(entityId, Collections.emptyList());\n \n \t\t\t\tif(!events.isEmpty()) {\n \t\t\t\t\twriteRowsToFile(out, imp, entityId, events);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412220444", "bodyText": "Ich glaube hier ist Map<Integer, List<Object[]>> besser, da wir Die zweite Interpretation aus der Multimap-Doku benutzt wird und f\u00fcr diese eine Multimap nicht empfohlen wird", "author": "thoniTUB", "createdAt": "2020-04-21T14:12:07Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMTE0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412221146", "bodyText": "A collection that maps keys to values, similar to Map, but in which each key may beassociated with multiple values. You can visualize the contents of a multimap either as amap from keys to nonempty collections of values:\n\u2022a \u00e2\u2020\u2019 1, 2\n\u2022b \u00e2\u2020\u2019 3\n... or as a single \"flattened\" collection of key-value pairs: \u2022a \u00e2\u2020\u2019 1\n\u2022a \u00e2\u2020\u2019 2\n\u2022b \u00e2\u2020\u2019 3\n\n\nImportant: although the first interpretation resembles how most multimaps are implemented, the design of the Multimap API is based on the second form.So, using the multimap shown above as an example, the size is 3, not 2,and the values collection is [1, 2, 3], not [[1, 2], [3]]. For thosetimes when the first style is more useful, use the multimap's asMap view (or create a Map<K, Collection> in the first place).", "author": "thoniTUB", "createdAt": "2020-04-21T14:13:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMjQ1NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412222455", "bodyText": "Dann sparst du dir auch den cast in Zeile 67", "author": "thoniTUB", "createdAt": "2020-04-21T14:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1NTgxNA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416455814", "bodyText": "Was sind deine Gedanken hierzu?", "author": "thoniTUB", "createdAt": "2020-04-28T09:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\nindex 0158b0faa..12fb957af 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java\n\n@@ -29,9 +28,8 @@ public class Preprocessed {\n \tprivate final PPColumn[] columns;\n \tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n-\tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();\n+\tprivate Int2ObjectMap<List<Object[]>> entries = new Int2ObjectAVLTreeMap<>();\n \t\n \tprivate final Output buffer = new Output((int) Size.megabytes(50).toBytes());\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzMTUwOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412231509", "bodyText": "Mache ambesten aus dem table Array in RequiredData gleich eine Liste, dann musst du hier nichts wrappen", "author": "thoniTUB", "createdAt": "2020-04-21T14:24:52Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java", "diffHunk": "@@ -81,18 +82,18 @@\n \t@Override\n \tpublic void importRequiredData(StandaloneSupport support) throws Exception {\n \n-\t\tLoadingUtil.importTables(support, content);\n+\t\tIntegrationUtils.importTables(support, content);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT TABLES\", getLabel());\n \n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT CONCEPTS\", getLabel());\n \n-\t\tLoadingUtil.importTableContents(support, content);\n+\t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java b/backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java\nindex 069186218..b3734b2eb 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java\n\n@@ -90,7 +89,7 @@ public class FormTest extends ConqueryTestSpec {\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT CONCEPTS\", getLabel());\n \n-\t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n+\t\tIntegrationUtils.importTableContents(support, content.getTables(), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT TABLE CONTENTS\", getLabel());\n \t\tIntegrationUtils.importPreviousQueries(support, content);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412234238", "bodyText": "Gibt es auch einen Test in dem tag != null ist?", "author": "thoniTUB", "createdAt": "2020-04-21T14:28:02Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "diffHunk": "@@ -177,29 +176,13 @@ public void execute(String name, TestConquery testConquery) throws Exception {\n \t\t\tFileUtils.copyInputStreamToFile(In.resource(path.substring(0, path.lastIndexOf(\"/\")) + \"/\" + \"content2.2.csv\")\n \t\t\t\t\t\t\t\t\t\t\t  .asStream(), new File(conquery.getTmpDir(), import2Table.getCsv().getName()));\n \n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(conquery.getConfig().getPreprocessor().getDirectories()[0], importId.getTag());\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(import2Table.getName() + \"_import\");\n-\t\t\tdesc.setTable(import2Table.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(IntegrationUtils.copyOutput(0, import2Table.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), import2Table.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new Output[import2Table.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < import2Table.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = IntegrationUtils.copyOutput(i + 1, import2Table.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n \n \t\t\t//preprocess\n \t\t\tconquery.preprocessTmp();\n \n \t\t\t//import preprocessedFiles\n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), inputFile.getPreprocessedFile());\n+\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgwMzkwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415803900", "bodyText": "Daf\u00fcr m\u00fcssten wir so tief in die Tests eingreifen f\u00fcr ein convenience feature", "author": "awildturtok", "createdAt": "2020-04-27T13:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU2NDk1OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416564958", "bodyText": "aber ich sollte die methode selber unit testen", "author": "awildturtok", "createdAt": "2020-04-28T12:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java b/backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java\nindex b7e16749a..c203bc5dd 100644\n--- a/backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java\n+++ b/backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java\n\n@@ -182,7 +181,7 @@ public class ImportDeletionTest implements ProgrammaticIntegrationTest {\n \n \t\t\t//import preprocessedFiles\n \n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null, \"csv\\\\.gz\"));\n \t\t\tconquery.waitUntilWorkDone();\n \t\t}\n \n"}}, {"oid": "afa650605a2bf69184aeaa8749d4ddf26d18db18", "url": "https://github.com/bakdata/conquery/commit/afa650605a2bf69184aeaa8749d4ddf26d18db18", "message": "reimplement DateRangeOutput as Epoch dates were a stupid idea to begin with.", "committedDate": "2020-04-21T15:42:09Z", "type": "commit"}, {"oid": "e45d13d5419f4b53f208009fe5f8fa529521d7f0", "url": "https://github.com/bakdata/conquery/commit/e45d13d5419f4b53f208009fe5f8fa529521d7f0", "message": "removed weakKeys as they do not work properly with strings.", "committedDate": "2020-04-22T09:53:32Z", "type": "commit"}, {"oid": "e183e4bb19ec5b0b1f37e7ed1ce789f9cff00d48", "url": "https://github.com/bakdata/conquery/commit/e183e4bb19ec5b0b1f37e7ed1ce789f9cff00d48", "message": "add handling of normal exceptions.", "committedDate": "2020-04-22T14:45:42Z", "type": "commit"}, {"oid": "4fc24dd031a03eabf7cdc14eadfc469eca8d3916", "url": "https://github.com/bakdata/conquery/commit/4fc24dd031a03eabf7cdc14eadfc469eca8d3916", "message": "moved error output up so we have analysis earlier (becasue the code now below it is having problems)", "committedDate": "2020-04-22T14:52:41Z", "type": "commit"}, {"oid": "195d2186714d70bf8eea7d7e35c5b43681f545e2", "url": "https://github.com/bakdata/conquery/commit/195d2186714d70bf8eea7d7e35c5b43681f545e2", "message": "fixed illegal calls on open dateranges", "committedDate": "2020-04-22T15:03:44Z", "type": "commit"}, {"oid": "c0902acf94c0bdbce36020e649e45de6ba9a8459", "url": "https://github.com/bakdata/conquery/commit/c0902acf94c0bdbce36020e649e45de6ba9a8459", "message": "improve error logging", "committedDate": "2020-04-22T15:16:57Z", "type": "commit"}, {"oid": "2852d3928e10b199ec39955c2069eb4349a9ea30", "url": "https://github.com/bakdata/conquery/commit/2852d3928e10b199ec39955c2069eb4349a9ea30", "message": "more logging", "committedDate": "2020-04-22T15:32:40Z", "type": "commit"}, {"oid": "0513d63a2d8f91ae75f3521777b9617da5b18ba3", "url": "https://github.com/bakdata/conquery/commit/0513d63a2d8f91ae75f3521777b9617da5b18ba3", "message": "fixed an overflow bug", "committedDate": "2020-04-22T15:47:02Z", "type": "commit"}, {"oid": "dbdb4f7e50e2a6bf915cdd61ccdbdc8e9fc1be0a", "url": "https://github.com/bakdata/conquery/commit/dbdb4f7e50e2a6bf915cdd61ccdbdc8e9fc1be0a", "message": "add better checks for open DateRanges", "committedDate": "2020-04-27T10:06:47Z", "type": "commit"}, {"oid": "6f3ee9a108a4d02a3df7beb33dda7eb299e9a88e", "url": "https://github.com/bakdata/conquery/commit/6f3ee9a108a4d02a3df7beb33dda7eb299e9a88e", "message": "refactors and cleanups.", "committedDate": "2020-04-27T10:39:18Z", "type": "commit"}, {"oid": "ed8118001e5dea03abd73f2a5adabaf189d5a2e1", "url": "https://github.com/bakdata/conquery/commit/ed8118001e5dea03abd73f2a5adabaf189d5a2e1", "message": "create missing Storage directory if it does not yet exist.", "committedDate": "2020-04-27T10:47:37Z", "type": "commit"}, {"oid": "8e80bfb48ef8ec911dee911011ac10c136920f02", "url": "https://github.com/bakdata/conquery/commit/8e80bfb48ef8ec911dee911011ac10c136920f02", "message": "minor orphan cleanups", "committedDate": "2020-04-27T10:47:57Z", "type": "commit"}, {"oid": "2259374250d0edff244cd686130375cc0385ceb8", "url": "https://github.com/bakdata/conquery/commit/2259374250d0edff244cd686130375cc0385ceb8", "message": "renamed PreprocessingDirectories fields to better communicate that they are directories", "committedDate": "2020-04-27T10:49:30Z", "type": "commit"}, {"oid": "d8394cf443cfcf89f77459b8ad2e29a0b2d06754", "url": "https://github.com/bakdata/conquery/commit/d8394cf443cfcf89f77459b8ad2e29a0b2d06754", "message": "unify usage of IntegrationUtils as much as possible.", "committedDate": "2020-04-27T12:18:48Z", "type": "commit"}, {"oid": "5644bf9c6eef4fbbe8cab93c9effe408a2c5f8f8", "url": "https://github.com/bakdata/conquery/commit/5644bf9c6eef4fbbe8cab93c9effe408a2c5f8f8", "message": "fix usage of compute in intmap", "committedDate": "2020-04-27T12:28:42Z", "type": "commit"}, {"oid": "3331ab54b7613fefc92b80531e29bfb1a52b0d75", "url": "https://github.com/bakdata/conquery/commit/3331ab54b7613fefc92b80531e29bfb1a52b0d75", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-27T12:29:11Z", "type": "commit"}, {"oid": "dc9650da31a1e7f449d084051a7ca91cb4282b22", "url": "https://github.com/bakdata/conquery/commit/dc9650da31a1e7f449d084051a7ca91cb4282b22", "message": "Merge 3331ab54b7613fefc92b80531e29bfb1a52b0d75 into 6463a75e42e344e6a09a2eecb2b0a07c995b321d", "committedDate": "2020-04-27T12:29:14Z", "type": "commit"}, {"oid": "266f9e9efbae42b0c5cdc19e48a47f2b469c2e24", "url": "https://github.com/bakdata/conquery/commit/266f9e9efbae42b0c5cdc19e48a47f2b469c2e24", "message": "automatic update to docs", "committedDate": "2020-04-27T12:31:14Z", "type": "commit"}, {"oid": "b6edb442d3a611e075242b084189d990dd0449a9", "url": "https://github.com/bakdata/conquery/commit/b6edb442d3a611e075242b084189d990dd0449a9", "message": "better tag insertion via regex", "committedDate": "2020-04-27T12:37:26Z", "type": "commit"}, {"oid": "15cd72a8e06ce283a3bcb8617989a81d6af50223", "url": "https://github.com/bakdata/conquery/commit/15cd72a8e06ce283a3bcb8617989a81d6af50223", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-04-27T12:37:38Z", "type": "commit"}, {"oid": "eb45e9d2afed1a93203b6c10281e84d30e895409", "url": "https://github.com/bakdata/conquery/commit/eb45e9d2afed1a93203b6c10281e84d30e895409", "message": "better tag insertion via regex FIX", "committedDate": "2020-04-27T13:00:46Z", "type": "commit"}, {"oid": "f2e65b93a6e7abe39e4f40b7c124f90d7187ff1f", "url": "https://github.com/bakdata/conquery/commit/f2e65b93a6e7abe39e4f40b7c124f90d7187ff1f", "message": "add error output at the end of PreprocessorCommand", "committedDate": "2020-04-27T13:27:02Z", "type": "commit"}, {"oid": "a28e42e887c6e3741b83c2a0d5c3aedb2ce74b6a", "url": "https://github.com/bakdata/conquery/commit/a28e42e887c6e3741b83c2a0d5c3aedb2ce74b6a", "message": "more error reporting and fix output file naming", "committedDate": "2020-04-27T13:38:50Z", "type": "commit"}, {"oid": "fcc0903271d5a4965affcc1fe350946acc2ef9fa", "url": "https://github.com/bakdata/conquery/commit/fcc0903271d5a4965affcc1fe350946acc2ef9fa", "message": "fix replacement of file ext", "committedDate": "2020-04-27T13:44:29Z", "type": "commit"}, {"oid": "4a5c2ebadc6cddd1beba88212f9725e383f40eb9", "url": "https://github.com/bakdata/conquery/commit/4a5c2ebadc6cddd1beba88212f9725e383f40eb9", "message": "fix file naming again", "committedDate": "2020-04-27T13:58:38Z", "type": "commit"}, {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634", "url": "https://github.com/bakdata/conquery/commit/a93fe05c228c8a178feb08abac75743fe4b3b634", "message": "clear MDC after Preprocessing", "committedDate": "2020-04-27T14:17:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r413239901", "bodyText": "Der Cast macht nichts", "author": "thoniTUB", "createdAt": "2020-04-22T18:57:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -72,23 +71,27 @@ public Integer transform(CDateRange value) {\n \t\t\t\ttype\n \t\t\t);\n \t\t}\n-\t\tif(maxValue - minValue <PackedUnsigned1616.MAX_VALUE) {\n+\t\t// min or max can be Integer.MIN/MAX_VALUE when this happens, the left expression overflows causing it to be true when it is not.\n+\t\tif ((long) maxValue - (long) minValue < (long) PackedUnsigned1616.MAX_VALUE) {\n \t\t\tDateRangeTypePacked type = new DateRangeTypePacked();\n \t\t\ttype.setMinValue(minValue);\n \t\t\ttype.setMaxValue(maxValue);\n+\n+\t\t\tlog.debug(\"Decided for Packed: min={}, max={}\", minValue, maxValue);\n+\n \t\t\treturn new Decision<>(\n-\t\t\t\tnew Transformer<CDateRange, Integer>() {\n-\t\t\t\t\t@Override\n-\t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\tCDateRange v = (CDateRange) value;\n-\t\t\t\t\t\tif(v.getMaxValue()>Integer.MAX_VALUE || v.getMinValue()<Integer.MIN_VALUE) {\n-\t\t\t\t\t\t\tthrow new IllegalArgumentException(value+\" is out of range\");\n+\t\t\t\t\tnew Transformer<CDateRange, Integer>() {\n+\t\t\t\t\t\t@Override\n+\t\t\t\t\t\tpublic Integer transform(CDateRange value) {\n+\t\t\t\t\t\t\tCDateRange v = (CDateRange) value;", "originalCommit": "0513d63a2d8f91ae75f3521777b9617da5b18ba3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NTQzMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416545431", "bodyText": "hast recht, das sollte auch weg.", "author": "awildturtok", "createdAt": "2020-04-28T11:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex bd9f62282..bc9f3c1f8 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -59,20 +67,30 @@ public class DateRangeParser extends Parser<CDateRange> {\n \t\n \t@Override\n \tprotected Decision<CDateRange, ?, ? extends CType<CDateRange, ?>> decideType() {\n+\t\t// We cannot yet do meaningful compression for open dateranges.\n+\t\t// TODO: 27.04.2020 consider packed compression with extra value as null value.\n+\t\tif(!onlyClosed) {\n+\t\t\treturn new Decision<>(\n+\t\t\t\t\tnew NoopTransformer<>(),\n+\t\t\t\t\tnew DateRangeTypeDateRange()\n+\t\t\t);\n+\t\t}\n+\n \t\tif(onlyQuarters) {\n \t\t\tDateRangeTypeQuarter type = new DateRangeTypeQuarter();\n \t\t\treturn new Decision<>(\n \t\t\t\tnew Transformer<CDateRange, Integer>() {\n \t\t\t\t\t@Override\n \t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\treturn ((CDateRange)value).getMinValue();\n+\t\t\t\t\t\treturn value.getMinValue();\n \t\t\t\t\t}\n \t\t\t\t},\n \t\t\t\ttype\n \t\t\t);\n \t\t}\n \t\t// min or max can be Integer.MIN/MAX_VALUE when this happens, the left expression overflows causing it to be true when it is not.\n-\t\tif ((long) maxValue - (long) minValue < (long) PackedUnsigned1616.MAX_VALUE) {\n+\t\t// We allow this exception to happen as it would imply erroneous data.\n+\t\tif (Math.subtractExact(maxValue, minValue) < PackedUnsigned1616.MAX_VALUE) {\n \t\t\tDateRangeTypePacked type = new DateRangeTypePacked();\n \t\t\ttype.setMinValue(minValue);\n \t\t\ttype.setMaxValue(maxValue);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0MzQ3MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416443470", "bodyText": "Ist in dieser Datei einfach nur ein Import dazugekommen?", "author": "thoniTUB", "createdAt": "2020-04-28T08:51:39Z", "path": "backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java", "diffHunk": "@@ -1,5 +1,7 @@\n package com.bakdata.conquery.apiv1;\n \n+import static com.bakdata.conquery.models.auth.AuthorizationHelper.*;", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java b/backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java\nindex f7a81783d..9275dbb18 100644\n--- a/backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java\n+++ b/backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java\n\n@@ -1,7 +1,5 @@\n package com.bakdata.conquery.apiv1;\n \n-import static com.bakdata.conquery.models.auth.AuthorizationHelper.*;\n-\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.List;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0NDgwMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416444801", "bodyText": "Ah nice! Insgeheim ein lang ersehntes Feature f\u00fcr mich ;)", "author": "thoniTUB", "createdAt": "2020-04-28T08:53:45Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java", "diffHunk": "@@ -91,6 +91,10 @@ public void run(ConqueryConfig config, Environment environment) {\n \t\t\n \t\tenvironment.lifecycle().manage(this);\n \n+\t\tif(config.getStorage().getDirectory().mkdirs()){\n+\t\t\tlog.warn(\"Had to create Storage Dir at `{}`\", config.getStorage().getDirectory());", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java\nindex edd758fdc..b7dd7fdac 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java\n\n@@ -91,10 +91,6 @@ public class MasterCommand extends IoHandlerAdapter implements Managed {\n \t\t\n \t\tenvironment.lifecycle().manage(this);\n \n-\t\tif(config.getStorage().getDirectory().mkdirs()){\n-\t\t\tlog.warn(\"Had to create Storage Dir at `{}`\", config.getStorage().getDirectory());\n-\t\t}\n-\n \t\tlog.info(\"Started meta storage\");\n \t\tfor (File directory : config.getStorage().getDirectory().listFiles()) {\n \t\t\tif (directory.getName().startsWith(\"dataset_\")) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0OTUwMw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416449503", "bodyText": "Hier w\u00fcrde ich eine Message zusammenbauen und die als einzelnen Error absetzten. Das machst die Logs besser durchsuchbar und lesbarer, da die Zusammenfassung als ein Block erkennbar ist.", "author": "thoniTUB", "createdAt": "2020-04-28T09:00:49Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex 671f6e126..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -126,7 +126,7 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\tif (!failed.isEmpty()) {\n \t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n \t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n-\t\t\tSystem.exit(Math.min(failed.size(), 255));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416450110", "bodyText": "Hier kollidierst du potentiell mit speziellen StatusCodes: http://tldp.org/LDP/abs/html/exitcodes.html", "author": "thoniTUB", "createdAt": "2020-04-28T09:01:38Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MTQ3OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416451478", "bodyText": "Zwar gilt das nur f\u00fcr bash, aber wo werden die StatusCodes sonst so eifrig benutzt?", "author": "thoniTUB", "createdAt": "2020-04-28T09:03:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODc1MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468751", "bodyText": "https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\nHabe mich hier von inspirieren lassen, das ist eher ein easter egg, es geht f\u00fcr mich nur um != 0 aber ichw erde es auf 100 truncaten so wie parallel die wissen was sie machen", "author": "awildturtok", "createdAt": "2020-04-28T09:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNzQyOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417127429", "bodyText": "Okay :D so kann man es nat\u00fcrlich auch machen und einen Gro\u00dfteil der codes erhalten", "author": "thoniTUB", "createdAt": "2020-04-29T07:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\nindex 671f6e126..5a3ab0c5f 100644\n--- a/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n+++ b/backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java\n\n@@ -126,7 +126,7 @@ public class PreprocessorCommand extends ConqueryCommand {\n \t\tif (!failed.isEmpty()) {\n \t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n \t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n-\t\t\tSystem.exit(Math.min(failed.size(), 255));\n+\t\t\tSystem.exit(Math.min(failed.size(), 100)); // Inspired by: https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\n \t\t}\n \t}\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MjkzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416452938", "bodyText": "\ud83d\udc4d", "author": "thoniTUB", "createdAt": "2020-04-28T09:06:00Z", "path": "backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java", "diffHunk": "@@ -3,15 +3,13 @@\n import javax.validation.Valid;\n import javax.validation.constraints.Min;\n \n-import org.hibernate.validator.constraints.NotEmpty;\n-\n import lombok.Getter;\n import lombok.Setter;\n import lombok.ToString;\n \n @Getter @Setter @ToString\n public class PreprocessingConfig {\n-\t@NotEmpty @Valid\n+\t@Valid", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java b/backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java\nindex 34f17b435..96285f34e 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java\n\n@@ -3,13 +3,15 @@ package com.bakdata.conquery.models.config;\n import javax.validation.Valid;\n import javax.validation.constraints.Min;\n \n+import org.hibernate.validator.constraints.NotEmpty;\n+\n import lombok.Getter;\n import lombok.Setter;\n import lombok.ToString;\n \n @Getter @Setter @ToString\n public class PreprocessingConfig {\n-\t@Valid\n+\t@NotEmpty @Valid\n \tprivate PreprocessingDirectories[] directories;\n \t@Min(1)\n \tprivate int threads = Runtime.getRuntime().availableProcessors();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416458552", "bodyText": "Hast du das mit Lennart abgesprochen?, ich wei\u00df nicht wann die OpenRanges auftreten aber potentiell k\u00f6nnen dadurch dann Daten in den Formularen fehlen.", "author": "thoniTUB", "createdAt": "2020-04-28T09:14:29Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -320,6 +320,11 @@ public boolean hasLowerBound() {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n+\t\tif(isOpen()){\n+\t\t\t// TODO: 22.04.2020 throw exceptiopn?", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODIzMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468230", "bodyText": "wenn wir es nicht testen ist das eine endlosschleife.", "author": "awildturtok", "createdAt": "2020-04-28T09:29:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzMDA3NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417130075", "bodyText": "Ok, ich habe gerade auch nochmal nachgeschaut in dem Context, sollten eigentlich auch keine offnen Ranges auf treten", "author": "thoniTUB", "createdAt": "2020-04-29T07:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\nindex 19d310bf6..565e04143 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java\n\n@@ -320,11 +320,6 @@ public abstract class CDateRange implements IRange<LocalDate, CDateRange> {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n-\t\tif(isOpen()){\n-\t\t\t// TODO: 22.04.2020 throw exceptiopn?\n-\t\t\treturn Collections.emptyList();\n-\t\t}\n-\n \t\tint startYear = this.getMin().getYear();\n \t\tint endYear = this.getMax().getYear();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416462485", "bodyText": "Angenommen es w\u00e4ren nur komplett offene DateRanges vorhanden, dann ist das ganz sch\u00f6n teuer die null abzuspeichern, aber ein Edgecase", "author": "thoniTUB", "createdAt": "2020-04-28T09:20:43Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -57,6 +67,15 @@ public static CDateRange parseISORange(String value) throws ParsingException {\n \t\n \t@Override\n \tprotected Decision<CDateRange, ?, ? extends CType<CDateRange, ?>> decideType() {\n+\t\t// We cannot yet do meaningful compression for open dateranges.\n+\t\t// TODO: 27.04.2020 consider packed compression with extra value as null value.\n+\t\tif(!onlyClosed) {", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NDY3Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416544672", "bodyText": "ganz offene meinst du -inf/+inf, das werden wir nicht zulassen, das sind m\u00fcll daten. Bzw die werden wir so hoffentlich nie zulassen, da w\u00fcrden sehr viele komische sachen bei rauskommen.", "author": "awildturtok", "createdAt": "2020-04-28T11:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\nindex 0356759fc..bc9f3c1f8 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java\n\n@@ -82,7 +82,7 @@ public class DateRangeParser extends Parser<CDateRange> {\n \t\t\t\tnew Transformer<CDateRange, Integer>() {\n \t\t\t\t\t@Override\n \t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\treturn ((CDateRange)value).getMinValue();\n+\t\t\t\t\t\treturn value.getMinValue();\n \t\t\t\t\t}\n \t\t\t\t},\n \t\t\t\ttype\n"}}, {"oid": "759a119f42b8ceec222d11216ef86abab75064a8", "url": "https://github.com/bakdata/conquery/commit/759a119f42b8ceec222d11216ef86abab75064a8", "message": "Update PreprocessorCommand.java", "committedDate": "2020-04-28T09:31:05Z", "type": "commit"}, {"oid": "2b67c79bbe626ee88390d45555ba4b5b3d4f2227", "url": "https://github.com/bakdata/conquery/commit/2b67c79bbe626ee88390d45555ba4b5b3d4f2227", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-28T11:41:46Z", "type": "commit"}, {"oid": "ae46979ca41566bf502b73186bf044a373101f8b", "url": "https://github.com/bakdata/conquery/commit/ae46979ca41566bf502b73186bf044a373101f8b", "message": "fix reading of empty ranges", "committedDate": "2020-04-28T13:46:30Z", "type": "commit"}, {"oid": "ee07107edc0e45f8020bf20406c2f4706edf7889", "url": "https://github.com/bakdata/conquery/commit/ee07107edc0e45f8020bf20406c2f4706edf7889", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-04-28T13:46:42Z", "type": "commit"}, {"oid": "ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "url": "https://github.com/bakdata/conquery/commit/ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "message": "test for Preprocessor", "committedDate": "2020-04-28T14:32:54Z", "type": "commit"}, {"oid": "ad1a715ab9346a0f8eedd804d239400a6d3dc204", "url": "https://github.com/bakdata/conquery/commit/ad1a715ab9346a0f8eedd804d239400a6d3dc204", "message": "migrate usage of Preprocessed to Map of list as its not the appropriate use case for Multimaps.\nRemove redundant cast in DateRangeParser and remove code that does nothing.", "committedDate": "2020-04-29T07:29:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNjAwNQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417126005", "bodyText": "Hier verstehe ich die Logik gerade nicht.\nM\u00fcsstest du nicht erst pr\u00fcfen ob beides  NullorEmpty ist und offene ranges erlaubt sind und CDateRange.of() zur\u00fcckgeben anstatt null.\nDie nachfolgende If-Bedingung, sollte die nicht als erstes gepr\u00fcft werden?", "author": "thoniTUB", "createdAt": "2020-04-29T07:46:01Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -39,14 +40,13 @@ public Output createForHeaders(Object2IntArrayMap<String> headers) {\n \t\treturn new Output() {\n \t\t\t@Override\n \t\t\tprotected Object parseLine(String[] row, Parser<?> type, long sourceLine) throws ParsingException {\n-\t\t\t\tif (!allowOpen && (row[startIndex] == null || row[endIndex] == null)) {\n-\t\t\t\t\tthrow new IllegalArgumentException(\"Open Ranges are not allowed.\");\n-\t\t\t\t}\n-\n-\t\t\t\tif (row[startIndex] == null && row[endIndex] == null) {\n+\t\t\t\tif (Strings.isNullOrEmpty(row[startIndex]) && Strings.isNullOrEmpty(row[endIndex])) {", "originalCommit": "ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "51b3039b65f1b69b073c51a62e7355bf17fea516", "chunk": "diff --git a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\nindex 8369472a6..2e9bdf76a 100644\n--- a/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\n+++ b/backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java\n\n@@ -40,14 +40,25 @@ public class EpochDateRangeOutput extends OutputDescription {\n \t\treturn new Output() {\n \t\t\t@Override\n \t\t\tprotected Object parseLine(String[] row, Parser<?> type, long sourceLine) throws ParsingException {\n-\t\t\t\tif (Strings.isNullOrEmpty(row[startIndex]) && Strings.isNullOrEmpty(row[endIndex])) {\n+\t\t\t\tfinal boolean startNull = Strings.isNullOrEmpty(row[startIndex]);\n+\t\t\t\tfinal boolean endNull = Strings.isNullOrEmpty(row[endIndex]);\n+\n+\t\t\t\tif (startNull && endNull) {\n \t\t\t\t\treturn null;\n \t\t\t\t}\n \n-\t\t\t\tif (!allowOpen && (Strings.isNullOrEmpty(row[startIndex]) || Strings.isNullOrEmpty(row[endIndex]))) {\n+\t\t\t\tif (!allowOpen && (startNull || endNull)) {\n \t\t\t\t\tthrow new IllegalArgumentException(\"Open Ranges are not allowed.\");\n \t\t\t\t}\n \n+\t\t\t\tif(startNull){\n+\t\t\t\t\treturn CDateRange.atMost(Integer.parseInt(row[endIndex]));\n+\t\t\t\t}\n+\n+\t\t\t\tif(endNull){\n+\t\t\t\t\treturn CDateRange.atLeast(Integer.parseInt(row[startIndex]));\n+\t\t\t\t}\n+\n \t\t\t\tint start = Integer.parseInt(row[startIndex]);\n \t\t\t\tint end = Integer.parseInt(row[endIndex]);\n \n"}}, {"oid": "7a28f358eaa03655debe83123a017db9d02ced5b", "url": "https://github.com/bakdata/conquery/commit/7a28f358eaa03655debe83123a017db9d02ced5b", "message": "new spanning logic that ignores +-Infinity in CDateRange. and accompanying tests", "committedDate": "2020-04-29T10:42:19Z", "type": "commit"}, {"oid": "27edc0503ae00e70a8e133c620cc4a92f77fb746", "url": "https://github.com/bakdata/conquery/commit/27edc0503ae00e70a8e133c620cc4a92f77fb746", "message": "remove unused eventrange", "committedDate": "2020-04-29T12:15:36Z", "type": "commit"}, {"oid": "4d6b95d1fd97ff69abc43dd7b9ea2bb6c4cba858", "url": "https://github.com/bakdata/conquery/commit/4d6b95d1fd97ff69abc43dd7b9ea2bb6c4cba858", "message": "minor code style fixes", "committedDate": "2020-04-29T13:27:00Z", "type": "commit"}, {"oid": "f55704328b1e32b5b579996cc615f186bbcba1b9", "url": "https://github.com/bakdata/conquery/commit/f55704328b1e32b5b579996cc615f186bbcba1b9", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-29T13:27:49Z", "type": "commit"}, {"oid": "d2d80aae819809cbb5d3a959d774829dd6d26589", "url": "https://github.com/bakdata/conquery/commit/d2d80aae819809cbb5d3a959d774829dd6d26589", "message": "fix empty rows", "committedDate": "2020-04-30T10:02:09Z", "type": "commit"}, {"oid": "4d202cdc955a53ac75fff9544cfeec7bc67e0e45", "url": "https://github.com/bakdata/conquery/commit/4d202cdc955a53ac75fff9544cfeec7bc67e0e45", "message": "add better PreprocessedHeader validation for imports", "committedDate": "2020-04-30T11:24:04Z", "type": "commit"}, {"oid": "5c3b36775cc2bf1d10025493ace1f3d0e4ef8872", "url": "https://github.com/bakdata/conquery/commit/5c3b36775cc2bf1d10025493ace1f3d0e4ef8872", "message": "fixed trying to get cause where we didn't know there could be one.", "committedDate": "2020-05-06T14:38:11Z", "type": "commit"}]}