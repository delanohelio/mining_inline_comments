{"pr_number": 2631, "pr_title": "Bigtable Keyviz Art Codelab", "pr_createdAt": "2020-04-11T18:37:07Z", "pr_url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631", "timeline": [{"oid": "3bcf97ad220640b592d456f6a56b725b8c48e438", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/3bcf97ad220640b592d456f6a56b725b8c48e438", "message": "working keyviz art", "committedDate": "2020-04-01T15:46:58Z", "type": "commit"}, {"oid": "d3f74d676c7103ae850526e52194658d8d5fb8e9", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/d3f74d676c7103ae850526e52194658d8d5fb8e9", "message": "Cleanup ReadData with comments and private functions", "committedDate": "2020-04-02T19:02:25Z", "type": "commit"}, {"oid": "bfd8c0803622dbcb9cfc0931a0048a88a27599ac", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/bfd8c0803622dbcb9cfc0931a0048a88a27599ac", "message": "Add more comments and cleanup code", "committedDate": "2020-04-08T14:45:33Z", "type": "commit"}, {"oid": "a584266e77cfc7de0002d6665599faac09b930ab", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/a584266e77cfc7de0002d6665599faac09b930ab", "message": "Updating types, working on tests", "committedDate": "2020-04-10T18:39:56Z", "type": "commit"}, {"oid": "7865d3a038cdfc7a6d5cee623f8c3f6f6ba412aa", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7865d3a038cdfc7a6d5cee623f8c3f6f6ba412aa", "message": "Package for code and working tests", "committedDate": "2020-04-10T20:16:25Z", "type": "commit"}, {"oid": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/ad9fc3c0e3737db6a014ad4dee880731838fb56e", "message": "Cleanup commented code and import", "committedDate": "2020-04-13T15:34:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407554236", "bodyText": "Not our preferred package name, but we seem to be moving in this direction.", "author": "lesv", "createdAt": "2020-04-13T15:57:33Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NjI0OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407676249", "bodyText": "I actually wasn't going to even have a package but needed it due to the way I am testing. Can change if preferred?", "author": "billyjacobson", "createdAt": "2020-04-13T19:40:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE3OTE4Nw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409179187", "bodyText": "This is good now.", "author": "lesv", "createdAt": "2020-04-15T22:49:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NDIzNg=="}], "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\nindex a4fedc6e2..ef33be31a 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\n\n@@ -55,6 +55,9 @@ public class LoadData {\n     long rowSize = options.getMegabytesPerRow() * ONE_MB;\n     final long max =\n         (Math.round((options.getGigabytesWritten() * ONE_GB)) / rowSize);\n+    // Make each number the same length by padding with 0s\n+    int maxLength = (\"\" + max).length();\n+    String numberFormat = \"%0\" + maxLength + \"d\";\n \n     p.apply(GenerateSequence.from(0).to(max))\n         .apply(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NTg5Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407555893", "bodyText": "Should this be done outside the processElement() so it's cached?", "author": "lesv", "createdAt": "2020-04-13T16:00:36Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.CloudBigtableIO;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.util.Random;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.client.Mutation;\n+import org.apache.hadoop.hbase.client.Put;\n+import org.apache.hadoop.hbase.util.Bytes;\n+\n+/**\n+ * A Beam job that loads random data into Cloud Bigtable.\n+ */\n+public class LoadData {\n+\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  public static void main(String[] args) {\n+\n+    WriteDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(WriteDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    long rowSize = options.getMegabytesPerRow() * ONE_MB;\n+    final long max =\n+        (Math.round((options.getGigabytesWritten() * ONE_GB)) / rowSize);\n+\n+    p.apply(GenerateSequence.from(0).to(max))\n+        .apply(\n+            ParDo.of(\n+                new DoFn<Long, Mutation>() {\n+                  @ProcessElement\n+                  public void processElement(@Element Long rowkey, OutputReceiver<Mutation> out) {\n+                    // Make each number the same length by padding with 0s\n+                    int maxLength = (\"\" + max).length();\n+                    String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", rowkey);", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\nindex a4fedc6e2..ef33be31a 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/LoadData.java\n\n@@ -55,6 +55,9 @@ public class LoadData {\n     long rowSize = options.getMegabytesPerRow() * ONE_MB;\n     final long max =\n         (Math.round((options.getGigabytesWritten() * ONE_GB)) / rowSize);\n+    // Make each number the same length by padding with 0s\n+    int maxLength = (\"\" + max).length();\n+    String numberFormat = \"%0\" + maxLength + \"d\";\n \n     p.apply(GenerateSequence.from(0).to(max))\n         .apply(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NjM3OQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407556379", "bodyText": "xtra line?", "author": "lesv", "createdAt": "2020-04-13T16:01:26Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\nindex 7c7a54da2..c6099694d 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n\n@@ -120,6 +120,7 @@ public class ReadData {\n         Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n         ResultScanner imageData = table.getScanner(scan);\n \n+        // Iterate over stream of rows to count them.\n         for (Result row : imageData) {\n           count++;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NzQ5MA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407557490", "bodyText": "Isn't there a better way to do this?  If not, perhaps an explanation of what's going on would be useful?", "author": "lesv", "createdAt": "2020-04-13T16:03:36Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NjgwNw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407676807", "bodyText": "Yeah, this is the way to do it. Can leave a comment", "author": "billyjacobson", "createdAt": "2020-04-13T19:41:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NzQ5MA=="}], "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\nindex 7c7a54da2..c6099694d 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n\n@@ -120,6 +120,7 @@ public class ReadData {\n         Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n         ResultScanner imageData = table.getScanner(scan);\n \n+        // Iterate over stream of rows to count them.\n         for (Result row : imageData) {\n           count++;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODAzMg==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558032", "bodyText": "Should this be more specific?  Can we mitigate issues?  Should we just let this happen?", "author": "lesv", "createdAt": "2020-04-13T16:04:33Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY3NzA0Mw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407677043", "bodyText": "I think it's fine to just let it happen for this codelab", "author": "billyjacobson", "createdAt": "2020-04-13T19:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODAzMg=="}], "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\nindex 7c7a54da2..c6099694d 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n\n@@ -120,6 +120,7 @@ public class ReadData {\n         Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n         ResultScanner imageData = table.getScanner(scan);\n \n+        // Iterate over stream of rows to count them.\n         for (Result row : imageData) {\n           count++;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1ODUyMQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407558521", "bodyText": "More specificity?\nSpecific causes?\nmitigation?\nDon't bother catching?", "author": "lesv", "createdAt": "2020-04-13T16:05:26Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\nindex 7c7a54da2..c6099694d 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n\n@@ -120,6 +120,7 @@ public class ReadData {\n         Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n         ResultScanner imageData = table.getScanner(scan);\n \n+        // Iterate over stream of rows to count them.\n         for (Result row : imageData) {\n           count++;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1OTEwNw==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r407559107", "bodyText": "Should the concatenation really be in a loop?", "author": "lesv", "createdAt": "2020-04-13T16:06:31Z", "path": "bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Google LLC\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package keyviz;\n+\n+import com.google.cloud.bigtable.beam.AbstractCloudBigtableTableDoFn;\n+import com.google.cloud.bigtable.beam.CloudBigtableConfiguration;\n+import com.google.cloud.bigtable.beam.CloudBigtableTableConfiguration;\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.channels.Channels;\n+import java.nio.channels.ReadableByteChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Calendar;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.GenerateSequence;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.hadoop.hbase.TableName;\n+import org.apache.hadoop.hbase.client.Result;\n+import org.apache.hadoop.hbase.client.ResultScanner;\n+import org.apache.hadoop.hbase.client.Scan;\n+import org.apache.hadoop.hbase.client.Table;\n+import org.apache.hadoop.hbase.filter.Filter;\n+import org.apache.hadoop.hbase.filter.FilterList;\n+import org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter;\n+import org.apache.hadoop.hbase.filter.MultiRowRangeFilter.RowRange;\n+import org.apache.hadoop.hbase.util.Bytes;\n+import org.joda.time.Duration;\n+\n+public class ReadData {\n+\n+  static final long KEY_VIZ_WINDOW_MINUTES = 15;\n+  static final long ONE_MB = 1000 * 1000;\n+  static final long ONE_GB = 1000 * ONE_MB;\n+  static final String COLUMN_FAMILY = \"cf\";\n+\n+  static final long START_TIME = getStartTime();\n+\n+  public static void main(String[] args) {\n+    ReadDataOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(ReadDataOptions.class);\n+    Pipeline p = Pipeline.create(options);\n+    CloudBigtableTableConfiguration bigtableTableConfig =\n+        new CloudBigtableTableConfiguration.Builder()\n+            .withProjectId(options.getBigtableProjectId())\n+            .withInstanceId(options.getBigtableInstanceId())\n+            .withTableId(options.getBigtableTableId())\n+            .build();\n+\n+    // Initiates a new pipeline every second\n+    p.apply(GenerateSequence.from(0).withRate(1, new Duration(1000)))\n+        .apply(ParDo.of(new ReadFromTableFn(bigtableTableConfig, options)));\n+    p.run().waitUntilFinish();\n+  }\n+\n+  public static class ReadFromTableFn extends AbstractCloudBigtableTableDoFn<Long, Void> {\n+\n+    List<List<Float>> imageData = new ArrayList<>();\n+    String[] keys;\n+\n+    public ReadFromTableFn(CloudBigtableConfiguration config, ReadDataOptions readDataOptions) {\n+      super(config);\n+      keys = new String[Math.toIntExact(getNumRows(readDataOptions))];\n+      downloadImageData(readDataOptions.getFilePath());\n+      generateRowkeys(getNumRows(readDataOptions));\n+\n+    }\n+\n+    @ProcessElement\n+    public void processElement(PipelineOptions po) {\n+      // Determine which column will be drawn based on runtime of job.\n+      long timestampDiff = System.currentTimeMillis() - START_TIME;\n+      long minutes = (timestampDiff / 1000) / 60;\n+      int timeOffsetIndex = Math.toIntExact(minutes / KEY_VIZ_WINDOW_MINUTES);\n+\n+      ReadDataOptions options = po.as(ReadDataOptions.class);\n+      long count = 0;\n+\n+      List<RowRange> ranges = getRangesForTimeIndex(timeOffsetIndex, getNumRows(options));\n+      if (ranges.size() == 0) {\n+        return;\n+      }\n+\n+      try {\n+        // Scan with a filter that will only return the first key from each row. This filter is used\n+        // to more efficiently perform row count operations.\n+        Filter rangeFilters = new MultiRowRangeFilter(ranges);\n+        FilterList firstKeyFilterWithRanges = new FilterList(new FirstKeyOnlyFilter(),\n+            rangeFilters);\n+        Scan scan =\n+            new Scan()\n+                .addFamily(Bytes.toBytes(COLUMN_FAMILY))\n+                .setFilter(firstKeyFilterWithRanges);\n+\n+        Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n+        ResultScanner imageData = table.getScanner(scan);\n+\n+        for (Result row : imageData) {\n+          count++;\n+        }\n+      } catch (Exception e) {\n+        System.out.println(\"Error reading.\");\n+        e.printStackTrace();\n+      }\n+      System.out.printf(\"got %d rows\\n\", count);\n+    }\n+\n+    /**\n+     * Download the image data as a grid of weights and store them in a 2D array.\n+     */\n+    private void downloadImageData(String artUrl) {\n+      try {\n+        ReadableByteChannel chan =\n+            FileSystems.open(\n+                FileSystems.matchNewResource(artUrl, false /* is_directory */));\n+        InputStream is = Channels.newInputStream(chan);\n+        BufferedReader br = new BufferedReader(new InputStreamReader(is));\n+\n+        String line;\n+        while ((line = br.readLine()) != null) {\n+          imageData.add(\n+              Arrays.stream(line.split(\",\"))\n+                  .map(Float::valueOf)\n+                  .collect(Collectors.toList()));\n+        }\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+      }\n+    }\n+\n+    /**\n+     * Generates an array with the rowkeys that were loaded into the specified Bigtable. This is\n+     * used to create the correct intervals for scanning equal sections of rowkeys. Since Bigtable\n+     * sorts keys lexicographically if we just used standard intervals, each section would have\n+     * different sizes.\n+     */\n+    private void generateRowkeys(long maxInput) {\n+      int maxLength = (\"\" + maxInput).length();\n+      for (int i = 0; i < maxInput; i++) {\n+        // Make each number the same length by padding with 0s.\n+        String paddedRowkey = String.format(\"%0\" + maxLength + \"d\", i);", "originalCommit": "ad9fc3c0e3737db6a014ad4dee880731838fb56e", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "chunk": "diff --git a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\nindex 7c7a54da2..c6099694d 100644\n--- a/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n+++ b/bigtable/beam/keyviz-art/src/main/java/keyviz/ReadData.java\n\n@@ -120,6 +120,7 @@ public class ReadData {\n         Table table = getConnection().getTable(TableName.valueOf(options.getBigtableTableId()));\n         ResultScanner imageData = table.getScanner(scan);\n \n+        // Iterate over stream of rows to count them.\n         for (Result row : imageData) {\n           count++;\n         }\n"}}, {"oid": "7d1e86e2441864bdacf5c5afef0b304391f58e55", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/7d1e86e2441864bdacf5c5afef0b304391f58e55", "message": "Cache number format, comment on row counting", "committedDate": "2020-04-13T19:46:15Z", "type": "commit"}, {"oid": "c8a39ba96e8a0339048f69ee28ba6fd3178f8eb2", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/c8a39ba96e8a0339048f69ee28ba6fd3178f8eb2", "message": "empty line", "committedDate": "2020-04-13T19:46:52Z", "type": "commit"}, {"oid": "578922c87135900f5a704bc435bd8272b84ce628", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/commit/578922c87135900f5a704bc435bd8272b84ce628", "message": "Fix filter for efficient row counting", "committedDate": "2020-04-15T14:47:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODY1NQ==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r409748655", "bodyText": "just want to confirm this is the default, and that's why we're deleting it?", "author": "kolea2", "createdAt": "2020-04-16T18:02:30Z", "path": "bigtable/beam/helloworld/src/test/java/HelloWorldTest.java", "diffHunk": "@@ -104,8 +104,7 @@ public void testWrite() {\n         new String[] {\n           \"--bigtableProjectId=\" + projectId,\n           \"--bigtableInstanceId=\" + instanceId,\n-          \"--bigtableTableId=\" + TABLE_ID,\n-          \"--runner=DirectRunner\"", "originalCommit": "578922c87135900f5a704bc435bd8272b84ce628", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIxMzkxOA==", "url": "https://github.com/GoogleCloudPlatform/java-docs-samples/pull/2631#discussion_r413213918", "bodyText": "correct", "author": "billyjacobson", "createdAt": "2020-04-22T18:20:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc0ODY1NQ=="}], "type": "inlineReview", "revised_code": null}]}