{"pr_number": 1259, "pr_title": "SAMZA-2444: JobModel save in CoordinatorStreamStore resulting flush for each message", "pr_createdAt": "2020-01-23T22:04:08Z", "pr_url": "https://github.com/apache/samza/pull/1259", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM2NDQ4OA==", "url": "https://github.com/apache/samza/pull/1259#discussion_r371364488", "bodyText": "Is this operation meant to be atomic? Does an impl of MetadataStore ensure that either all or none of this map is deleted?\nIs this operation meant to be idempotent?", "author": "lakshmi-manasa-g", "createdAt": "2020-01-27T17:02:32Z", "path": "samza-api/src/main/java/org/apache/samza/metadatastore/MetadataStore.java", "diffHunk": "@@ -67,6 +68,16 @@ default void putAll(Map<String, byte[]> entries) {\n    */\n   void delete(String key);\n \n+  /**\n+   * Deletes the mapping with the specified set.", "originalCommit": "cd0aaac214f31cb1e77f812d3af2015e9eb6e7fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "chunk": "diff --git a/samza-api/src/main/java/org/apache/samza/metadatastore/MetadataStore.java b/samza-api/src/main/java/org/apache/samza/metadatastore/MetadataStore.java\nindex 402b2d87..1358aa52 100644\n--- a/samza-api/src/main/java/org/apache/samza/metadatastore/MetadataStore.java\n+++ b/samza-api/src/main/java/org/apache/samza/metadatastore/MetadataStore.java\n\n@@ -68,16 +67,6 @@ public interface MetadataStore {\n    */\n   void delete(String key);\n \n-  /**\n-   * Deletes the mapping with the specified set.\n-   * @param keys the keys for what the mappings are to be deleted.\n-   */\n-  default void deleteAll(Collection<String> keys) {\n-    for (String key : keys) {\n-      delete(key);\n-    }\n-  }\n-\n   /**\n    * Returns all the entries in this metadata store.\n    *\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4NjU2OQ==", "url": "https://github.com/apache/samza/pull/1259#discussion_r371386569", "bodyText": "minor: might be good to add {@inheritdoc}", "author": "lakshmi-manasa-g", "createdAt": "2020-01-27T17:45:11Z", "path": "samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java", "diffHunk": "@@ -155,6 +156,15 @@ public void delete(String namespacedKey) {\n     put(namespacedKey, null);\n   }\n \n+  @Override", "originalCommit": "cd0aaac214f31cb1e77f812d3af2015e9eb6e7fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\nindex c749be32..1df4f701 100644\n--- a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n+++ b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n\n@@ -151,18 +137,16 @@ public class CoordinatorStreamStore implements MetadataStore {\n   }\n \n   @Override\n-  public void delete(String namespacedKey) {\n-    // Since kafka doesn't support individual message deletion, store value as null for a namespacedKey to delete.\n-    put(namespacedKey, null);\n+  public void putAll(Map<String, byte[]> entries) {\n+    for (Map.Entry<String, byte[]> entry : entries.entrySet()) {\n+      put(entry.getKey(), entry.getValue());\n+    }\n   }\n \n   @Override\n-  public void deleteAll(Collection<String> namespacedKeys) {\n-    Map<String, byte[]> entries = new HashMap<>(namespacedKeys.size());\n-    for (String namespacedKey : namespacedKeys) {\n-      entries.put(namespacedKey, null);\n-    }\n-    putAll(entries);\n+  public void delete(String namespacedKey) {\n+    // Since kafka doesn't support individual message deletion, store value as null for a namespacedKey to delete.\n+    put(namespacedKey, null);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4ODA5Ng==", "url": "https://github.com/apache/samza/pull/1259#discussion_r371388096", "bodyText": "this is not atomic right. cause, theflush() is when the entries are guaranteed to be persisted on the coordinator stream and putAll does flush only at the end of all put. So if this is the expectation, then the MetadataStore interface should allow it.", "author": "lakshmi-manasa-g", "createdAt": "2020-01-27T17:48:14Z", "path": "samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java", "diffHunk": "@@ -155,6 +156,15 @@ public void delete(String namespacedKey) {\n     put(namespacedKey, null);\n   }\n \n+  @Override\n+  public void deleteAll(Collection<String> namespacedKeys) {\n+    Map<String, byte[]> entries = new HashMap<>(namespacedKeys.size());\n+    for (String namespacedKey : namespacedKeys) {\n+      entries.put(namespacedKey, null);\n+    }\n+    putAll(entries);", "originalCommit": "cd0aaac214f31cb1e77f812d3af2015e9eb6e7fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\nindex c749be32..1df4f701 100644\n--- a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n+++ b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n\n@@ -151,18 +137,16 @@ public class CoordinatorStreamStore implements MetadataStore {\n   }\n \n   @Override\n-  public void delete(String namespacedKey) {\n-    // Since kafka doesn't support individual message deletion, store value as null for a namespacedKey to delete.\n-    put(namespacedKey, null);\n+  public void putAll(Map<String, byte[]> entries) {\n+    for (Map.Entry<String, byte[]> entry : entries.entrySet()) {\n+      put(entry.getKey(), entry.getValue());\n+    }\n   }\n \n   @Override\n-  public void deleteAll(Collection<String> namespacedKeys) {\n-    Map<String, byte[]> entries = new HashMap<>(namespacedKeys.size());\n-    for (String namespacedKey : namespacedKeys) {\n-      entries.put(namespacedKey, null);\n-    }\n-    putAll(entries);\n+  public void delete(String namespacedKey) {\n+    // Since kafka doesn't support individual message deletion, store value as null for a namespacedKey to delete.\n+    put(namespacedKey, null);\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM5NjQzNw==", "url": "https://github.com/apache/samza/pull/1259#discussion_r371396437", "bodyText": "would it be good to replace wirteTaskContainerMapping with the new wirteTaskContainerMappings altogether?\nthere seems to be only one usage of this method and that has been replaced to use the new one in JobModelManager.\nis there a reason to keep this old method?", "author": "lakshmi-manasa-g", "createdAt": "2020-01-27T18:05:30Z", "path": "samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java", "diffHunk": "@@ -103,21 +106,53 @@ public TaskAssignmentManager(MetadataStore taskContainerMappingMetadataStore, Me\n    * @param taskMode the mode of the task\n    */\n   public void writeTaskContainerMapping(String taskName, String containerId, TaskMode taskMode) {\n-    String existingContainerId = taskNameToContainerId.get(taskName);\n-    if (existingContainerId != null && !existingContainerId.equals(containerId)) {\n-      LOG.info(\"Task \\\"{}\\\" in mode {} moved from container {} to container {}\", new Object[]{taskName, taskMode, existingContainerId, containerId});\n-    } else {\n-      LOG.debug(\"Task \\\"{}\\\" in mode {} assigned to container {}\", taskName, taskMode, containerId);\n-    }\n+    writeTaskContainerMappings(ImmutableMap.of(containerId, ImmutableMap.of(taskName, taskMode)));", "originalCommit": "cd0aaac214f31cb1e77f812d3af2015e9eb6e7fc", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java\nindex 9414d0c7..16f8a519 100644\n--- a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java\n+++ b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskAssignmentManager.java\n\n@@ -106,53 +103,21 @@ public class TaskAssignmentManager {\n    * @param taskMode the mode of the task\n    */\n   public void writeTaskContainerMapping(String taskName, String containerId, TaskMode taskMode) {\n-    writeTaskContainerMappings(ImmutableMap.of(containerId, ImmutableMap.of(taskName, taskMode)));\n-  }\n-\n-  /**\n-   * Method to batch write task container info to {@link MetadataStore}.\n-   * @param mappings the task and container mappings: (ContainerId, (TaskName, TaskMode))\n-   */\n-  public void writeTaskContainerMappings(Map<String, Map<String, TaskMode>> mappings) {\n-    Set<String> taskContainerMappingsToBeDeleted = new HashSet<>();\n-    Set<String> taskModeMappingsToBeDeleted = new HashSet<>();\n-    Map<String, byte[]> taskContainerMappingsToBeStored = new HashMap<>();\n-    Map<String, byte[]> taskModeMappingsToBeStored = new HashMap<>();\n-\n-    for (String containerId : mappings.keySet()) {\n-      Map<String, TaskMode> tasks = mappings.get(containerId);\n-      for (String taskName : tasks.keySet()) {\n-        TaskMode taskMode = tasks.get(taskName);\n-\n-        String existingContainerId = taskNameToContainerId.get(taskName);\n-        if (existingContainerId != null && !existingContainerId.equals(containerId)) {\n-          LOG.info(\"Task \\\"{}\\\" in mode {} moved from container {} to container {}\", new Object[]{taskName, taskMode, existingContainerId, containerId});\n-        } else {\n-          LOG.debug(\"Task \\\"{}\\\" in mode {} assigned to container {}\", taskName, taskMode, containerId);\n-        }\n-\n-        if (containerId == null) {\n-          taskContainerMappingsToBeDeleted.add(taskName);\n-          taskModeMappingsToBeDeleted.add(taskName);\n-          taskNameToContainerId.remove(taskName);\n-        } else {\n-          taskContainerMappingsToBeStored.put(taskName, containerIdSerde.toBytes(containerId));\n-          taskModeMappingsToBeStored.put(taskName, taskModeSerde.toBytes(taskMode.toString()));\n-          taskNameToContainerId.put(taskName, containerId);\n-        }\n-      }\n-    }\n-    if (!taskContainerMappingsToBeDeleted.isEmpty()) {\n-      taskContainerMappingMetadataStore.deleteAll(taskContainerMappingsToBeDeleted);\n-    }\n-    if (!taskModeMappingsToBeDeleted.isEmpty()) {\n-      taskModeMappingMetadataStore.deleteAll(taskModeMappingsToBeDeleted);\n-    }\n-    if (!taskContainerMappingsToBeStored.isEmpty()) {\n-      taskContainerMappingMetadataStore.putAll(taskContainerMappingsToBeStored);\n+    String existingContainerId = taskNameToContainerId.get(taskName);\n+    if (existingContainerId != null && !existingContainerId.equals(containerId)) {\n+      LOG.info(\"Task \\\"{}\\\" in mode {} moved from container {} to container {}\", new Object[]{taskName, taskMode, existingContainerId, containerId});\n+    } else {\n+      LOG.debug(\"Task \\\"{}\\\" in mode {} assigned to container {}\", taskName, taskMode, containerId);\n     }\n-    if (!taskModeMappingsToBeStored.isEmpty()) {\n-      taskModeMappingMetadataStore.putAll(taskModeMappingsToBeStored);\n+\n+    if (containerId == null) {\n+      taskContainerMappingMetadataStore.delete(taskName);\n+      taskModeMappingMetadataStore.delete(taskName);\n+      taskNameToContainerId.remove(taskName);\n+    } else {\n+      taskContainerMappingMetadataStore.put(taskName, containerIdSerde.toBytes(containerId));\n+      taskModeMappingMetadataStore.put(taskName, taskModeSerde.toBytes(taskMode.toString()));\n+      taskNameToContainerId.put(taskName, containerId);\n     }\n   }\n \n"}}, {"oid": "b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "url": "https://github.com/apache/samza/commit/b7efe8db5b5a878c84eaf52e16f0d15ffafe7518", "message": "Remove flush operation out of put functions\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T19:37:54Z", "type": "commit"}, {"oid": "413c834857f6f7a17d8563fe15cb92863560cb91", "url": "https://github.com/apache/samza/commit/413c834857f6f7a17d8563fe15cb92863560cb91", "message": "Explicitly call flush method after calling put/putAll/delete methods\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T21:45:01Z", "type": "commit"}, {"oid": "4f57592d6cac0596ce970448ece2f37e0699dea7", "url": "https://github.com/apache/samza/commit/4f57592d6cac0596ce970448ece2f37e0699dea7", "message": "Check flush call in unit tests\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T21:52:31Z", "type": "commit"}, {"oid": "7466ca31b30248e756b618da707064d49b43c2e1", "url": "https://github.com/apache/samza/commit/7466ca31b30248e756b618da707064d49b43c2e1", "message": "Improve performance with batch udpate\n\n1. Batch write task partition assignments information to metadata store.\n2. Batch write task container information to metadata store.\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T23:09:50Z", "type": "commit"}, {"oid": "a6696017fb3320f09a0b966dbb7c7c6ae9ae9fb0", "url": "https://github.com/apache/samza/commit/a6696017fb3320f09a0b966dbb7c7c6ae9ae9fb0", "message": "Fix checkstyle issue\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T23:15:08Z", "type": "commit"}, {"oid": "a6696017fb3320f09a0b966dbb7c7c6ae9ae9fb0", "url": "https://github.com/apache/samza/commit/a6696017fb3320f09a0b966dbb7c7c6ae9ae9fb0", "message": "Fix checkstyle issue\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T23:15:08Z", "type": "forcePushed"}, {"oid": "e34e917e499c143841ca0bb588dc1323de9dbb17", "url": "https://github.com/apache/samza/commit/e34e917e499c143841ca0bb588dc1323de9dbb17", "message": "Fix unit test failures\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-27T23:29:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwNDI0OQ==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372004249", "bodyText": "why do we catch only exceptions from metadatstore.put and not from its delete or flush?", "author": "lakshmi-manasa-g", "createdAt": "2020-01-28T19:18:00Z", "path": "samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java", "diffHunk": "@@ -66,28 +66,33 @@ public TaskPartitionAssignmentManager(MetadataStore metadataStore) {\n   }\n \n   /**\n-   * Stores the task to partition assignments to the metadata store.\n-   * @param partition the system stream partition.\n-   * @param taskNames the task names to which the partition is assigned to.\n+   * Stores the task names to {@link SystemStreamPartition} assignments to the metadata store.\n+   * @param sspToTaskNameMapping the mapped assignments to write to the metadata store. If the task name list is empty,\n+   *                             then the entry is deleted from the metadata store.\n    */\n-  public void writeTaskPartitionAssignment(SystemStreamPartition partition, List<String> taskNames) {\n-    // For broadcast streams, a input system stream partition will be mapped to more than one tasks in a\n-    // SamzaContainer. Rather than storing taskName to list of SystemStreamPartitions in metadata store, here\n-    // systemStreamPartition to list of taskNames is stored. This was done due to 1 MB limit on value size in kafka.\n-    String serializedSSPAsJson = serializeSSPToJson(partition);\n-    if (taskNames == null || taskNames.isEmpty()) {\n-      LOG.info(\"Deleting the key: {} from the metadata store.\", partition);\n-      metadataStore.delete(serializedSSPAsJson);\n-    } else {\n-      try {\n-        String taskNamesAsString = taskNamesMapper.writeValueAsString(taskNames);\n-        byte[] taskNamesAsBytes = valueSerde.toBytes(taskNamesAsString);\n-        LOG.info(\"Storing the partition: {} and taskNames: {} into the metadata store.\", serializedSSPAsJson, taskNames);\n-        metadataStore.put(serializedSSPAsJson, taskNamesAsBytes);\n-      } catch (Exception e) {\n-        throw new SamzaException(\"Exception occurred when writing task to partition assignment.\", e);\n+  public void writeTaskPartitionAssignments(Map<SystemStreamPartition, List<String>> sspToTaskNameMapping) {\n+    for (SystemStreamPartition partition: sspToTaskNameMapping.keySet()) {\n+      List<String> taskNames = sspToTaskNameMapping.get(partition);\n+      LOG.info(\"Storing ssp: {} and task: {} into metadata store\", partition, taskNames);\n+      // For broadcast streams, a input system stream partition will be mapped to more than one tasks in a\n+      // SamzaContainer. Rather than storing taskName to list of SystemStreamPartitions in metadata store, here\n+      // systemStreamPartition to list of taskNames is stored. This was done due to 1 MB limit on value size in kafka.\n+      String serializedSSPAsJson = serializeSSPToJson(partition);\n+      if (taskNames == null || taskNames.isEmpty()) {\n+        LOG.info(\"Deleting the key: {} from the metadata store.\", partition);\n+        metadataStore.delete(serializedSSPAsJson);\n+      } else {\n+        try {", "originalCommit": "e34e917e499c143841ca0bb588dc1323de9dbb17", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA1MTIxOA==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372051218", "bodyText": "In order to not break anything here, I just move the whole code block to the new function.\nThe statement String taskNamesAsString = taskNamesMapper.writeValueAsString(taskNames) throws exceptions explicitly, so exception need to be catched explicitly. I think original idea is not for metadatstore.put method.", "author": "alnzng", "createdAt": "2020-01-28T20:57:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwNDI0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA1MjYzOQ==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372052639", "bodyText": "ah, got it. thanks!", "author": "lakshmi-manasa-g", "createdAt": "2020-01-28T21:00:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwNDI0OQ=="}], "type": "inlineReview", "revised_code": {"commit": "dfe49bcbd59eff0df67b15880e45b8d82fbf1ceb", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\nindex 484a824b..9b9c71e4 100644\n--- a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\n+++ b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\n\n@@ -73,7 +73,6 @@ public class TaskPartitionAssignmentManager {\n   public void writeTaskPartitionAssignments(Map<SystemStreamPartition, List<String>> sspToTaskNameMapping) {\n     for (SystemStreamPartition partition: sspToTaskNameMapping.keySet()) {\n       List<String> taskNames = sspToTaskNameMapping.get(partition);\n-      LOG.info(\"Storing ssp: {} and task: {} into metadata store\", partition, taskNames);\n       // For broadcast streams, a input system stream partition will be mapped to more than one tasks in a\n       // SamzaContainer. Rather than storing taskName to list of SystemStreamPartitions in metadata store, here\n       // systemStreamPartition to list of taskNames is stored. This was done due to 1 MB limit on value size in kafka.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwNjE1Nw==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372006157", "bodyText": "do we need this impl of putAll because MetadataStore.putAll default impl in the interface MetadatStore is the same?", "author": "lakshmi-manasa-g", "createdAt": "2020-01-28T19:21:48Z", "path": "samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java", "diffHunk": "@@ -149,6 +136,13 @@ private void putWithoutFlush(String namespacedKey, byte[] value) {\n     systemProducer.send(SOURCE, envelope);\n   }\n \n+  @Override\n+  public void putAll(Map<String, byte[]> entries) {", "originalCommit": "e34e917e499c143841ca0bb588dc1323de9dbb17", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA0NzYwOQ==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372047609", "bodyText": "Good catch @lakshmi-manasa-g . This function is duplicated with the one in MetadataStore, I will delete it.", "author": "alnzng", "createdAt": "2020-01-28T20:49:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAwNjE1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "dfe49bcbd59eff0df67b15880e45b8d82fbf1ceb", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\nindex 1df4f701..24ce4576 100644\n--- a/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n+++ b/samza-core/src/main/java/org/apache/samza/coordinator/metadatastore/CoordinatorStreamStore.java\n\n@@ -136,13 +136,6 @@ public class CoordinatorStreamStore implements MetadataStore {\n     systemProducer.send(SOURCE, envelope);\n   }\n \n-  @Override\n-  public void putAll(Map<String, byte[]> entries) {\n-    for (Map.Entry<String, byte[]> entry : entries.entrySet()) {\n-      put(entry.getKey(), entry.getValue());\n-    }\n-  }\n-\n   @Override\n   public void delete(String namespacedKey) {\n     // Since kafka doesn't support individual message deletion, store value as null for a namespacedKey to delete.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNjg2NA==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372016864", "bodyText": "minor: is this log statement really needed? Because it will be followed by another log statement immediately that says either the key is being deleted or partition is being stored (only that the SSP is a JSON in the storing log statement).", "author": "lakshmi-manasa-g", "createdAt": "2020-01-28T19:42:48Z", "path": "samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java", "diffHunk": "@@ -66,28 +66,33 @@ public TaskPartitionAssignmentManager(MetadataStore metadataStore) {\n   }\n \n   /**\n-   * Stores the task to partition assignments to the metadata store.\n-   * @param partition the system stream partition.\n-   * @param taskNames the task names to which the partition is assigned to.\n+   * Stores the task names to {@link SystemStreamPartition} assignments to the metadata store.\n+   * @param sspToTaskNameMapping the mapped assignments to write to the metadata store. If the task name list is empty,\n+   *                             then the entry is deleted from the metadata store.\n    */\n-  public void writeTaskPartitionAssignment(SystemStreamPartition partition, List<String> taskNames) {\n-    // For broadcast streams, a input system stream partition will be mapped to more than one tasks in a\n-    // SamzaContainer. Rather than storing taskName to list of SystemStreamPartitions in metadata store, here\n-    // systemStreamPartition to list of taskNames is stored. This was done due to 1 MB limit on value size in kafka.\n-    String serializedSSPAsJson = serializeSSPToJson(partition);\n-    if (taskNames == null || taskNames.isEmpty()) {\n-      LOG.info(\"Deleting the key: {} from the metadata store.\", partition);\n-      metadataStore.delete(serializedSSPAsJson);\n-    } else {\n-      try {\n-        String taskNamesAsString = taskNamesMapper.writeValueAsString(taskNames);\n-        byte[] taskNamesAsBytes = valueSerde.toBytes(taskNamesAsString);\n-        LOG.info(\"Storing the partition: {} and taskNames: {} into the metadata store.\", serializedSSPAsJson, taskNames);\n-        metadataStore.put(serializedSSPAsJson, taskNamesAsBytes);\n-      } catch (Exception e) {\n-        throw new SamzaException(\"Exception occurred when writing task to partition assignment.\", e);\n+  public void writeTaskPartitionAssignments(Map<SystemStreamPartition, List<String>> sspToTaskNameMapping) {\n+    for (SystemStreamPartition partition: sspToTaskNameMapping.keySet()) {\n+      List<String> taskNames = sspToTaskNameMapping.get(partition);\n+      LOG.info(\"Storing ssp: {} and task: {} into metadata store\", partition, taskNames);", "originalCommit": "e34e917e499c143841ca0bb588dc1323de9dbb17", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjA1Mzk3NA==", "url": "https://github.com/apache/samza/pull/1259#discussion_r372053974", "bodyText": "Hmm, I think you are right. Let me remove this log line. Thanks.", "author": "alnzng", "createdAt": "2020-01-28T21:02:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjAxNjg2NA=="}], "type": "inlineReview", "revised_code": {"commit": "dfe49bcbd59eff0df67b15880e45b8d82fbf1ceb", "chunk": "diff --git a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\nindex 484a824b..9b9c71e4 100644\n--- a/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\n+++ b/samza-core/src/main/java/org/apache/samza/container/grouper/task/TaskPartitionAssignmentManager.java\n\n@@ -73,7 +73,6 @@ public class TaskPartitionAssignmentManager {\n   public void writeTaskPartitionAssignments(Map<SystemStreamPartition, List<String>> sspToTaskNameMapping) {\n     for (SystemStreamPartition partition: sspToTaskNameMapping.keySet()) {\n       List<String> taskNames = sspToTaskNameMapping.get(partition);\n-      LOG.info(\"Storing ssp: {} and task: {} into metadata store\", partition, taskNames);\n       // For broadcast streams, a input system stream partition will be mapped to more than one tasks in a\n       // SamzaContainer. Rather than storing taskName to list of SystemStreamPartitions in metadata store, here\n       // systemStreamPartition to list of taskNames is stored. This was done due to 1 MB limit on value size in kafka.\n"}}, {"oid": "dfe49bcbd59eff0df67b15880e45b8d82fbf1ceb", "url": "https://github.com/apache/samza/commit/dfe49bcbd59eff0df67b15880e45b8d82fbf1ceb", "message": "Remove duplicated codes and useless log\n\nSigned-off-by: Alan Zhang <shuai.xyz@gmail.com>", "committedDate": "2020-01-28T21:07:48Z", "type": "commit"}]}