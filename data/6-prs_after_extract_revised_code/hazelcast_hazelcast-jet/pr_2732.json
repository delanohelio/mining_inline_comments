{"pr_number": 2732, "pr_title": "Reduce poll and init timeout in Kafka source", "pr_createdAt": "2020-12-04T15:16:25Z", "pr_url": "https://github.com/hazelcast/hazelcast-jet/pull/2732", "timeline": [{"oid": "ef38930e1e3892c7b0d205728b51cb42cbc6786f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/ef38930e1e3892c7b0d205728b51cb42cbc6786f", "message": "Reduce poll and init timeout in Kafka source\n\nWe reduced the `poll` timeout from 50ms to 0ms. The consumer receives\nrecords on the background, there's no need to block for more records,\njust grab those that are immediately available. This reduces snapshot\nlatency.\n\nSecondly, it fixes the issue when the `consumer.partitionsFor()` can\nblock for up to 60 seconds waiting for the metadata in the `init()`\nmethod. We reduced the waiting to 1 second, which can easily not be\nenough. As as consequence, we had to adapt the processor to live without\npartition information and try to get it later (we already did that when\nchecking for partitions added at runtime). The major change was that\nnow, when restoring offsets from the snapshot, we can't rely on\n`currentAssignment`, but we'll simply assume the topic has at least as\nmany partitions as the number of offsets we restored for that topic.\n\nAs a side effect, this also fixes the issue when a partition was added\nwhile the job was down. Take this scenario:\n- topic T has 1 partition\n- the job is suspended\n- while suspended, another partition is added\n- the job wakes up\n\nNow there's the change: Previously, the processor found out in the\n`init` method that there are 2 partitions. These partitions would start\nat default positions. Then, offset for partition 1 was restored, but no\noffset for partition 2. If the `auto.offset.reset` parameter was\n`latest` (the default), we could miss the messages in partition 2 before\nthe job restarted.\n\nWith current implementation we don't check partition counts in `init`.\nWhen the job restarts, it will restore offsets for partition 1, but not\nfor 2. The partition count for the topic will be inferred to 1. Later,\nafter the snapshot is restored, it will query the metadata for the topic\nand find out there are 2 partitions. The 2nd partition will be added and\nsought to the beginning and all items from it will be consumed.\n\nFixes #2724", "committedDate": "2020-12-04T15:15:32Z", "type": "commit"}, {"oid": "3264f3848301e6aa981c5881071e991be541028d", "url": "https://github.com/hazelcast/hazelcast-jet/commit/3264f3848301e6aa981c5881071e991be541028d", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into kafka-fixes", "committedDate": "2020-12-07T08:20:50Z", "type": "commit"}, {"oid": "c1521e199747448356f51f35eaedf60c7ffa7b5f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/c1521e199747448356f51f35eaedf60c7ffa7b5f", "message": "Add tests and final fixes", "committedDate": "2020-12-07T13:35:13Z", "type": "commit"}, {"oid": "e0d12238f53bc5ee9251cb76af167f90a3c98ef4", "url": "https://github.com/hazelcast/hazelcast-jet/commit/e0d12238f53bc5ee9251cb76af167f90a3c98ef4", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into kafka-fixes\n\n# Conflicts:\n#\thazelcast-jet-core/src/test/java/com/hazelcast/jet/pipeline/MockLoggingFactory.java", "committedDate": "2020-12-07T13:38:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzY2MDgwOQ==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2732#discussion_r537660809", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    getLogger().info(\"New partition(s) handled: \" + newAssignments);\n          \n          \n            \n                    getLogger().info(\"New partition(s) assigned: \" + newAssignments);", "author": "gierlachg", "createdAt": "2020-12-07T16:50:05Z", "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/StreamKafkaP.java", "diffHunk": "@@ -107,60 +106,63 @@ protected void init(@Nonnull Context context) {\n         processorIndex = context.globalProcessorIndex();\n         totalParallelism = context.totalParallelism();\n         consumer = new KafkaConsumer<>(properties);\n-        assignPartitions(false);\n     }\n \n-    private void assignPartitions(boolean seekToBeginning) {\n+    private void assignPartitions() {\n         if (System.nanoTime() < nextMetadataCheck) {\n             return;\n         }\n-        boolean allEqual = true;\n-        for (int i = 0; i < topics.size(); i++) {\n-            int newCount = consumer.partitionsFor(topics.get(i)).size();\n-            allEqual &= partitionCounts[i] == newCount;\n-            partitionCounts[i] = newCount;\n-        }\n-        if (allEqual) {\n-            return;\n-        }\n-\n-        KafkaPartitionAssigner assigner = new KafkaPartitionAssigner(topics, partitionCounts, totalParallelism);\n-        Set<TopicPartition> newAssignments = assigner.topicPartitionsFor(processorIndex);\n-        logFinest(getLogger(), \"Currently assigned partitions: %s\", newAssignments);\n-\n-        newAssignments.removeAll(currentAssignment.keySet());\n-        if (!newAssignments.isEmpty()) {\n-            getLogger().info(\"Partition assignments changed, added partitions: \" + newAssignments);\n-            for (TopicPartition tp : newAssignments) {\n-                currentAssignment.put(tp, currentAssignment.size());\n-            }\n-            eventTimeMapper.addPartitions(newAssignments.size());\n-            consumer.assign(currentAssignment.keySet());\n-            if (seekToBeginning) {\n-                // for newly detected partitions, we should always seek to the beginning\n-                consumer.seekToBeginning(newAssignments);\n+        for (int topicIndex = 0; topicIndex < topics.size(); topicIndex++) {\n+            int newPartitionCount;\n+            String topicName = topics.get(topicIndex);\n+            try {\n+                newPartitionCount = consumer.partitionsFor(topicName, Duration.ofSeconds(1)).size();\n+            } catch (TimeoutException e) {\n+                // If we fail to get the metadata, don't try other topics (they are likely to fail too)\n+                getLogger().warning(\"Unable to get partition metadata, ignoring: \" + e, e);\n+                return;\n             }\n+\n+            handleNewPartitions(topicIndex, newPartitionCount);\n         }\n \n-        createOrExtendOffsetsArrays();\n         nextMetadataCheck = System.nanoTime() + METADATA_CHECK_INTERVAL_NANOS;\n     }\n \n-    private void createOrExtendOffsetsArrays() {\n-        for (int topicIdx = 0; topicIdx < partitionCounts.length; topicIdx++) {\n-            int newPartitionCount = partitionCounts[topicIdx];\n-            String topicName = topics.get(topicIdx);\n-            long[] oldOffsets = offsets.get(topicName);\n-            if (oldOffsets != null && oldOffsets.length == newPartitionCount) {\n-                continue;\n-            }\n-            long[] newOffsets = new long[newPartitionCount];\n-            Arrays.fill(newOffsets, -1);\n-            if (oldOffsets != null) {\n-                arraycopy(oldOffsets, 0, newOffsets, 0, oldOffsets.length);\n+    private void handleNewPartitions(int topicIndex, int newPartitionCount) {\n+        String topicName = topics.get(topicIndex);\n+        long[] oldTopicOffsets = offsets.get(topicName);\n+        if (oldTopicOffsets.length >= newPartitionCount) {\n+            return;\n+        }\n+        // extend the offsets array for this topic\n+        long[] newOffsets = Arrays.copyOf(oldTopicOffsets, newPartitionCount);\n+        Arrays.fill(newOffsets, oldTopicOffsets.length, newOffsets.length, -1);\n+        offsets.put(topicName, newOffsets);\n+        Collection<TopicPartition> newAssignments = new ArrayList<>();\n+        for (int partition = oldTopicOffsets.length; partition < newPartitionCount; partition++) {\n+            if (handledByThisProcessor(topicIndex, partition)) {\n+                TopicPartition tp = new TopicPartition(topicName, partition);\n+                currentAssignment.put(tp, currentAssignment.size());\n+                newAssignments.add(tp);\n             }\n-            offsets.put(topicName, newOffsets);\n         }\n+        if (newAssignments.isEmpty()) {\n+            return;\n+        }\n+        getLogger().info(\"New partition(s) handled: \" + newAssignments);", "originalCommit": "e0d12238f53bc5ee9251cb76af167f90a3c98ef4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aee8632dfbeec6e82af0358cf134dac7ed6cf27f", "chunk": "diff --git a/extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/StreamKafkaP.java b/extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/StreamKafkaP.java\nindex 555c14463..45d4afbaf 100644\n--- a/extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/StreamKafkaP.java\n+++ b/extensions/kafka/src/main/java/com/hazelcast/jet/kafka/impl/StreamKafkaP.java\n\n@@ -123,13 +123,13 @@ public final class StreamKafkaP<K, V, T> extends AbstractProcessor {\n                 return;\n             }\n \n-            handleNewPartitions(topicIndex, newPartitionCount);\n+            handleNewPartitions(topicIndex, newPartitionCount, false);\n         }\n \n         nextMetadataCheck = System.nanoTime() + METADATA_CHECK_INTERVAL_NANOS;\n     }\n \n-    private void handleNewPartitions(int topicIndex, int newPartitionCount) {\n+    private void handleNewPartitions(int topicIndex, int newPartitionCount, boolean isRestoring) {\n         String topicName = topics.get(topicIndex);\n         long[] oldTopicOffsets = offsets.get(topicName);\n         if (oldTopicOffsets.length >= newPartitionCount) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODEwMzkzMw==", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2732#discussion_r538103933", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    assertTrue(processor.   currentAssignment.isEmpty());\n          \n          \n            \n                    assertTrue(processor.currentAssignment.isEmpty());", "author": "gierlachg", "createdAt": "2020-12-08T07:40:17Z", "path": "extensions/kafka/src/test/java/com/hazelcast/jet/kafka/impl/StreamKafkaPTest.java", "diffHunk": "@@ -244,7 +246,7 @@ public void when_noAssignedPartitionAndAddedLater_then_resumesFromIdle() throws\n                 .setTotalParallelism(INITIAL_PARTITION_COUNT + 1)\n                 .setGlobalProcessorIndex(INITIAL_PARTITION_COUNT));\n \n-        assertTrue(processor.currentAssignment.isEmpty());\n+        assertTrue(processor.   currentAssignment.isEmpty());", "originalCommit": "e0d12238f53bc5ee9251cb76af167f90a3c98ef4", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f16e8a3b66e47b02ee983e68e46800948bd76b71", "chunk": "diff --git a/extensions/kafka/src/test/java/com/hazelcast/jet/kafka/impl/StreamKafkaPTest.java b/extensions/kafka/src/test/java/com/hazelcast/jet/kafka/impl/StreamKafkaPTest.java\nindex fc739d843..b76fff47f 100644\n--- a/extensions/kafka/src/test/java/com/hazelcast/jet/kafka/impl/StreamKafkaPTest.java\n+++ b/extensions/kafka/src/test/java/com/hazelcast/jet/kafka/impl/StreamKafkaPTest.java\n\n@@ -246,7 +246,7 @@ public class StreamKafkaPTest extends SimpleTestInClusterSupport {\n                 .setTotalParallelism(INITIAL_PARTITION_COUNT + 1)\n                 .setGlobalProcessorIndex(INITIAL_PARTITION_COUNT));\n \n-        assertTrue(processor.   currentAssignment.isEmpty());\n+        assertTrue(processor.currentAssignment.isEmpty());\n         assertEquals(IDLE_MESSAGE, consumeEventually(processor, outbox));\n \n         kafkaTestSupport.setPartitionCount(topic1Name, INITIAL_PARTITION_COUNT + 1);\n"}}, {"oid": "aee8632dfbeec6e82af0358cf134dac7ed6cf27f", "url": "https://github.com/hazelcast/hazelcast-jet/commit/aee8632dfbeec6e82af0358cf134dac7ed6cf27f", "message": "Remove double seeking", "committedDate": "2020-12-08T07:51:44Z", "type": "commit"}, {"oid": "398f0b8f3f77cb144e6d13c96fbc77521b3582e6", "url": "https://github.com/hazelcast/hazelcast-jet/commit/398f0b8f3f77cb144e6d13c96fbc77521b3582e6", "message": "Merge remote-tracking branch 'remotes/hazelcast/master' into kafka-fixes", "committedDate": "2020-12-09T09:48:31Z", "type": "commit"}, {"oid": "f16e8a3b66e47b02ee983e68e46800948bd76b71", "url": "https://github.com/hazelcast/hazelcast-jet/commit/f16e8a3b66e47b02ee983e68e46800948bd76b71", "message": "Address review comments", "committedDate": "2020-12-09T09:52:01Z", "type": "commit"}]}