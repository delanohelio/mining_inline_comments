{"pr_number": 1382, "pr_title": "Minor changes to support recovering from replica addition failure", "pr_createdAt": "2020-02-12T18:53:07Z", "pr_url": "https://github.com/linkedin/ambry/pull/1382", "timeline": [{"oid": "b1b3eb9025eac7b3e70b518e907be5c4fa24fb3d", "url": "https://github.com/linkedin/ambry/commit/b1b3eb9025eac7b3e70b518e907be5c4fa24fb3d", "message": "Minor changes to support recovering from replica addition failure\n\nThis PR adds minor changes to resume dynamic replica addition if it\nfailed last time before updating InstanceConfig(clustermap update in\nHelix). The logic is simple: if diskManager finds there is a existing\ndir associated with replica to add, it first deletes it and recreates a\nnew one. (The files in previous dir may not be reliable)", "committedDate": "2020-02-12T18:48:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTAzNTA4OQ==", "url": "https://github.com/linkedin/ambry/pull/1382#discussion_r379035089", "bodyText": "Just validating my though?\nIs the assumption here that, if there is some old state of a replica on a node's storage, and storage manager or disk manager don't know about it, then it could be a corrupted state, and hence remove it.\nThe existing state might help in case where let's say a node crashed and comes back up after sometime, and is assigned same replica back again. In this case, even though there is some state on disk, we will delete it. The state on disk might have helped this replica in catching up with existing replicas quicker. But I assuming that we are ignoring this scenario because we don't have a way to identify this scenario. Is this correct?", "author": "ankagrawal", "createdAt": "2020-02-13T18:18:26Z", "path": "ambry-store/src/main/java/com.github.ambry.store/DiskManager.java", "diffHunk": "@@ -311,11 +312,23 @@ boolean addBlobStore(ReplicaId replica) {\n       if (!running) {\n         logger.error(\"Failed to add {} because disk manager is not running\", replica.getPartitionId());\n       } else {\n+        // Clean up existing dir associated with this replica to add. Here we re-create a new store because we don't\n+        // know the state of files in old directory. (The old directory was created last time when adding this replica", "originalCommit": "b1b3eb9025eac7b3e70b518e907be5c4fa24fb3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTA0ODAwMw==", "url": "https://github.com/linkedin/ambry/pull/1382#discussion_r379048003", "bodyText": "You are right.\nThe state on disk might have helped this replica in catching up with existing replicas quicker this is true when the added replica was previously in BOOTSTRAP -> STANDBY and was in the middle of caught-up. Note that, if this added replica was ready in BOOTSTRAP -> STANDBY before node crashed, that means it should have already updated InstanceConfig in Helix (this is the last step in OFFLINE -> BOOTSTRAP). So, after node reboots, storage manager/disk manager should be aware of this replica and don't attempt to add it again.\nThe failure I want to address in this PR occurs before new added replica updates InstanceConfig, so the new replica may be created but still empty. We don't have any loss by deleting its dir when node is restarted. (we can discuss it offline if this didn't answer your question)", "author": "jsjtzyy", "createdAt": "2020-02-13T18:42:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTAzNTA4OQ=="}], "type": "inlineReview", "revised_code": null}]}