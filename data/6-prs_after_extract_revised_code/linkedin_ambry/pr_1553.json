{"pr_number": 1553, "pr_title": "Checkpoint Azure compaction progress", "pr_createdAt": "2020-06-05T22:24:25Z", "pr_url": "https://github.com/linkedin/ambry/pull/1553", "timeline": [{"oid": "7adcdca4782a45d006cdef1fbf15b15753a24319", "url": "https://github.com/linkedin/ambry/commit/7adcdca4782a45d006cdef1fbf15b15753a24319", "message": "Checkpoint Azure compaction progress", "committedDate": "2020-06-05T22:21:56Z", "type": "commit"}, {"oid": "d34e15d61539621f4a4c3c373d5196cd168671fd", "url": "https://github.com/linkedin/ambry/commit/d34e15d61539621f4a4c3c373d5196cd168671fd", "message": "Add configurable limit to number of blobs purged in a partition each compaction round.", "committedDate": "2020-06-08T04:28:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU1NTE0MA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437555140", "bodyText": "retries here for retry-able error?", "author": "ankagrawal", "createdAt": "2020-06-09T16:16:07Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -312,11 +295,13 @@ public class AzureStorageCompactor {\n    * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n    * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n-   * @throws BlobStorageException if the checkpoint blob\n+   * @throws CloudStorageException if the operation fails.\n    */\n-  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n-    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n     if (!hasCheckpoint) {\n       return emptyCheckpoints;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU1NjI2OA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437556268", "bodyText": "why 64? Also maybe we can make it private static final", "author": "ankagrawal", "createdAt": "2020-06-09T16:17:57Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2NzE3Nw==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438467177", "bodyText": "approximate size of payload, made it static.", "author": "lightningrob", "createdAt": "2020-06-10T23:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU1NjI2OA=="}], "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -312,11 +295,13 @@ public class AzureStorageCompactor {\n    * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n    * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n-   * @throws BlobStorageException if the checkpoint blob\n+   * @throws CloudStorageException if the operation fails.\n    */\n-  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n-    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n     if (!hasCheckpoint) {\n       return emptyCheckpoints;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MTEyOA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437561128", "bodyText": "We should cache this information instead of doing a get always. Only when we dont see a cache for this partition, should we do getCompactionInProgress", "author": "ankagrawal", "createdAt": "2020-06-09T16:25:33Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n+      }\n+      return checkpoints;\n+    } catch (IOException e) {\n+      logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n+      azureMetrics.compactionProgressReadErrorCount.inc();\n+      return emptyCheckpoints;\n+    }\n+  }\n+\n+  /**\n+   * Update the compaction progress for a partition.\n+   * @param partitionPath the partition to update.\n+   * @param fieldName the compaction field (deletion or expiration time).\n+   * @param checkpointTime the updated progress time.\n    */\n-  public CloudBlobMetadata getOldestDeadlob(String partitionPath, String fieldName) throws CloudStorageException {\n-    // TODO: once we have Cosmos compaction table, can query that.\n-    List<CloudBlobMetadata> deadBlobs =\n-        requestAgent.doWithRetries(() -> getDeadBlobs(partitionPath, fieldName, 1, System.currentTimeMillis(), 1),\n-            \"GetDeadBlobs\", partitionPath);\n-    return deadBlobs.isEmpty() ? null : deadBlobs.get(0);\n+  boolean updateCompactionProgress(String partitionPath, String fieldName, long checkpointTime) {\n+    try {\n+      // load existing progress checkpoint.\n+      Map<String, Long> checkpoints = getCompactionProgress(partitionPath);", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ2Nzc5Mw==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438467793", "bodyText": "I'm hesitant to cache it since the partition could get reassigned back and forth between runs and be out of date.   I'd probably want to read it anyway to make sure.", "author": "lightningrob", "createdAt": "2020-06-10T23:50:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MTEyOA=="}], "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -312,11 +295,13 @@ public class AzureStorageCompactor {\n    * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n    * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n-   * @throws BlobStorageException if the checkpoint blob\n+   * @throws CloudStorageException if the operation fails.\n    */\n-  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n-    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n     if (!hasCheckpoint) {\n       return emptyCheckpoints;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MTU4MA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437561580", "bodyText": "Also maybe retry for retry-able error?", "author": "ankagrawal", "createdAt": "2020-06-09T16:26:10Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +309,61 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws BlobStorageException if the checkpoint blob\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n+    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n+      }\n+      return checkpoints;\n+    } catch (IOException e) {\n+      logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n+      azureMetrics.compactionProgressReadErrorCount.inc();\n+      return emptyCheckpoints;\n+    }\n+  }\n+\n+  /**\n+   * Update the compaction progress for a partition.\n+   * @param partitionPath the partition to update.\n+   * @param fieldName the compaction field (deletion or expiration time).\n+   * @param checkpointTime the updated progress time.\n    */\n-  public CloudBlobMetadata getOldestDeadlob(String partitionPath, String fieldName) throws CloudStorageException {\n-    // TODO: once we have Cosmos compaction table, can query that.\n-    List<CloudBlobMetadata> deadBlobs =\n-        requestAgent.doWithRetries(() -> getDeadBlobs(partitionPath, fieldName, 1, System.currentTimeMillis(), 1),\n-            \"GetDeadBlobs\", partitionPath);\n-    return deadBlobs.isEmpty() ? null : deadBlobs.get(0);\n+  boolean updateCompactionProgress(String partitionPath, String fieldName, long checkpointTime) {\n+    try {\n+      // load existing progress checkpoint.\n+      Map<String, Long> checkpoints = getCompactionProgress(partitionPath);\n+      // Ensure we don't downgrade progress already recorded.\n+      if (checkpointTime <= checkpoints.getOrDefault(fieldName, DEFAULT_TIME)) {\n+        logger.info(\"Skipping update of compaction progress for {} because saved {} is more recent.\", partitionPath,\n+            fieldName);\n+        return false;\n+      }\n+      checkpoints.put(fieldName, Math.max(checkpointTime, checkpoints.get(fieldName)));\n+      String json = objectMapper.writeValueAsString(checkpoints);\n+      ByteArrayInputStream bais = new ByteArrayInputStream(json.getBytes());\n+      azureBlobDataAccessor.uploadFile(CHECKPOINT_CONTAINER, partitionPath, bais);", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -312,11 +295,13 @@ public class AzureStorageCompactor {\n    * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n    * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n-   * @throws BlobStorageException if the checkpoint blob\n+   * @throws CloudStorageException if the operation fails.\n    */\n-  Map<String, Long> getCompactionProgress(String partitionPath) throws BlobStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(64);\n-    boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n     if (!hasCheckpoint) {\n       return emptyCheckpoints;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2MzY4MA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437563680", "bodyText": "We don't need to compute compactionTime at the end of this method anymore.", "author": "ankagrawal", "createdAt": "2020-06-09T16:29:24Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n-      logger.info(\"Skipping compaction due to shut down.\");\n+      logger.info(\"Skipping compaction of {} due to shut down.\", partitionPath);\n       return 0;\n     }\n \n+    Map<String, Long> checkpoints;\n+    try {\n+      checkpoints = getCompactionProgress(partitionPath);\n+    } catch (BlobStorageException | UncheckedIOException e) {\n+      // If checkpoint couldn't be read, skip and try later.\n+      throw new CloudStorageException(\"Compaction of \" + partitionPath + \" failed reading checkpoint\", e);\n+    }\n+\n     long now = System.currentTimeMillis();\n     long compactionStartTime = now;\n+    // FIXME: incorrect because time limit is for all partitions\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n     long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n-    Date queryStartDate = new Date(queryStartTime);\n     Date queryEndDate = new Date(queryEndTime);\n     int totalBlobsPurged = 0;\n-    logger.info(\"Compacting partition {} over time range {} - {}\", partitionPath, queryStartDate, queryEndDate);\n     try {\n+      long deletionStartTime = Math.max(queryStartTime, checkpoints.get(CloudBlobMetadata.FIELD_DELETION_TIME));\n       int numPurged =\n-          compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, queryStartTime, queryEndTime,\n+          compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, deletionStartTime, queryEndTime,\n               timeToQuit);\n       logger.info(\"Purged {} deleted blobs in partition {} up to {}\", numPurged, partitionPath, queryEndDate);\n       totalBlobsPurged += numPurged;\n-      numPurged = compactPartition(partitionPath, CloudBlobMetadata.FIELD_EXPIRATION_TIME, queryStartTime, queryEndTime,\n-          timeToQuit);\n+      long expirationStartTime = Math.max(queryStartTime, checkpoints.get(CloudBlobMetadata.FIELD_EXPIRATION_TIME));\n+      numPurged =\n+          compactPartition(partitionPath, CloudBlobMetadata.FIELD_EXPIRATION_TIME, expirationStartTime, queryEndTime,\n+              timeToQuit);", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -102,6 +99,7 @@ public class AzureStorageCompactor {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n+   * @throws CloudStorageException if the compaction fails.\n    */\n   public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NTQzOQ==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437575439", "bodyText": "javadocs need update.", "author": "ankagrawal", "createdAt": "2020-06-09T16:49:01Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -102,6 +99,7 @@ public class AzureStorageCompactor {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n+   * @throws CloudStorageException if the compaction fails.\n    */\n   public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NTc4MQ==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437575781", "bodyText": "javadocs of the constructor needs update.", "author": "ankagrawal", "createdAt": "2020-06-09T16:49:35Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -56,11 +74,15 @@ public AzureStorageCompactor(AzureBlobDataAccessor azureBlobDataAccessor, Cosmos\n     this.azureMetrics = azureMetrics;\n     this.retentionPeriodMs = TimeUnit.DAYS.toMillis(cloudConfig.cloudDeletedBlobRetentionDays);\n     this.queryLimit = cloudConfig.cloudBlobCompactionQueryLimit;\n+    this.purgeLimit = cloudConfig.cloudCompactionPurgeLimit;", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -77,8 +76,6 @@ public class AzureStorageCompactor {\n     this.purgeLimit = cloudConfig.cloudCompactionPurgeLimit;\n     this.queryBucketDays = cloudConfig.cloudCompactionQueryBucketDays;\n     this.lookbackDays = cloudConfig.cloudCompactionLookbackDays;\n-    // TODO: change this\n-    compactionTimeLimitMs = TimeUnit.HOURS.toMillis(cloudConfig.cloudBlobCompactionIntervalHours);\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n     emptyCheckpoints = new HashMap<>();\n     emptyCheckpoints.put(CloudBlobMetadata.FIELD_DELETION_TIME, DEFAULT_TIME);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU3NzA5MQ==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437577091", "bodyText": "the compactionStartTime variable above is redundant now.", "author": "ankagrawal", "createdAt": "2020-06-09T16:51:41Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -81,29 +103,39 @@ boolean isShuttingDown() {\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n    */\n-  public int compactPartition(String partitionPath) {\n+  public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n-      logger.info(\"Skipping compaction due to shut down.\");\n+      logger.info(\"Skipping compaction of {} due to shut down.\", partitionPath);\n       return 0;\n     }\n \n+    Map<String, Long> checkpoints;\n+    try {\n+      checkpoints = getCompactionProgress(partitionPath);\n+    } catch (BlobStorageException | UncheckedIOException e) {\n+      // If checkpoint couldn't be read, skip and try later.\n+      throw new CloudStorageException(\"Compaction of \" + partitionPath + \" failed reading checkpoint\", e);\n+    }\n+\n     long now = System.currentTimeMillis();\n     long compactionStartTime = now;\n+    // FIXME: incorrect because time limit is for all partitions", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b550e038a4694e98e19c898d57d60bf094f0bac8", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 3e117bd43..82bf6417f 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -102,6 +99,7 @@ public class AzureStorageCompactor {\n   /**\n    * Purge the inactive blobs in the specified partition.\n    * @return the total number of blobs purged.\n+   * @throws CloudStorageException if the compaction fails.\n    */\n   public int compactPartition(String partitionPath) throws CloudStorageException {\n     if (isShuttingDown()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwMjEyOA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r437602128", "bodyText": "Not seeing any caller for this API except UT in the current diff. Will this be used in future PR?", "author": "ssen-li", "createdAt": "2020-06-09T17:33:37Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -237,6 +237,22 @@ public boolean downloadFile(String containerName, String fileName, OutputStream\n     }\n   }\n \n+  /**\n+   * Delete a file from blob storage, if it exists.\n+   * @param containerName name of the container containing file to delete.\n+   * @param fileName name of the file to delete.\n+   * @return true if the file was deleted, otherwise false.\n+   * @throws BlobStorageException for any error on ABS side.\n+   */\n+  boolean deleteFile(String containerName, String fileName) throws BlobStorageException {", "originalCommit": "d34e15d61539621f4a4c3c373d5196cd168671fd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODUyNTk3Mg==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438525972", "bodyText": "The integration test uses it to wipe out the checkpoint during cleanup.  It could be used, I could actually wrap deleteBlob around it in a future PR.", "author": "lightningrob", "createdAt": "2020-06-11T03:44:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwMjEyOA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "b550e038a4694e98e19c898d57d60bf094f0bac8", "url": "https://github.com/linkedin/ambry/commit/b550e038a4694e98e19c898d57d60bf094f0bac8", "message": "Address review comments.", "committedDate": "2020-06-10T23:47:45Z", "type": "commit"}, {"oid": "9d457b806f23574e4c16d42a2270b162da3c7324", "url": "https://github.com/linkedin/ambry/commit/9d457b806f23574e4c16d42a2270b162da3c7324", "message": "Merge branch 'master' of github.com:linkedin/ambry into compaction-progress", "committedDate": "2020-06-11T00:52:45Z", "type": "commit"}, {"oid": "e5601860ff562740068c4cf471b9667b2439b7bf", "url": "https://github.com/linkedin/ambry/commit/e5601860ff562740068c4cf471b9667b2439b7bf", "message": "Bug fix and cleanup", "committedDate": "2020-06-11T07:40:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMzk5Mw==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438923993", "bodyText": "Since this is only used for integration tests right now, it may not matter too much, but would it be possible to delete the blob without making a call to check for existence first (and checking delete status code).", "author": "cgtz", "createdAt": "2020-06-11T16:40:55Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureBlobDataAccessor.java", "diffHunk": "@@ -238,6 +238,22 @@ public boolean downloadFile(String containerName, String fileName, OutputStream\n     }\n   }\n \n+  /**\n+   * Delete a file from blob storage, if it exists.\n+   * @param containerName name of the container containing file to delete.\n+   * @param fileName name of the file to delete.\n+   * @return true if the file was deleted, otherwise false.\n+   * @throws BlobStorageException for any error on ABS side.\n+   */\n+  boolean deleteFile(String containerName, String fileName) throws BlobStorageException {\n+    BlockBlobClient blobClient = getBlockBlobClient(containerName, fileName, false);\n+    if (blobClient.exists()) {\n+      blobClient.delete();", "originalCommit": "e5601860ff562740068c4cf471b9667b2439b7bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzNjU5Ng==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r439136596", "bodyText": "Fair point.  I meant it as deleteIfNeeded.  I can change it in a follow up.", "author": "lightningrob", "createdAt": "2020-06-12T00:09:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMzk5Mw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyNDg3Ng==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438924876", "bodyText": "I guess this is to handle cases when there are two metadata objects in the list with the same ID?", "author": "cgtz", "createdAt": "2020-06-11T16:42:26Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureCloudDestination.java", "diffHunk": "@@ -221,7 +221,7 @@ public short undeleteBlob(BlobId blobId, short lifeVersion, CloudUpdateValidator\n     for (List<BlobId> batchOfBlobs : chunkedBlobIdList) {\n       metadataList.addAll(getBlobMetadataChunked(batchOfBlobs));\n     }\n-    return metadataList.stream().collect(Collectors.toMap(CloudBlobMetadata::getId, Function.identity()));\n+    return metadataList.stream().collect(Collectors.toMap(CloudBlobMetadata::getId, Function.identity(), (x, y) -> x));", "originalCommit": "e5601860ff562740068c4cf471b9667b2439b7bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTEzNzY2MA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r439137660", "bodyText": "Yes, I added that for easier test handling, it shouldn't happen in prod flow.  It's not the prettiest but don't think it hurts.", "author": "lightningrob", "createdAt": "2020-06-12T00:11:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyNDg3Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk0NTIzNg==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438945236", "bodyText": "since references to this map get returned via different method calls, consider wrapping it in Collections.unmodifiableMap It also seems like it could be static.", "author": "cgtz", "createdAt": "2020-06-11T17:18:37Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -56,11 +73,13 @@ public AzureStorageCompactor(AzureBlobDataAccessor azureBlobDataAccessor, Cosmos\n     this.azureMetrics = azureMetrics;\n     this.retentionPeriodMs = TimeUnit.DAYS.toMillis(cloudConfig.cloudDeletedBlobRetentionDays);\n     this.queryLimit = cloudConfig.cloudBlobCompactionQueryLimit;\n+    this.purgeLimit = cloudConfig.cloudCompactionPurgeLimit;\n     this.queryBucketDays = cloudConfig.cloudCompactionQueryBucketDays;\n     this.lookbackDays = cloudConfig.cloudCompactionLookbackDays;\n-    // TODO: change this\n-    compactionTimeLimitMs = TimeUnit.HOURS.toMillis(cloudConfig.cloudBlobCompactionIntervalHours);\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n+    emptyCheckpoints = new HashMap<>();", "originalCommit": "e5601860ff562740068c4cf471b9667b2439b7bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0MzQzNA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r439143434", "bodyText": "ok", "author": "lightningrob", "createdAt": "2020-06-12T00:22:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk0NTIzNg=="}], "type": "inlineReview", "revised_code": {"commit": "4bc404628de9702411e8939c34ed90dd3c45017d", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 7371ab7c9..26d812133 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -77,9 +87,6 @@ public class AzureStorageCompactor {\n     this.queryBucketDays = cloudConfig.cloudCompactionQueryBucketDays;\n     this.lookbackDays = cloudConfig.cloudCompactionLookbackDays;\n     requestAgent = new CloudRequestAgent(cloudConfig, vcrMetrics);\n-    emptyCheckpoints = new HashMap<>();\n-    emptyCheckpoints.put(CloudBlobMetadata.FIELD_DELETION_TIME, DEFAULT_TIME);\n-    emptyCheckpoints.put(CloudBlobMetadata.FIELD_EXPIRATION_TIME, DEFAULT_TIME);\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk2NDkzNw==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438964937", "bodyText": "Using the same ByteArrayOutputStream for all retries could cause issues if the first try partially wrote to BAOS and then the second try will just start appending to the same stream.\nInstead you could do something like this:\n    ByteArrayOutputStream baos = requestAgent.doWithRetries(() -> {\n      ByteArrayOutputStream os = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n      boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, os, false);\n      return hasCheckpoint ? os : null;\n    }, \"Download compaction checkpoint\", partitionPath);", "author": "cgtz", "createdAt": "2020-06-11T17:49:18Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +289,67 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws CloudStorageException if the operation fails.\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),", "originalCommit": "e5601860ff562740068c4cf471b9667b2439b7bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIxMDIzMg==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r439210232", "bodyText": "Good catch.  Moved the outputstream inside the lambda.", "author": "lightningrob", "createdAt": "2020-06-12T05:13:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk2NDkzNw=="}], "type": "inlineReview", "revised_code": {"commit": "4bc404628de9702411e8939c34ed90dd3c45017d", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 7371ab7c9..26d812133 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -295,26 +301,29 @@ public class AzureStorageCompactor {\n    * @throws CloudStorageException if the operation fails.\n    */\n   Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n-    boolean hasCheckpoint = requestAgent.doWithRetries(\n-        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+    // TODO: change return type to POJO with getters and serde methods\n+    String payload = requestAgent.doWithRetries(\n+        () -> {\n+          ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+          boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+          return hasCheckpoint ? baos.toString() : null;\n+        },\n         \"Download compaction checkpoint\", partitionPath);\n-    if (!hasCheckpoint) {\n-      return emptyCheckpoints;\n+    if (payload == null) {\n+      return new HashMap(emptyCheckpoints);\n     }\n     try {\n       // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n-      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(payload);\n       Map<String, Long> checkpoints = new HashMap<>();\n-      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n-          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+      for (String fieldName : compactionFields) {\n         checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n       }\n       return checkpoints;\n     } catch (IOException e) {\n       logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n       azureMetrics.compactionProgressReadErrorCount.inc();\n-      return emptyCheckpoints;\n+      return new HashMap(emptyCheckpoints);\n     }\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk3MTg3OA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r438971878", "bodyText": "For me, it seems cleaner for checkpoints to be a simple object (instead of a map) with getDeletionTimeCheckpoint and getFieldExpirationTime and toBytes method so the logic around serde/defaults can all be in one place.", "author": "cgtz", "createdAt": "2020-06-11T17:57:26Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java", "diffHunk": "@@ -257,17 +289,67 @@ int purgeBlobs(List<CloudBlobMetadata> blobMetadataList) throws CloudStorageExce\n   }\n \n   /**\n-   * Returns the dead blob in the specified partition with the earliest expiration or deletion time.\n+   * Get the current compaction progress for a partition.\n    * @param partitionPath the partition to check.\n-   * @param fieldName the field name to use (expiration or deletion time).\n-   * @return the {@link CloudBlobMetadata} for the dead blob, or NULL if none was found.\n-   * @throws CloudStorageException\n+   * @return a {@link Map} containing the progress time for compaction based on deletion and expiration time.\n+   * @throws CloudStorageException if the operation fails.\n+   */\n+  Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+    boolean hasCheckpoint = requestAgent.doWithRetries(\n+        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+        \"Download compaction checkpoint\", partitionPath);\n+    if (!hasCheckpoint) {\n+      return emptyCheckpoints;\n+    }\n+    try {\n+      // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      Map<String, Long> checkpoints = new HashMap<>();\n+      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n+          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+        checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);", "originalCommit": "e5601860ff562740068c4cf471b9667b2439b7bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE0NDE1OA==", "url": "https://github.com/linkedin/ambry/pull/1553#discussion_r439144158", "bodyText": "Good idea.  I'll change it in a follow up PR.", "author": "lightningrob", "createdAt": "2020-06-12T00:24:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk3MTg3OA=="}], "type": "inlineReview", "revised_code": {"commit": "4bc404628de9702411e8939c34ed90dd3c45017d", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\nindex 7371ab7c9..26d812133 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/azure/AzureStorageCompactor.java\n\n@@ -295,26 +301,29 @@ public class AzureStorageCompactor {\n    * @throws CloudStorageException if the operation fails.\n    */\n   Map<String, Long> getCompactionProgress(String partitionPath) throws CloudStorageException {\n-    ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n-    boolean hasCheckpoint = requestAgent.doWithRetries(\n-        () -> azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false),\n+    // TODO: change return type to POJO with getters and serde methods\n+    String payload = requestAgent.doWithRetries(\n+        () -> {\n+          ByteArrayOutputStream baos = new ByteArrayOutputStream(CHECKPOINT_BUFFER_SIZE);\n+          boolean hasCheckpoint = azureBlobDataAccessor.downloadFile(CHECKPOINT_CONTAINER, partitionPath, baos, false);\n+          return hasCheckpoint ? baos.toString() : null;\n+        },\n         \"Download compaction checkpoint\", partitionPath);\n-    if (!hasCheckpoint) {\n-      return emptyCheckpoints;\n+    if (payload == null) {\n+      return new HashMap(emptyCheckpoints);\n     }\n     try {\n       // Payload format: {\"expirationTime\" : 12345, \"deletionTime\" : 67890}\n-      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(baos.toString());\n+      ObjectNode jsonNode = (ObjectNode) objectMapper.readTree(payload);\n       Map<String, Long> checkpoints = new HashMap<>();\n-      for (String fieldName : new String[]{CloudBlobMetadata.FIELD_DELETION_TIME,\n-          CloudBlobMetadata.FIELD_EXPIRATION_TIME}) {\n+      for (String fieldName : compactionFields) {\n         checkpoints.put(fieldName, jsonNode.has(fieldName) ? jsonNode.get(fieldName).longValue() : DEFAULT_TIME);\n       }\n       return checkpoints;\n     } catch (IOException e) {\n       logger.error(\"Could not retrieve compaction progress for {}\", partitionPath, e);\n       azureMetrics.compactionProgressReadErrorCount.inc();\n-      return emptyCheckpoints;\n+      return new HashMap(emptyCheckpoints);\n     }\n   }\n \n"}}, {"oid": "4bc404628de9702411e8939c34ed90dd3c45017d", "url": "https://github.com/linkedin/ambry/commit/4bc404628de9702411e8939c34ed90dd3c45017d", "message": "Address review comments", "committedDate": "2020-06-12T05:09:56Z", "type": "commit"}]}