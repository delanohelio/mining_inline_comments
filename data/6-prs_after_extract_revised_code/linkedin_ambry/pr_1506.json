{"pr_number": 1506, "pr_title": "Break compaction query into smaller time buckets", "pr_createdAt": "2020-05-06T06:53:11Z", "pr_url": "https://github.com/linkedin/ambry/pull/1506", "timeline": [{"oid": "1114f1ce514441d954a4758fd56e8516101af009", "url": "https://github.com/linkedin/ambry/commit/1114f1ce514441d954a4758fd56e8516101af009", "message": "Break cloud compaction into smaller query buckets to avoid throttling", "committedDate": "2020-05-06T06:46:53Z", "type": "commit"}, {"oid": "f225dd78a34348f73ddea2b75125d5389522e841", "url": "https://github.com/linkedin/ambry/commit/f225dd78a34348f73ddea2b75125d5389522e841", "message": "Merge branch 'master' of github.com:linkedin/ambry into compactor-query-buckets", "committedDate": "2020-05-06T06:48:10Z", "type": "commit"}, {"oid": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "url": "https://github.com/linkedin/ambry/commit/63b5edcb847442a763f6a4f859ca58e2dba0cee6", "message": "Test fixes", "committedDate": "2020-05-06T07:23:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwMzIwMg==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423403202", "bodyText": "Is this statement a TODO? We should mark it as such in that case.", "author": "ankagrawal", "createdAt": "2020-05-12T00:59:27Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -124,30 +128,30 @@ public int compactPartitions() {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: we can cache the latest timestamp that we know we have cleared and use that on subsequent calls\n+    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n     // Starting from beginning of time is too expensive\n     // Order partitions by earliest time at which dead blob exists", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NTQ0Mg==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424075442", "bodyText": "I'm removing these comments.  I will file a ticket for the checkpointing enhancement", "author": "lightningrob", "createdAt": "2020-05-12T22:43:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwMzIwMg=="}], "type": "inlineReview", "revised_code": {"commit": "c31a42cba7b63485607d66a53e2f726f5b114d40", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\nindex 8cc83bf2e..e948f1cda 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n\n@@ -128,9 +128,6 @@ public class CloudStorageCompactor implements Runnable {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n-    // Starting from beginning of time is too expensive\n-    // Order partitions by earliest time at which dead blob exists\n     long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n     Date queryStartDate = new Date(queryStartTime);\n     Date queryEndDate = new Date(queryEndTime);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwNDUwOA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423404508", "bodyText": "We should move the logic that actually does the compaction (which is basically everything outside the if statement in line 218), in a separate method of its own, unless we have reason not to do so. Makes code easy to read, keeps methods small and separates out the control part of compaction logic from the actual compaction.", "author": "ankagrawal", "createdAt": "2020-05-12T01:04:27Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -212,6 +214,28 @@ public int compactPartition(String partitionPath, String fieldName, long querySt\n \n     // Iterate until returned list size < limit, time runs out or we get shut down\n     int totalPurged = 0;\n+    long chunkTimeRange = TimeUnit.DAYS.toMillis(queryBucketDays);\n+    if (queryEndTime - queryStartTime > chunkTimeRange) {\n+      logger.debug(\"Dividing compaction query for {} into buckets of {} days\", partitionPath, queryBucketDays);\n+      long chunkedStartTime = queryStartTime;\n+      while (chunkedStartTime < queryEndTime) {\n+        long chunkedEndTime = Math.min(chunkedStartTime + chunkTimeRange, queryEndTime);\n+        int numPurged = compactPartition(partitionPath, fieldName, chunkedStartTime, chunkedEndTime, timeToQuit);", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NzUzMA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424077530", "bodyText": "I had it that way initially but then consolidated it into single method.  I can change it back if it helps readability.", "author": "lightningrob", "createdAt": "2020-05-12T22:49:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQwNDUwOA=="}], "type": "inlineReview", "revised_code": {"commit": "c31a42cba7b63485607d66a53e2f726f5b114d40", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\nindex 8cc83bf2e..e948f1cda 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n\n@@ -214,28 +212,45 @@ public class CloudStorageCompactor implements Runnable {\n \n     // Iterate until returned list size < limit, time runs out or we get shut down\n     int totalPurged = 0;\n-    long chunkTimeRange = TimeUnit.DAYS.toMillis(queryBucketDays);\n-    if (queryEndTime - queryStartTime > chunkTimeRange) {\n-      logger.debug(\"Dividing compaction query for {} into buckets of {} days\", partitionPath, queryBucketDays);\n-      long chunkedStartTime = queryStartTime;\n-      while (chunkedStartTime < queryEndTime) {\n-        long chunkedEndTime = Math.min(chunkedStartTime + chunkTimeRange, queryEndTime);\n-        int numPurged = compactPartition(partitionPath, fieldName, chunkedStartTime, chunkedEndTime, timeToQuit);\n-        totalPurged += numPurged;\n-        chunkedStartTime += chunkTimeRange;\n-        if (isShuttingDown() || System.currentTimeMillis() >= timeToQuit) {\n-          break;\n-        }\n-        if (numPurged == 0) {\n-          // TODO: Back off since the last index scan might have been expensive\n-        } else {\n-          logger.info(\"Purged {} blobs in partition {} up to {} {}\", totalPurged, partitionPath, fieldName,\n-              new Date(chunkedEndTime));\n-        }\n+    long bucketTimeRange = TimeUnit.DAYS.toMillis(queryBucketDays);\n+    logger.debug(\"Dividing compaction query for {} into buckets of {} days\", partitionPath, queryBucketDays);\n+    long bucketStartTime = queryStartTime;\n+    while (bucketStartTime < queryEndTime) {\n+      long bucketEndTime = Math.min(bucketStartTime + bucketTimeRange, queryEndTime);\n+      int numPurged = compactPartitionBucketed(partitionPath, fieldName, bucketStartTime, bucketEndTime, timeToQuit);\n+      totalPurged += numPurged;\n+      bucketStartTime += bucketTimeRange;\n+      if (isShuttingDown() || System.currentTimeMillis() >= timeToQuit) {\n+        break;\n+      }\n+      if (numPurged == 0) {\n+        // TODO: Consider backing off since the last query might have been expensive\n+      } else {\n+        logger.info(\"Purged {} blobs in partition {} up to {} {}\", totalPurged, partitionPath, fieldName,\n+            new Date(bucketEndTime));\n       }\n-      return totalPurged;\n     }\n+    return totalPurged;\n+  }\n \n+  /**\n+   * Purge the inactive blobs in the specified partition.\n+   * @param partitionPath the partition to compact.\n+   * @param fieldName the field name to query on. Allowed values are {@link CloudBlobMetadata#FIELD_DELETION_TIME}\n+   *                      and {@link CloudBlobMetadata#FIELD_EXPIRATION_TIME}.\n+   * @param queryStartTime the initial query start time, which will be adjusted as compaction progresses.\n+   * @param queryEndTime the query end time.\n+   * @param timeToQuit the time at which compaction should terminate.\n+   * @return the number of blobs purged or found.\n+   */\n+  private int compactPartitionBucketed(String partitionPath, String fieldName, long queryStartTime, long queryEndTime,\n+      long timeToQuit) throws CloudStorageException {\n+\n+    if (queryEndTime - queryStartTime > TimeUnit.DAYS.toMillis(queryBucketDays)) {\n+      throw new IllegalArgumentException(\"Time window is longer than \" + queryBucketDays + \" days\");\n+    }\n+\n+    int totalPurged = 0;\n     while (System.currentTimeMillis() < timeToQuit && !isShuttingDown()) {\n \n       Callable<List<CloudBlobMetadata>> deadBlobsLambda =\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQxMjAyOA==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r423412028", "bodyText": "Can there be a case where we cannot complete compaction of all partitions in 24 hours?  In that case, it's possible that same that partitions that are at the end of the list of partitions, miss the compaction every time. There are atleast 2 ways we can approach this:\n\nIf instead of draining out each partition in a loop, we do n batches of compactPartittion calls for each partition, before looping through the list of partitions again, it will keep every partition compacted uniformly.\nAnother way we can solve this could be by setting cloudConfig.cloudBlobCompactionIntervalHours large enough so that this scenario can never happen. For this, it might be good to have metrics that can tell us how many times we were able to successfully loop through all partitions, and how many times we couldn't loop through all partitions.", "author": "ankagrawal", "createdAt": "2020-05-12T01:32:12Z", "path": "ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java", "diffHunk": "@@ -124,30 +128,30 @@ public int compactPartitions() {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: we can cache the latest timestamp that we know we have cleared and use that on subsequent calls\n+    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n     // Starting from beginning of time is too expensive\n     // Order partitions by earliest time at which dead blob exists\n-    // Can start with retention period and go back additional retention periods until no more found\n-    long queryStartTime = 1;\n+    long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n+    Date queryStartDate = new Date(queryStartTime);\n+    Date queryEndDate = new Date(queryEndTime);\n     int totalBlobsPurged = 0;\n     for (PartitionId partitionId : partitionsSnapshot) {\n       String partitionPath = partitionId.toPathString();\n       if (!partitions.contains(partitionId)) {\n         // Looks like partition was reassigned since the loop started, so skip it\n         continue;\n       }\n-      logger.info(\"Running compaction on partition {}\", partitionPath);\n+      logger.info(\"Compacting partition {} over time range {} - {}\", partitionPath, queryStartDate, queryEndDate);\n       try {\n-        // TODO: before compacting, call getOldestBlob to get queryStartTime\n         int numPurged =\n             compactPartition(partitionPath, CloudBlobMetadata.FIELD_DELETION_TIME, queryStartTime, queryEndTime,", "originalCommit": "63b5edcb847442a763f6a4f859ca58e2dba0cee6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3NjEwMw==", "url": "https://github.com/linkedin/ambry/pull/1506#discussion_r424076103", "bodyText": "I think the first suggestion is a good idea.  I will file a separate ticket to do that in a follow up PR.  We can limit each partition by either number of blobs cleared or number of days scanned.", "author": "lightningrob", "createdAt": "2020-05-12T22:45:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQxMjAyOA=="}], "type": "inlineReview", "revised_code": {"commit": "c31a42cba7b63485607d66a53e2f726f5b114d40", "chunk": "diff --git a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\nindex 8cc83bf2e..e948f1cda 100644\n--- a/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n+++ b/ambry-cloud/src/main/java/com/github/ambry/cloud/CloudStorageCompactor.java\n\n@@ -128,9 +128,6 @@ public class CloudStorageCompactor implements Runnable {\n     long compactionStartTime = now;\n     long timeToQuit = now + compactionTimeLimitMs;\n     long queryEndTime = now - retentionPeriodMs;\n-    // TODO: checkpoint the latest timestamp and use that on subsequent compactions\n-    // Starting from beginning of time is too expensive\n-    // Order partitions by earliest time at which dead blob exists\n     long queryStartTime = now - TimeUnit.DAYS.toMillis(lookbackDays);\n     Date queryStartDate = new Date(queryStartTime);\n     Date queryEndDate = new Date(queryEndTime);\n"}}, {"oid": "c31a42cba7b63485607d66a53e2f726f5b114d40", "url": "https://github.com/linkedin/ambry/commit/c31a42cba7b63485607d66a53e2f726f5b114d40", "message": "Address Ankur review comments", "committedDate": "2020-05-12T23:12:53Z", "type": "commit"}]}