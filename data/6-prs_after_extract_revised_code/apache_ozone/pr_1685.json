{"pr_number": 1685, "pr_title": "HDDS-4552. Read data from chunk into ByteBuffer[] instead of single ByteBuffer.", "pr_createdAt": "2020-12-10T20:14:09Z", "pr_url": "https://github.com/apache/ozone/pull/1685", "timeline": [{"oid": "6320079208cdc9aeacd4c11e71b5b9b67832f767", "url": "https://github.com/apache/ozone/commit/6320079208cdc9aeacd4c11e71b5b9b67832f767", "message": "HDDS-4552. Read data from chunk into ByteBuffer[] instead of single ByteBuffer.", "committedDate": "2020-12-10T21:02:49Z", "type": "forcePushed"}, {"oid": "577ddad3313d19ce7d4fe3dfec7565ce68623e27", "url": "https://github.com/apache/ozone/commit/577ddad3313d19ce7d4fe3dfec7565ce68623e27", "message": "CI fixes", "committedDate": "2020-12-10T23:56:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU2NzUxNw==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546567517", "bodyText": "LOG.info -> LOG.warn?", "author": "bshashikant", "createdAt": "2020-12-21T08:20:29Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -134,7 +135,12 @@ public synchronized void initialize() throws IOException {\n     try {\n       chunks = getChunkInfos();\n     } catch (ContainerNotFoundException ioEx) {\n-      refreshPipeline(ioEx);\n+      LOG.info(\"Unable to read information for block {} from pipeline {}: {}.\" +", "originalCommit": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "chunk": "diff --git a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java\nindex c4b4e5f77..cdbbbfaf4 100644\n--- a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java\n+++ b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java\n\n@@ -135,12 +141,7 @@ public synchronized void initialize() throws IOException {\n     try {\n       chunks = getChunkInfos();\n     } catch (ContainerNotFoundException ioEx) {\n-      LOG.info(\"Unable to read information for block {} from pipeline {}: {}.\" +\n-              \" Trying to refresh the pipeline.\", blockID, pipeline.getId(),\n-          ioEx.getMessage());\n-      if (!refreshPipeline()) {\n-        throw ioEx;\n-      }\n+      refreshPipeline(ioEx);\n       chunks = getChunkInfos();\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546577336", "bodyText": "This will do an additional buffer copy here. Let's see if we can explore anything here to avoid buffer copy here:\nhttps://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/UnsafeByteOperations", "author": "bshashikant", "createdAt": "2020-12-21T08:43:40Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "originalCommit": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODI1NzE0Mg==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548257142", "bodyText": "We get ByteString from the response. But the returned ByteString does not have the underlying buffer boundary information. Hence ByteString#asReadOnlyByteBufferList() will return only one ByteBuffer with all the data irrespective of the backing arrays used to construct the ByteString.", "author": "hanishakoneru", "createdAt": "2020-12-23T21:32:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3NzM0NQ==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548377345", "bodyText": "What if we read in small buffers on the server side itself and send it across as a list of bytestrings to the client?\nCopying a big buffer on the client read path will be slowing down the read.  Probably we should do some benchmarking to understand the effects of all these.", "author": "bshashikant", "createdAt": "2020-12-24T04:30:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3NzczMA==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548377730", "bodyText": "In case, this turns out to be unavoidable, we can also think about doing bytebuffer.compact()  which also does an intrinsic buffer copy to release the buffers but the logic would be more simpler.", "author": "bshashikant", "createdAt": "2020-12-24T04:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzAyNTU5Mw==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r553025593", "bodyText": "What if we read in small buffers on the server side itself and send it across as a list of bytestrings to the client?\n\nThis might work but would require a change in the DN-Client protocol. Would have to analyze the compatibility issues and how to address them.\nIn case, this turns out to be unavoidable, we can also think about doing bytebuffer.compact() which also does an intrinsic buffer copy to release the buffers but the logic would be more simpler\n\nI am not sure if there is much gain in doing this. The code changes in this PR were required because the logic was inaccurate. It was working because there was always only one ByteBuffer.", "author": "hanishakoneru", "createdAt": "2021-01-07T00:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxOTk1Ng==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r553719956", "bodyText": "The basic problem we are trying to solve here is to minimize the memory overhead in the client. In order to solve this, adding an extra buffer copy overhead(with the patch) does not seem to be a reasonable idea to me. Let's discuss it in some more detail on how to address this.", "author": "bshashikant", "createdAt": "2021-01-08T03:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}], "type": "inlineReview", "revised_code": {"commit": "e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "chunk": "diff --git a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\nindex b4ee3eee6..be1dd6219 100644\n--- a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n+++ b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n\n@@ -348,10 +323,10 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + buffersSize;\n     }\n \n-    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // bufferOffsetWrtChunkData and buffersSize are updated after the data\n     // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n"}}, {"oid": "e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "url": "https://github.com/apache/ozone/commit/e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "message": "Unit tests for InputStreams", "committedDate": "2021-02-18T20:27:59Z", "type": "forcePushed"}, {"oid": "3b5253efdb566de2e02237390d54335274ba94e3", "url": "https://github.com/apache/ozone/commit/3b5253efdb566de2e02237390d54335274ba94e3", "message": "Set ReadChunkVersion explicitly", "committedDate": "2021-03-10T20:25:13Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTEwMQ==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065101", "bodyText": "Let's -> \"Let's say\"", "author": "bshashikant", "createdAt": "2021-03-15T05:50:27Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -66,13 +76,21 @@\n \n   // Index of the buffers corresponding to the current position of the buffers\n   private int bufferIndex;\n+  // bufferOffsets[i] stores the index of the first data byte in buffer i\n+  // (buffers.get(i)) w.r.t first byte in the buffers.\n+  // Let's each buffer has a capacity of 40 bytes. The bufferOffset for", "originalCommit": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "chunk": "diff --git a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\nindex 46c8cd235..657269d37 100644\n--- a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n+++ b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n\n@@ -76,21 +66,13 @@\n \n   // Index of the buffers corresponding to the current position of the buffers\n   private int bufferIndex;\n-  // bufferOffsets[i] stores the index of the first data byte in buffer i\n-  // (buffers.get(i)) w.r.t first byte in the buffers.\n-  // Let's each buffer has a capacity of 40 bytes. The bufferOffset for\n-  // the first buffer would always be 0 as this would be the first data byte\n-  // in buffers. BufferOffset for the 2nd buffer would be 40 as bytes 0-39\n-  // would be stored in buffer 0. Hence, bufferOffsets[0] = 0,\n-  // bufferOffsets[1] = 40, bufferOffsets[2] = 80, etc.\n-  private long[] bufferOffsets = null;\n \n   // The offset of the current data residing in the buffers w.r.t the start\n   // of chunk data\n-  private long bufferOffsetWrtChunkData;\n+  private long bufferOffset;\n \n   // The number of bytes of chunk data residing in the buffers currently\n-  private long buffersSize;\n+  private long bufferLength;\n \n   // Position of the ChunkInputStream is maintained by this variable (if a\n   // seek is performed. This position is w.r.t to the chunk only and not the\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTI0MQ==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065241", "bodyText": "Unintended change?", "author": "bshashikant", "createdAt": "2021-03-15T05:50:59Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -87,7 +105,8 @@\n   ChunkInputStream(ChunkInfo chunkInfo, BlockID blockId,\n       XceiverClientFactory xceiverClientFactory,\n       Supplier<Pipeline> pipelineSupplier,\n-      boolean verifyChecksum, Token<? extends TokenIdentifier> token) {\n+      boolean verifyChecksum,\n+      Token<? extends TokenIdentifier> token) {", "originalCommit": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY0MDU5OA==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594640598", "bodyText": "Yup. Reverted.", "author": "hanishakoneru", "createdAt": "2021-03-15T19:55:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTI0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "chunk": "diff --git a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\nindex 46c8cd235..657269d37 100644\n--- a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n+++ b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java\n\n@@ -105,8 +87,7 @@\n   ChunkInputStream(ChunkInfo chunkInfo, BlockID blockId,\n       XceiverClientFactory xceiverClientFactory,\n       Supplier<Pipeline> pipelineSupplier,\n-      boolean verifyChecksum,\n-      Token<? extends TokenIdentifier> token) {\n+      boolean verifyChecksum, Token<? extends TokenIdentifier> token) {\n     this.chunkInfo = chunkInfo;\n     this.length = chunkInfo.getLen();\n     this.blockID = blockId;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2ODM2Mg==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594068362", "bodyText": "Let's not change the default for now. We can change once we do some tests and analyze performance .", "author": "bshashikant", "createdAt": "2021-03-15T06:01:15Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java", "diffHunk": "@@ -109,13 +109,13 @@\n   private String checksumType = ChecksumType.CRC32.name();\n \n   @Config(key = \"bytes.per.checksum\",\n-      defaultValue = \"1MB\",\n+      defaultValue = \"256KB\",", "originalCommit": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY0MjM1OQ==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594642359", "bodyText": "Sure. Reverted back to 1MB.", "author": "hanishakoneru", "createdAt": "2021-03-15T19:57:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2ODM2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "chunk": "diff --git a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java\nindex 098101b0f..f39ec8613 100644\n--- a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java\n+++ b/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java\n\n@@ -109,13 +109,13 @@\n   private String checksumType = ChecksumType.CRC32.name();\n \n   @Config(key = \"bytes.per.checksum\",\n-      defaultValue = \"256KB\",\n+      defaultValue = \"1MB\",\n       type = ConfigType.SIZE,\n       description = \"Checksum will be computed for every bytes per checksum \"\n           + \"number of bytes and stored sequentially. The minimum value for \"\n           + \"this config is 16KB.\",\n       tags = ConfigTag.CLIENT)\n-  private int bytesPerChecksum = 256 * 1024;\n+  private int bytesPerChecksum = 1024 * 1024;\n \n   @Config(key = \"verify.checksum\",\n       defaultValue = \"true\",\n"}}, {"oid": "0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "url": "https://github.com/apache/ozone/commit/0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "message": "Change default bytes per checksum to 256KB and reduce min bytes per checksum to 16KB", "committedDate": "2021-03-17T17:49:16Z", "type": "commit"}, {"oid": "f6b0f31a2c64fae2b5376a46b4decfbb383e0e30", "url": "https://github.com/apache/ozone/commit/f6b0f31a2c64fae2b5376a46b4decfbb383e0e30", "message": "1. Introduce ReadChunk Versions -\n   V0 for returning data as single ByteString (old format).\n   V1 for returning data as a list of ByteStrings, with each ByteString length = number of bytes per checksum.\n2. If chunk does not have checksums, then set buffer capacity to a default (64KB).\n3. Return data from chunk as a list of ByteBuffers instead of a single ByteBuffer.", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "03a3d82777731f0dcfce0ef2b849cb7cf027932d", "url": "https://github.com/apache/ozone/commit/03a3d82777731f0dcfce0ef2b849cb7cf027932d", "message": "Unit tests for InputStreams", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "bb672c04160654dd4ff60c4126bdcbe9ebecaf93", "url": "https://github.com/apache/ozone/commit/bb672c04160654dd4ff60c4126bdcbe9ebecaf93", "message": "CI fixes", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "f19c58e91b8039cdcad958e8cecf8057c7ae734a", "url": "https://github.com/apache/ozone/commit/f19c58e91b8039cdcad958e8cecf8057c7ae734a", "message": "CI fixes 2", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "cc460c8b651f84467c320c51f37098b64e2337a2", "url": "https://github.com/apache/ozone/commit/cc460c8b651f84467c320c51f37098b64e2337a2", "message": "CI fix 3", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "710e999afe1c009fa98b8801bd5a402f36404b88", "url": "https://github.com/apache/ozone/commit/710e999afe1c009fa98b8801bd5a402f36404b88", "message": "Set ReadChunkVersion explicitly", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "d4020effb5591967181a442e1246bfb1f69649b3", "url": "https://github.com/apache/ozone/commit/d4020effb5591967181a442e1246bfb1f69649b3", "message": "CI fixes + check if ReadChunkResponse hasData before calling getData", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "url": "https://github.com/apache/ozone/commit/6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "message": "Review comments and CI fixes", "committedDate": "2021-03-17T17:49:56Z", "type": "commit"}, {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "url": "https://github.com/apache/ozone/commit/6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "message": "Review comments and CI fixes", "committedDate": "2021-03-17T17:49:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMjU0Mw==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597612543", "bodyText": "If read from first location fails and we have to fall back to the temp chunk file, this would cause exception.", "author": "adoroszlai", "createdAt": "2021-03-19T11:41:24Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -255,16 +287,16 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n         if (file.exists()) {\n           long offset = info.getOffset() - chunkFileOffset;\n           Preconditions.checkState(offset >= 0);\n-          ChunkUtils.readData(file, data, offset, len, volumeIOStats);\n-          return ChunkBuffer.wrap(data);\n+          ChunkUtils.readData(file, dataBuffers, offset, len, volumeIOStats);\n+          return ChunkBuffer.wrap(Lists.newArrayList(dataBuffers));\n         }\n       } catch (StorageContainerException ex) {\n         //UNABLE TO FIND chunk is not a problem as we will try with the\n         //next possible location\n         if (ex.getResult() != UNABLE_TO_FIND_CHUNK) {\n           throw ex;\n         }\n-        data.clear();\n+        dataBuffers = null;", "originalCommit": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMzQzOQ==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597613439", "bodyText": "This block seems to be duplicated from FilePerBlock....  Can it be extracted?", "author": "adoroszlai", "createdAt": "2021-03-19T11:43:14Z", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -222,7 +228,33 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     }\n \n     long len = info.getLen();\n-    ByteBuffer data = ByteBuffer.allocate((int) len);\n+\n+    long bufferCapacity = 0;\n+    if (info.isReadDataIntoSingleBuffer()) {\n+      // Older client - read all chunk data into one single buffer.\n+      bufferCapacity = len;\n+    } else {\n+      // Set buffer capacity to checksum boundary size so that each buffer\n+      // corresponds to one checksum. If checksum is NONE, then set buffer\n+      // capacity to default (OZONE_CHUNK_READ_BUFFER_DEFAULT_SIZE_KEY = 64KB).\n+      ChecksumData checksumData = info.getChecksumData();\n+\n+      if (checksumData != null) {\n+        if (checksumData.getChecksumType() ==\n+            ContainerProtos.ChecksumType.NONE) {\n+          bufferCapacity = defaultReadBufferCapacity;\n+        } else {\n+          bufferCapacity = checksumData.getBytesPerChecksum();\n+        }\n+      }\n+    }\n+    // If the buffer capacity is 0, set all the data into one ByteBuffer\n+    if (bufferCapacity == 0) {\n+      bufferCapacity = len;\n+    }", "originalCommit": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxNjkwMA==", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597616900", "bodyText": "I think we should avoid streams on read/write path.  Earlier these were found to cause CPU usage hotspots.  See eg. HDDS-3702.\n(Also in few other instances below.)", "author": "adoroszlai", "createdAt": "2021-03-19T11:49:48Z", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -364,7 +404,18 @@ protected ByteString readChunk(ChunkInfo readChunkInfo) throws IOException {\n       throw new IOException(\"Unexpected OzoneException: \" + e.toString(), e);\n     }\n \n-    return readChunkResponse.getData();\n+    if (readChunkResponse.hasData()) {\n+      return readChunkResponse.getData().asReadOnlyByteBufferList();\n+    } else if (readChunkResponse.hasDataBuffers()) {\n+      List<ByteString> buffersList = readChunkResponse.getDataBuffers()\n+          .getBuffersList();\n+      return buffersList.stream()\n+          .map(ByteString::asReadOnlyByteBuffer)\n+          .collect(Collectors.toList());", "originalCommit": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}]}