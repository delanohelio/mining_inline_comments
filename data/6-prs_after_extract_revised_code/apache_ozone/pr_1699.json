{"pr_number": 1699, "pr_title": "HDDS-4583. TableCache Refactor to fix issues in cleanup never policy.", "pr_createdAt": "2020-12-14T19:17:23Z", "pr_url": "https://github.com/apache/ozone/pull/1699", "timeline": [{"oid": "fa23f68fc75997dded5d8d822880dcaf283c20da", "url": "https://github.com/apache/ozone/commit/fa23f68fc75997dded5d8d822880dcaf283c20da", "message": "HDDS-4583. TableCache Refactor to fix issues in cleanup never policy.", "committedDate": "2020-12-14T19:14:48Z", "type": "commit"}, {"oid": "be08a1761fcda66277d37d9668bf1731a4b1502d", "url": "https://github.com/apache/ozone/commit/be08a1761fcda66277d37d9668bf1731a4b1502d", "message": "fix cs", "committedDate": "2020-12-14T19:43:36Z", "type": "commit"}, {"oid": "2be87b1d4e68c5e77fbe2a77113865b94a3e4eb3", "url": "https://github.com/apache/ozone/commit/2be87b1d4e68c5e77fbe2a77113865b94a3e4eb3", "message": "fix cs and javadoc", "committedDate": "2020-12-14T20:04:17Z", "type": "commit"}, {"oid": "91e5feaac9b1330df1951a4563b25f73f7a7b98c", "url": "https://github.com/apache/ozone/commit/91e5feaac9b1330df1951a4563b25f73f7a7b98c", "message": "fix cs and add epoch entry for volume/bucket for all ops to cleanup epoch entry", "committedDate": "2020-12-14T21:03:30Z", "type": "commit"}, {"oid": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "url": "https://github.com/apache/ozone/commit/1df7910e8294d0c7b7258444087ebaf385cb04ff", "message": "fix cs", "committedDate": "2020-12-14T21:07:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NjU3Mw==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546956573", "bodyText": "Can we add new JavaDoc here.", "author": "hanishakoneru", "createdAt": "2020-12-21T22:15:39Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java", "diffHunk": "@@ -61,14 +60,9 @@\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,\n       Class<KEY> keyType, Class<VALUE> valueType) throws IOException;\n \n-  /**\n-   * Gets an existing TableStore with implicit key/value conversion and\n-   * with specified cleanup policy for cache.\n-   * @throws IOException\n-   */\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java\nindex 2de7070d2..f0096ed9d 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/DBStore.java\n\n@@ -60,6 +60,16 @@\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,\n       Class<KEY> keyType, Class<VALUE> valueType) throws IOException;\n \n+  /**\n+   * Gets an existing TableStore with implicit key/value conversion and\n+   * with specified cache type.\n+   * @param name - Name of the TableStore to get\n+   * @param keyType\n+   * @param valueType\n+   * @param cacheType\n+   * @return - TableStore.\n+   * @throws IOException\n+   */\n   <KEY, VALUE> Table<KEY, VALUE> getTable(String name,\n       Class<KEY> keyType, Class<VALUE> valueType,\n       TableCache.CacheType cacheType) throws IOException;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1NzM5Mg==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546957392", "bodyText": "Any reason for removing the Override flag?", "author": "hanishakoneru", "createdAt": "2020-12-21T22:17:55Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java", "diffHunk": "@@ -307,12 +307,11 @@ protected ObjectName getStatMBeanName() {\n         valueType);\n   }\n \n-  @Override", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java\nindex 679b91f3e..252363c4e 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/RDBStore.java\n\n@@ -307,6 +307,7 @@ protected ObjectName getStatMBeanName() {\n         valueType);\n   }\n \n+  @Override\n   public <K, V> Table<K, V> getTable(String name,\n       Class<K> keyType, Class<V> valueType,\n       TableCache.CacheType cacheType) throws IOException {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk1ODQ1Ng==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546958456", "bodyText": "JavaDoc says default cache type is FullCache but the calling parameter is set to PartialCache.", "author": "hanishakoneru", "createdAt": "2020-12-21T22:20:54Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java", "diffHunk": "@@ -61,8 +62,7 @@\n \n   /**\n    * Create an TypedTable from the raw table.\n-   * Default cleanup policy used for the table is\n-   * {@link CacheCleanupPolicy#MANUAL}.\n+   * Default cache type for the table is {@link CacheType#FullCache}.", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java\nindex 34f17f40d..88e13fdb3 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/TypedTable.java\n\n@@ -62,7 +62,7 @@\n \n   /**\n    * Create an TypedTable from the raw table.\n-   * Default cache type for the table is {@link CacheType#FullCache}.\n+   * Default cache type for the table is {@link CacheType#PartialCache}.\n    * @param rawTable\n    * @param codecRegistry\n    * @param keyType\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk5NzMxNg==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r546997316", "bodyText": "NIT: look ups", "author": "hanishakoneru", "createdAt": "2020-12-22T00:18:45Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -40,64 +41,74 @@\n import org.slf4j.LoggerFactory;\n \n /**\n- * Cache implementation for the table. Depending on the cache clean up policy\n- * this cache will be full cache or partial cache.\n- *\n- * If cache cleanup policy is set as {@link CacheCleanupPolicy#MANUAL},\n- * this will be a partial cache.\n- *\n- * If cache cleanup policy is set as {@link CacheCleanupPolicy#NEVER},\n- * this will be a full cache.\n+ * Cache implementation for the table. Full Table cache, where the DB state\n+ * and cache state will be same for these tables.\n  */\n @Private\n @Evolving\n-public class TableCacheImpl<CACHEKEY extends CacheKey,\n+public class FullTableCache<CACHEKEY extends CacheKey,\n     CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n \n   public static final Logger LOG =\n-      LoggerFactory.getLogger(TableCacheImpl.class);\n+      LoggerFactory.getLogger(FullTableCache.class);\n \n   private final Map<CACHEKEY, CACHEVALUE> cache;\n   private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n   private ExecutorService executorService;\n-  private CacheCleanupPolicy cleanupPolicy;\n \n+  private final ReadWriteLock lock;\n \n \n-  public TableCacheImpl(CacheCleanupPolicy cleanupPolicy) {\n-\n+  public FullTableCache() {\n     // As for full table cache only we need elements to be inserted in sorted\n-    // manner, so that list will be easy. For other we can go with Hash map.\n-    if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-      cache = new ConcurrentSkipListMap<>();\n-    } else {\n-      cache = new ConcurrentHashMap<>();\n-    }\n+    // manner, so that list will be easy. But looks up have log(N) time", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\nindex 01bbb0044..2754b593e 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n\n@@ -61,7 +60,7 @@\n \n   public FullTableCache() {\n     // As for full table cache only we need elements to be inserted in sorted\n-    // manner, so that list will be easy. But looks up have log(N) time\n+    // manner, so that list will be easy. But look ups have log(N) time\n     // complexity.\n \n     // Here lock is required to protect cache because cleanup is not done\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwMDU5Mw==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547000593", "bodyText": "I think with the new log4j, we do not need this check for isDebugEnabled(). Parameters will be evaluated only if Debug is enabled.", "author": "hanishakoneru", "createdAt": "2020-12-22T00:31:19Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            removed.set(true);\n+            if (LOG.isDebugEnabled()) {", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\nindex 01bbb0044..2754b593e 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n\n@@ -128,7 +127,6 @@ public int size() {\n   @VisibleForTesting\n   public void evictCache(List<Long> epochs) {\n     EpochEntry<CACHEKEY> currentEntry;\n-    final AtomicBoolean removed = new AtomicBoolean();\n     CACHEKEY cachekey;\n     long lastEpoch = epochs.get(epochs.size() - 1);\n     for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwNDA4MA==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547004080", "bodyText": "Can we add the comment back - // Remove only entries which are marked for delete. and elaborate more that only the epoch entry corresponding to the current CacheValue will be removed here.", "author": "hanishakoneru", "createdAt": "2020-12-22T00:44:32Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzQwNw==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557407", "bodyText": "Updated.", "author": "bharatviswa504", "createdAt": "2020-12-22T23:34:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwNDA4MA=="}], "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\nindex 01bbb0044..2754b593e 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n\n@@ -128,7 +127,6 @@ public int size() {\n   @VisibleForTesting\n   public void evictCache(List<Long> epochs) {\n     EpochEntry<CACHEKEY> currentEntry;\n-    final AtomicBoolean removed = new AtomicBoolean();\n     CACHEKEY cachekey;\n     long lastEpoch = epochs.get(epochs.size() - 1);\n     for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTk1Mg==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547481952", "bodyText": "In the non-ratis OM cluster, we might get evictCache in non sorted order. Let's say we get evictCache (1,3,4). Then we might remove epoch entry 2 also even though it has not been flushed by DoubleBuffer, right?\nInstead of having this removed boolean, would it be easier if we remove the epoch entry whenever it is there in evictCache list and only then? Something like this check before the cache.computeIfPresent()...\nlock.writeLock().lock();\nif (epochs.contains(currentEpoch) {\n   iterator.remove();\n   cache.computeIfPresent(cachekey, ((k, v) -> {\n   ........\n}", "author": "hanishakoneru", "createdAt": "2020-12-22T20:01:50Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java", "diffHunk": "@@ -125,43 +136,44 @@ protected void evictCache(List<Long> epochs) {\n       currentEntry = iterator.next();\n       cachekey = currentEntry.getCachekey();\n       long currentEpoch = currentEntry.getEpoch();\n-      CacheValue cacheValue = cache.computeIfPresent(cachekey, ((k, v) -> {\n-        if (cleanupPolicy == CacheCleanupPolicy.MANUAL) {\n-          if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            iterator.remove();\n-            removed.set(true);\n-            return null;\n-          }\n-        } else if (cleanupPolicy == CacheCleanupPolicy.NEVER) {\n-          // Remove only entries which are marked for delete.\n+\n+      // Acquire lock to avoid race between cleanup and add to cache entry by\n+      // client requests.\n+      try {\n+        lock.writeLock().lock();\n+        cache.computeIfPresent(cachekey, ((k, v) -> {\n           if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())\n               && v.getCacheValue() == null) {\n-            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n-                k.getCacheKey(), currentEpoch);\n-            removed.set(true);\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                  k.getCacheKey(), currentEpoch);\n+            }\n             iterator.remove();\n+            removed.set(true);\n             return null;\n           }\n+          return v;\n+        }));\n+      } finally {\n+        lock.writeLock().unlock();\n+      }\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzMyNA==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557324", "bodyText": "Good idea, updated code.", "author": "bharatviswa504", "createdAt": "2020-12-22T23:34:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MTk1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\nindex 01bbb0044..2754b593e 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/FullTableCache.java\n\n@@ -128,7 +127,6 @@ public int size() {\n   @VisibleForTesting\n   public void evictCache(List<Long> epochs) {\n     EpochEntry<CACHEKEY> currentEntry;\n-    final AtomicBoolean removed = new AtomicBoolean();\n     CACHEKEY cachekey;\n     long lastEpoch = epochs.get(epochs.size() - 1);\n     for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwNjk0Mg==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547506942", "bodyText": "Typo: Partial table cache", "author": "hanishakoneru", "createdAt": "2020-12-22T21:04:41Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\nindex 8a9c35b15..0bf03c50c 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n\n@@ -29,7 +29,6 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.atomic.AtomicBoolean;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwODIzMw==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547508233", "bodyText": "LOG.isDebugEnabled check can be removed here.", "author": "hanishakoneru", "createdAt": "2020-12-22T21:08:14Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.\n+  }\n+\n+  @Override\n+  public void put(CACHEKEY cacheKey, CACHEVALUE value) {\n+    cache.put(cacheKey, value);\n+    epochEntries.add(new EpochEntry<>(value.getEpoch(), cacheKey));\n+  }\n+\n+  public void cleanup(List<Long> epochs) {\n+    executorService.execute(() -> evictCache(epochs));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return cache.size();\n+  }\n+\n+  @Override\n+  public Iterator<Map.Entry<CACHEKEY, CACHEVALUE>> iterator() {\n+    return cache.entrySet().iterator();\n+  }\n+\n+  @VisibleForTesting\n+  public void evictCache(List<Long> epochs) {\n+    EpochEntry<CACHEKEY> currentEntry;\n+    final AtomicBoolean removed = new AtomicBoolean();\n+    CACHEKEY cachekey;\n+    long lastEpoch = epochs.get(epochs.size() - 1);\n+    for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n+         iterator.hasNext();) {\n+      currentEntry = iterator.next();\n+      cachekey = currentEntry.getCachekey();\n+      long currentEpoch = currentEntry.getEpoch();\n+\n+      // As ConcurrentHashMap computeIfPresent is atomic, there is no race\n+      // condition between cache cleanup and requests updating same cache entry.\n+\n+      cache.computeIfPresent(cachekey, ((k, v) -> {\n+        if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                k.getCacheKey(), currentEpoch);\n+          }\n+          iterator.remove();\n+          removed.set(true);\n+          return null;\n+        }\n+        return v;\n+      }));\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {\n+        if (LOG.isDebugEnabled()) {", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\nindex 8a9c35b15..0bf03c50c 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n\n@@ -29,7 +29,6 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.atomic.AtomicBoolean;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwODQ1NA==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547508454", "bodyText": "Same suggestion as in FullTableCache for removed usage.", "author": "hanishakoneru", "createdAt": "2020-12-22T21:08:50Z", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hdds.utils.db.cache;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NavigableSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hdds.annotation.InterfaceAudience.Private;\n+import org.apache.hadoop.hdds.annotation.InterfaceStability.Evolving;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Cache implementation for the table. Partial Table cache, where the DB state\n+ * and cache state will not be same. Partial table cache holds entries until\n+ * flush to DB happens.\n+ */\n+@Private\n+@Evolving\n+public class PartialTableCache<CACHEKEY extends CacheKey,\n+    CACHEVALUE extends CacheValue> implements TableCache<CACHEKEY, CACHEVALUE> {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(PartialTableCache.class);\n+\n+  private final Map<CACHEKEY, CACHEVALUE> cache;\n+  private final NavigableSet<EpochEntry<CACHEKEY>> epochEntries;\n+  private ExecutorService executorService;\n+\n+\n+  public PartialTableCache() {\n+    // We use concurrent Hash map for O(1) lookup for get API.\n+    // During list operation for partial cache we anyway merge between DB and\n+    // cache state. So entries in cache does not need to be in sorted order.\n+\n+    // And as concurrentHashMap computeIfPresent which is used by cleanup is\n+    // atomic operation, and ozone level locks like bucket/volume locks\n+    // protect updating same key, here it is not required to hold cache\n+    // level locks during update/cleanup operation.\n+\n+    // 1. During update, it is caller responsibility to hold volume/bucket\n+    // locks.\n+    // 2. During cleanup which removes entry, while request is updating cache\n+    // that should be guarded by concurrentHashMap guaranty.\n+    cache = new ConcurrentHashMap<>();\n+\n+    epochEntries = new ConcurrentSkipListSet<>();\n+    // Created a singleThreadExecutor, so one cleanup will be running at a\n+    // time.\n+    ThreadFactory build = new ThreadFactoryBuilder().setDaemon(true)\n+        .setNameFormat(\"PartialTableCache Cleanup Thread - %d\").build();\n+    executorService = Executors.newSingleThreadExecutor(build);\n+  }\n+\n+  @Override\n+  public CACHEVALUE get(CACHEKEY cachekey) {\n+    return cache.get(cachekey);\n+  }\n+\n+  @Override\n+  public void loadInitial(CACHEKEY cacheKey, CACHEVALUE cacheValue) {\n+    // Do nothing for full table cache.\n+  }\n+\n+  @Override\n+  public void put(CACHEKEY cacheKey, CACHEVALUE value) {\n+    cache.put(cacheKey, value);\n+    epochEntries.add(new EpochEntry<>(value.getEpoch(), cacheKey));\n+  }\n+\n+  public void cleanup(List<Long> epochs) {\n+    executorService.execute(() -> evictCache(epochs));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return cache.size();\n+  }\n+\n+  @Override\n+  public Iterator<Map.Entry<CACHEKEY, CACHEVALUE>> iterator() {\n+    return cache.entrySet().iterator();\n+  }\n+\n+  @VisibleForTesting\n+  public void evictCache(List<Long> epochs) {\n+    EpochEntry<CACHEKEY> currentEntry;\n+    final AtomicBoolean removed = new AtomicBoolean();\n+    CACHEKEY cachekey;\n+    long lastEpoch = epochs.get(epochs.size() - 1);\n+    for (Iterator<EpochEntry<CACHEKEY>> iterator = epochEntries.iterator();\n+         iterator.hasNext();) {\n+      currentEntry = iterator.next();\n+      cachekey = currentEntry.getCachekey();\n+      long currentEpoch = currentEntry.getEpoch();\n+\n+      // As ConcurrentHashMap computeIfPresent is atomic, there is no race\n+      // condition between cache cleanup and requests updating same cache entry.\n+\n+      cache.computeIfPresent(cachekey, ((k, v) -> {\n+        if (v.getEpoch() == currentEpoch && epochs.contains(v.getEpoch())) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"CacheKey {} with epoch {} is removed from cache\",\n+                k.getCacheKey(), currentEpoch);\n+          }\n+          iterator.remove();\n+          removed.set(true);\n+          return null;\n+        }\n+        return v;\n+      }));\n+\n+      // If currentEntry epoch is greater than last epoch provided, we have\n+      // deleted all entries less than specified epoch. So, we can break.\n+      if (currentEpoch > lastEpoch) {\n+        break;\n+      }\n+\n+      // When epoch entry is not removed, this might be a override entry in\n+      // cache. Clean that epoch entry.\n+      if (!removed.get()) {", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\nindex 8a9c35b15..0bf03c50c 100644\n--- a/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n+++ b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/db/cache/PartialTableCache.java\n\n@@ -29,7 +29,6 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.atomic.AtomicBoolean;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwOTk4NA==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547509984", "bodyText": "Can we add the case where evictCache is called with an missing entry from consecutive list of epochs.", "author": "hanishakoneru", "createdAt": "2020-12-22T21:13:28Z", "path": "hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java", "diffHunk": "@@ -380,6 +383,33 @@ public void testPartialTableCacheParallel() throws Exception {\n \n   }\n \n+  @Test\n+  public void testTableCache() {\n+\n+    // In non-HA epoch entries might be out of order.\n+    // Scenario is like create vol, set vol, set vol, delete vol\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(0)), 0));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(1)), 1));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(2)), 3));\n+\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.absent(), 2));\n+\n+    List<Long> epochs = new ArrayList<>();\n+    epochs.add(0L);\n+    epochs.add(1L);\n+    epochs.add(2L);\n+    epochs.add(3L);\n+", "originalCommit": "1df7910e8294d0c7b7258444087ebaf385cb04ff", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzU1NzI4OQ==", "url": "https://github.com/apache/ozone/pull/1699#discussion_r547557289", "bodyText": "Added a new test", "author": "bharatviswa504", "createdAt": "2020-12-22T23:34:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzUwOTk4NA=="}], "type": "inlineReview", "revised_code": {"commit": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "chunk": "diff --git a/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java b/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java\nindex 2a4238709..b908ef84e 100644\n--- a/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java\n+++ b/hadoop-hdds/framework/src/test/java/org/apache/hadoop/hdds/utils/db/cache/TestTableCache.java\n\n@@ -410,6 +410,68 @@ public void testTableCache() {\n     Assert.assertTrue(tableCache.getEpochEntrySet().size() == 0);\n   }\n \n+\n+  @Test\n+  public void testTableCacheWithNonConsecutiveEpochList() {\n+\n+    // In non-HA epoch entries might be out of order.\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(0)), 0));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(1)), 1));\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+        new CacheValue<>(Optional.of(Long.toString(3)), 3));\n+\n+    tableCache.put(new CacheKey<>(Long.toString(0)),\n+          new CacheValue<>(Optional.of(Long.toString(2)), 2));\n+\n+    tableCache.put(new CacheKey<>(Long.toString(1)),\n+        new CacheValue<>(Optional.of(Long.toString(1)), 4));\n+\n+    List<Long> epochs = new ArrayList<>();\n+    epochs.add(0L);\n+    epochs.add(1L);\n+    epochs.add(3L);\n+\n+    tableCache.evictCache(epochs);\n+\n+    Assert.assertTrue(tableCache.size() == 2);\n+    Assert.assertTrue(tableCache.getEpochEntrySet().size() == 2);\n+\n+    Assert.assertNotNull(tableCache.get(new CacheKey<>(Long.toString(0))));\n+    Assert.assertEquals(2,\n+        tableCache.get(new CacheKey<>(Long.toString(0))).getEpoch());\n+\n+    Assert.assertNotNull(tableCache.get(new CacheKey<>(Long.toString(1))));\n+    Assert.assertEquals(4,\n+        tableCache.get(new CacheKey<>(Long.toString(1))).getEpoch());\n+\n+    // now evict 2,4\n+    epochs = new ArrayList<>();\n+    epochs.add(2L);\n+    epochs.add(4L);\n+\n+    tableCache.evictCache(epochs);\n+\n+    if(cacheType == TableCache.CacheType.PartialCache) {\n+      Assert.assertTrue(tableCache.size() == 0);\n+      Assert.assertTrue(tableCache.getEpochEntrySet().size() == 0);\n+    } else {\n+      Assert.assertTrue(tableCache.size() == 2);\n+      Assert.assertTrue(tableCache.getEpochEntrySet().size() == 0);\n+\n+      // Entries should exist, as the entries are not delete entries\n+      Assert.assertNotNull(tableCache.get(new CacheKey<>(Long.toString(0))));\n+      Assert.assertEquals(2,\n+          tableCache.get(new CacheKey<>(Long.toString(0))).getEpoch());\n+\n+      Assert.assertNotNull(tableCache.get(new CacheKey<>(Long.toString(1))));\n+      Assert.assertEquals(4,\n+          tableCache.get(new CacheKey<>(Long.toString(1))).getEpoch());\n+    }\n+\n+  }\n+\n   private int writeToCache(int count, int startVal, long sleep)\n       throws InterruptedException {\n     int counter = 1;\n"}}, {"oid": "7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "url": "https://github.com/apache/ozone/commit/7bdade07b42278c6b0c7fde8a78b02b5291e9b73", "message": "fix review comments", "committedDate": "2020-12-22T23:27:06Z", "type": "commit"}, {"oid": "5d97aa690aea478c84f6e5ae0d694b02b3288261", "url": "https://github.com/apache/ozone/commit/5d97aa690aea478c84f6e5ae0d694b02b3288261", "message": "make enum caps", "committedDate": "2020-12-22T23:32:47Z", "type": "commit"}, {"oid": "d62d8747c9bac340c9263d2a086e0a027facca39", "url": "https://github.com/apache/ozone/commit/d62d8747c9bac340c9263d2a086e0a027facca39", "message": "Fix cs", "committedDate": "2020-12-23T07:28:47Z", "type": "commit"}]}