{"pr_number": 6044, "pr_title": "Support for multi-threaded Group By reducer for SQL.", "pr_createdAt": "2020-09-22T04:56:18Z", "pr_url": "https://github.com/apache/pinot/pull/6044", "timeline": [{"oid": "0192716f7e55f8c0a367b16056c240ceadce64b3", "url": "https://github.com/apache/pinot/commit/0192716f7e55f8c0a367b16056c240ceadce64b3", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-09-22T05:05:22Z", "type": "forcePushed"}, {"oid": "8b72494adf2992c66fd30bafada7acd4b308d6d4", "url": "https://github.com/apache/pinot/commit/8b72494adf2992c66fd30bafada7acd4b308d6d4", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by the reduce phase.\n\n- In this PR, the executor service defaults to have a single thread, until the performance\n  impact can be studied under various conditions (eg high qps, where brokers have higher\n  CPU usage).\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-09-22T05:08:00Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1MzczNg==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r492853736", "bodyText": "Actually, on second thought, the default value of 1 is not good, as it will make reduce across concurrent queries as sequential. Moreover, if we add more threads, then it may cause contention in case of high qps use cases.\nWhile we tune this, perhaps the behavior should be:\n\nIf config not explicitly specified, then preserve current behavior without executor service, or perhaps using MoreExecutors.newDirectExecutorService() that uses the calling thread to execute the Runnable.\nIf config specified, use executor service with num threads specified in the config.\n\nThoughts @kishoreg  @Jackie-Jiang ?\n(I have updated the PR with the approach above).", "author": "mayankshriv", "createdAt": "2020-09-22T15:59:08Z", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -161,6 +161,10 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_NUM_REDUCE_THREADS = \"pinot.broker.num.reduce.threads\";\n+    public static final int DEFAULT_NUM_REDUCE_THREADS = 1; // TBD: Change to a more appropriate default (eg numCores).", "originalCommit": "8b72494adf2992c66fd30bafada7acd4b308d6d4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyMjM4NA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r493022384", "bodyText": "This config is right but the implementation can be changed. This should be something similar to what we have in combine operator - Executor pool is cached or capped at a high number based on the number of cores. But the number of callables we create be based on this config.", "author": "kishoreg", "createdAt": "2020-09-22T20:44:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg1MzczNg=="}], "type": "inlineReview", "revised_code": {"commit": "c73604641d5d84656b0e980b95329dc3152c66f7", "chunk": "diff --git a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\nindex 01c4380065..84b601cf78 100644\n--- a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n+++ b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n\n@@ -162,8 +167,9 @@ public class CommonConstants {\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n     // Config for number of threads to use for Broker reduce-phase.\n-    public static final String CONFIG_OF_NUM_REDUCE_THREADS = \"pinot.broker.num.reduce.threads\";\n-    public static final int DEFAULT_NUM_REDUCE_THREADS = 1; // TBD: Change to a more appropriate default (eg numCores).\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n+    public static final int MAX_REDUCE_THREADS_PER_QUERY =\n+        Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2)); // Same logic as CombineOperatorUtils\n \n     public static class Request {\n       public static final String PQL = \"pql\";\n"}}, {"oid": "3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "url": "https://github.com/apache/pinot/commit/3e13eb46ffb1b57587d7116de043ef79d0a6afe9", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-09-22T16:22:19Z", "type": "forcePushed"}, {"oid": "105fe42f549b6a40313d051250cd2059c537041d", "url": "https://github.com/apache/pinot/commit/105fe42f549b6a40313d051250cd2059c537041d", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-09-22T17:33:38Z", "type": "forcePushed"}, {"oid": "5174dceac6a0b13011a31febcd4b235a2f9bb96c", "url": "https://github.com/apache/pinot/commit/5174dceac6a0b13011a31febcd4b235a2f9bb96c", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase.\n\n- Added a broker side config to specify the number of threads to be used for reduce phase.\n  `pinot.broker.num.reduce.threads`\n\n- The default behavior in this PR is to use `MoreExecutors.newDirectExecutorService()` which\n  uses the calling thread to execute the `Runnable`, mainting the existing behavior.\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-09-24T16:11:26Z", "type": "forcePushed"}, {"oid": "c73604641d5d84656b0e980b95329dc3152c66f7", "url": "https://github.com/apache/pinot/commit/c73604641d5d84656b0e980b95329dc3152c66f7", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-08T04:45:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjI0Ng==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726246", "bodyText": "This can still be final?", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:11:40Z", "path": "pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java", "diffHunk": "@@ -102,7 +102,7 @@\n \n   protected final AtomicLong _requestIdGenerator = new AtomicLong();\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-  protected final BrokerReduceService _brokerReduceService = new BrokerReduceService();\n+  protected BrokerReduceService _brokerReduceService;", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNTUyNQ==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503635525", "bodyText": "Yeah, not sure what happened there. Probably a side effect of trying out some intermediate code.", "author": "mayankshriv", "createdAt": "2020-10-13T02:46:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjI0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java b/pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java\nindex b1be3e6fb8..3dfa4f1db9 100644\n--- a/pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java\n+++ b/pinot-broker/src/main/java/org/apache/pinot/broker/requesthandler/BaseBrokerRequestHandler.java\n\n@@ -102,7 +102,7 @@ public abstract class BaseBrokerRequestHandler implements BrokerRequestHandler {\n \n   protected final AtomicLong _requestIdGenerator = new AtomicLong();\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-  protected BrokerReduceService _brokerReduceService;\n+  protected final BrokerReduceService _brokerReduceService;\n \n   protected final String _brokerId;\n   protected final long _brokerTimeoutMs;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjkzOQ==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502726939", "bodyText": "pinot.broker.max.reduce.threads.per.query for clarity?", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:18:00Z", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNjA5MA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503636090", "bodyText": "I debated about it, and felt that sub-setting becomes interesting (e.g. what does 'per' mean). But seems like there's other configs that also follow this, so will change.", "author": "mayankshriv", "createdAt": "2020-10-13T02:48:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNjkzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\nindex 84b601cf78..0d23bb2d75 100644\n--- a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n+++ b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n\n@@ -167,8 +167,8 @@ public class CommonConstants {\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n     // Config for number of threads to use for Broker reduce-phase.\n-    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n-    public static final int MAX_REDUCE_THREADS_PER_QUERY =\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads.per.query\";\n+    public static final int DEFAULT_MAX_REDUCE_THREADS_PER_QUERY =\n         Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2)); // Same logic as CombineOperatorUtils\n \n     public static class Request {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzE5Nw==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727197", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static final int MAX_REDUCE_THREADS_PER_QUERY =\n          \n          \n            \n                public static final int DEFAULT_MAX_REDUCE_THREADS_PER_QUERY =", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:20:04Z", "path": "pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java", "diffHunk": "@@ -166,6 +166,11 @@\n     public static final double DEFAULT_BROKER_MIN_RESOURCE_PERCENT_FOR_START = 100.0;\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n+    // Config for number of threads to use for Broker reduce-phase.\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n+    public static final int MAX_REDUCE_THREADS_PER_QUERY =", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\nindex 84b601cf78..0d23bb2d75 100644\n--- a/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n+++ b/pinot-common/src/main/java/org/apache/pinot/common/utils/CommonConstants.java\n\n@@ -167,8 +167,8 @@ public class CommonConstants {\n     public static final String CONFIG_OF_ENABLE_QUERY_LIMIT_OVERRIDE = \"pinot.broker.enable.query.limit.override\";\n \n     // Config for number of threads to use for Broker reduce-phase.\n-    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads\";\n-    public static final int MAX_REDUCE_THREADS_PER_QUERY =\n+    public static final String CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY = \"pinot.broker.max.reduce.threads.per.query\";\n+    public static final int DEFAULT_MAX_REDUCE_THREADS_PER_QUERY =\n         Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2)); // Same logic as CombineOperatorUtils\n \n     public static class Request {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzM0Ng==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727346", "bodyText": "Log both number or worker threads and threads per query?\nAlso, if it is single-threaded, no need to launch the executor service.", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:21:28Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNzY4MA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503637680", "bodyText": "Initially, I had Guava's MoreExecutor.directorExecutor() that uses the current thread to run the task, in case of single thread. I decided to just keep it simple and have the exact same code in case of single vs multi-thread (with exception of index table). We can revisit that if needed.", "author": "mayankshriv", "createdAt": "2020-10-13T02:55:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzM0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\nindex db239be900..497fa6fcaa 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n\n@@ -61,6 +61,9 @@ public class BrokerReduceService {\n \n   // brw -> Shorthand for broker reduce worker threads.\n   private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+\n+  // Set the reducer priority higher than NORM but lower than MAX, because if a query is complete\n+  // we want to deserialize and return response as soon. This is the same as server side 'pqr' threads.\n   protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n \n   private final ListeningExecutorService _reduceExecutorService;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzQ0NQ==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727445", "bodyText": "Any specific reason for this priority? Some comments will be appreciated", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:22:21Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzNzc4Mg==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503637782", "bodyText": "This is the same as the server side code, will add comments.", "author": "mayankshriv", "createdAt": "2020-10-13T02:55:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzQ0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\nindex db239be900..497fa6fcaa 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n\n@@ -61,6 +61,9 @@ public class BrokerReduceService {\n \n   // brw -> Shorthand for broker reduce worker threads.\n   private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+\n+  // Set the reducer priority higher than NORM but lower than MAX, because if a query is complete\n+  // we want to deserialize and return response as soon. This is the same as server side 'pqr' threads.\n   protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n \n   private final ListeningExecutorService _reduceExecutorService;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyNzY1MA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502727650", "bodyText": "I don't think we need to use the ListeningExecutorService here, ExecutorService should be enough with lower overhead", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:24:08Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java", "diffHunk": "@@ -47,8 +57,32 @@\n @ThreadSafe\n public class BrokerReduceService {\n \n+  private static final Logger LOGGER = LoggerFactory.getLogger(BrokerReduceService.class);\n+\n+  // brw -> Shorthand for broker reduce worker threads.\n+  private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+  protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n+\n+  private final ListeningExecutorService _reduceExecutorService;\n+  private final int _maxReduceThreadsPerQuery;\n+\n+  public BrokerReduceService(PinotConfiguration config) {\n+    _maxReduceThreadsPerQuery = config.getProperty(CommonConstants.Broker.CONFIG_OF_MAX_REDUCE_THREADS_PER_QUERY,\n+        CommonConstants.Broker.MAX_REDUCE_THREADS_PER_QUERY);\n+    LOGGER.info(\"Initializing BrokerReduceService with {} reduce threads.\", _maxReduceThreadsPerQuery);\n+\n+    ThreadFactory reduceThreadFactory =\n+        new ThreadFactoryBuilder().setDaemon(false).setPriority(QUERY_RUNNER_THREAD_PRIORITY)\n+            .setNameFormat(REDUCE_THREAD_NAME_FORMAT).build();\n+\n+    // ExecutorService is initialized with numThreads sames availableProcessors.\n+    ExecutorService delegate =\n+        Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors(), reduceThreadFactory);\n+    _reduceExecutorService = MoreExecutors.listeningDecorator(delegate);", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\nindex db239be900..497fa6fcaa 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/BrokerReduceService.java\n\n@@ -61,6 +61,9 @@ public class BrokerReduceService {\n \n   // brw -> Shorthand for broker reduce worker threads.\n   private static final String REDUCE_THREAD_NAME_FORMAT = \"brw-%d\";\n+\n+  // Set the reducer priority higher than NORM but lower than MAX, because if a query is complete\n+  // we want to deserialize and return response as soon. This is the same as server side 'pqr' threads.\n   protected static final int QUERY_RUNNER_THREAD_PRIORITY = 7;\n \n   private final ListeningExecutorService _reduceExecutorService;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502728458", "bodyText": "Return 1? You need at least one thread", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:30:22Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {\n+        if (!future.isDone()) {\n+          future.cancel(true);\n+        }\n       }\n     }\n+\n     indexedTable.finish(true);\n     return indexedTable;\n   }\n \n+  /**\n+   * Computes the number of reduce threads to use per query.\n+   * <ul>\n+   *   <li> Use single thread if number of data tables to reduce is less than {@value #MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE}.</li>\n+   *   <li> Else, use min of max allowed reduce threads per query, and number of data tables.</li>\n+   * </ul>\n+   *\n+   * @param numDataTables Number of data tables to reduce\n+   * @param maxReduceThreadsPerQuery Max allowed reduce threads per query\n+   * @return Number of reduce threads to use for the query\n+   */\n+  private int getNumReduceThreadsToUse(int numDataTables, int maxReduceThreadsPerQuery) {\n+    // Use single thread if number of data tables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE.\n+    if (numDataTables < MIN_DATA_TABLES_FOR_CONCURRENT_REDUCE) {\n+      return Math.min(1, numDataTables); // Number of data tables can be zero.", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDIzNg==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503640236", "bodyText": "Unit test fails, seems numDataTables can be zero.", "author": "mayankshriv", "createdAt": "2020-10-13T03:04:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTAwNw==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339007", "bodyText": "Yes, numDataTables can be zero, but I think returning 0 or 1 should both work. Actually a better approach should be just short-circuit the zero data table case.", "author": "Jackie-Jiang", "createdAt": "2020-10-14T00:54:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODQ1OA=="}], "type": "inlineReview", "revised_code": {"commit": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\nindex f09635446e..7cc423a7be 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n\n@@ -240,7 +256,8 @@ public class GroupByDataTableReducer implements DataTableReducer {\n   }\n \n   private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n-      DataTableReducerContext reducerContext) {\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n     long start = System.currentTimeMillis();\n     int numDataTables = dataTablesToReduce.size();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTA0Mg==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729042", "bodyText": "(Critical) You need to put the timeout exception into the query response, or the response will be wrong and there is no way to detect that", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:35:13Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];\n+    CountDownLatch countDownLatch = new CountDownLatch(numDataTables);\n+\n+    // Create groups of data tables that each thread can process concurrently.\n+    // Given that numReduceThreads is <= numDataTables, each group will have at least one data table.\n+    ArrayList<DataTable> dataTables = new ArrayList<>(dataTablesToReduce);\n+    List<List<DataTable>> reduceGroups = new ArrayList<>(numReduceThreadsToUse);\n+\n+    for (int i = 0; i < numReduceThreadsToUse; i++) {\n+      reduceGroups.add(new ArrayList<>());\n+    }\n+    for (int i = 0; i < numDataTables; i++) {\n+      reduceGroups.get(i % numReduceThreadsToUse).add(dataTables.get(i));\n+    }\n+\n+    int cnt = 0;\n     ColumnDataType[] columnDataTypes = dataSchema.getColumnDataTypes();\n-    for (DataTable dataTable : dataTables) {\n-      int numRows = dataTable.getNumberOfRows();\n-      for (int rowId = 0; rowId < numRows; rowId++) {\n-        Object[] values = new Object[_numColumns];\n-        for (int colId = 0; colId < _numColumns; colId++) {\n-          switch (columnDataTypes[colId]) {\n-            case INT:\n-              values[colId] = dataTable.getInt(rowId, colId);\n-              break;\n-            case LONG:\n-              values[colId] = dataTable.getLong(rowId, colId);\n-              break;\n-            case FLOAT:\n-              values[colId] = dataTable.getFloat(rowId, colId);\n-              break;\n-            case DOUBLE:\n-              values[colId] = dataTable.getDouble(rowId, colId);\n-              break;\n-            case STRING:\n-              values[colId] = dataTable.getString(rowId, colId);\n-              break;\n-            case BYTES:\n-              values[colId] = dataTable.getBytes(rowId, colId);\n-              break;\n-            case OBJECT:\n-              values[colId] = dataTable.getObject(rowId, colId);\n-              break;\n-            // Add other aggregation intermediate result / group-by column type supports here\n-            default:\n-              throw new IllegalStateException();\n+    for (List<DataTable> reduceGroup : reduceGroups) {\n+      futures[cnt++] = reducerContext.getExecutorService().submit(new TraceRunnable() {\n+        @Override\n+        public void runJob() {\n+          for (DataTable dataTable : reduceGroup) {\n+            int numRows = dataTable.getNumberOfRows();\n+\n+            try {\n+              for (int rowId = 0; rowId < numRows; rowId++) {\n+                Object[] values = new Object[_numColumns];\n+                for (int colId = 0; colId < _numColumns; colId++) {\n+                  switch (columnDataTypes[colId]) {\n+                    case INT:\n+                      values[colId] = dataTable.getInt(rowId, colId);\n+                      break;\n+                    case LONG:\n+                      values[colId] = dataTable.getLong(rowId, colId);\n+                      break;\n+                    case FLOAT:\n+                      values[colId] = dataTable.getFloat(rowId, colId);\n+                      break;\n+                    case DOUBLE:\n+                      values[colId] = dataTable.getDouble(rowId, colId);\n+                      break;\n+                    case STRING:\n+                      values[colId] = dataTable.getString(rowId, colId);\n+                      break;\n+                    case BYTES:\n+                      values[colId] = dataTable.getBytes(rowId, colId);\n+                      break;\n+                    case OBJECT:\n+                      values[colId] = dataTable.getObject(rowId, colId);\n+                      break;\n+                    // Add other aggregation intermediate result / group-by column type supports here\n+                    default:\n+                      throw new IllegalStateException();\n+                  }\n+                }\n+                indexedTable.upsert(new Record(values));\n+              }\n+            } finally {\n+              countDownLatch.countDown();\n+            }\n           }\n         }\n-        indexedTable.upsert(new Record(values));\n+      });\n+    }\n+\n+    try {\n+      long timeOutMs = reducerContext.getReduceTimeOutMs() - (System.currentTimeMillis() - start);\n+      countDownLatch.await(timeOutMs, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException e) {\n+      for (Future future : futures) {", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\nindex f09635446e..7cc423a7be 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n\n@@ -240,7 +256,8 @@ public class GroupByDataTableReducer implements DataTableReducer {\n   }\n \n   private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n-      DataTableReducerContext reducerContext) {\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n     long start = System.currentTimeMillis();\n     int numDataTables = dataTablesToReduce.size();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729127", "bodyText": "Don't use the executor service for single-threaded case. There is overhead of using that instead of the current thread, which might cause performance degradation.", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:36:14Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +239,130 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext) {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();\n+\n+    // Get the number of threads to use for reducing.\n+    int numReduceThreadsToUse = getNumReduceThreadsToUse(numDataTables, reducerContext.getMaxReduceThreadsPerQuery());\n+\n+    // In case of single reduce thread, fall back to SimpleIndexedTable to avoid redundant locking/unlocking calls.\n     int capacity = GroupByUtils.getTableCapacity(_queryContext);\n-    IndexedTable indexedTable = new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+    IndexedTable indexedTable =\n+        (numReduceThreadsToUse > 1) ? new ConcurrentIndexedTable(dataSchema, _queryContext, capacity)\n+            : new SimpleIndexedTable(dataSchema, _queryContext, capacity);\n+\n+    Future[] futures = new Future[numDataTables];", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY0MDYxNA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r503640614", "bodyText": "I really don't want to have two separate implementations (one for single thread and one for multi-thread). I did some benchmark with high qps use case, and the overhead is not measurable. Will keep it like this for now, until we find evidence that it hurts performance.", "author": "mayankshriv", "createdAt": "2020-10-13T03:06:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMwODQyOA==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504308428", "bodyText": "Did some perf benchmarking on high throughput use case. The overhead does not seem to register. Will leave it as-is for now.", "author": "mayankshriv", "createdAt": "2020-10-13T23:07:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTEyNw=="}], "type": "inlineReview", "revised_code": {"commit": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\nindex f09635446e..7cc423a7be 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java\n\n@@ -240,7 +256,8 @@ public class GroupByDataTableReducer implements DataTableReducer {\n   }\n \n   private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n-      DataTableReducerContext reducerContext) {\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n     long start = System.currentTimeMillis();\n     int numDataTables = dataTablesToReduce.size();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyOTUxOQ==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r502729519", "bodyText": "I feel the original formatting is better. You can skip the reformatting by adding //@formatter:off, see AggregationFunctionUtils.isFitForDictionaryBasedComputation() for details.", "author": "Jackie-Jiang", "createdAt": "2020-10-10T01:39:36Z", "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,13 +321,13 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n-    testDistinctInnerSegmentHelper(new String[]{\n-        \"SELECT DISTINCT(intColumn, longColumn, floatColumn, doubleColumn, stringColumn, bytesColumn) FROM testTable LIMIT 10000\",\n-        \"SELECT DISTINCT(stringColumn, bytesColumn, floatColumn) FROM testTable WHERE intColumn >= 60 LIMIT 10000\",\n-        \"SELECT DISTINCT(intColumn, bytesColumn) FROM testTable ORDER BY bytesColumn LIMIT 5\",\n-        \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\",\n-        \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"\n-    }, true);\n+    testDistinctInnerSegmentHelper(", "originalCommit": "c73604641d5d84656b0e980b95329dc3152c66f7", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "chunk": "diff --git a/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java b/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\nindex e403461104..9a8f14b0c5 100644\n--- a/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\n+++ b/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\n\n@@ -321,13 +321,14 @@ public class DistinctQueriesTest extends BaseQueriesTest {\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n-    testDistinctInnerSegmentHelper(\n-        new String[]{\"SELECT DISTINCT(intColumn, longColumn, floatColumn, doubleColumn, stringColumn, bytesColumn) FROM testTable LIMIT 10000\", //\n-            \"SELECT DISTINCT(stringColumn, bytesColumn, floatColumn) FROM testTable WHERE intColumn >= 60 LIMIT 10000\", //\n-            \"SELECT DISTINCT(intColumn, bytesColumn) FROM testTable ORDER BY bytesColumn LIMIT 5\", //\n-            \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\", //\n-            \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"},\n-        true);\n+    //@formatter:off\n+    testDistinctInnerSegmentHelper(new String[]{\n+        \"SELECT DISTINCT(intColumn, longColumn, floatColumn, doubleColumn, stringColumn, bytesColumn) FROM testTable LIMIT 10000\",\n+        \"SELECT DISTINCT(stringColumn, bytesColumn, floatColumn) FROM testTable WHERE intColumn >= 60 LIMIT 10000\",\n+        \"SELECT DISTINCT(intColumn, bytesColumn) FROM testTable ORDER BY bytesColumn LIMIT 5\",\n+        \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\",\n+        \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"\n+    }, true);\n   }\n \n   /**\n"}}, {"oid": "1ad015b3866ff92c1c0db6f139a4ab71035eace9", "url": "https://github.com/apache/pinot/commit/1ad015b3866ff92c1c0db6f139a4ab71035eace9", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-13T03:35:20Z", "type": "forcePushed"}, {"oid": "39626d41c7c909b7f1e9697b69119f8eedc366b7", "url": "https://github.com/apache/pinot/commit/39626d41c7c909b7f1e9697b69119f8eedc366b7", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-13T04:47:02Z", "type": "forcePushed"}, {"oid": "bb955b505c1d720158ec6642a2502b78b839e733", "url": "https://github.com/apache/pinot/commit/bb955b505c1d720158ec6642a2502b78b839e733", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-13T21:39:05Z", "type": "forcePushed"}, {"oid": "e20f784d0300eb26ab31391156e75bd3915da646", "url": "https://github.com/apache/pinot/commit/e20f784d0300eb26ab31391156e75bd3915da646", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-13T23:06:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzODY2Ng==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504338666", "bodyText": "You need to have another comment //@formatter:on to turn the formatter on after the queries", "author": "Jackie-Jiang", "createdAt": "2020-10-14T00:52:40Z", "path": "pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java", "diffHunk": "@@ -319,6 +321,7 @@ private void testDistinctInnerSegmentHelper(String[] queries, boolean isPql)\n   @Test\n   public void testDistinctInnerSegment()\n       throws Exception {\n+    //@formatter:off", "originalCommit": "e20f784d0300eb26ab31391156e75bd3915da646", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "283c8e35c959a2f969b3231c143f7bfbf22bd113", "chunk": "diff --git a/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java b/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\nindex 9a8f14b0c5..d3006ab110 100644\n--- a/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\n+++ b/pinot-core/src/test/java/org/apache/pinot/queries/DistinctQueriesTest.java\n\n@@ -329,6 +329,7 @@ public class DistinctQueriesTest extends BaseQueriesTest {\n         \"SELECT DISTINCT(ADD ( intColumn,  floatColumn  ), stringColumn) FROM testTable WHERE longColumn < 60 ORDER BY stringColumn DESC, ADD(intColumn, floatColumn) ASC LIMIT 10\",\n         \"SELECT DISTINCT(floatColumn, longColumn) FROM testTable WHERE stringColumn = 'a' ORDER BY longColumn LIMIT 10\"\n     }, true);\n+    //@formatter:on\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTMxNw==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504339317", "bodyText": "We can short circuit the single data table case by directly return the new SimpleIndexedTable(dataSchema, _queryContext, capacity)", "author": "Jackie-Jiang", "createdAt": "2020-10-14T00:55:16Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/query/reduce/GroupByDataTableReducer.java", "diffHunk": "@@ -231,58 +255,134 @@ private DataSchema getPrePostAggregationDataSchema(DataSchema dataSchema) {\n     return new DataSchema(columnNames, columnDataTypes);\n   }\n \n-  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTables) {\n+  private IndexedTable getIndexedTable(DataSchema dataSchema, Collection<DataTable> dataTablesToReduce,\n+      DataTableReducerContext reducerContext)\n+      throws TimeoutException {\n+    long start = System.currentTimeMillis();\n+    int numDataTables = dataTablesToReduce.size();", "originalCommit": "e20f784d0300eb26ab31391156e75bd3915da646", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2MTU4Mw==", "url": "https://github.com/apache/pinot/pull/6044#discussion_r504361583", "bodyText": "I did that initially. But then it requires indexTable.finish(). And in future anytime we have other such tasks that need to be done before returning, they will need to be done at two places. So I chose to avoid that.", "author": "mayankshriv", "createdAt": "2020-10-14T02:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzOTMxNw=="}], "type": "inlineReview", "revised_code": null}, {"oid": "283c8e35c959a2f969b3231c143f7bfbf22bd113", "url": "https://github.com/apache/pinot/commit/283c8e35c959a2f969b3231c143f7bfbf22bd113", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-14T02:19:22Z", "type": "commit"}, {"oid": "283c8e35c959a2f969b3231c143f7bfbf22bd113", "url": "https://github.com/apache/pinot/commit/283c8e35c959a2f969b3231c143f7bfbf22bd113", "message": "Support for multi-threaded Group By reducer for SQL.\n\nThe existing implementation of Broker reduce phase is single-threaded.\nFor group-by queries where large response are being sent back from multiple servers,\nthis could become a bottlenect.\n\nGiven that brokers are generally light on CPU usage, making the reduce phase\nmulti-threaded would be a good idea to boost performance. This PR adds a multi-threaded\nimplementation for the Group-By reducer for SQL.\n\n- Added an executor service in BrokerReduceService that can be used by multi-threaded\n  execution of the broker reduce phase. This is initialized with number of threads as:\n  `Runtime.getRuntime().availableProcessors()`.\n\n- Added a broker side config to specify max number of threads per query to be used for reduce phase.\n  `pinot.broker.max.reduce.threads.per.query`. This has a same default value as server side combine phase:\n  `Math.max(1, Math.min(10, Runtime.getRuntime().availableProcessors() / 2))`\n\n   For reverting to single threaded reduce, set this config to 1.\n\n- The GroupByDataTableReducer uses the following algorithm for determining number of\n  threads to use in reduce phase (per query):\n  - If there are less than 2 data tables to reduce, it uses single threaded run.\n  - Else, it uses `Math.min(pinot.broker.max.reduce.threads.per.query, numDataTables).\n\n- For testing, explicitly sets num threads to reduce to be > 1 to ensure functional\n  correctness is tested.", "committedDate": "2020-10-14T02:19:22Z", "type": "forcePushed"}]}