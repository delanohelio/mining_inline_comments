{"pr_number": 5774, "pr_title": "Config recommendation engine", "pr_createdAt": "2020-07-30T18:42:04Z", "pr_url": "https://github.com/apache/pinot/pull/5774", "timeline": [{"oid": "94edd3c8127a90cbb23e9ec260043bc263ecd2fd", "url": "https://github.com/apache/pinot/commit/94edd3c8127a90cbb23e9ec260043bc263ecd2fd", "message": "Rebase config recommender on master", "committedDate": "2020-08-04T21:16:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzY1NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465793655", "bodyText": "We should also use @consumes annotation right?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:02:36Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/api/resources/PinotTableRestletResource.java", "diffHunk": "@@ -141,6 +142,18 @@ public SuccessResponse addTable(String tableConfigStr) {\n     }\n   }\n \n+  @PUT\n+  @Produces(MediaType.APPLICATION_JSON)", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MDAwNg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465940006", "bodyText": "got it", "author": "jasperjiaguo", "createdAt": "2020-08-05T19:02:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzY1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDI2Mg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r467300262", "bodyText": "I stayed with only @Produces(MediaType.APPLICATION_JSON) to follow the pattern of other APIs", "author": "jasperjiaguo", "createdAt": "2020-08-07T21:56:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzY1NQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzczOA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465793738", "bodyText": "A concise javadoc would be helpful explaining the purpose of each class. No need to explain the algorithm, but bullet-list items explaining the responsibility of the class", "author": "siddharthteotia", "createdAt": "2020-08-05T15:02:42Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/RecommenderDriver.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.SerializationFeature;\n+import java.io.IOException;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+\n+public class RecommenderDriver {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDM0Nw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r467300347", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-07T21:56:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzczOA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/RecommenderDriver.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/RecommenderDriver.java\nindex e740015fbb..df6d2803ba 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/RecommenderDriver.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/RecommenderDriver.java\n\n@@ -25,7 +25,7 @@ import java.io.IOException;\n import java.lang.reflect.InvocationTargetException;\n import java.lang.reflect.Method;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.io.InputManager;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzgxNw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465793817", "bodyText": "IIUC, this is the output side right?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:02:49Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/ConfigManager.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import org.apache.pinot.controller.recommender.rules.io.FlaggedQueries;\n+import org.apache.pinot.controller.recommender.rules.io.configs.IndexConfig;\n+import org.apache.pinot.controller.recommender.rules.io.configs.PartitionConfig;\n+\n+\n+public class ConfigManager {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkyMTQ0NA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465921444", "bodyText": "Yes I will add java docs...", "author": "jasperjiaguo", "createdAt": "2020-08-05T18:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5MzgxNw=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/ConfigManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/ConfigManager.java\nindex 0ff141d6ab..4331cfdda0 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/ConfigManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/ConfigManager.java\n\n@@ -25,6 +25,15 @@ import org.apache.pinot.controller.recommender.rules.io.configs.IndexConfig;\n import org.apache.pinot.controller.recommender.rules.io.configs.PartitionConfig;\n \n \n+/**\n+ * Manager for the overwritten configs in the input and output\n+ * overwritten configs:\n+ * Devs/sres/advanced user want to use the rule engine to recommend configs.\n+ * However, based on their experience or due to a special optimization for a use case,\n+ * they know that it will help to have inverted index on a particular column.\n+ * But they still want to run the engine to recommend inverted indexes on other columns (if applicable) and recommend other configs (sorted, bloom etc).\n+ * The engine will do it's job of recommending by taking into account the overwritten config and honoring it.\n+ */\n public class ConfigManager {\n   IndexConfig _indexConfig = new IndexConfig();\n   PartitionConfig _partitionConfig = new PartitionConfig();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5Mzg3Ng==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465793876", "bodyText": "javadoc please", "author": "siddharthteotia", "createdAt": "2020-08-05T15:02:53Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzODYxMg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466638612", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:30:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5Mzg3Ng=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5Mzk2OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465793969", "bodyText": "consider naming it numMessagesPerSecInKafKaTopic and add a comment stating this is applicable to realtime/hybrid table", "author": "siddharthteotia", "createdAt": "2020-08-05T15:02:59Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MDMyMw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465940323", "bodyText": "got it", "author": "jasperjiaguo", "createdAt": "2020-08-05T19:03:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5Mzk2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDAxNQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794015", "bodyText": "I think we need to make the purpose of overwrittenConfigs more clear. IIUC, this works as follows:\nI as a user/dev wants to use the rule engine to recommend configs. However, based on my experience or due to a special optimization for a use case, I know that it will help to have inverted index on a particular column. But I still want to run the engine to recommend inverted indexes on other columns (if applicable) and recommend other configs (sorted, bloom etc). The engine will do it's job of recommending by taking into account the overwritten config and honoring it. In other words, the recommended config is going to be a super-set of the overwritten config. Is this understanding correct?\nWe should highlight the purpose clearly in comments", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:04Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume\n+  public long _numRecordsPerPush = DEFAULT_NUM_RECORDS_PER_PUSH; // records per push for offline part of a table\n+  public long _latencySLA = DEFAULT_LATENCY_SLA; // latency sla in ms\n+  public int _numKafkaPartitions = DEFAULT_NUM_KAFKA_PARTITIONS;\n+\n+  // The parameters of rules\n+  public PartitionRuleParams _partitionRuleParams = new PartitionRuleParams();\n+  public InvertedSortedIndexJointRuleParams _invertedSortedIndexJointRuleParams =\n+      new InvertedSortedIndexJointRuleParams();\n+  public BloomFilterRuleParams _bloomFilterRuleParams = new BloomFilterRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRuleParams _noDictionaryOnHeapDictionaryJointRuleParams =\n+      new NoDictionaryOnHeapDictionaryJointRuleParams();\n+  public FlagQueryRuleParams _flagQueryRuleParams = new FlagQueryRuleParams();\n+\n+  // For forward compatibility: 1. dev/sre to overwrite field(s) 2. incremental recommendation on existing/staging tables\n+  public ConfigManager _overWrittenConfigs = new ConfigManager();", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA5NjQyNA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466096424", "bodyText": "added comments", "author": "jasperjiaguo", "createdAt": "2020-08-06T01:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzODU0MA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466638540", "bodyText": "Added code comments", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:30:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDAxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDA1OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794058", "bodyText": "I don't think I fully understand why this should be ignored by the deserializer", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:07Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume\n+  public long _numRecordsPerPush = DEFAULT_NUM_RECORDS_PER_PUSH; // records per push for offline part of a table\n+  public long _latencySLA = DEFAULT_LATENCY_SLA; // latency sla in ms\n+  public int _numKafkaPartitions = DEFAULT_NUM_KAFKA_PARTITIONS;\n+\n+  // The parameters of rules\n+  public PartitionRuleParams _partitionRuleParams = new PartitionRuleParams();\n+  public InvertedSortedIndexJointRuleParams _invertedSortedIndexJointRuleParams =\n+      new InvertedSortedIndexJointRuleParams();\n+  public BloomFilterRuleParams _bloomFilterRuleParams = new BloomFilterRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRuleParams _noDictionaryOnHeapDictionaryJointRuleParams =\n+      new NoDictionaryOnHeapDictionaryJointRuleParams();\n+  public FlagQueryRuleParams _flagQueryRuleParams = new FlagQueryRuleParams();\n+\n+  // For forward compatibility: 1. dev/sre to overwrite field(s) 2. incremental recommendation on existing/staging tables\n+  public ConfigManager _overWrittenConfigs = new ConfigManager();\n+\n+  /******************************Ignored by deserializer****************************************/", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkyNjc2Ng==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465926766", "bodyText": "The fields following this line are pre-processed from the input and used as algorithm input only, so no need to serialize/deserialize them using jackson.", "author": "jasperjiaguo", "createdAt": "2020-08-05T18:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDA1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTQ2OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465941468", "bodyText": "Got it. Thanks.", "author": "siddharthteotia", "createdAt": "2020-08-05T19:05:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDA1OA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDEwNQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794105", "bodyText": "Please see my comment above w.r.t explaining the purpose and usage of overwritten configs.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:10Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume\n+  public long _numRecordsPerPush = DEFAULT_NUM_RECORDS_PER_PUSH; // records per push for offline part of a table\n+  public long _latencySLA = DEFAULT_LATENCY_SLA; // latency sla in ms\n+  public int _numKafkaPartitions = DEFAULT_NUM_KAFKA_PARTITIONS;\n+\n+  // The parameters of rules\n+  public PartitionRuleParams _partitionRuleParams = new PartitionRuleParams();\n+  public InvertedSortedIndexJointRuleParams _invertedSortedIndexJointRuleParams =\n+      new InvertedSortedIndexJointRuleParams();\n+  public BloomFilterRuleParams _bloomFilterRuleParams = new BloomFilterRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRuleParams _noDictionaryOnHeapDictionaryJointRuleParams =\n+      new NoDictionaryOnHeapDictionaryJointRuleParams();\n+  public FlagQueryRuleParams _flagQueryRuleParams = new FlagQueryRuleParams();\n+\n+  // For forward compatibility: 1. dev/sre to overwrite field(s) 2. incremental recommendation on existing/staging tables\n+  public ConfigManager _overWrittenConfigs = new ConfigManager();\n+\n+  /******************************Ignored by deserializer****************************************/\n+  public Map<String, ColumnMetaData> _metaDataMap = new HashMap<>(); // meta data per column, complement to schema\n+  long _sizePerRecord = 0;\n+  Map<String, FieldSpec.DataType> _colnameFieldTypeMap = new HashMap<>();\n+  Set<String> _dimNames = null;\n+  Set<String> _metricNames = null;\n+  Set<String> _dateTimeNames = null;\n+  Set<String> _dimNamesInveredSortedIndexApplicable = null;\n+  Map<String, Integer> _colNameToIntMap = null;\n+  String[] _intToColNameMap = null;\n+  Map<FieldSpec.DataType, Integer> _dataTypeSizeMap = new HashMap<FieldSpec.DataType, Integer>() {{\n+    put(FieldSpec.DataType.INT, DEFAULT_INT_SIZE);\n+    put(FieldSpec.DataType.LONG, DEFAULT_LONG_SIZE);\n+    put(FieldSpec.DataType.FLOAT, DEFAULT_FLOAT_SIZE);\n+    put(FieldSpec.DataType.DOUBLE, DEFAULT_DOUBLE_SIZE);\n+    put(FieldSpec.DataType.BYTES, DEFAULT_BYTE_SIZE);\n+    put(FieldSpec.DataType.STRING, DEFAULT_CHAR_SIZE);\n+    put(null, DEFAULT_NULL_SIZE);\n+  }};\n+\n+  /**\n+   * Process the dependencies incurred by overwritten configs.", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDE4OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794188", "bodyText": "Why did we move this class to the recommender? Is it not being used elsewhere?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:18Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/exceptions/InvalidInputException.java", "diffHunk": "@@ -16,14 +16,12 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.tools.tuner.query.src.stats.wrapper;\n+package org.apache.pinot.controller.recommender.io.exceptions;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkzMDAzMw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465930033", "bodyText": "The tuner package was the old code for index recommender. I did reuse some of the old code, so github shows I'm moving some files from the old package. But I'm not sure why this happens in InvalidInputException class as it is completely new. The old  index recommender is not used anywhere right now so I removed  the sources completely.", "author": "jasperjiaguo", "createdAt": "2020-08-05T18:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTU5Ng==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465941596", "bodyText": "Got it. Thanks", "author": "siddharthteotia", "createdAt": "2020-08-05T19:05:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDE4OA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/exceptions/InvalidInputException.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/exceptions/InvalidInputException.java\nsimilarity index 86%\nrename from pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/exceptions/InvalidInputException.java\nrename to pinot-controller/src/main/java/org/apache/pinot/controller/recommender/exceptions/InvalidInputException.java\nindex 2ec6745060..bd872cb0fd 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/exceptions/InvalidInputException.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/exceptions/InvalidInputException.java\n\n@@ -16,10 +16,11 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.controller.recommender.io.exceptions;\n-\n-\n+package org.apache.pinot.controller.recommender.exceptions;\n \n+/**\n+ * Reflects conflicts in input, e.g. we cannot have both no-dict and index on one dimension\n+ */\n public class InvalidInputException extends Exception {\n   public InvalidInputException(String pattern, Object ... arguments) {\n     super(java.text.MessageFormat.format(pattern,arguments));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDI0MA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794240", "bodyText": "We have an existing class ColumnMetadata in pinot-core. Although, this is in a different package so there shouldn't be any conflict. But, just to avoid any confusion (intellij will display both files as the user starts typing the name in file search), please consider renaming it.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:23Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/ColumnMetaData.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io.metadata;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnoreProperties;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import org.apache.pinot.spi.data.FieldSpec;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.DEFAULT_CARDINALITY;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.DEFAULT_DATA_LENGTH;\n+\n+\n+/**\n+ * The metadata of a column\n+ */\n+@JsonIgnoreProperties(ignoreUnknown = true)\n+public class ColumnMetaData extends FieldSpec {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTg2OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465941869", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T19:06:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDI0MA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/ColumnMetaData.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/FieldMetadata.java\nsimilarity index 93%\nrename from pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/ColumnMetaData.java\nrename to pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/FieldMetadata.java\nindex 111111bf95..f39ede5995 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/ColumnMetaData.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/metadata/FieldMetadata.java\n\n@@ -29,10 +29,10 @@ import static org.apache.pinot.controller.recommender.rules.io.params.Recommende\n \n \n /**\n- * The metadata of a column\n+ * The field information metadata piggybacked on a schema FieldSpec. For parsing the \"schema with metadata\" in the input.\n  */\n @JsonIgnoreProperties(ignoreUnknown = true)\n-public class ColumnMetaData extends FieldSpec {\n+public class FieldMetadata extends FieldSpec {\n   double _cardinality = DEFAULT_CARDINALITY;\n   int _averageLength = DEFAULT_DATA_LENGTH;\n   double _numValuesPerEntry = DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY; // for multi-values\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDMwNQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794305", "bodyText": "This should be a new class/interface for the recommender right? Why are we moving an existing class? or is this a github issue?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:28Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/AbstractRule.java", "diffHunk": "@@ -16,22 +16,19 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.pinot.tools.tuner.query.src.parser;\n+package org.apache.pinot.controller.recommender.rules;\n \n-import javax.annotation.Nullable;\n-import org.apache.pinot.tools.tuner.query.src.stats.wrapper.AbstractQueryStats;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n \n \n-/**\n- * Parser interface for a query line\n- */\n-public interface QueryParser {\n-  /**\n-   * parse the the complete log line to a parsed obj\n-   * @param line the complete log line to be parsed, InputIterator should put broken lines together\n-   * @return the parsed log line obj\n-   */\n-  @Nullable\n-  AbstractQueryStats parse(String line);\n-}\n+public abstract class AbstractRule {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkzMjAzMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465932031", "bodyText": "Please see the above conversation.", "author": "jasperjiaguo", "createdAt": "2020-08-05T18:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDMwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTk2Nw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465941967", "bodyText": "Got it. They came from tuner", "author": "siddharthteotia", "createdAt": "2020-08-05T19:06:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDMwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/AbstractRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/AbstractRule.java\nindex 786a19db4a..679cd739bc 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/AbstractRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/AbstractRule.java\n\n@@ -18,17 +18,21 @@\n  */\n package org.apache.pinot.controller.recommender.rules;\n \n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n \n \n+/**\n+ * Interface for a rule\n+ */\n public abstract class AbstractRule {\n-  protected InputManager _inputManager;\n-  protected ConfigManager _outputManager;\n-  public abstract void run();\n+  protected InputManager _input;\n+  protected ConfigManager _output;\n+  public abstract void run() throws InvalidInputException;\n \n-  public AbstractRule(InputManager inputManager, ConfigManager outputManager) {\n-    _inputManager = inputManager;\n-    _outputManager = outputManager;\n+  public AbstractRule(InputManager input, ConfigManager output) {\n+    _input = input;\n+    _output = output;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDM3OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794379", "bodyText": "Javadoc would be nice", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:33Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.impl.BloomFilterRule;\n+import org.apache.pinot.controller.recommender.rules.impl.FlagQueryRule;\n+import org.apache.pinot.controller.recommender.rules.impl.InvertedSortedIndexJointRule;\n+import org.apache.pinot.controller.recommender.rules.impl.KafkaPartitionRule;\n+import org.apache.pinot.controller.recommender.rules.impl.NoDictionaryOnHeapDictionaryJointRule;\n+import org.apache.pinot.controller.recommender.rules.impl.PinotTablePartitionRule;\n+import org.apache.pinot.controller.recommender.rules.impl.VariedLengthDictionaryRule;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.RulesToExecute.*;\n+\n+\n+public class RulesToExecute {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyNDkyNg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466024926", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:51:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDM3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\nindex 1c37b2d33f..bb8657d293 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\n\n@@ -33,9 +33,15 @@ import org.apache.pinot.controller.recommender.rules.impl.VariedLengthDictionary\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.RulesToExecute.*;\n \n \n+/**\n+ * In this class we will have\n+ * RuleFactory: factory for all rules, please add constructor for new rules in this factory wen extending\n+ * booleans to dictate whether a rule needs to be fired\n+ * Rule: a enum with all the rule names\n+ */\n public class RulesToExecute {\n   public static class RuleFactory {\n-    public static AbstractRule getRule(Rules rule, InputManager inputManager, ConfigManager outputManager) {\n+    public static AbstractRule getRule(Rule rule, InputManager inputManager, ConfigManager outputManager) {\n       switch (rule) {\n         case FlagQueryRule:\n           return new FlagQueryRule(inputManager, outputManager);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDQ5NA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794494", "bodyText": "The enum name shouldn't really have \"plural\". The enum although defines multiple constants, it represents only 1 of them at a give time. So, we should simply call it Rule?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:43Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.impl.BloomFilterRule;\n+import org.apache.pinot.controller.recommender.rules.impl.FlagQueryRule;\n+import org.apache.pinot.controller.recommender.rules.impl.InvertedSortedIndexJointRule;\n+import org.apache.pinot.controller.recommender.rules.impl.KafkaPartitionRule;\n+import org.apache.pinot.controller.recommender.rules.impl.NoDictionaryOnHeapDictionaryJointRule;\n+import org.apache.pinot.controller.recommender.rules.impl.PinotTablePartitionRule;\n+import org.apache.pinot.controller.recommender.rules.impl.VariedLengthDictionaryRule;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.RulesToExecute.*;\n+\n+\n+public class RulesToExecute {\n+  public static class RuleFactory {\n+    public static AbstractRule getRule(Rules rule, InputManager inputManager, ConfigManager outputManager) {\n+      switch (rule) {\n+        case FlagQueryRule:\n+          return new FlagQueryRule(inputManager, outputManager);\n+        case InvertedSortedIndexJointRule:\n+          return new InvertedSortedIndexJointRule(inputManager, outputManager);\n+        case KafkaPartitionRule:\n+          return new KafkaPartitionRule(inputManager, outputManager);\n+        case PinotTablePartitionRule:\n+          return new PinotTablePartitionRule(inputManager, outputManager);\n+        case BloomFilterRule:\n+          return new BloomFilterRule(inputManager, outputManager);\n+        case NoDictionaryOnHeapDictionaryJointRule:\n+          return new NoDictionaryOnHeapDictionaryJointRule(inputManager, outputManager);\n+        case VariedLengthDictionaryRule:\n+          return new VariedLengthDictionaryRule(inputManager, outputManager);\n+        default:\n+          return null;\n+      }\n+    }\n+  }\n+  // All rules will execute by default unless explicitly specifying \"recommendInvertedSortedIndexJoint\" = \"false\"\n+  boolean _recommendKafkaPartition = DEFAULT_RECOMMEND_KAFKA_PARTITION;\n+  boolean _recommendPinotTablePartition = DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION;\n+  boolean _recommendInvertedSortedIndexJoint = DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT;\n+  boolean _recommendBloomFilter = DEFAULT_RECOMMEND_BLOOM_FILTER;\n+  boolean _recommendNoDictionaryOnHeapDictionaryJoint = DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT;\n+  boolean _recommendVariedLengthDictionary = DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY;\n+  boolean _recommendFlagQuery = DEFAULT_RECOMMEND_FLAG_QUERY;\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendVariedLengthDictionary(boolean recommendVariedLengthDictionary) {\n+    _recommendVariedLengthDictionary = recommendVariedLengthDictionary;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendFlagQuery(boolean recommendFlagQuery) {\n+    _recommendFlagQuery = recommendFlagQuery;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendNoDictionaryOnHeapDictionaryJoint(boolean recommendNoDictionaryOnHeapDictionaryJoint) {\n+    _recommendNoDictionaryOnHeapDictionaryJoint = recommendNoDictionaryOnHeapDictionaryJoint;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendKafkaPartition(boolean recommendKafkaPartition) {\n+    _recommendKafkaPartition = recommendKafkaPartition;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendInvertedSortedIndexJoint(boolean recommendInvertedSortedIndexJoint) {\n+    _recommendInvertedSortedIndexJoint = recommendInvertedSortedIndexJoint;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendPinotTablePartition(boolean recommendPinotTablePartition) {\n+    _recommendPinotTablePartition = recommendPinotTablePartition;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRecommendBloomFilter(boolean recommendBloomFilter) {\n+    _recommendBloomFilter = recommendBloomFilter;\n+  }\n+\n+  public boolean isRecommendVariedLengthDictionary() {\n+    return _recommendVariedLengthDictionary;\n+  }\n+\n+  public boolean isRecommendFlagQuery() {\n+    return _recommendFlagQuery;\n+  }\n+\n+  public boolean isRecommendNoDictionaryOnHeapDictionaryJoint() {\n+    return _recommendNoDictionaryOnHeapDictionaryJoint;\n+  }\n+\n+  public boolean isRecommendKafkaPartition() {\n+    return _recommendKafkaPartition;\n+  }\n+\n+  public boolean isRecommendInvertedSortedIndexJoint() {\n+    return _recommendInvertedSortedIndexJoint;\n+  }\n+\n+  public boolean isRecommendPinotTablePartition() {\n+    return _recommendPinotTablePartition;\n+  }\n+\n+  public boolean isRecommendBloomFilter() {\n+    return _recommendBloomFilter;\n+  }\n+\n+  public enum Rules {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyNDg4NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466024885", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:51:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDQ5NA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\nindex 1c37b2d33f..bb8657d293 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/RulesToExecute.java\n\n@@ -33,9 +33,15 @@ import org.apache.pinot.controller.recommender.rules.impl.VariedLengthDictionary\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.RulesToExecute.*;\n \n \n+/**\n+ * In this class we will have\n+ * RuleFactory: factory for all rules, please add constructor for new rules in this factory wen extending\n+ * booleans to dictate whether a rule needs to be fired\n+ * Rule: a enum with all the rule names\n+ */\n public class RulesToExecute {\n   public static class RuleFactory {\n-    public static AbstractRule getRule(Rules rule, InputManager inputManager, ConfigManager outputManager) {\n+    public static AbstractRule getRule(Rule rule, InputManager inputManager, ConfigManager outputManager) {\n       switch (rule) {\n         case FlagQueryRule:\n           return new FlagQueryRule(inputManager, outputManager);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDUzOA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794538", "bodyText": "Please add javadoc and brief explanation of the rule's algorithm. We already have that in the design, so just englishize it here :)", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:47Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class  BloomFilterRule extends AbstractRule {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA5OTcwNA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466099704", "bodyText": "Done", "author": "jasperjiaguo", "createdAt": "2020-08-06T01:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzODI1NA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466638254", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:29:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDUzOA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\nindex a689e19141..ad165ccc5b 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\n\n@@ -19,46 +19,47 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n-import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n import org.apache.pinot.core.query.request.context.ExpressionContext;\n import org.apache.pinot.core.query.request.context.FilterContext;\n import org.apache.pinot.core.query.request.context.QueryContext;\n import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n-import org.apache.pinot.controller.recommender.io.ConfigManager;\n-import org.apache.pinot.controller.recommender.io.InputManager;\n-import org.apache.pinot.controller.recommender.rules.AbstractRule;\n-import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n \n+/**\n+ * Create bloomfilter for dimensions frequently used in EQ predicate\n+ *    The partitioned dimension should be frequently used in the \u201c=\u201d\n+ *    Skip the no dictionary columns\n+ */\n public class  BloomFilterRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(BloomFilterRule.class);\n   private final BloomFilterRuleParams _params;\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n \n-  public BloomFilterRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getBloomFilterRuleParams();\n+  public BloomFilterRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getBloomFilterRuleParams();\n   }\n \n   @Override\n   public void run() {\n-    int numDims = _inputManager.getNumDims();\n+    int numDims = _input.getNumDims();\n     double[] weights = new double[numDims];\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n     // For each query, find out the dimensions used in 'EQ'\n     // and accumulate the (weighted) frequencies\n-    _inputManager.getQueryWeightMap().forEach((query,weight) -> {\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n       totalWeight.addAndGet(weight);\n-      FixedLenBitset fixedLenBitset = parseQuery(query);\n+      FixedLenBitset fixedLenBitset = parseQuery(_input.getQueryContext(query));\n       LOGGER.debug(\"fixedLenBitset {}\", fixedLenBitset);\n       for (Integer i : fixedLenBitset.getOffsets()) {\n         weights[i] += weight;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDYwMA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794600", "bodyText": "Mark this is as a TODO since it can be easily spotted in the code if someone stumbles upon it in the future.\nTODO: once Pinot starts supporting bloom filter based pruning for IN, !=, NOT IN, we should enhance the algorithm of this rule.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:03:53Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class  BloomFilterRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(BloomFilterRule.class);\n+  private final BloomFilterRuleParams _params;\n+  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+\n+  public BloomFilterRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getBloomFilterRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    int numDims = _inputManager.getNumDims();\n+    double[] weights = new double[numDims];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    // For each query, find out the dimensions used in 'EQ'\n+    // and accumulate the (weighted) frequencies\n+    _inputManager.getQueryWeightMap().forEach((query,weight) -> {\n+      totalWeight.addAndGet(weight);\n+      FixedLenBitset fixedLenBitset = parseQuery(query);\n+      LOGGER.debug(\"fixedLenBitset {}\", fixedLenBitset);\n+      for (Integer i : fixedLenBitset.getOffsets()) {\n+        weights[i] += weight;\n+      }\n+    });\n+    LOGGER.debug(\"Weight: {}, Total {}\", weights, totalWeight);\n+\n+    for (int i = 0; i < numDims; i++) {\n+      String dimName = _inputManager.intToColName(i);\n+      if (((weights[i] / totalWeight.get()) > _params.THRESHOLD_MIN_PERCENT_EQ_BLOOMFILTER)\n+          //The partitioned dimension should be frequently > P used\n+          && (_inputManager.getCardinality(dimName)\n+          < _params.THRESHOLD_MAX_CARDINALITY_BLOOMFILTER)) { //The Cardinality < C (1 million for 1MB size)\n+        _outputManager.getIndexConfig().getBloomFilterColumns().add(dimName);\n+      }\n+    }\n+  }\n+\n+  public FixedLenBitset parseQuery(String queryString) {\n+    LOGGER.debug(\"Parsing query: {}\", queryString);\n+    if (queryString == null) {\n+      return FixedLenBitset.IMMUTABLE_EMPTY_SET;\n+    }\n+\n+    BrokerRequest brokerRequest;\n+    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n+    try {\n+      brokerRequest = parser.compileToBrokerRequest(queryString);\n+    } catch (SqlCompilationException e) {\n+      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n+      return FixedLenBitset.IMMUTABLE_EMPTY_SET;\n+    }\n+    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n+    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n+\n+    if (queryContext.getFilter() == null) {\n+      return FixedLenBitset.IMMUTABLE_EMPTY_SET;\n+    }\n+\n+    LOGGER.trace(\"Parsing Where Clause: {}\", queryContext.getFilter().toString());\n+    return parsePredicateList(queryContext.getFilter());\n+  }\n+\n+  /**\n+   * The partitioned dimension should used in the \u201c=\u201d \uff08IN, NOT IN, != are not using bloom filter in Pinot for now) filter.\n+   * @param filterContext filterContext", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3OTA3Mg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466079072", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T00:37:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDYwMA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\nindex a689e19141..ad165ccc5b 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/BloomFilterRule.java\n\n@@ -19,46 +19,47 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n-import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n import org.apache.pinot.core.query.request.context.ExpressionContext;\n import org.apache.pinot.core.query.request.context.FilterContext;\n import org.apache.pinot.core.query.request.context.QueryContext;\n import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n-import org.apache.pinot.controller.recommender.io.ConfigManager;\n-import org.apache.pinot.controller.recommender.io.InputManager;\n-import org.apache.pinot.controller.recommender.rules.AbstractRule;\n-import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n \n+/**\n+ * Create bloomfilter for dimensions frequently used in EQ predicate\n+ *    The partitioned dimension should be frequently used in the \u201c=\u201d\n+ *    Skip the no dictionary columns\n+ */\n public class  BloomFilterRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(BloomFilterRule.class);\n   private final BloomFilterRuleParams _params;\n   protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n \n-  public BloomFilterRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getBloomFilterRuleParams();\n+  public BloomFilterRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getBloomFilterRuleParams();\n   }\n \n   @Override\n   public void run() {\n-    int numDims = _inputManager.getNumDims();\n+    int numDims = _input.getNumDims();\n     double[] weights = new double[numDims];\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n     // For each query, find out the dimensions used in 'EQ'\n     // and accumulate the (weighted) frequencies\n-    _inputManager.getQueryWeightMap().forEach((query,weight) -> {\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n       totalWeight.addAndGet(weight);\n-      FixedLenBitset fixedLenBitset = parseQuery(query);\n+      FixedLenBitset fixedLenBitset = parseQuery(_input.getQueryContext(query));\n       LOGGER.debug(\"fixedLenBitset {}\", fixedLenBitset);\n       for (Integer i : fixedLenBitset.getOffsets()) {\n         weights[i] += weight;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDkwMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794901", "bodyText": "Please add javadoc and brief explanation of the algorithm (please do this for all rules)", "author": "siddharthteotia", "createdAt": "2020-08-05T15:04:19Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzODMyNw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466638327", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:30:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDkwMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDk1MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465794951", "bodyText": "Not sure if I follow this. If the column is in filter and group by, why do we have to consider the frequency?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:04:23Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n+  private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+  private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n+\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n+\n+    int numCols = _inputManager.getNumCols();\n+    double[] filterGroupByWeights = new double[numCols];\n+    double[] selectionWeights = new double[numCols];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    //**********No dictionary recommendation*******/\n+    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+\n+    //Exclude cols with index\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n+    // TODO: Remove this after range index is implemented for no-dictionary\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+    //Find out columns used in filter&groupby and selection and corresponding frequencies\n+    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n+      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n+    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n+    for (int i = 0; i < numCols; i++) {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MzYxNg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465943616", "bodyText": "Add a comment explaining the rationale", "author": "siddharthteotia", "createdAt": "2020-08-05T19:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDk1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3ODM5MA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466078390", "bodyText": "resolved", "author": "jasperjiaguo", "createdAt": "2020-08-06T00:34:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NDk1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NjYzMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465796631", "bodyText": "Might want to add comment about the experiment done during the design phase -- for a column heavily used in selection only (not part of filter or group by), making it no dictionary reduces the latency by 20% (I guess) since we avoid the 2 lookups.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:06:49Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n+  private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+  private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n+\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n+\n+    int numCols = _inputManager.getNumCols();\n+    double[] filterGroupByWeights = new double[numCols];\n+    double[] selectionWeights = new double[numCols];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    //**********No dictionary recommendation*******/\n+    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+\n+    //Exclude cols with index\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n+    // TODO: Remove this after range index is implemented for no-dictionary\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+    //Find out columns used in filter&groupby and selection and corresponding frequencies\n+    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n+      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n+    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n+    for (int i = 0; i < numCols; i++) {\n+      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n+      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n+        noDictCols.remove(_inputManager.intToColName(i));\n+      }\n+    }\n+\n+    LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n+        totalWeight);\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+\n+    for (int i = 0; i < numCols; i++) {\n+      // No dictionary on columns frequently used in selection", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0NzYyNA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465947624", "bodyText": "It looks like we can improve the performance of this loop starting at line 94 by going over the remaining columns", "author": "siddharthteotia", "createdAt": "2020-08-05T19:17:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NjYzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3ODMyMg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466078322", "bodyText": "added", "author": "jasperjiaguo", "createdAt": "2020-08-06T00:34:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5NjYzMQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5ODI3MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465798271", "bodyText": "General question - Would it be possible to parse the query exactly once before the execution of first rule begins? Right now, it seems like as the rules are fired in order, each rule will parse the input query set? Even though the algorithm of each rule is different, is it possible to parse once and extract all the common info needed by all the rules?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:09:12Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n+  private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+  private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n+\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n+\n+    int numCols = _inputManager.getNumCols();\n+    double[] filterGroupByWeights = new double[numCols];\n+    double[] selectionWeights = new double[numCols];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    //**********No dictionary recommendation*******/\n+    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+\n+    //Exclude cols with index\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n+    // TODO: Remove this after range index is implemented for no-dictionary\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+    //Find out columns used in filter&groupby and selection and corresponding frequencies\n+    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n+      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n+    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n+    for (int i = 0; i < numCols; i++) {\n+      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n+      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n+        noDictCols.remove(_inputManager.intToColName(i));\n+      }\n+    }\n+\n+    LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n+        totalWeight);\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+\n+    for (int i = 0; i < numCols; i++) {\n+      // No dictionary on columns frequently used in selection\n+      double selectionFreq = selectionWeights[i] / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n+        continue;\n+      }\n+\n+      // Add dictionary on columns NOT frequently used in selection\n+      // AND can save storage > threshold\n+      String colName = _inputManager.intToColName(i);\n+      double noDictSize;\n+      double withDictSize;\n+      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n+      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n+      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n+      long dictionarySize = _inputManager.getDictionarySize(colName);\n+      double cardinality = _inputManager.getCardinality(colName);\n+      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n+      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n+      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+      LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n+      LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n+\n+      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+        //TODO: improve this estimation\n+        noDictSize = // size of one segment flushed ith no dictionary\n+            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        withDictSize = // size of one flushed segment with dictionary\n+            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+      } else { // For hybrid or offline table, nodictionary follows the offline side\n+        noDictSize = // size of all segments in one push  with no dictionary\n+            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n+        withDictSize = // size of all segments in one push with dictionary\n+            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n+                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+      }\n+\n+      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n+          storageSaved);\n+\n+      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n+        noDictCols.remove(colName);\n+      }\n+    }\n+\n+    // Add the no dictionary cols to config\n+    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+\n+    //**********On heap dictionary recommendation*******/\n+    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n+        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+        {\n+          long dictionarySize = _inputManager.getDictionarySize(colName);\n+          int colId = _inputManager.colNameToInt(colName);\n+          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n+              && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n+            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n+    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n+        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n+  }\n+\n+  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzODExNg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466638116", "bodyText": "resolved - the code will parse only once", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:29:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc5ODI3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwMDI3OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465800278", "bodyText": "We can simplify this by not bringing numValuesPerEntry into the equation. numValuesPerEntry is applicable to MV columns right? Pinot currently doesn't support raw MV columns so they are always dictionary encoded", "author": "siddharthteotia", "createdAt": "2020-08-05T15:11:58Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n+  private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+  private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n+\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n+\n+    int numCols = _inputManager.getNumCols();\n+    double[] filterGroupByWeights = new double[numCols];\n+    double[] selectionWeights = new double[numCols];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    //**********No dictionary recommendation*******/\n+    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+\n+    //Exclude cols with index\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n+    // TODO: Remove this after range index is implemented for no-dictionary\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+    //Find out columns used in filter&groupby and selection and corresponding frequencies\n+    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n+      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n+    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n+    for (int i = 0; i < numCols; i++) {\n+      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n+      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n+        noDictCols.remove(_inputManager.intToColName(i));\n+      }\n+    }\n+\n+    LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n+        totalWeight);\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+\n+    for (int i = 0; i < numCols; i++) {\n+      // No dictionary on columns frequently used in selection\n+      double selectionFreq = selectionWeights[i] / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n+        continue;\n+      }\n+\n+      // Add dictionary on columns NOT frequently used in selection\n+      // AND can save storage > threshold\n+      String colName = _inputManager.intToColName(i);\n+      double noDictSize;\n+      double withDictSize;\n+      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n+      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n+      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n+      long dictionarySize = _inputManager.getDictionarySize(colName);\n+      double cardinality = _inputManager.getCardinality(colName);\n+      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n+      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n+      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+      LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n+      LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n+\n+      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+        //TODO: improve this estimation\n+        noDictSize = // size of one segment flushed ith no dictionary\n+            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwMDkxNQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465800915", "bodyText": "We should add a TODO though -- stating to enhance this to consider MV columns as noDictionary in the future when Pinot supports that", "author": "siddharthteotia", "createdAt": "2020-08-05T15:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwMDI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1ODI3MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466058271", "bodyText": "Done. Thanks for bring out this!", "author": "jasperjiaguo", "createdAt": "2020-08-05T23:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwMDI3OA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwNjI0Mw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465806243", "bodyText": "Why is DEFAULT_NUM_PARTITIONS used here?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:20:02Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java", "diffHunk": "@@ -0,0 +1,245 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashSet;\n+import java.util.Set;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static java.lang.Math.min;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n+  private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+  private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n+\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n+\n+    int numCols = _inputManager.getNumCols();\n+    double[] filterGroupByWeights = new double[numCols];\n+    double[] selectionWeights = new double[numCols];\n+    AtomicDouble totalWeight = new AtomicDouble(0);\n+\n+    //**********No dictionary recommendation*******/\n+    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+\n+    //Exclude cols with index\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n+    // TODO: Remove this after range index is implemented for no-dictionary\n+    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+    //Find out columns used in filter&groupby and selection and corresponding frequencies\n+    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n+      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n+    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n+    for (int i = 0; i < numCols; i++) {\n+      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n+      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n+        noDictCols.remove(_inputManager.intToColName(i));\n+      }\n+    }\n+\n+    LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n+        totalWeight);\n+    LOGGER.debug(\"noDictCols {}\", noDictCols);\n+\n+    for (int i = 0; i < numCols; i++) {\n+      // No dictionary on columns frequently used in selection\n+      double selectionFreq = selectionWeights[i] / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n+        continue;\n+      }\n+\n+      // Add dictionary on columns NOT frequently used in selection\n+      // AND can save storage > threshold\n+      String colName = _inputManager.intToColName(i);\n+      double noDictSize;\n+      double withDictSize;\n+      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n+      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n+      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n+      long dictionarySize = _inputManager.getDictionarySize(colName);\n+      double cardinality = _inputManager.getCardinality(colName);\n+      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n+      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n+      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+      LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n+      LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n+\n+      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+        //TODO: improve this estimation\n+        noDictSize = // size of one segment flushed ith no dictionary\n+            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        withDictSize = // size of one flushed segment with dictionary\n+            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+      } else { // For hybrid or offline table, nodictionary follows the offline side\n+        noDictSize = // size of all segments in one push  with no dictionary\n+            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n+        withDictSize = // size of all segments in one push with dictionary\n+            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\nindex 4ad9b28c7a..ae78c7dd55 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/NoDictionaryOnHeapDictionaryJointRule.java\n\n@@ -19,172 +19,168 @@\n package org.apache.pinot.controller.recommender.rules.impl;\n \n import com.google.common.util.concurrent.AtomicDouble;\n+import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static java.lang.Math.max;\n-import static java.lang.Math.min;\n-import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.REALTIME;\n \n \n+/**\n+ * Recommend no dictionary columns and on heap dictionary columns\n+ * Name of the column(s) not to create a dictionary on:\n+ *    EXCLUDE the columns we will create bitmap/sorted indices on.\n+ *    If a column (fixed width or variable width) is used in filter and/or group by, create a dictionary.\n+ *    If a column is not used in filter and group by: there are two cases:\n+ *      If the column is used heavily in selection, then don't create a dictionary\n+ *      If the column is not used in selection, then create a dictionary only if by creating a dictionary we can save > p% of storage\n+ *\n+ * Name of the column(s) with dictionary on heap\n+ *    We want the table\u2019s QPS > Q\n+ *    The memory footprint should be < M (configurable)\n+ *    The column is frequently > F queried in filter/group by\n+ */\n public class NoDictionaryOnHeapDictionaryJointRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(NoDictionaryOnHeapDictionaryJointRule.class);\n   private final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n   private final NoDictionaryOnHeapDictionaryJointRuleParams _params;\n \n-  public NoDictionaryOnHeapDictionaryJointRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    _params = inputManager.getNoDictionaryOnHeapDictionaryJointRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    _params = input.getNoDictionaryOnHeapDictionaryJointRuleParams();\n   }\n \n   @Override\n   public void run() {\n     LOGGER.info(\"Recommending no dictionary and on-heap dictionaries\");\n \n-    int numCols = _inputManager.getNumCols();\n-    double[] filterGroupByWeights = new double[numCols];\n-    double[] selectionWeights = new double[numCols];\n+    int numCols = _input.getNumCols();\n+    Map<String, Double> filterGroupByWeights = new HashMap<>();\n+    Map<String, Double> selectionWeights = new HashMap<>();\n     AtomicDouble totalWeight = new AtomicDouble(0);\n \n+    //****Find out columns used in filter&groupby and selection and corresponding frequencies*****/\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight = _input.getQueryWeight(query);\n+      parseQuery(_input.getQueryContext(query), weight, filterGroupByWeights, selectionWeights);\n+      totalWeight.addAndGet(weight);\n+    });\n+\n     //**********No dictionary recommendation*******/\n-    Set<String> noDictCols = new HashSet<>(_inputManager.getColNameToIntMap().keySet());\n+    Set<String> noDictCols = new HashSet<>(_input.getColNameToIntMap().keySet());\n \n-    //Exclude cols with index\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getInvertedIndexColumns());\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getSortedColumn());\n-    // TODO: Remove this after range index is implemented for no-dictionary\n-    noDictCols.removeAll(_outputManager.getIndexConfig().getRangeIndexColumns());\n+    // Exclude cols already having index\n+    noDictCols.removeAll(_output.getIndexConfig().getInvertedIndexColumns());\n+    noDictCols.removeAll(_output.getIndexConfig().getSortedColumn());\n+    noDictCols.removeAll(_output.getIndexConfig()\n+        .getRangeIndexColumns()); // TODO: Remove this after range index is implemented for no-dictionary\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n-    //Find out columns used in filter&groupby and selection and corresponding frequencies\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      parseQuery(query, weight, filterGroupByWeights, selectionWeights);\n-      totalWeight.addAndGet(weight);\n-    });\n \n-    //Add dictionary on columns used in filter&groupby , with frequency > threshold\n-    for (int i = 0; i < numCols; i++) {\n-      double filterGroupByFreq = filterGroupByWeights[i] / totalWeight.get();\n-      if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY) {\n-        noDictCols.remove(_inputManager.intToColName(i));\n-      }\n-    }\n+    // Exclude MV cols TODO: currently no index column is only applicable for SV columns, change this after it's supported for MV\n+    noDictCols.removeIf(colName -> !_input.isSingleValueColumn(colName));\n+\n+    // Exclude columns used in filter&groupby, with frequency > threshold\n+    // Our study shows: [With a dictionary setup, at segment level the server\n+    // can early return an empty result if the value specified in the query\n+    // does not match with any value in the dictionary]\n+    noDictCols.removeIf(colName -> {\n+      double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      return filterGroupByFreq\n+          > _params.THRESHOLD_MIN_FILTER_FREQ_DICTIONARY; // THRESHOLD_MIN_FILTER_FREQ_DICTIONARY is default to 0\n+    });\n \n     LOGGER.debug(\"filterGroupByWeights {}, selectionWeights{}, totalWeight{} \", filterGroupByWeights, selectionWeights,\n         totalWeight);\n     LOGGER.debug(\"noDictCols {}\", noDictCols);\n \n-    for (int i = 0; i < numCols; i++) {\n-      // No dictionary on columns frequently used in selection\n-      double selectionFreq = selectionWeights[i] / totalWeight.get();\n-      if (selectionFreq > _params.THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY) {\n-        continue;\n+    noDictCols.removeIf(colName -> {\n+      // For columns frequently used in selection, use no dictionary\n+      // Our study shows: [for a column heavily used in selection only (not part of filter or group by)\n+      // making it no dictionary reduces the latency by ~25%]\n+      double selectionFreq = selectionWeights.getOrDefault(colName, 0d) / totalWeight.get();\n+      if (selectionFreq > _params.THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY) {\n+        return false;\n       }\n \n-      // Add dictionary on columns NOT frequently used in selection\n-      // AND can save storage > threshold\n-      String colName = _inputManager.intToColName(i);\n+      // For columns NOT frequently used in selection\n+      // Add dictionary only if doing so can save us > THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE of space\n       double noDictSize;\n       double withDictSize;\n-      long colDataSizeWithoutDictionary = _inputManager.getColDataSizeWithoutDictionary(colName);\n-      double numValuesPerEntry = _inputManager.getNumValuesPerEntry(colName);\n-      int bitCompressedDataSize = _inputManager.getBitCompressedDataSize(colName);\n-      long dictionarySize = _inputManager.getDictionarySize(colName);\n-      double cardinality = _inputManager.getCardinality(colName);\n-      long numRecordsPerPush = _inputManager.getNumRecordsPerPush();\n-      LOGGER.debug(\"colDataSizeWithoutDictionary {}\", colDataSizeWithoutDictionary);\n-      LOGGER.debug(\"bitCompressedDataSize {}\", bitCompressedDataSize);\n+\n+      long svColRawSizePerDoc = 0;\n+      try {\n+        svColRawSizePerDoc = _input.getColRawSizePerDoc(colName);\n+      } catch (InvalidInputException e) {\n+        return true; // If this column is a MV column, it cannot be used as noDict column\n+      }\n+\n+      double numValuesPerEntry = _input.getNumValuesPerEntry(colName);\n+      int dictionaryEncodedForwardIndexSize = _input.getDictionaryEncodedForwardIndexSize(colName);\n+      long dictionarySize = _input.getDictionarySize(colName);\n+      long numRecordsPerPush;\n+      LOGGER.debug(\"svColRawSizePerDoc {}\", svColRawSizePerDoc);\n+      LOGGER.debug(\"dictionaryEncodedForwardIndexSize {}\", dictionaryEncodedForwardIndexSize);\n       LOGGER.debug(\"dictionarySize {}\", dictionarySize);\n       LOGGER.debug(\"numValuesPerEntry {}\", numValuesPerEntry);\n \n-      if (_inputManager.getTableType().equalsIgnoreCase(REALTIME)) {\n+      if (_input.getTableType().equalsIgnoreCase(REALTIME)) {\n         //TODO: improve this estimation\n-        noDictSize = // size of one segment flushed ith no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n-        withDictSize = // size of one flushed segment with dictionary\n-            dictionarySize + bitCompressedDataSize * numValuesPerEntry * _params.SEGMENT_FLUSH_TIME;\n+        numRecordsPerPush = _input._numMessagesPerSecInKafKaTopic * _params.SEGMENT_FLUSH_TIME;\n       } else { // For hybrid or offline table, nodictionary follows the offline side\n-        noDictSize = // size of all segments in one push  with no dictionary\n-            colDataSizeWithoutDictionary * numValuesPerEntry * numRecordsPerPush;\n-        withDictSize = // size of all segments in one push with dictionary\n-            dictionarySize * dictionaryCoefficient(cardinality, numRecordsPerPush) * DEFAUlT_NUM_PARTITIONS\n-                + bitCompressedDataSize * numValuesPerEntry * numRecordsPerPush;\n+        numRecordsPerPush = _input._numRecordsPerPush;\n       }\n \n-      double storageSaved = (double) (noDictSize - withDictSize) / noDictSize;\n+      noDictSize = numRecordsPerPush * svColRawSizePerDoc;\n+      withDictSize = numRecordsPerPush * dictionaryEncodedForwardIndexSize + svColRawSizePerDoc * dictionarySize * _params.DICTIONARY_COEFFICIENT;\n+\n+      double storageSaved = (noDictSize - withDictSize) / noDictSize;\n       LOGGER.debug(\"colName {}, noDictSize {}, withDictSize{}, storageSaved{}\", colName, noDictSize, withDictSize,\n           storageSaved);\n \n-      if (storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-        noDictCols.remove(colName);\n-      }\n-    }\n+      return storageSaved > _params.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+    });\n \n-    // Add the no dictionary cols to config\n-    _outputManager.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n+    // Add the no dictionary cols to output config\n+    _output.getIndexConfig().getNoDictionaryColumns().addAll(noDictCols);\n \n     //**********On heap dictionary recommendation*******/\n-    if (_inputManager.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n-      for (String colName : _inputManager.getColNameToIntMap().keySet()) {\n-        if (!_outputManager.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n+    if (_input.getQps() > _params.THRESHOLD_MIN_QPS_ON_HEAP) { // QPS > THRESHOLD_MIN_QPS_ON_HEAP\n+      for (String colName : _input.getColNameToIntMap().keySet()) {\n+        if (!_output.getIndexConfig().getNoDictionaryColumns().contains(colName)) //exclude no dictionary column\n         {\n-          long dictionarySize = _inputManager.getDictionarySize(colName);\n-          int colId = _inputManager.colNameToInt(colName);\n-          double filterGroupByFreq = filterGroupByWeights[colId] / totalWeight.get();\n+          long dictionarySize = _input.getDictionarySize(colName);\n+          double filterGroupByFreq = filterGroupByWeights.getOrDefault(colName, 0d) / totalWeight.get();\n           if (filterGroupByFreq > _params.THRESHOLD_MIN_FILTER_FREQ_ON_HEAP  //frequently used in filter/group by\n               && dictionarySize < _params.THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP) { // memory foot print < threshold\n-            _outputManager.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n+            _output.getIndexConfig().getOnHeapDictionaryColumns().add(colName);\n           }\n         }\n       }\n     }\n   }\n \n-  private double dictionaryCoefficient(double cardinality, long numRecordsPerPush) {\n-    return 1 - min(max(DEFAULT_DICT_COEFF_A * Math.log(DEFAULT_DICT_COEFF_B * cardinality / numRecordsPerPush),\n-        DEFAULT_DICT_LOWER), DEFAULT_DICT_UPPER);\n-  }\n-\n-  public void parseQuery(String queryString, double weight, double[] filterGroupByWeights, double[] selectionWeights) {\n-    LOGGER.trace(\"Parsing query: {}\", queryString);\n-    if (queryString == null) {\n-      return;\n-    }\n-\n-    BrokerRequest brokerRequest;\n-    AbstractCompiler parser = PinotQueryParserFactory.get(_inputManager.getQueryType());\n-    try {\n-      brokerRequest = parser.compileToBrokerRequest(queryString);\n-    } catch (SqlCompilationException e) {\n-      LOGGER.error(\"Error parsing query: {}, {}\", queryString, e.getMessage());\n-      return;\n-    }\n-    BrokerRequest optimizedRequest = _brokerRequestOptimizer.optimize(brokerRequest, _inputManager.getTimeCol());\n-    QueryContext queryContext = BrokerRequestToQueryContextConverter.convert(optimizedRequest);\n-\n+  public void parseQuery(QueryContext queryContext, double weight, Map<String, Double> filterGroupByWeights,\n+      Map<String, Double> selectionWeights) {\n     if (queryContext.getSelectExpressions() != null) {\n       queryContext.getSelectExpressions().forEach(selectExpression -> {\n         Set<String> colNames = new HashSet<>();\n         selectExpression.getColumns(colNames);\n         colNames.forEach(colName -> {\n-          selectionWeights[_inputManager.colNameToInt(colName)] += weight;\n+          selectionWeights.merge(colName, weight, Double::sum);\n         });\n       });\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwNzQxMw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465807413", "bodyText": "you can probably remove the \"*\". It just adds minor overhead to the logger", "author": "siddharthteotia", "createdAt": "2020-08-05T15:21:39Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Optional;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+public class PinotTablePartitionRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n+  PartitionRuleParams _params;\n+\n+  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+\n+  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    this._params = inputManager.getPartitionRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    //**********Calculate size per record***************/\n+    _inputManager.estimateSizePerRecord();\n+    //**************************************************/\n+\n+    LOGGER.info(\"Recommending partition configurations\");\n+\n+    if (_inputManager.getQps()\n+        < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+          _params.THRESHOLD_MIN_QPS_PARTITION);\n+      return;\n+    }\n+    if (_inputManager.getLatencySLA()\n+        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n+          _params.THRESHOLD_MAX_SLA_PARTITION);\n+      return;\n+    }\n+\n+    LOGGER.info(\"*Recommending partition number\");", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwOTk1Ng==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465809956", "bodyText": "(nit): suggest changing it to \"Recommending number of partitions\"", "author": "siddharthteotia", "createdAt": "2020-08-05T15:25:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwNzQxMw=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\nindex 3c40e801cd..4000b4a7cc 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n\n@@ -23,88 +23,114 @@ import java.util.Comparator;\n import java.util.List;\n import java.util.Optional;\n import org.apache.commons.lang3.tuple.Pair;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n \n \n+/**\n+ * Recommend pinot number table partitions on realtime and/or offline side of the table\n+ * Recommend column to partition on:\n+ *  For a table whose QPS < Q (say 200 or 300) NO partitioning is needed\n+ *  For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+ *  The partitioned dimension D should appear frequently in EQ or IN, according to the PartitionSegmentPruner::isPartitionMatch():\n+ *  Should have sufficient cardinality to guarantee balanced partitioning\n+ * Number of partitions:\n+ *  Realtime:\n+ *    num of partitions for real time should be the same value as the number of kafka partitions\n+ *  Offline:\n+ *    **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+ *    Offline partition num is dependent on the amount of data coming in on a given day.\n+ *    Should give a reasonable partition value that makes the resulting partition size optimal\n+ *  Hybrid:\n+ *    The minimum of Realtime and Offline\n+ */\n public class PinotTablePartitionRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n   PartitionRuleParams _params;\n \n-  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-\n-  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    this._params = inputManager.getPartitionRuleParams();\n+  public PinotTablePartitionRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    this._params = input.getPartitionRuleParams();\n   }\n \n   @Override\n-  public void run() {\n+  public void run()\n+      throws InvalidInputException {\n     //**********Calculate size per record***************/\n-    _inputManager.estimateSizePerRecord();\n+    _input.estimateSizePerRecord();\n     //**************************************************/\n \n     LOGGER.info(\"Recommending partition configurations\");\n \n-    if (_inputManager.getQps()\n+    if (_input.getQps()\n         < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n-      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _input.getQps(),\n           _params.THRESHOLD_MIN_QPS_PARTITION);\n       return;\n     }\n-    if (_inputManager.getLatencySLA()\n-        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n-      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n-          _params.THRESHOLD_MAX_SLA_PARTITION);\n+    if (_input.getLatencySLA()\n+        > _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _input.getLatencySLA(),\n+          _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION);\n       return;\n     }\n \n-    LOGGER.info(\"*Recommending partition number\");\n-    if (_inputManager.getTableType().equalsIgnoreCase(\n-        REALTIME)) { //real time partition num should be the same value as the number of kafka partitions\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-    } else if (_inputManager.getTableType().equalsIgnoreCase(OFFLINE)) {\n+\n+\n+    /*For realtime/hybrid, the number of partitions on realtime Pinot table side is same as number of kafka partitions.\n+    This is generally the case unless there is a reason for them to be different. We saw only one outlier. So generally on\n+     the realtime side the number of partitions should match with the kafka partitions */\n+\n+    /*The number of partitions on offline table side is dependent on the amount of data.\n+     **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+    For hybrid table, we have seen cases where this value = number of kafka partitions = number of realtime table partitions.\n+    For hybrid table, we have also seen cases, where the value for offline is lower than realtime since the data\n+    generated on a given day is low volume and using a high count of number of partitions would lead to too many small\n+    sized segments since we typically have data from one partition in a segment. So we can conclude on the offline side, if the\n+    data amount is sufficient, the number of partitions will match with the realtime side. Otherwise the partition size will shrink\n+    so that data_size_per_push/num_offline_partitions >=  OPTIMAL_SIZE_PER_SEGMENT.*/\n+\n+    LOGGER.info(\"*Recommending number of partitions \");\n+    int numKafkaPartitions = _output.getPartitionConfig().getNumKafkaPartitions();\n+    long offLineDataSizePerPush = _input.getNumRecordsPerPush() * _input.getSizePerRecord();\n+    int optimalOfflinePartitions = (int)Math.ceil((double) offLineDataSizePerPush / _params.OPTIMAL_SIZE_PER_SEGMENT);\n+    if (_input.getTableType().equalsIgnoreCase(REALTIME) || _input.getTableType()\n+        .equalsIgnoreCase(HYBRID)) {\n+      //real time num of partitions should be the same value as the number of kafka partitions\n+      _output.getPartitionConfig().setNumPartitionsRealtime(numKafkaPartitions);\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(OFFLINE)) {\n       //Offline partition num is dependent on the amount of data coming in on a given day.\n       //Using a very high value of numPartitions for small dataset size will result in too many small sized segments.\n       //We define have a desirable segment size OPTIMAL_SIZE_PER_SEGMENT\n       //Divide the size of data coming in on a given day by OPTIMAL_SIZE_PER_SEGMENT we get the number of partitions.\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT));\n-    } else {\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-      //real time partition num should be same as the number of kafka partitions\n-\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline(\n-          Math.min((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT),\n-              _outputManager.getPartitionConfig().getNumKafkaPartitions()));\n+      _output.getPartitionConfig()\n+          .setNumPartitionsOffline((int) (optimalOfflinePartitions));\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(HYBRID)) {\n+      _output.getPartitionConfig().setNumPartitionsOffline(\n+          Math.min(optimalOfflinePartitions, numKafkaPartitions));\n     }\n \n     LOGGER.info(\"*Recommending column to partition\");\n \n-    double[] weights = new double[_inputManager.getNumDims()];\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      FixedLenBitset fixedLenBitset = parseQuery(query);\n+    double[] weights = new double[_input.getNumDims()];\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight =_input.getQueryWeight(query);\n+      FixedLenBitset fixedLenBitset = parseQuery(_input.getQueryContext(query));\n       LOGGER.debug(\"fixedLenBitset:{}\", fixedLenBitset);\n       if (fixedLenBitset != null) {\n         for (Integer i : fixedLenBitset.getOffsets()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwOTE5NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465809195", "bodyText": "We can actually simplify the code here. Currently we are writing the code for offline and realtime.\nWe can compute once for offline (if type is OFFLINE or HYBRID)\nWe can compute once for realtime (if type if REALTIME or HYBRID)\nThis will clenup the if-else block here", "author": "siddharthteotia", "createdAt": "2020-08-05T15:24:11Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Optional;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+public class PinotTablePartitionRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n+  PartitionRuleParams _params;\n+\n+  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+\n+  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    this._params = inputManager.getPartitionRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    //**********Calculate size per record***************/\n+    _inputManager.estimateSizePerRecord();\n+    //**************************************************/\n+\n+    LOGGER.info(\"Recommending partition configurations\");\n+\n+    if (_inputManager.getQps()\n+        < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+          _params.THRESHOLD_MIN_QPS_PARTITION);\n+      return;\n+    }\n+    if (_inputManager.getLatencySLA()\n+        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n+          _params.THRESHOLD_MAX_SLA_PARTITION);\n+      return;\n+    }\n+\n+    LOGGER.info(\"*Recommending partition number\");\n+    if (_inputManager.getTableType().equalsIgnoreCase(\n+        REALTIME)) { //real time partition num should be the same value as the number of kafka partitions", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA3NjA3MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466076071", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T00:26:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgwOTE5NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\nindex 3c40e801cd..4000b4a7cc 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n\n@@ -23,88 +23,114 @@ import java.util.Comparator;\n import java.util.List;\n import java.util.Optional;\n import org.apache.commons.lang3.tuple.Pair;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n \n \n+/**\n+ * Recommend pinot number table partitions on realtime and/or offline side of the table\n+ * Recommend column to partition on:\n+ *  For a table whose QPS < Q (say 200 or 300) NO partitioning is needed\n+ *  For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+ *  The partitioned dimension D should appear frequently in EQ or IN, according to the PartitionSegmentPruner::isPartitionMatch():\n+ *  Should have sufficient cardinality to guarantee balanced partitioning\n+ * Number of partitions:\n+ *  Realtime:\n+ *    num of partitions for real time should be the same value as the number of kafka partitions\n+ *  Offline:\n+ *    **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+ *    Offline partition num is dependent on the amount of data coming in on a given day.\n+ *    Should give a reasonable partition value that makes the resulting partition size optimal\n+ *  Hybrid:\n+ *    The minimum of Realtime and Offline\n+ */\n public class PinotTablePartitionRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n   PartitionRuleParams _params;\n \n-  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-\n-  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    this._params = inputManager.getPartitionRuleParams();\n+  public PinotTablePartitionRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    this._params = input.getPartitionRuleParams();\n   }\n \n   @Override\n-  public void run() {\n+  public void run()\n+      throws InvalidInputException {\n     //**********Calculate size per record***************/\n-    _inputManager.estimateSizePerRecord();\n+    _input.estimateSizePerRecord();\n     //**************************************************/\n \n     LOGGER.info(\"Recommending partition configurations\");\n \n-    if (_inputManager.getQps()\n+    if (_input.getQps()\n         < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n-      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _input.getQps(),\n           _params.THRESHOLD_MIN_QPS_PARTITION);\n       return;\n     }\n-    if (_inputManager.getLatencySLA()\n-        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n-      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n-          _params.THRESHOLD_MAX_SLA_PARTITION);\n+    if (_input.getLatencySLA()\n+        > _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _input.getLatencySLA(),\n+          _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION);\n       return;\n     }\n \n-    LOGGER.info(\"*Recommending partition number\");\n-    if (_inputManager.getTableType().equalsIgnoreCase(\n-        REALTIME)) { //real time partition num should be the same value as the number of kafka partitions\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-    } else if (_inputManager.getTableType().equalsIgnoreCase(OFFLINE)) {\n+\n+\n+    /*For realtime/hybrid, the number of partitions on realtime Pinot table side is same as number of kafka partitions.\n+    This is generally the case unless there is a reason for them to be different. We saw only one outlier. So generally on\n+     the realtime side the number of partitions should match with the kafka partitions */\n+\n+    /*The number of partitions on offline table side is dependent on the amount of data.\n+     **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+    For hybrid table, we have seen cases where this value = number of kafka partitions = number of realtime table partitions.\n+    For hybrid table, we have also seen cases, where the value for offline is lower than realtime since the data\n+    generated on a given day is low volume and using a high count of number of partitions would lead to too many small\n+    sized segments since we typically have data from one partition in a segment. So we can conclude on the offline side, if the\n+    data amount is sufficient, the number of partitions will match with the realtime side. Otherwise the partition size will shrink\n+    so that data_size_per_push/num_offline_partitions >=  OPTIMAL_SIZE_PER_SEGMENT.*/\n+\n+    LOGGER.info(\"*Recommending number of partitions \");\n+    int numKafkaPartitions = _output.getPartitionConfig().getNumKafkaPartitions();\n+    long offLineDataSizePerPush = _input.getNumRecordsPerPush() * _input.getSizePerRecord();\n+    int optimalOfflinePartitions = (int)Math.ceil((double) offLineDataSizePerPush / _params.OPTIMAL_SIZE_PER_SEGMENT);\n+    if (_input.getTableType().equalsIgnoreCase(REALTIME) || _input.getTableType()\n+        .equalsIgnoreCase(HYBRID)) {\n+      //real time num of partitions should be the same value as the number of kafka partitions\n+      _output.getPartitionConfig().setNumPartitionsRealtime(numKafkaPartitions);\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(OFFLINE)) {\n       //Offline partition num is dependent on the amount of data coming in on a given day.\n       //Using a very high value of numPartitions for small dataset size will result in too many small sized segments.\n       //We define have a desirable segment size OPTIMAL_SIZE_PER_SEGMENT\n       //Divide the size of data coming in on a given day by OPTIMAL_SIZE_PER_SEGMENT we get the number of partitions.\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT));\n-    } else {\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-      //real time partition num should be same as the number of kafka partitions\n-\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline(\n-          Math.min((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT),\n-              _outputManager.getPartitionConfig().getNumKafkaPartitions()));\n+      _output.getPartitionConfig()\n+          .setNumPartitionsOffline((int) (optimalOfflinePartitions));\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(HYBRID)) {\n+      _output.getPartitionConfig().setNumPartitionsOffline(\n+          Math.min(optimalOfflinePartitions, numKafkaPartitions));\n     }\n \n     LOGGER.info(\"*Recommending column to partition\");\n \n-    double[] weights = new double[_inputManager.getNumDims()];\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      FixedLenBitset fixedLenBitset = parseQuery(query);\n+    double[] weights = new double[_input.getNumDims()];\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight =_input.getQueryWeight(query);\n+      FixedLenBitset fixedLenBitset = parseQuery(_input.getQueryContext(query));\n       LOGGER.debug(\"fixedLenBitset:{}\", fixedLenBitset);\n       if (fixedLenBitset != null) {\n         for (Integer i : fixedLenBitset.getOffsets()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTEwOQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465811109", "bodyText": "I think our check-style will complain for using * import. We should import specifically", "author": "siddharthteotia", "createdAt": "2020-08-05T15:26:54Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.PartitionRule.*;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjczMDkzMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466730931", "bodyText": "mvn checkstyle:check success so we should be fine", "author": "jasperjiaguo", "createdAt": "2020-08-06T23:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTEwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\nindex 2c85d28e90..df261c2c49 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\n\n@@ -23,16 +23,33 @@ import com.fasterxml.jackson.annotation.Nulls;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.PartitionRule.*;\n \n-\n+/**\n+ * Thresholds and parameters used in PartitionRule\n+ */\n public class PartitionRuleParams {\n-  public Long THRESHOLD_MAX_SLA_PARTITION = DEFAULT_THRESHOLD_MAX_SLA_PARTITION;\n+  // Above this latency SLA we do not need any partitioning\n+  public Long THRESHOLD_MAX_LATENCY_SLA_PARTITION = DEFAULT_THRESHOLD_MAX_LATENCY_SLA_PARTITION;\n+\n+  // Below this QPS we do not need any partitioning\n   public Long THRESHOLD_MIN_QPS_PARTITION = DEFAULT_THRESHOLD_MIN_QPS_PARTITION;\n+\n+  // The optimal size for an offline segment\n   public Long OPTIMAL_SIZE_PER_SEGMENT = DEFAULT_OPTIMAL_SIZE_PER_SEGMENT;\n+\n+  // In the over all recommendation for partitioning, iff the frequency of top N-th candidate is larger than\n+  // THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES * frequency_of_top_one_candidate,\n+  // we will pick from [1st, nth] candidates with the largest cardinality as partitioning column\n   public Double THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES =\n       DEFAULT_THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES;\n-  public Long KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION;\n+\n+  // For IN predicate with literals larger than this we will not recommend partitioning on that\n+  // column because the query will fan out to  a large number of partitions,\n+  // making partitioning pointless\n   public Integer THRESHOLD_MAX_IN_LENGTH = DEFAULT_THRESHOLD_MAX_IN_LENGTH;\n \n+  // The desirable msgs/sec for each kafka partition\n+  public Long KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION;\n+\n   public Integer getTHRESHOLD_MAX_IN_LENGTH() {\n     return THRESHOLD_MAX_IN_LENGTH;\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTgyOQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465811829", "bodyText": "Please add javadoc and a short one-line comment explaining the purpose of each configuration. Please try to do this for all param classes.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:27:58Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.PartitionRule.*;\n+\n+\n+public class PartitionRuleParams {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzNzgyMA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466637820", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:29:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTgyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\nindex 2c85d28e90..df261c2c49 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/PartitionRuleParams.java\n\n@@ -23,16 +23,33 @@ import com.fasterxml.jackson.annotation.Nulls;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.PartitionRule.*;\n \n-\n+/**\n+ * Thresholds and parameters used in PartitionRule\n+ */\n public class PartitionRuleParams {\n-  public Long THRESHOLD_MAX_SLA_PARTITION = DEFAULT_THRESHOLD_MAX_SLA_PARTITION;\n+  // Above this latency SLA we do not need any partitioning\n+  public Long THRESHOLD_MAX_LATENCY_SLA_PARTITION = DEFAULT_THRESHOLD_MAX_LATENCY_SLA_PARTITION;\n+\n+  // Below this QPS we do not need any partitioning\n   public Long THRESHOLD_MIN_QPS_PARTITION = DEFAULT_THRESHOLD_MIN_QPS_PARTITION;\n+\n+  // The optimal size for an offline segment\n   public Long OPTIMAL_SIZE_PER_SEGMENT = DEFAULT_OPTIMAL_SIZE_PER_SEGMENT;\n+\n+  // In the over all recommendation for partitioning, iff the frequency of top N-th candidate is larger than\n+  // THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES * frequency_of_top_one_candidate,\n+  // we will pick from [1st, nth] candidates with the largest cardinality as partitioning column\n   public Double THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES =\n       DEFAULT_THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES;\n-  public Long KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION;\n+\n+  // For IN predicate with literals larger than this we will not recommend partitioning on that\n+  // column because the query will fan out to  a large number of partitions,\n+  // making partitioning pointless\n   public Integer THRESHOLD_MAX_IN_LENGTH = DEFAULT_THRESHOLD_MAX_IN_LENGTH;\n \n+  // The desirable msgs/sec for each kafka partition\n+  public Long KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION;\n+\n   public Integer getTHRESHOLD_MAX_IN_LENGTH() {\n     return THRESHOLD_MAX_IN_LENGTH;\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTkwOQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465811909", "bodyText": "Please add javadoc", "author": "siddharthteotia", "createdAt": "2020-08-05T15:28:06Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYzNzc1OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466637759", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-06T19:29:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTkwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTk0NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465811945", "bodyText": "a very short one line comment above each configuration (at least for the ones that are not self explanatory and intuitive from the name)", "author": "siddharthteotia", "createdAt": "2020-08-05T15:28:09Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxODU0Nw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466018547", "bodyText": "Will add the comments in the *Params class for parameters", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:37:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxMTk0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxNzYwNA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465817604", "bodyText": "typo in name?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:36:31Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/NoDictionaryOnHeapDictionaryJointRuleParams.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n+\n+\n+public class NoDictionaryOnHeapDictionaryJointRuleParams {\n+  public Double THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNzQ3NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466017475", "bodyText": "fixed", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:34:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxNzYwNA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/NoDictionaryOnHeapDictionaryJointRuleParams.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/NoDictionaryOnHeapDictionaryJointRuleParams.java\nindex 36187b7b04..0f7d340bc7 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/NoDictionaryOnHeapDictionaryJointRuleParams.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/NoDictionaryOnHeapDictionaryJointRuleParams.java\n\n@@ -23,23 +23,57 @@ import com.fasterxml.jackson.annotation.Nulls;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.NoDictionaryOnHeapDictionaryJointRule.*;\n \n-\n+/**\n+ * Thresholds and parameters used in NoDictionaryOnHeapDictionaryJointRule\n+ */\n public class NoDictionaryOnHeapDictionaryJointRuleParams {\n-  public Double THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE;\n+\n+\n+  // We won't consider on heap dictionaries if table QPS < this threshold\n   public Long THRESHOLD_MIN_QPS_ON_HEAP = DEFAULT_THRESHOLD_MIN_QPS_ON_HEAP;\n+\n+  // We won't consider on heap dictionaries the frequency of this column used in filter < this threshold\n   public Double THRESHOLD_MIN_FILTER_FREQ_ON_HEAP = DEFAULT_THRESHOLD_MIN_FILTER_FREQ_ON_HEAP;\n+\n+  // The maximum acceptable memory footprint on heap\n   public Long THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP = DEFAULT_THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP;\n-  public Double THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY = DEFAULT_THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY;\n+\n+  // For columns used in selection, if frequency >this threshold, we will apply no dictionary on it\n+  public Double THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY = DEFAULT_THRESHOLD_MIN_SELECTION_FREQ_NO_DICTIONARY;\n+\n+  // For cols frequently used in filter or groupby, we will add dictionary on that, now default to 0,\n+  // meaning all cols ever used in filter or groupby will have dictionary\n   public Double THRESHOLD_MIN_FILTER_FREQ_DICTIONARY = DEFAULT_THRESHOLD_MIN_FILTER_FREQ_DICTIONARY;\n+\n+  // The default time threshold after which consumed kafka messages will be packed to segments\n+  // (consuming segments -> online segments)\n   public Integer SEGMENT_FLUSH_TIME = DEFAULT_SEGMENT_FLUSH_TIME;\n \n-  public Double getTHRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE() {\n-    return THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE;\n+  // The accumulated size of dictionaries of all segments in one push is generally smaller than the whole big dictionary size\n+  // (due to that the cardinality we have is the cardianlity for the whole dataset not per segment)\n+  // Use factor to shrink the size\n+  // TODO: improve this estimation if possible\n+  public Double DICTIONARY_COEFFICIENT = DEFAULT_DICTIONARY_COEFFICIENT;\n+\n+  // For colums not used in filter and selection, apply on heap dictionary only if it can save storage % > this threshold\n+  public Double THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE = DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n+\n+  public Double getDICTIONARY_COEFFICIENT() {\n+    return DICTIONARY_COEFFICIENT;\n+  }\n+\n+  @JsonSetter(value = \"DICTIONARY_COEFFICIENT\", nulls = Nulls.SKIP)\n+  public void setDICTIONARY_COEFFICIENT(Double DICTIONARY_COEFFICIENT) {\n+    this.DICTIONARY_COEFFICIENT = DICTIONARY_COEFFICIENT;\n+  }\n+\n+  public Double getTHRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE() {\n+    return THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n   }\n \n-  @JsonSetter(value = \"THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE\", nulls = Nulls.SKIP)\n-  public void setTHRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE(Double THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE) {\n-    this.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE;\n+  @JsonSetter(value = \"THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE\", nulls = Nulls.SKIP)\n+  public void setTHRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE(Double THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE) {\n+    this.THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE = THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SAVE;\n   }\n \n   public Integer getSEGMENT_FLUSH_TIME() {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxODYzMw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465818633", "bodyText": "include latency in the name", "author": "siddharthteotia", "createdAt": "2020-08-05T15:37:59Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_TEXT_MATCH = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_RANGE = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_REGEX = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_ISNULL = 0.5d;\n+    public static final double DEFAULT_THRESHOLD_MIN_AND_PREDICATE_INCREMENTAL_VOTE = 0.6d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_AND_PREDICATE_TOP_CANDIDATES = 0.8d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_GAIN_DIFF_BETWEEN_ITERATION = 0.05d;\n+    public static final int DEFAULT_MAX_NUM_ITERATION_WITHOUT_GAIN = 3;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_NESI_FOR_TOP_CANDIDATES = 0.7d;\n+  }\n+\n+  public static class RulesToExecute {\n+    public static final boolean DEFAULT_RECOMMEND_FLAG_QUERY = true;\n+    public static final boolean DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY = true;\n+    public static final boolean DEFAULT_RECOMMEND_KAFKA_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT = true;\n+    public static final boolean DEFAULT_RECOMMEND_BLOOM_FILTER = true;\n+    public static final boolean DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT = true;\n+  }\n+\n+  public static class PartitionRule {\n+    public static final int DEFAULT_NUM_PARTITIONS = 0;\n+\n+    public static final long DEFAULT_THRESHOLD_MAX_SLA_PARTITION = 1000;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNzE3OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466017179", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:34:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgxODYzMw=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMDU1Mg==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465820552", "bodyText": "I don't think we should accept input if QPS, latency, num messages per second in kafka topic and number of records pushed per day are not specified. It is something user (or whoever is getting the recommendation) needs to know. So, we should not have any defaults for these 4.", "author": "siddharthteotia", "createdAt": "2020-08-05T15:40:45Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_TEXT_MATCH = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_RANGE = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_REGEX = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_ISNULL = 0.5d;\n+    public static final double DEFAULT_THRESHOLD_MIN_AND_PREDICATE_INCREMENTAL_VOTE = 0.6d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_AND_PREDICATE_TOP_CANDIDATES = 0.8d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_GAIN_DIFF_BETWEEN_ITERATION = 0.05d;\n+    public static final int DEFAULT_MAX_NUM_ITERATION_WITHOUT_GAIN = 3;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_NESI_FOR_TOP_CANDIDATES = 0.7d;\n+  }\n+\n+  public static class RulesToExecute {\n+    public static final boolean DEFAULT_RECOMMEND_FLAG_QUERY = true;\n+    public static final boolean DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY = true;\n+    public static final boolean DEFAULT_RECOMMEND_KAFKA_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT = true;\n+    public static final boolean DEFAULT_RECOMMEND_BLOOM_FILTER = true;\n+    public static final boolean DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT = true;\n+  }\n+\n+  public static class PartitionRule {\n+    public static final int DEFAULT_NUM_PARTITIONS = 0;\n+\n+    public static final long DEFAULT_THRESHOLD_MAX_SLA_PARTITION = 1000;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_PARTITION = 200;\n+    public static final long DEFAULT_OPTIMAL_SIZE_PER_SEGMENT = 2000_000_000; //2GB\n+    public static final long DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = 250;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES = 0.8d;\n+    public static final int DEFAULT_THRESHOLD_MAX_IN_LENGTH = 4;\n+  }\n+\n+  public static class BloomFilterRule {\n+    public static final long DEFAULT_THRESHOLD_MAX_CARDINALITY_BLOOMFILTER = 1000_000;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_EQ_BLOOMFILTER = 0.5d;\n+  }\n+\n+  public static class NoDictionaryOnHeapDictionaryJointRule {\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_DICTIONARY = 0d;\n+    public static final double DEFAULT_THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY = 0.3d;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_ON_HEAP = 10_000;\n+    public static final long DEFAULT_THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP = 1000_000L;\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_ON_HEAP = 0.3d;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = 0.95;\n+\n+    public static final double DEFAULT_DICT_COEFF_A = 0.217769;\n+    public static final double DEFAULT_DICT_COEFF_B = 89.0975;\n+    public static final double DEFAULT_DICT_LOWER = 0;\n+    public static final double DEFAULT_DICT_UPPER = 0;\n+    public static final int DEFAUlT_NUM_PARTITIONS = 16;\n+\n+    public static final int DEFAULT_SEGMENT_FLUSH_TIME = 86400;\n+  }\n+\n+  public static class FlagQueryRuleParams{\n+    public static final long DEFAULT_THRESHOLD_MAX_LIMIT_SIZE = 100000;\n+    public static final String WARNING_NO_FILTERING = \"Warning: No filtering in ths query\";\n+    public static final String WARNING_NO_TIME_COL = \"Warning: No time column used in ths query\";\n+    public static final String WARNING_TOO_LONG_LIMIT = \"Warning: The size of LIMIT is longer than \" + DEFAULT_THRESHOLD_MAX_LIMIT_SIZE;\n+    public static final String ERROR_INVALID_QUERY = \"Error: query not able to parse, skipped\";\n+  }\n+\n+  public static final String PQL = \"pql\";\n+  public static final String SQL = \"sql\";\n+  public static final String OFFLINE = \"offline\";\n+  public static final String REALTIME = \"realtime\";\n+  public static final String HYBRID = \"hybrid\";\n+  public static final int NO_SUCH_COL = -1;\n+  public static final double DEFAULT_CARDINALITY = 1;\n+  public static final double MIN_CARDINALITY = 1;\n+  public static final double DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY = 1d;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMDA5OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466030098", "bodyText": "done, I think on Dino side he can probably make these fields \"required\"", "author": "jasperjiaguo", "createdAt": "2020-08-05T22:03:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMDU1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMjA5Mw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465822093", "bodyText": "I don't think you need to have DEFAULT_FLOAT/INT/LONG etc size. These are fixed width columns and will always be same as specified in their respective classes. -- Float.BYTES, Long.BYTES etc", "author": "siddharthteotia", "createdAt": "2020-08-05T15:42:55Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_TEXT_MATCH = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_RANGE = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_REGEX = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_ISNULL = 0.5d;\n+    public static final double DEFAULT_THRESHOLD_MIN_AND_PREDICATE_INCREMENTAL_VOTE = 0.6d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_AND_PREDICATE_TOP_CANDIDATES = 0.8d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_GAIN_DIFF_BETWEEN_ITERATION = 0.05d;\n+    public static final int DEFAULT_MAX_NUM_ITERATION_WITHOUT_GAIN = 3;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_NESI_FOR_TOP_CANDIDATES = 0.7d;\n+  }\n+\n+  public static class RulesToExecute {\n+    public static final boolean DEFAULT_RECOMMEND_FLAG_QUERY = true;\n+    public static final boolean DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY = true;\n+    public static final boolean DEFAULT_RECOMMEND_KAFKA_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT = true;\n+    public static final boolean DEFAULT_RECOMMEND_BLOOM_FILTER = true;\n+    public static final boolean DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT = true;\n+  }\n+\n+  public static class PartitionRule {\n+    public static final int DEFAULT_NUM_PARTITIONS = 0;\n+\n+    public static final long DEFAULT_THRESHOLD_MAX_SLA_PARTITION = 1000;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_PARTITION = 200;\n+    public static final long DEFAULT_OPTIMAL_SIZE_PER_SEGMENT = 2000_000_000; //2GB\n+    public static final long DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = 250;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES = 0.8d;\n+    public static final int DEFAULT_THRESHOLD_MAX_IN_LENGTH = 4;\n+  }\n+\n+  public static class BloomFilterRule {\n+    public static final long DEFAULT_THRESHOLD_MAX_CARDINALITY_BLOOMFILTER = 1000_000;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_EQ_BLOOMFILTER = 0.5d;\n+  }\n+\n+  public static class NoDictionaryOnHeapDictionaryJointRule {\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_DICTIONARY = 0d;\n+    public static final double DEFAULT_THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY = 0.3d;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_ON_HEAP = 10_000;\n+    public static final long DEFAULT_THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP = 1000_000L;\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_ON_HEAP = 0.3d;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = 0.95;\n+\n+    public static final double DEFAULT_DICT_COEFF_A = 0.217769;\n+    public static final double DEFAULT_DICT_COEFF_B = 89.0975;\n+    public static final double DEFAULT_DICT_LOWER = 0;\n+    public static final double DEFAULT_DICT_UPPER = 0;\n+    public static final int DEFAUlT_NUM_PARTITIONS = 16;\n+\n+    public static final int DEFAULT_SEGMENT_FLUSH_TIME = 86400;\n+  }\n+\n+  public static class FlagQueryRuleParams{\n+    public static final long DEFAULT_THRESHOLD_MAX_LIMIT_SIZE = 100000;\n+    public static final String WARNING_NO_FILTERING = \"Warning: No filtering in ths query\";\n+    public static final String WARNING_NO_TIME_COL = \"Warning: No time column used in ths query\";\n+    public static final String WARNING_TOO_LONG_LIMIT = \"Warning: The size of LIMIT is longer than \" + DEFAULT_THRESHOLD_MAX_LIMIT_SIZE;\n+    public static final String ERROR_INVALID_QUERY = \"Error: query not able to parse, skipped\";\n+  }\n+\n+  public static final String PQL = \"pql\";\n+  public static final String SQL = \"sql\";\n+  public static final String OFFLINE = \"offline\";\n+  public static final String REALTIME = \"realtime\";\n+  public static final String HYBRID = \"hybrid\";\n+  public static final int NO_SUCH_COL = -1;\n+  public static final double DEFAULT_CARDINALITY = 1;\n+  public static final double MIN_CARDINALITY = 1;\n+  public static final double DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY = 1d;\n+  public static final int DEFAULT_QPS = 100;\n+  public static final int DEFAULT_LATENCY_SLA = 1000;\n+  public static final int DEFAULT_NUM_MSG_PER_SEC = 250;\n+  public static final int DEFAULT_NUM_RECORDS_PER_PUSH = 10000;\n+  public static final int DEFAULT_INT_SIZE = Integer.BYTES;\n+  public static final int DEFAULT_NULL_SIZE = 0;", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNTc4NQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466015785", "bodyText": "Done", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:30:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMjA5Mw=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMzE3MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465823171", "bodyText": "How are these coefficients used?", "author": "siddharthteotia", "createdAt": "2020-08-05T15:44:36Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_TEXT_MATCH = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_RANGE = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_REGEX = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_ISNULL = 0.5d;\n+    public static final double DEFAULT_THRESHOLD_MIN_AND_PREDICATE_INCREMENTAL_VOTE = 0.6d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_AND_PREDICATE_TOP_CANDIDATES = 0.8d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_GAIN_DIFF_BETWEEN_ITERATION = 0.05d;\n+    public static final int DEFAULT_MAX_NUM_ITERATION_WITHOUT_GAIN = 3;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_NESI_FOR_TOP_CANDIDATES = 0.7d;\n+  }\n+\n+  public static class RulesToExecute {\n+    public static final boolean DEFAULT_RECOMMEND_FLAG_QUERY = true;\n+    public static final boolean DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY = true;\n+    public static final boolean DEFAULT_RECOMMEND_KAFKA_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT = true;\n+    public static final boolean DEFAULT_RECOMMEND_BLOOM_FILTER = true;\n+    public static final boolean DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT = true;\n+  }\n+\n+  public static class PartitionRule {\n+    public static final int DEFAULT_NUM_PARTITIONS = 0;\n+\n+    public static final long DEFAULT_THRESHOLD_MAX_SLA_PARTITION = 1000;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_PARTITION = 200;\n+    public static final long DEFAULT_OPTIMAL_SIZE_PER_SEGMENT = 2000_000_000; //2GB\n+    public static final long DEFAULT_KAFKA_NUM_MESSAGES_PER_SEC_PER_PARTITION = 250;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_DIMENSION_PARTITION_TOP_CANDIDATES = 0.8d;\n+    public static final int DEFAULT_THRESHOLD_MAX_IN_LENGTH = 4;\n+  }\n+\n+  public static class BloomFilterRule {\n+    public static final long DEFAULT_THRESHOLD_MAX_CARDINALITY_BLOOMFILTER = 1000_000;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_EQ_BLOOMFILTER = 0.5d;\n+  }\n+\n+  public static class NoDictionaryOnHeapDictionaryJointRule {\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_DICTIONARY = 0d;\n+    public static final double DEFAULT_THRESHOLD_MAX_SELECTION_FREQ_DICTIONARY = 0.3d;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_ON_HEAP = 10_000;\n+    public static final long DEFAULT_THRESHOLD_MAX_DICTIONARY_SIZE_ON_HEAP = 1000_000L;\n+    public static final double DEFAULT_THRESHOLD_MIN_FILTER_FREQ_ON_HEAP = 0.3d;\n+    public static final double DEFAULT_THRESHOLD_MIN_PERCENT_DICTIONARY_STORAGE_SVAE = 0.95;\n+", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxMjc4Mw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466012783", "bodyText": "Will revisit this later today", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgyMzE3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgzNDk1MQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465834951", "bodyText": "It will also be very useful for reference to quote the study and analysis we had done for use case at Li. Please don't quote the table names. However, we should include the following general comments:\n\nFor realtime/hybrid, the number of partitions on realtime Pinot table side is same as number of kafka partitions. This is generally the case unless there is a reason for them to be different. We saw one outlier\nFor offline, the number of partitions on offline Pinot table side is dependent on the amount of data. For hybrid table, we have seen cases where this value = number of kafka partitions  = number of realtime table partitions. For hybrid table, we have also seen cases, where the value for offline is lower than realtime since the data generated on a given day is low volume and using a high count of number of partitions would lead to too many small sized segments since we typically have data from one partition in a segment.", "author": "siddharthteotia", "createdAt": "2020-08-05T16:01:52Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.impl;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Optional;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n+import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n+import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n+import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n+import org.apache.pinot.parsers.AbstractCompiler;\n+import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.io.ConfigManager;\n+import org.apache.pinot.controller.recommender.io.InputManager;\n+import org.apache.pinot.controller.recommender.rules.AbstractRule;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+public class PinotTablePartitionRule extends AbstractRule {\n+  private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n+  PartitionRuleParams _params;\n+\n+  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n+\n+  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n+    super(inputManager, outputManager);\n+    this._params = inputManager.getPartitionRuleParams();\n+  }\n+\n+  @Override\n+  public void run() {\n+    //**********Calculate size per record***************/\n+    _inputManager.estimateSizePerRecord();\n+    //**************************************************/\n+\n+    LOGGER.info(\"Recommending partition configurations\");\n+\n+    if (_inputManager.getQps()\n+        < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+          _params.THRESHOLD_MIN_QPS_PARTITION);\n+      return;\n+    }\n+    if (_inputManager.getLatencySLA()\n+        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n+          _params.THRESHOLD_MAX_SLA_PARTITION);\n+      return;\n+    }\n+\n+    LOGGER.info(\"*Recommending partition number\");\n+    if (_inputManager.getTableType().equalsIgnoreCase(\n+        REALTIME)) { //real time partition num should be the same value as the number of kafka partitions\n+      _outputManager.getPartitionConfig()\n+          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n+    } else if (_inputManager.getTableType().equalsIgnoreCase(OFFLINE)) {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxMjMxMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466012311", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:23:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgzNDk1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\nindex 3c40e801cd..4000b4a7cc 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/impl/PinotTablePartitionRule.java\n\n@@ -23,88 +23,114 @@ import java.util.Comparator;\n import java.util.List;\n import java.util.Optional;\n import org.apache.commons.lang3.tuple.Pair;\n-import org.apache.pinot.common.request.BrokerRequest;\n-import org.apache.pinot.core.query.request.context.ExpressionContext;\n-import org.apache.pinot.core.query.request.context.FilterContext;\n-import org.apache.pinot.core.query.request.context.QueryContext;\n-import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n-import org.apache.pinot.core.query.request.context.predicate.Predicate;\n-import org.apache.pinot.core.query.request.context.utils.BrokerRequestToQueryContextConverter;\n-import org.apache.pinot.core.requesthandler.BrokerRequestOptimizer;\n-import org.apache.pinot.core.requesthandler.PinotQueryParserFactory;\n-import org.apache.pinot.parsers.AbstractCompiler;\n-import org.apache.pinot.sql.parsers.SqlCompilationException;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n import org.apache.pinot.controller.recommender.io.ConfigManager;\n import org.apache.pinot.controller.recommender.io.InputManager;\n import org.apache.pinot.controller.recommender.rules.AbstractRule;\n import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.core.query.request.context.ExpressionContext;\n+import org.apache.pinot.core.query.request.context.FilterContext;\n+import org.apache.pinot.core.query.request.context.QueryContext;\n+import org.apache.pinot.core.query.request.context.predicate.InPredicate;\n+import org.apache.pinot.core.query.request.context.predicate.Predicate;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n \n \n+/**\n+ * Recommend pinot number table partitions on realtime and/or offline side of the table\n+ * Recommend column to partition on:\n+ *  For a table whose QPS < Q (say 200 or 300) NO partitioning is needed\n+ *  For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+ *  The partitioned dimension D should appear frequently in EQ or IN, according to the PartitionSegmentPruner::isPartitionMatch():\n+ *  Should have sufficient cardinality to guarantee balanced partitioning\n+ * Number of partitions:\n+ *  Realtime:\n+ *    num of partitions for real time should be the same value as the number of kafka partitions\n+ *  Offline:\n+ *    **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+ *    Offline partition num is dependent on the amount of data coming in on a given day.\n+ *    Should give a reasonable partition value that makes the resulting partition size optimal\n+ *  Hybrid:\n+ *    The minimum of Realtime and Offline\n+ */\n public class PinotTablePartitionRule extends AbstractRule {\n   private final Logger LOGGER = LoggerFactory.getLogger(PinotTablePartitionRule.class);\n   PartitionRuleParams _params;\n \n-  protected final BrokerRequestOptimizer _brokerRequestOptimizer = new BrokerRequestOptimizer();\n-\n-  public PinotTablePartitionRule(InputManager inputManager, ConfigManager outputManager) {\n-    super(inputManager, outputManager);\n-    this._params = inputManager.getPartitionRuleParams();\n+  public PinotTablePartitionRule(InputManager input, ConfigManager output) {\n+    super(input, output);\n+    this._params = input.getPartitionRuleParams();\n   }\n \n   @Override\n-  public void run() {\n+  public void run()\n+      throws InvalidInputException {\n     //**********Calculate size per record***************/\n-    _inputManager.estimateSizePerRecord();\n+    _input.estimateSizePerRecord();\n     //**************************************************/\n \n     LOGGER.info(\"Recommending partition configurations\");\n \n-    if (_inputManager.getQps()\n+    if (_input.getQps()\n         < _params.THRESHOLD_MIN_QPS_PARTITION) { //For a table whose QPS < Q (say 200 or 300) NO partitioning is needed.\n-      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _inputManager.getQps(),\n+      LOGGER.info(\"*Input QPS {} < threshold {}, no partition needed\", _input.getQps(),\n           _params.THRESHOLD_MIN_QPS_PARTITION);\n       return;\n     }\n-    if (_inputManager.getLatencySLA()\n-        > _params.THRESHOLD_MAX_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n-      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _inputManager.getLatencySLA(),\n-          _params.THRESHOLD_MAX_SLA_PARTITION);\n+    if (_input.getLatencySLA()\n+        > _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION) { //For a table whose latency SLA > L (say 1000ms) NO partitioning is needed.\n+      LOGGER.info(\"*Input SLA {} > threshold {}, no partition needed\", _input.getLatencySLA(),\n+          _params.THRESHOLD_MAX_LATENCY_SLA_PARTITION);\n       return;\n     }\n \n-    LOGGER.info(\"*Recommending partition number\");\n-    if (_inputManager.getTableType().equalsIgnoreCase(\n-        REALTIME)) { //real time partition num should be the same value as the number of kafka partitions\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-    } else if (_inputManager.getTableType().equalsIgnoreCase(OFFLINE)) {\n+\n+\n+    /*For realtime/hybrid, the number of partitions on realtime Pinot table side is same as number of kafka partitions.\n+    This is generally the case unless there is a reason for them to be different. We saw only one outlier. So generally on\n+     the realtime side the number of partitions should match with the kafka partitions */\n+\n+    /*The number of partitions on offline table side is dependent on the amount of data.\n+     **We are assuming one segment per partition per push on offline side and this is generally true in Pinot**\n+    For hybrid table, we have seen cases where this value = number of kafka partitions = number of realtime table partitions.\n+    For hybrid table, we have also seen cases, where the value for offline is lower than realtime since the data\n+    generated on a given day is low volume and using a high count of number of partitions would lead to too many small\n+    sized segments since we typically have data from one partition in a segment. So we can conclude on the offline side, if the\n+    data amount is sufficient, the number of partitions will match with the realtime side. Otherwise the partition size will shrink\n+    so that data_size_per_push/num_offline_partitions >=  OPTIMAL_SIZE_PER_SEGMENT.*/\n+\n+    LOGGER.info(\"*Recommending number of partitions \");\n+    int numKafkaPartitions = _output.getPartitionConfig().getNumKafkaPartitions();\n+    long offLineDataSizePerPush = _input.getNumRecordsPerPush() * _input.getSizePerRecord();\n+    int optimalOfflinePartitions = (int)Math.ceil((double) offLineDataSizePerPush / _params.OPTIMAL_SIZE_PER_SEGMENT);\n+    if (_input.getTableType().equalsIgnoreCase(REALTIME) || _input.getTableType()\n+        .equalsIgnoreCase(HYBRID)) {\n+      //real time num of partitions should be the same value as the number of kafka partitions\n+      _output.getPartitionConfig().setNumPartitionsRealtime(numKafkaPartitions);\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(OFFLINE)) {\n       //Offline partition num is dependent on the amount of data coming in on a given day.\n       //Using a very high value of numPartitions for small dataset size will result in too many small sized segments.\n       //We define have a desirable segment size OPTIMAL_SIZE_PER_SEGMENT\n       //Divide the size of data coming in on a given day by OPTIMAL_SIZE_PER_SEGMENT we get the number of partitions.\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT));\n-    } else {\n-      long size = _inputManager.getNumRecordsPerPush() * _inputManager.getSizePerRecord();\n-      _outputManager.getPartitionConfig()\n-          .setNumPartitionsRealtime(_outputManager.getPartitionConfig().getNumKafkaPartitions());\n-      //real time partition num should be same as the number of kafka partitions\n-\n-      _outputManager.getPartitionConfig().setNumPartitionsOffline(\n-          Math.min((int) (size / _params.OPTIMAL_SIZE_PER_SEGMENT),\n-              _outputManager.getPartitionConfig().getNumKafkaPartitions()));\n+      _output.getPartitionConfig()\n+          .setNumPartitionsOffline((int) (optimalOfflinePartitions));\n+    }\n+    if (_input.getTableType().equalsIgnoreCase(HYBRID)) {\n+      _output.getPartitionConfig().setNumPartitionsOffline(\n+          Math.min(optimalOfflinePartitions, numKafkaPartitions));\n     }\n \n     LOGGER.info(\"*Recommending column to partition\");\n \n-    double[] weights = new double[_inputManager.getNumDims()];\n-    _inputManager.getQueryWeightMap().forEach((query, weight) -> {\n-      FixedLenBitset fixedLenBitset = parseQuery(query);\n+    double[] weights = new double[_input.getNumDims()];\n+    _input.getParsedQueries().forEach(query -> {\n+      Double weight =_input.getQueryWeight(query);\n+      FixedLenBitset fixedLenBitset = parseQuery(_input.getQueryContext(query));\n       LOGGER.debug(\"fixedLenBitset:{}\", fixedLenBitset);\n       if (fixedLenBitset != null) {\n         for (Integer i : fixedLenBitset.getOffsets()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgzNTg0OA==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465835848", "bodyText": "I think 2GB is on the high end. Let's just start with 1GB probably", "author": "siddharthteotia", "createdAt": "2020-08-05T16:03:20Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.rules.io.params;\n+\n+public class RecommenderConstants {\n+  public static class InvertedSortedIndexJointRule {\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_TEXT_MATCH = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_RANGE = 0.5d;\n+    public static final double DEAFULT_PERCENT_SELECT_FOR_REGEX = 0.5d;\n+    public static final double DEFAULT_PERCENT_SELECT_FOR_ISNULL = 0.5d;\n+    public static final double DEFAULT_THRESHOLD_MIN_AND_PREDICATE_INCREMENTAL_VOTE = 0.6d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_AND_PREDICATE_TOP_CANDIDATES = 0.8d;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_GAIN_DIFF_BETWEEN_ITERATION = 0.05d;\n+    public static final int DEFAULT_MAX_NUM_ITERATION_WITHOUT_GAIN = 3;\n+    public static final double DEFAULT_THRESHOLD_RATIO_MIN_NESI_FOR_TOP_CANDIDATES = 0.7d;\n+  }\n+\n+  public static class RulesToExecute {\n+    public static final boolean DEFAULT_RECOMMEND_FLAG_QUERY = true;\n+    public static final boolean DEFAULT_RECOMMEND_VARIED_LENGTH_DICTIONARY = true;\n+    public static final boolean DEFAULT_RECOMMEND_KAFKA_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_PINOT_TABLE_PARTITION = true;\n+    public static final boolean DEFAULT_RECOMMEND_INVERTED_SORTED_INDEX_JOINT = true;\n+    public static final boolean DEFAULT_RECOMMEND_BLOOM_FILTER = true;\n+    public static final boolean DEFAULT_RECOMMEND_NO_DICTIONARY_ONHEAP_DICTIONARY_JOINT = true;\n+  }\n+\n+  public static class PartitionRule {\n+    public static final int DEFAULT_NUM_PARTITIONS = 0;\n+\n+    public static final long DEFAULT_THRESHOLD_MAX_SLA_PARTITION = 1000;\n+    public static final long DEFAULT_THRESHOLD_MIN_QPS_PARTITION = 200;\n+    public static final long DEFAULT_OPTIMAL_SIZE_PER_SEGMENT = 2000_000_000; //2GB", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzYwMQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466007601", "bodyText": "changed to 1 GB", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:13:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTgzNTg0OA=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\nindex b8954229cb..8ae807a4be 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/rules/io/params/RecommenderConstants.java\n\n@@ -18,6 +18,10 @@\n  */\n package org.apache.pinot.controller.recommender.rules.io.params;\n \n+/**\n+ * The default parameters used int each algorithm and default values for general inputs\n+ * parameters usage are explained in the *Params class\n+ */\n public class RecommenderConstants {\n   public static class InvertedSortedIndexJointRule {\n     public static final double DEFAULT_PERCENT_SELECT_FOR_FUNCTION = 0.5d;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NDQyOQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465954429", "bodyText": "This function is only computing the size of dictionary right? We should not include the size of bit compressed forward index", "author": "siddharthteotia", "createdAt": "2020-08-05T19:30:17Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume\n+  public long _numRecordsPerPush = DEFAULT_NUM_RECORDS_PER_PUSH; // records per push for offline part of a table\n+  public long _latencySLA = DEFAULT_LATENCY_SLA; // latency sla in ms\n+  public int _numKafkaPartitions = DEFAULT_NUM_KAFKA_PARTITIONS;\n+\n+  // The parameters of rules\n+  public PartitionRuleParams _partitionRuleParams = new PartitionRuleParams();\n+  public InvertedSortedIndexJointRuleParams _invertedSortedIndexJointRuleParams =\n+      new InvertedSortedIndexJointRuleParams();\n+  public BloomFilterRuleParams _bloomFilterRuleParams = new BloomFilterRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRuleParams _noDictionaryOnHeapDictionaryJointRuleParams =\n+      new NoDictionaryOnHeapDictionaryJointRuleParams();\n+  public FlagQueryRuleParams _flagQueryRuleParams = new FlagQueryRuleParams();\n+\n+  // For forward compatibility: 1. dev/sre to overwrite field(s) 2. incremental recommendation on existing/staging tables\n+  public ConfigManager _overWrittenConfigs = new ConfigManager();\n+\n+  /******************************Ignored by deserializer****************************************/\n+  public Map<String, ColumnMetaData> _metaDataMap = new HashMap<>(); // meta data per column, complement to schema\n+  long _sizePerRecord = 0;\n+  Map<String, FieldSpec.DataType> _colnameFieldTypeMap = new HashMap<>();\n+  Set<String> _dimNames = null;\n+  Set<String> _metricNames = null;\n+  Set<String> _dateTimeNames = null;\n+  Set<String> _dimNamesInveredSortedIndexApplicable = null;\n+  Map<String, Integer> _colNameToIntMap = null;\n+  String[] _intToColNameMap = null;\n+  Map<FieldSpec.DataType, Integer> _dataTypeSizeMap = new HashMap<FieldSpec.DataType, Integer>() {{\n+    put(FieldSpec.DataType.INT, DEFAULT_INT_SIZE);\n+    put(FieldSpec.DataType.LONG, DEFAULT_LONG_SIZE);\n+    put(FieldSpec.DataType.FLOAT, DEFAULT_FLOAT_SIZE);\n+    put(FieldSpec.DataType.DOUBLE, DEFAULT_DOUBLE_SIZE);\n+    put(FieldSpec.DataType.BYTES, DEFAULT_BYTE_SIZE);\n+    put(FieldSpec.DataType.STRING, DEFAULT_CHAR_SIZE);\n+    put(null, DEFAULT_NULL_SIZE);\n+  }};\n+\n+  /**\n+   * Process the dependencies incurred by overwritten configs.\n+   * E.g. we will subtract the dimensions with overwritten indices from _dimNames to get _dimNamesIndexApplicable\n+   * This ensures we do not recommend indices on those dimensions\n+   */\n+  public void init()\n+      throws InvalidInputException {\n+    LOGGER.info(\"Preprocessing Input:\");\n+    reorderDimsAndBuildMap();\n+    registerColnameFieldType();\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setFlagQueryRuleParams(FlagQueryRuleParams flagQueryRuleParams) {\n+    _flagQueryRuleParams = flagQueryRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumKafkaPartitions(int numKafkaPartitions) {\n+    _numKafkaPartitions = numKafkaPartitions;\n+  }\n+\n+  @JsonSetter(value = \"queriesWithWeights\", nulls = Nulls.SKIP)\n+  public void setQueryWeightMap(Map<String, Double> queryWeightMap) {\n+    _queryWeightMap = queryWeightMap;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNoDictionaryOnHeapDictionaryJointRuleParams(\n+      NoDictionaryOnHeapDictionaryJointRuleParams noDictionaryOnHeapDictionaryJointRuleParams) {\n+    _noDictionaryOnHeapDictionaryJointRuleParams = noDictionaryOnHeapDictionaryJointRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setLatencySLA(int latencySLA) {\n+    _latencySLA = latencySLA;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setQps(long qps) {\n+    _qps = qps;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setBloomFilterRuleParams(BloomFilterRuleParams bloomFilterRuleParams) {\n+    _bloomFilterRuleParams = bloomFilterRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setPartitionRuleParams(PartitionRuleParams partitionRuleParams) {\n+    _partitionRuleParams = partitionRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setTableType(String tableType) {\n+    _tableType = tableType;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumMessagesPerSec(long numMessagesPerSec) {\n+    _numMessagesPerSec = numMessagesPerSec;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumRecordsPerPush(long numRecordsPerPush) {\n+    _numRecordsPerPush = numRecordsPerPush;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRulesToExecute(RulesToExecute rulesToExecute) {\n+    _rulesToExecute = rulesToExecute;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setSchema(JsonNode jsonNode)\n+      throws IOException {\n+    ObjectReader reader = new ObjectMapper().readerFor(Schema.class);\n+    this._schema=reader.readValue(jsonNode);\n+    reader = new ObjectMapper().readerFor(SchemaWithMetaData.class);\n+    this._schemaWithMetaData=reader.readValue(jsonNode);\n+    _schemaWithMetaData.getDimensionFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _schemaWithMetaData.getMetricFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _schemaWithMetaData.getDateTimeFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _metaDataMap.put(_schemaWithMetaData.getTimeFieldSpec().getName(), _schemaWithMetaData.getTimeFieldSpec());\n+  }\n+\n+  @JsonIgnore\n+  public void setMetaDataMap(Map<String, ColumnMetaData> metaDataMap) {\n+    _metaDataMap = metaDataMap;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setQueryType(String queryType) {\n+    _queryType = queryType;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setInvertedSortedIndexJointRuleParams(\n+      InvertedSortedIndexJointRuleParams invertedSortedIndexJointRuleParams) {\n+    _invertedSortedIndexJointRuleParams = invertedSortedIndexJointRuleParams;\n+  }\n+\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setOverWrittenConfigs(ConfigManager overWrittenConfigs) {\n+    _overWrittenConfigs = overWrittenConfigs;\n+  }\n+\n+\n+  public FlagQueryRuleParams getFlagQueryRuleParams() {\n+    return _flagQueryRuleParams;\n+  }\n+\n+\n+  public FieldSpec.DataType getFieldType(String colName){\n+    return _colnameFieldTypeMap.getOrDefault(colName, null);\n+  }\n+\n+  public Map<String, Integer> getColNameToIntMap() {\n+    return _colNameToIntMap;\n+  }\n+\n+  /**\n+   * Get the number of dimensions we can apply indices on.\n+   * @return total number of dimensions minus number of dimensions with overwritten indices\n+   */\n+  public int getNumDimsInvertedSortedApplicable() {\n+    return _dimNamesInveredSortedIndexApplicable.size();\n+  }\n+\n+  public NoDictionaryOnHeapDictionaryJointRuleParams getNoDictionaryOnHeapDictionaryJointRuleParams() {\n+    return _noDictionaryOnHeapDictionaryJointRuleParams;\n+  }\n+\n+  public int getNumDims() {\n+    return _dimNames.size();\n+  }\n+\n+  public int getNumCols() {\n+    return _colNameToIntMap.size();\n+  }\n+\n+  //TODO: Currently Pinot is using only ONE time column specified by TimeFieldSpec\n+  //TODO: Change the implementation after the new schema with multiple _dateTimeNames is in use\n+  public String getTimeCol() {\n+    return _schema.getTimeFieldSpec().getName();\n+  }\n+\n+  public Set<String> getColNamesNoDictionary() {\n+    return _overWrittenConfigs.getIndexConfig().getNoDictionaryColumns();\n+  }\n+\n+  public long getLatencySLA() {\n+    return _latencySLA;\n+  }\n+\n+  public long getQps() {\n+    return _qps;\n+  }\n+\n+  public BloomFilterRuleParams getBloomFilterRuleParams() {\n+    return _bloomFilterRuleParams;\n+  }\n+\n+  public PartitionRuleParams getPartitionRuleParams() {\n+    return _partitionRuleParams;\n+  }\n+\n+  public String getTableType() {\n+    return _tableType;\n+  }\n+\n+  public Map<String, Double> getQueryWeightMap() {\n+    return _queryWeightMap;\n+  }\n+\n+  public long getNumMessagesPerSec() {\n+    return _numMessagesPerSec;\n+  }\n+\n+  public long getNumRecordsPerPush() {\n+    return _numRecordsPerPush;\n+  }\n+\n+  public RulesToExecute getRulesToExecute() {\n+    return _rulesToExecute;\n+  }\n+\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  @JsonIgnore\n+  public Map<String, ColumnMetaData> getMetaDataMap() {\n+    return _metaDataMap;\n+  }\n+\n+  public String getQueryType() {\n+    return _queryType;\n+  }\n+\n+  public InvertedSortedIndexJointRuleParams getInvertedSortedIndexJointRuleParams() {\n+    return _invertedSortedIndexJointRuleParams;\n+  }\n+\n+  public ConfigManager getOverWrittenConfigs() {\n+    return _overWrittenConfigs;\n+  }\n+\n+  public long getSizePerRecord() {\n+    return _sizePerRecord;\n+  }\n+\n+  public double getCardinality(String columnName) {\n+    return max(_metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getCardinality(), MIN_CARDINALITY);\n+  }\n+\n+  public double getNumValuesPerEntry(String columnName) {\n+    return _metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getNumValuesPerEntry();\n+  }\n+\n+  public int getAverageDataLen(String columnName) {\n+    return _metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getAverageLength();\n+  }\n+\n+  public int getNumKafkaPartitions() {\n+    return _numKafkaPartitions;\n+  }\n+\n+  public boolean isIndexableDim(String colName) {\n+    return _dimNamesInveredSortedIndexApplicable.contains(colName);\n+  }\n+\n+  public boolean isSingleValueColumn(String colName){\n+    ColumnMetaData columnMetaData = _metaDataMap.getOrDefault(colName, new ColumnMetaData());\n+    return columnMetaData.isSingleValueField() && (columnMetaData.getNumValuesPerEntry() < DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY + EPSILON);\n+  }\n+\n+  /**\n+   * Map a index-applicable dimension name to an 0<=integer<getNumDimsInvertedSortedApplicable,\n+   * to be used with {@link FixedLenBitset}\n+   * @param colName a dimension with no overwritten index\n+   * @return a unique integer id\n+   */\n+  public int colNameToInt(String colName) {\n+    return _colNameToIntMap.getOrDefault(colName, NO_SUCH_COL);\n+  }\n+\n+  /**\n+   * A reverse process of colNameToInt\n+   * @param colID a unique integer id\n+   * @return column name\n+   */\n+  public String intToColName(int colID) {\n+    return _intToColNameMap[colID];\n+  }\n+\n+  /**\n+   * Test if colName is a valid dimension name\n+   */\n+  public boolean isDim(String colName) {\n+    return _dimNames.contains(colName);\n+  }\n+\n+  public boolean isDateTime(String colName) {\n+    return _schema.getTimeFieldSpec().getName().equals(colName);\n+  }\n+\n+  public void registerColnameFieldType() { // create a map from colname to data type\n+    for (DimensionFieldSpec dimensionFieldSpec : _schema.getDimensionFieldSpecs()) {\n+      _colnameFieldTypeMap.put(dimensionFieldSpec.getName(), dimensionFieldSpec.getDataType());\n+    }\n+    for (MetricFieldSpec metricFieldSpec : _schema.getMetricFieldSpecs()) {\n+      _colnameFieldTypeMap.put(metricFieldSpec.getName(), metricFieldSpec.getDataType());\n+    }\n+    //TODO: add support for multiple getDateTimeFieldSpecs\n+    _colnameFieldTypeMap.put(_schema.getTimeFieldSpec().getName(), _schema.getTimeFieldSpec().getDataType());\n+  }\n+\n+  public void estimateSizePerRecord() {\n+    for (String colName : _colnameFieldTypeMap.keySet()) {\n+      _sizePerRecord += getColDataSizeWithDictionary(colName);\n+      LOGGER.debug(\"{} {}\",colName, getColDataSizeWithDictionary(colName));\n+    }\n+    LOGGER.info(\"*Estimated size per record {} bytes\", _sizePerRecord);\n+  }\n+\n+  public long getColDataSizeWithoutDictionary(String colName) {\n+    //TODO: implement this after the complex is supported\n+    FieldSpec.DataType dataType = getFieldType(colName);\n+    if (dataType == FieldSpec.DataType.STRUCT || dataType == FieldSpec.DataType.MAP\n+        || dataType == FieldSpec.DataType.LIST) {\n+      return 0;\n+    } else {\n+      if (dataType == FieldSpec.DataType.BYTES || dataType == FieldSpec.DataType.STRING) {\n+        return _dataTypeSizeMap.get(dataType) * getAverageDataLen(colName);\n+      } else {\n+        return _dataTypeSizeMap.get(dataType);\n+      }\n+    }\n+  }\n+\n+  public long getColDataSizeWithDictionary(String colName) {\n+    //TODO: implement this after the complex is supported\n+    FieldSpec.DataType dataType = getFieldType(colName);\n+    int numValuesPerEntry = (int) Math.ceil(getNumValuesPerEntry(colName));\n+    LOGGER.trace(\"{} {}\", colName, numValuesPerEntry);\n+    if (dataType == FieldSpec.DataType.STRUCT || dataType == FieldSpec.DataType.MAP\n+        || dataType == FieldSpec.DataType.LIST) {\n+      return 0;\n+    } else if (!_overWrittenConfigs.getIndexConfig().getNoDictionaryColumns().contains(colName)) { // has dictionary\n+      return getBitCompressedDataSize(colName) * numValuesPerEntry;\n+    } else { // no dictionary\n+      if (dataType == FieldSpec.DataType.BYTES || dataType == FieldSpec.DataType.STRING) {\n+        return _dataTypeSizeMap.get(dataType) * numValuesPerEntry * getAverageDataLen(colName);\n+      } else {\n+        return _dataTypeSizeMap.get(dataType) * numValuesPerEntry;\n+      }\n+    }\n+  }\n+\n+  public int getBitCompressedDataSize(String colName) {\n+    return max((int) Math.ceil(Math.log(getCardinality(colName)) / (8 * Math.log(2))), 1);\n+  }\n+\n+  //\n+  public long getDictionarySize(String colName) {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzIwNQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r466007205", "bodyText": "Done! Thanks for pointing out this bug.", "author": "jasperjiaguo", "createdAt": "2020-08-05T21:12:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NDQyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NTkzNw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465955937", "bodyText": "You might want to rename it to getDictionaryEncodedForwardIndexSize", "author": "siddharthteotia", "createdAt": "2020-08-05T19:33:22Z", "path": "pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java", "diffHunk": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.controller.recommender.io;\n+\n+import com.fasterxml.jackson.annotation.JsonAutoDetect;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonSetter;\n+import com.fasterxml.jackson.annotation.Nulls;\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n+import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n+import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.FlagQueryRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.InvertedSortedIndexJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.NoDictionaryOnHeapDictionaryJointRuleParams;\n+import org.apache.pinot.controller.recommender.rules.io.params.PartitionRuleParams;\n+import org.apache.pinot.controller.recommender.rules.utils.FixedLenBitset;\n+import org.apache.pinot.spi.data.DimensionFieldSpec;\n+import org.apache.pinot.spi.data.FieldSpec;\n+import org.apache.pinot.spi.data.MetricFieldSpec;\n+import org.apache.pinot.spi.data.Schema;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static java.lang.Math.max;\n+import static org.apache.pinot.controller.recommender.rules.io.params.RecommenderConstants.*;\n+\n+\n+@JsonAutoDetect(fieldVisibility = JsonAutoDetect.Visibility.NONE)\n+public class InputManager {\n+  private final Logger LOGGER = LoggerFactory.getLogger(InputManager.class);\n+\n+  /******************************Deserialized from input json*********************************/\n+  // Basic input fields\n+  public RulesToExecute _rulesToExecute = new RulesToExecute(); // dictates which rules to execute\n+  public Schema _schema = new Schema();\n+  public SchemaWithMetaData _schemaWithMetaData = new SchemaWithMetaData();\n+\n+  public String _queryType = SQL; // SQL or PQL\n+  public long _qps = DEFAULT_QPS;\n+  public Map<String, Double> _queryWeightMap = new HashMap<>(); // {\"queryString\":\"queryWeight\"}\n+  public String _tableType = OFFLINE;\n+  public long _numMessagesPerSec = DEFAULT_NUM_MSG_PER_SEC; // messages per sec for kafka to consume\n+  public long _numRecordsPerPush = DEFAULT_NUM_RECORDS_PER_PUSH; // records per push for offline part of a table\n+  public long _latencySLA = DEFAULT_LATENCY_SLA; // latency sla in ms\n+  public int _numKafkaPartitions = DEFAULT_NUM_KAFKA_PARTITIONS;\n+\n+  // The parameters of rules\n+  public PartitionRuleParams _partitionRuleParams = new PartitionRuleParams();\n+  public InvertedSortedIndexJointRuleParams _invertedSortedIndexJointRuleParams =\n+      new InvertedSortedIndexJointRuleParams();\n+  public BloomFilterRuleParams _bloomFilterRuleParams = new BloomFilterRuleParams();\n+  public NoDictionaryOnHeapDictionaryJointRuleParams _noDictionaryOnHeapDictionaryJointRuleParams =\n+      new NoDictionaryOnHeapDictionaryJointRuleParams();\n+  public FlagQueryRuleParams _flagQueryRuleParams = new FlagQueryRuleParams();\n+\n+  // For forward compatibility: 1. dev/sre to overwrite field(s) 2. incremental recommendation on existing/staging tables\n+  public ConfigManager _overWrittenConfigs = new ConfigManager();\n+\n+  /******************************Ignored by deserializer****************************************/\n+  public Map<String, ColumnMetaData> _metaDataMap = new HashMap<>(); // meta data per column, complement to schema\n+  long _sizePerRecord = 0;\n+  Map<String, FieldSpec.DataType> _colnameFieldTypeMap = new HashMap<>();\n+  Set<String> _dimNames = null;\n+  Set<String> _metricNames = null;\n+  Set<String> _dateTimeNames = null;\n+  Set<String> _dimNamesInveredSortedIndexApplicable = null;\n+  Map<String, Integer> _colNameToIntMap = null;\n+  String[] _intToColNameMap = null;\n+  Map<FieldSpec.DataType, Integer> _dataTypeSizeMap = new HashMap<FieldSpec.DataType, Integer>() {{\n+    put(FieldSpec.DataType.INT, DEFAULT_INT_SIZE);\n+    put(FieldSpec.DataType.LONG, DEFAULT_LONG_SIZE);\n+    put(FieldSpec.DataType.FLOAT, DEFAULT_FLOAT_SIZE);\n+    put(FieldSpec.DataType.DOUBLE, DEFAULT_DOUBLE_SIZE);\n+    put(FieldSpec.DataType.BYTES, DEFAULT_BYTE_SIZE);\n+    put(FieldSpec.DataType.STRING, DEFAULT_CHAR_SIZE);\n+    put(null, DEFAULT_NULL_SIZE);\n+  }};\n+\n+  /**\n+   * Process the dependencies incurred by overwritten configs.\n+   * E.g. we will subtract the dimensions with overwritten indices from _dimNames to get _dimNamesIndexApplicable\n+   * This ensures we do not recommend indices on those dimensions\n+   */\n+  public void init()\n+      throws InvalidInputException {\n+    LOGGER.info(\"Preprocessing Input:\");\n+    reorderDimsAndBuildMap();\n+    registerColnameFieldType();\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setFlagQueryRuleParams(FlagQueryRuleParams flagQueryRuleParams) {\n+    _flagQueryRuleParams = flagQueryRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumKafkaPartitions(int numKafkaPartitions) {\n+    _numKafkaPartitions = numKafkaPartitions;\n+  }\n+\n+  @JsonSetter(value = \"queriesWithWeights\", nulls = Nulls.SKIP)\n+  public void setQueryWeightMap(Map<String, Double> queryWeightMap) {\n+    _queryWeightMap = queryWeightMap;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNoDictionaryOnHeapDictionaryJointRuleParams(\n+      NoDictionaryOnHeapDictionaryJointRuleParams noDictionaryOnHeapDictionaryJointRuleParams) {\n+    _noDictionaryOnHeapDictionaryJointRuleParams = noDictionaryOnHeapDictionaryJointRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setLatencySLA(int latencySLA) {\n+    _latencySLA = latencySLA;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setQps(long qps) {\n+    _qps = qps;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setBloomFilterRuleParams(BloomFilterRuleParams bloomFilterRuleParams) {\n+    _bloomFilterRuleParams = bloomFilterRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setPartitionRuleParams(PartitionRuleParams partitionRuleParams) {\n+    _partitionRuleParams = partitionRuleParams;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setTableType(String tableType) {\n+    _tableType = tableType;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumMessagesPerSec(long numMessagesPerSec) {\n+    _numMessagesPerSec = numMessagesPerSec;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setNumRecordsPerPush(long numRecordsPerPush) {\n+    _numRecordsPerPush = numRecordsPerPush;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setRulesToExecute(RulesToExecute rulesToExecute) {\n+    _rulesToExecute = rulesToExecute;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setSchema(JsonNode jsonNode)\n+      throws IOException {\n+    ObjectReader reader = new ObjectMapper().readerFor(Schema.class);\n+    this._schema=reader.readValue(jsonNode);\n+    reader = new ObjectMapper().readerFor(SchemaWithMetaData.class);\n+    this._schemaWithMetaData=reader.readValue(jsonNode);\n+    _schemaWithMetaData.getDimensionFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _schemaWithMetaData.getMetricFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _schemaWithMetaData.getDateTimeFieldSpecs()\n+        .forEach(columnMetaData -> {_metaDataMap.put(columnMetaData.getName(),columnMetaData);});\n+    _metaDataMap.put(_schemaWithMetaData.getTimeFieldSpec().getName(), _schemaWithMetaData.getTimeFieldSpec());\n+  }\n+\n+  @JsonIgnore\n+  public void setMetaDataMap(Map<String, ColumnMetaData> metaDataMap) {\n+    _metaDataMap = metaDataMap;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setQueryType(String queryType) {\n+    _queryType = queryType;\n+  }\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setInvertedSortedIndexJointRuleParams(\n+      InvertedSortedIndexJointRuleParams invertedSortedIndexJointRuleParams) {\n+    _invertedSortedIndexJointRuleParams = invertedSortedIndexJointRuleParams;\n+  }\n+\n+\n+  @JsonSetter(nulls = Nulls.SKIP)\n+  public void setOverWrittenConfigs(ConfigManager overWrittenConfigs) {\n+    _overWrittenConfigs = overWrittenConfigs;\n+  }\n+\n+\n+  public FlagQueryRuleParams getFlagQueryRuleParams() {\n+    return _flagQueryRuleParams;\n+  }\n+\n+\n+  public FieldSpec.DataType getFieldType(String colName){\n+    return _colnameFieldTypeMap.getOrDefault(colName, null);\n+  }\n+\n+  public Map<String, Integer> getColNameToIntMap() {\n+    return _colNameToIntMap;\n+  }\n+\n+  /**\n+   * Get the number of dimensions we can apply indices on.\n+   * @return total number of dimensions minus number of dimensions with overwritten indices\n+   */\n+  public int getNumDimsInvertedSortedApplicable() {\n+    return _dimNamesInveredSortedIndexApplicable.size();\n+  }\n+\n+  public NoDictionaryOnHeapDictionaryJointRuleParams getNoDictionaryOnHeapDictionaryJointRuleParams() {\n+    return _noDictionaryOnHeapDictionaryJointRuleParams;\n+  }\n+\n+  public int getNumDims() {\n+    return _dimNames.size();\n+  }\n+\n+  public int getNumCols() {\n+    return _colNameToIntMap.size();\n+  }\n+\n+  //TODO: Currently Pinot is using only ONE time column specified by TimeFieldSpec\n+  //TODO: Change the implementation after the new schema with multiple _dateTimeNames is in use\n+  public String getTimeCol() {\n+    return _schema.getTimeFieldSpec().getName();\n+  }\n+\n+  public Set<String> getColNamesNoDictionary() {\n+    return _overWrittenConfigs.getIndexConfig().getNoDictionaryColumns();\n+  }\n+\n+  public long getLatencySLA() {\n+    return _latencySLA;\n+  }\n+\n+  public long getQps() {\n+    return _qps;\n+  }\n+\n+  public BloomFilterRuleParams getBloomFilterRuleParams() {\n+    return _bloomFilterRuleParams;\n+  }\n+\n+  public PartitionRuleParams getPartitionRuleParams() {\n+    return _partitionRuleParams;\n+  }\n+\n+  public String getTableType() {\n+    return _tableType;\n+  }\n+\n+  public Map<String, Double> getQueryWeightMap() {\n+    return _queryWeightMap;\n+  }\n+\n+  public long getNumMessagesPerSec() {\n+    return _numMessagesPerSec;\n+  }\n+\n+  public long getNumRecordsPerPush() {\n+    return _numRecordsPerPush;\n+  }\n+\n+  public RulesToExecute getRulesToExecute() {\n+    return _rulesToExecute;\n+  }\n+\n+  public Schema getSchema() {\n+    return _schema;\n+  }\n+\n+  @JsonIgnore\n+  public Map<String, ColumnMetaData> getMetaDataMap() {\n+    return _metaDataMap;\n+  }\n+\n+  public String getQueryType() {\n+    return _queryType;\n+  }\n+\n+  public InvertedSortedIndexJointRuleParams getInvertedSortedIndexJointRuleParams() {\n+    return _invertedSortedIndexJointRuleParams;\n+  }\n+\n+  public ConfigManager getOverWrittenConfigs() {\n+    return _overWrittenConfigs;\n+  }\n+\n+  public long getSizePerRecord() {\n+    return _sizePerRecord;\n+  }\n+\n+  public double getCardinality(String columnName) {\n+    return max(_metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getCardinality(), MIN_CARDINALITY);\n+  }\n+\n+  public double getNumValuesPerEntry(String columnName) {\n+    return _metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getNumValuesPerEntry();\n+  }\n+\n+  public int getAverageDataLen(String columnName) {\n+    return _metaDataMap.getOrDefault(columnName, new ColumnMetaData()).getAverageLength();\n+  }\n+\n+  public int getNumKafkaPartitions() {\n+    return _numKafkaPartitions;\n+  }\n+\n+  public boolean isIndexableDim(String colName) {\n+    return _dimNamesInveredSortedIndexApplicable.contains(colName);\n+  }\n+\n+  public boolean isSingleValueColumn(String colName){\n+    ColumnMetaData columnMetaData = _metaDataMap.getOrDefault(colName, new ColumnMetaData());\n+    return columnMetaData.isSingleValueField() && (columnMetaData.getNumValuesPerEntry() < DEFAULT_AVERAGE_NUM_VALUES_PER_ENTRY + EPSILON);\n+  }\n+\n+  /**\n+   * Map a index-applicable dimension name to an 0<=integer<getNumDimsInvertedSortedApplicable,\n+   * to be used with {@link FixedLenBitset}\n+   * @param colName a dimension with no overwritten index\n+   * @return a unique integer id\n+   */\n+  public int colNameToInt(String colName) {\n+    return _colNameToIntMap.getOrDefault(colName, NO_SUCH_COL);\n+  }\n+\n+  /**\n+   * A reverse process of colNameToInt\n+   * @param colID a unique integer id\n+   * @return column name\n+   */\n+  public String intToColName(int colID) {\n+    return _intToColNameMap[colID];\n+  }\n+\n+  /**\n+   * Test if colName is a valid dimension name\n+   */\n+  public boolean isDim(String colName) {\n+    return _dimNames.contains(colName);\n+  }\n+\n+  public boolean isDateTime(String colName) {\n+    return _schema.getTimeFieldSpec().getName().equals(colName);\n+  }\n+\n+  public void registerColnameFieldType() { // create a map from colname to data type\n+    for (DimensionFieldSpec dimensionFieldSpec : _schema.getDimensionFieldSpecs()) {\n+      _colnameFieldTypeMap.put(dimensionFieldSpec.getName(), dimensionFieldSpec.getDataType());\n+    }\n+    for (MetricFieldSpec metricFieldSpec : _schema.getMetricFieldSpecs()) {\n+      _colnameFieldTypeMap.put(metricFieldSpec.getName(), metricFieldSpec.getDataType());\n+    }\n+    //TODO: add support for multiple getDateTimeFieldSpecs\n+    _colnameFieldTypeMap.put(_schema.getTimeFieldSpec().getName(), _schema.getTimeFieldSpec().getDataType());\n+  }\n+\n+  public void estimateSizePerRecord() {\n+    for (String colName : _colnameFieldTypeMap.keySet()) {\n+      _sizePerRecord += getColDataSizeWithDictionary(colName);\n+      LOGGER.debug(\"{} {}\",colName, getColDataSizeWithDictionary(colName));\n+    }\n+    LOGGER.info(\"*Estimated size per record {} bytes\", _sizePerRecord);\n+  }\n+\n+  public long getColDataSizeWithoutDictionary(String colName) {\n+    //TODO: implement this after the complex is supported\n+    FieldSpec.DataType dataType = getFieldType(colName);\n+    if (dataType == FieldSpec.DataType.STRUCT || dataType == FieldSpec.DataType.MAP\n+        || dataType == FieldSpec.DataType.LIST) {\n+      return 0;\n+    } else {\n+      if (dataType == FieldSpec.DataType.BYTES || dataType == FieldSpec.DataType.STRING) {\n+        return _dataTypeSizeMap.get(dataType) * getAverageDataLen(colName);\n+      } else {\n+        return _dataTypeSizeMap.get(dataType);\n+      }\n+    }\n+  }\n+\n+  public long getColDataSizeWithDictionary(String colName) {\n+    //TODO: implement this after the complex is supported\n+    FieldSpec.DataType dataType = getFieldType(colName);\n+    int numValuesPerEntry = (int) Math.ceil(getNumValuesPerEntry(colName));\n+    LOGGER.trace(\"{} {}\", colName, numValuesPerEntry);\n+    if (dataType == FieldSpec.DataType.STRUCT || dataType == FieldSpec.DataType.MAP\n+        || dataType == FieldSpec.DataType.LIST) {\n+      return 0;\n+    } else if (!_overWrittenConfigs.getIndexConfig().getNoDictionaryColumns().contains(colName)) { // has dictionary\n+      return getBitCompressedDataSize(colName) * numValuesPerEntry;\n+    } else { // no dictionary\n+      if (dataType == FieldSpec.DataType.BYTES || dataType == FieldSpec.DataType.STRING) {\n+        return _dataTypeSizeMap.get(dataType) * numValuesPerEntry * getAverageDataLen(colName);\n+      } else {\n+        return _dataTypeSizeMap.get(dataType) * numValuesPerEntry;\n+      }\n+    }\n+  }\n+\n+  public int getBitCompressedDataSize(String colName) {", "originalCommit": "c3c722c824480ab95851c0df2cef36adb7f29e65", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MDc4OQ==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465960789", "bodyText": "This function doesn't actually return the total bit compressed size.\nWhat we need is the following:\n\nget the number of bits per value\nnumber of records\n\nmultiply both\nnumber of bits per value  can be calculated by a function PinotDataBitSet", "author": "siddharthteotia", "createdAt": "2020-08-05T19:42:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NTkzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5ODUwMw==", "url": "https://github.com/apache/pinot/pull/5774#discussion_r465998503", "bodyText": "done", "author": "jasperjiaguo", "createdAt": "2020-08-05T20:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NTkzNw=="}], "type": "inlineReview", "revised_code": {"commit": "1886536a82a50ad3721c3576b1264791218fe4f6", "chunk": "diff --git a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\nindex e7c299f30d..8a7c59e56d 100644\n--- a/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n+++ b/pinot-controller/src/main/java/org/apache/pinot/controller/recommender/io/InputManager.java\n\n@@ -25,14 +25,19 @@ import com.fasterxml.jackson.annotation.Nulls;\n import com.fasterxml.jackson.databind.JsonNode;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import com.fasterxml.jackson.databind.ObjectReader;\n+import com.google.common.annotations.VisibleForTesting;\n import java.io.IOException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n import java.util.Map;\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.pinot.controller.recommender.io.exceptions.InvalidInputException;\n-import org.apache.pinot.controller.recommender.io.metadata.ColumnMetaData;\n+import org.apache.commons.lang3.tuple.Triple;\n+import org.apache.pinot.common.request.BrokerRequest;\n+import org.apache.pinot.controller.recommender.exceptions.InvalidInputException;\n+import org.apache.pinot.controller.recommender.io.metadata.FieldMetadata;\n import org.apache.pinot.controller.recommender.io.metadata.SchemaWithMetaData;\n import org.apache.pinot.controller.recommender.rules.RulesToExecute;\n import org.apache.pinot.controller.recommender.rules.io.params.BloomFilterRuleParams;\n"}}, {"oid": "1886536a82a50ad3721c3576b1264791218fe4f6", "url": "https://github.com/apache/pinot/commit/1886536a82a50ad3721c3576b1264791218fe4f6", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nremove log printing\n\nfixed a bug in InputManager, added test cases for FlagQuery and VariedLengthDictionary\n\nrefactored code\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs", "committedDate": "2020-08-07T03:40:11Z", "type": "forcePushed"}, {"oid": "8fe31b734898b939edd39e8ef6113454ae7ce750", "url": "https://github.com/apache/pinot/commit/8fe31b734898b939edd39e8ef6113454ae7ce750", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nremove log printing\n\nfixed a bug in InputManager, added test cases for FlagQuery and VariedLengthDictionary\n\nrefactored code\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nAddressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfixed testcases", "committedDate": "2020-08-07T03:43:57Z", "type": "forcePushed"}, {"oid": "76184a54001d770efbfb576ed3ff274dae86a666", "url": "https://github.com/apache/pinot/commit/76184a54001d770efbfb576ed3ff274dae86a666", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase", "committedDate": "2020-08-07T04:19:32Z", "type": "forcePushed"}, {"oid": "85628c8c8e4ad62766cfa88b391250b737689c4e", "url": "https://github.com/apache/pinot/commit/85628c8c8e4ad62766cfa88b391250b737689c4e", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license", "committedDate": "2020-08-07T04:51:37Z", "type": "forcePushed"}, {"oid": "0879d3ad9cfa6cac163ca77a0b44963279671803", "url": "https://github.com/apache/pinot/commit/0879d3ad9cfa6cac163ca77a0b44963279671803", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license\n\nadded cardinality regulator", "committedDate": "2020-08-07T09:48:01Z", "type": "forcePushed"}, {"oid": "3873a1b35c0202a60965f1869b47c783b962d97d", "url": "https://github.com/apache/pinot/commit/3873a1b35c0202a60965f1869b47c783b962d97d", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license\n\nadded cardinality regulator\n\nrestart test\n\nrestart test\n\nresolve comments", "committedDate": "2020-08-07T21:44:30Z", "type": "forcePushed"}, {"oid": "b9540c67efd462fa57c776113cddf3340636430e", "url": "https://github.com/apache/pinot/commit/b9540c67efd462fa57c776113cddf3340636430e", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license\n\nadded cardinality regulator\n\nrestart test\n\nrestart test\n\nresolve comments\n\nremoved logging", "committedDate": "2020-08-07T22:06:39Z", "type": "forcePushed"}, {"oid": "e518a71fdab60b4e5fdea13f79769d3cd7eef116", "url": "https://github.com/apache/pinot/commit/e518a71fdab60b4e5fdea13f79769d3cd7eef116", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license\n\nadded cardinality regulator\n\nrestart test\n\nrestart test\n\nresolve comments\n\nremoved logging\n\ntest", "committedDate": "2020-08-10T17:31:10Z", "type": "commit"}, {"oid": "e518a71fdab60b4e5fdea13f79769d3cd7eef116", "url": "https://github.com/apache/pinot/commit/e518a71fdab60b4e5fdea13f79769d3cd7eef116", "message": "Addressed issues in code review:\n1. for MV use dictionary encoding only\n2. renamed code\n3. cleaned up some conditions\n4. added a few java docs\n\nfix testcase\n\nadded license\n\nadded cardinality regulator\n\nrestart test\n\nrestart test\n\nresolve comments\n\nremoved logging\n\ntest", "committedDate": "2020-08-10T17:31:10Z", "type": "forcePushed"}]}