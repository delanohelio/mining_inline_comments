{"pr_number": 5409, "pr_title": "Faster vectorized bit unpacking (Part 1)", "pr_createdAt": "2020-05-18T22:59:26Z", "pr_url": "https://github.com/apache/pinot/pull/5409", "timeline": [{"oid": "a65409f873f3735d34378f4ab3526795f7a919a5", "url": "https://github.com/apache/pinot/commit/a65409f873f3735d34378f4ab3526795f7a919a5", "message": "Faster bit unpacking", "committedDate": "2020-05-18T21:51:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2NzQ2Mg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r426967462", "bodyText": "Consider writing a header for backward/forward compatibility.", "author": "mayankshriv", "createdAt": "2020-05-19T00:46:20Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public final class FixedBitIntReaderWriterV2 implements Closeable {\n+  private volatile PinotDataBitSetV2 _dataBitSet;\n+  private final int _numBitsPerValue;\n+\n+  public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {", "originalCommit": "a65409f873f3735d34378f4ab3526795f7a919a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NzUyNA==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r429077524", "bodyText": "Yes, in the follow-up when this code is wired with reader and writer (FixedBitSingleValueReader and FixedBitSingleValueWriter) and the scan operator, I will consider if the format has to be changed and bump the version and write a header.", "author": "siddharthteotia", "createdAt": "2020-05-22T07:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2NzQ2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDE3MzUyMg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430173522", "bodyText": "when you do this, please write a standalone header buffer that can be used in other places", "author": "kishoreg", "createdAt": "2020-05-26T06:03:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2NzQ2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "f0679eca59cacdd600fce7d43cd95a0e7244ea94", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\nindex d9ceec5525..e66154c067 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n\n@@ -27,13 +27,11 @@ import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n \n public final class FixedBitIntReaderWriterV2 implements Closeable {\n   private volatile PinotDataBitSetV2 _dataBitSet;\n-  private final int _numBitsPerValue;\n \n   public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n     Preconditions\n         .checkState(dataBuffer.size() == (int) (((long) numValues * numBitsPerValue + Byte.SIZE - 1) / Byte.SIZE));\n     _dataBitSet = PinotDataBitSetV2.createBitSet(dataBuffer, numBitsPerValue);\n-    _numBitsPerValue = numBitsPerValue;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2NzY0Ng==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r426967646", "bodyText": "Reasoning for magic number?", "author": "mayankshriv", "createdAt": "2020-05-19T00:47:08Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public final class FixedBitIntReaderWriterV2 implements Closeable {\n+  private volatile PinotDataBitSetV2 _dataBitSet;\n+  private final int _numBitsPerValue;\n+\n+  public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n+    Preconditions\n+        .checkState(dataBuffer.size() == (int) (((long) numValues * numBitsPerValue + Byte.SIZE - 1) / Byte.SIZE));\n+    _dataBitSet = PinotDataBitSetV2.createBitSet(dataBuffer, numBitsPerValue);\n+    _numBitsPerValue = numBitsPerValue;\n+  }\n+\n+  /**\n+   * Read dictionaryId for a particular docId\n+   * @param index docId to get the dictionaryId for\n+   * @return dictionaryId\n+   */\n+  public int readInt(int index) {\n+    return _dataBitSet.readInt(index);\n+  }\n+\n+  /**\n+   * Array based API to read dictionaryIds for a contiguous\n+   * range of docIds starting at startDocId for a given length\n+   * @param startDocId docId range start\n+   * @param length length of contiguous docId range\n+   * @param buffer out buffer to read dictionaryIds into\n+   */\n+  public void readInt(int startDocId, int length, int[] buffer) {\n+    _dataBitSet.readInt(startDocId, length, buffer);\n+  }\n+\n+  /**\n+   * Array based API to read dictionaryIds for an array of docIds\n+   * which are monotonically increasing but not necessarily contiguous\n+   * @param docIds array of docIds to read the dictionaryIds for\n+   * @param docIdStartIndex start index in docIds array\n+   * @param docIdLength length to process in docIds array\n+   * @param values out array to store the dictionaryIds into\n+   * @param valuesStartIndex start index in values array\n+   */\n+  public void readValues(int[] docIds, int docIdStartIndex, int docIdLength, int[] values, int valuesStartIndex) {\n+    int docIdEndIndex = docIdStartIndex + docIdLength - 1;\n+    if (shouldBulkRead(docIds, docIdStartIndex, docIdEndIndex)) {\n+      _dataBitSet.readInt(docIds, docIdStartIndex, docIdLength, values, valuesStartIndex);\n+    } else {\n+      for (int i = docIdStartIndex; i <= docIdEndIndex; i++) {\n+        values[valuesStartIndex++] = _dataBitSet.readInt(docIds[i]);\n+      }\n+    }\n+  }\n+\n+  private boolean shouldBulkRead(int[] docIds, int startIndex, int endIndex) {\n+    int numDocsToRead = endIndex - startIndex + 1;\n+    int docIdRange = docIds[endIndex] - docIds[startIndex] + 1;\n+    if (docIdRange > DocIdSetPlanNode.MAX_DOC_PER_CALL) {\n+      return false;\n+    }\n+    return numDocsToRead >= ((double)docIdRange * 0.7);", "originalCommit": "a65409f873f3735d34378f4ab3526795f7a919a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4NDg2MA==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r429084860", "bodyText": "I have provided details in javadoc. Let me know if that is helpful.", "author": "siddharthteotia", "createdAt": "2020-05-22T07:23:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2NzY0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "f0679eca59cacdd600fce7d43cd95a0e7244ea94", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\nindex d9ceec5525..e66154c067 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n\n@@ -27,13 +27,11 @@ import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n \n public final class FixedBitIntReaderWriterV2 implements Closeable {\n   private volatile PinotDataBitSetV2 _dataBitSet;\n-  private final int _numBitsPerValue;\n \n   public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n     Preconditions\n         .checkState(dataBuffer.size() == (int) (((long) numValues * numBitsPerValue + Byte.SIZE - 1) / Byte.SIZE));\n     _dataBitSet = PinotDataBitSetV2.createBitSet(dataBuffer, numBitsPerValue);\n-    _numBitsPerValue = numBitsPerValue;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2OTM3OQ==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r426969379", "bodyText": "Would be good to use JMH to make benchmark more accurate.", "author": "mayankshriv", "createdAt": "2020-05-19T00:53:33Z", "path": "pinot-perf/src/main/java/org/apache/pinot/perf/ForwardIndexBenchmark.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.perf;\n+\n+import com.google.common.base.Stopwatch;\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.FileOutputStream;\n+import java.io.FileReader;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Random;\n+import java.util.concurrent.TimeUnit;\n+import me.lemire.integercompression.BitPacking;\n+import org.apache.commons.math.util.MathUtils;\n+import org.apache.pinot.core.io.reader.impl.v1.FixedBitSingleValueReader;\n+import org.apache.pinot.core.io.util.FixedBitIntReaderWriter;\n+import org.apache.pinot.core.io.util.FixedBitIntReaderWriterV2;\n+import org.apache.pinot.core.io.util.FixedByteValueReaderWriter;\n+import org.apache.pinot.core.io.util.PinotDataBitSet;\n+import org.apache.pinot.core.io.writer.impl.v1.FixedBitSingleValueWriter;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+public class ForwardIndexBenchmark {", "originalCommit": "a65409f873f3735d34378f4ab3526795f7a919a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NzA1NA==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r429077054", "bodyText": "done", "author": "siddharthteotia", "createdAt": "2020-05-22T07:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2OTM3OQ=="}], "type": "inlineReview", "revised_code": {"commit": "14543bbe7905e74dc2439f06c26ad246c3462fba", "chunk": "diff --git a/pinot-perf/src/main/java/org/apache/pinot/perf/ForwardIndexBenchmark.java b/pinot-perf/src/main/java/org/apache/pinot/perf/ForwardIndexBenchmark.java\ndeleted file mode 100644\nindex 28f9cf16fa..0000000000\n--- a/pinot-perf/src/main/java/org/apache/pinot/perf/ForwardIndexBenchmark.java\n+++ /dev/null\n\n@@ -1,269 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.pinot.perf;\n-\n-import com.google.common.base.Stopwatch;\n-import java.io.BufferedReader;\n-import java.io.BufferedWriter;\n-import java.io.DataInputStream;\n-import java.io.DataOutputStream;\n-import java.io.File;\n-import java.io.FileInputStream;\n-import java.io.FileNotFoundException;\n-import java.io.FileOutputStream;\n-import java.io.FileReader;\n-import java.io.FileWriter;\n-import java.io.IOException;\n-import java.io.RandomAccessFile;\n-import java.nio.ByteBuffer;\n-import java.nio.ByteOrder;\n-import java.nio.channels.FileChannel;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.Random;\n-import java.util.concurrent.TimeUnit;\n-import me.lemire.integercompression.BitPacking;\n-import org.apache.commons.math.util.MathUtils;\n-import org.apache.pinot.core.io.reader.impl.v1.FixedBitSingleValueReader;\n-import org.apache.pinot.core.io.util.FixedBitIntReaderWriter;\n-import org.apache.pinot.core.io.util.FixedBitIntReaderWriterV2;\n-import org.apache.pinot.core.io.util.FixedByteValueReaderWriter;\n-import org.apache.pinot.core.io.util.PinotDataBitSet;\n-import org.apache.pinot.core.io.writer.impl.v1.FixedBitSingleValueWriter;\n-import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n-\n-public class ForwardIndexBenchmark {\n-\n-  static int ROWS = 4_000_000;\n-  static int MAX_VALUE = 11;\n-  static int NUM_BITS = PinotDataBitSet.getNumBitsPerValue(MAX_VALUE);\n-  static File rawFile = new File(\"/Users/steotia/fwd-index.test\");\n-  static File pinotOutFile = new File(rawFile.getAbsolutePath() + \".pinot.fwd\");\n-  static File bitPackedFile = new File(rawFile.getAbsolutePath() + \".fast.fwd\");\n-  static int[] rawValues = new int[ROWS];\n-\n-  static {\n-    rawFile.delete();\n-    pinotOutFile.delete();\n-    bitPackedFile.delete();\n-  }\n-\n-  static void generateRawFile()\n-      throws IOException {\n-\n-    rawFile.delete();\n-    BufferedWriter bw = new BufferedWriter(new FileWriter(rawFile));\n-    Random r = new Random();\n-    for (int i = 0; i < ROWS; i++) {\n-      rawValues[i] =  r.nextInt(MAX_VALUE);\n-      bw.write(\"\" + rawValues[i]);\n-      bw.write(\"\\n\");\n-    }\n-    bw.close();\n-  }\n-\n-  static void generatePinotFwdIndex()\n-      throws Exception {\n-    BufferedReader bfr = new BufferedReader(new FileReader(rawFile));\n-    FixedBitSingleValueWriter fixedBitSingleValueWriter = new FixedBitSingleValueWriter(pinotOutFile, ROWS, NUM_BITS);\n-    String line;\n-    int rowId = 0;\n-    while ((line = bfr.readLine()) != null) {\n-      fixedBitSingleValueWriter.setInt(rowId++, Integer.parseInt(line));\n-    }\n-    fixedBitSingleValueWriter.close();\n-    System.out.println(\"pinotOutFile.length = \" + pinotOutFile.length());\n-  }\n-\n-  static void generatePFORFwdIndex()\n-      throws Exception {\n-    BufferedReader bfr = new BufferedReader(new FileReader(rawFile));\n-    String line;\n-    int rowId = 0;\n-    int[] data = new int[ROWS];\n-    while ((line = bfr.readLine()) != null) {\n-      data[rowId++] = Integer.parseInt(line);\n-    }\n-    int bits = MathUtils.lcm(32, NUM_BITS);\n-    int inputSize = 32;\n-    int outputSize = 32 * NUM_BITS / 32;\n-    int[] raw = new int[inputSize];\n-    int[] bitPacked = new int[outputSize];\n-\n-    int totalNum = (NUM_BITS * ROWS + 31) / Integer.SIZE;\n-\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer\n-        .mapFile(bitPackedFile, false, 0, (long) totalNum * Integer.BYTES, ByteOrder.BIG_ENDIAN, \"bitpacking\");\n-\n-    FixedByteValueReaderWriter readerWriter = new FixedByteValueReaderWriter(pinotDataBuffer);\n-    int counter = 0;\n-    int outputCount = 0;\n-    for (int i = 0; i < data.length; i++) {\n-      raw[counter] = data[i];\n-      if (counter == raw.length - 1 || i == data.length - 1) {\n-        BitPacking.fastpack(raw, 0, bitPacked, 0, NUM_BITS);\n-        for (int j = 0; j < outputSize; j++) {\n-          readerWriter.writeInt(outputCount++, bitPacked[j]);\n-        }\n-        Arrays.fill(bitPacked, 0);\n-        counter = 0;\n-      } else {\n-        counter = counter + 1;\n-      }\n-    }\n-    readerWriter.close();\n-    System.out.println(\"bitPackedFile.length = \" + bitPackedFile.length());\n-  }\n-\n-  private static void compareFormats() throws Exception {\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer.loadBigEndianFile(pinotOutFile);\n-    FileChannel fileChannel = new RandomAccessFile(bitPackedFile, \"r\").getChannel();\n-    ByteBuffer buffer =\n-        fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, bitPackedFile.length()).order(ByteOrder.BIG_ENDIAN);\n-    long length = pinotDataBuffer.size();\n-    for (long i = 0; i < length; i += 4) {\n-      int val1 = pinotDataBuffer.getInt(i);\n-      int val2 = buffer.getInt((int)i);\n-      int v1 = val1 >>> 16;\n-      int v2 = val1 & (-1 >>> 16);\n-      int v3 = val2 >>> 16;\n-      int v4 = val2 & (-1 >>> 16);\n-      //35 -98 71 111 -- pinot buffer\n-      //71 111 35 -98 -- pfor buffer\n-      if (v1 != v4 || v2 != v3) {\n-        throw new IllegalStateException(\"detected different bytes at : \" + i);\n-      }\n-    }\n-  }\n-\n-  static void readRawFile()\n-      throws IOException {\n-    BufferedReader bfr = new BufferedReader(new FileReader(rawFile));\n-    String line;\n-    int rowId = 0;\n-    while ((line = bfr.readLine()) != null) {\n-      rawValues[rowId++] = Integer.parseInt(line);\n-    }\n-  }\n-\n-  static void readPinotFwdIndexSequentialContiguous()\n-      throws IOException {\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer.loadBigEndianFile(pinotOutFile);\n-    FixedBitIntReaderWriter reader = new FixedBitIntReaderWriter(pinotDataBuffer, ROWS, NUM_BITS);\n-    Stopwatch stopwatch = Stopwatch.createUnstarted();\n-    stopwatch.start();\n-    // sequentially unpack 1 integer at a time\n-    for (int startIndex = 0; startIndex < ROWS; startIndex++) {\n-      reader.readInt(startIndex);\n-    }\n-    stopwatch.stop();\n-    System.out.println(\"pinot took: \" + stopwatch.elapsed(TimeUnit.MILLISECONDS) + \"ms\");\n-  }\n-\n-  static void readPinotFwdIndexSequentialContiguousBulk()\n-      throws IOException {\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer.loadBigEndianFile(pinotOutFile);\n-    FixedBitIntReaderWriter reader = new FixedBitIntReaderWriter(pinotDataBuffer, ROWS, NUM_BITS);\n-    Stopwatch stopwatch = Stopwatch.createUnstarted();\n-    int[] unpacked = new int[32];\n-    stopwatch.start();\n-    // sequentially unpack 32 integers at a time\n-    for (int startIndex = 0; startIndex < ROWS; startIndex += 32) {\n-      reader.readInt(startIndex, 32, unpacked);\n-      //checkUnpackedValues(startIndex, unpacked);\n-    }\n-    stopwatch.stop();\n-    System.out.println(\"pinot took: \" + stopwatch.elapsed(TimeUnit.MILLISECONDS) + \"ms\");\n-  }\n-\n-  static void readFastPinotFwdIndexSequentialContiguous()\n-      throws IOException {\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer.loadBigEndianFile(pinotOutFile);\n-    FixedBitIntReaderWriterV2 reader = new FixedBitIntReaderWriterV2(pinotDataBuffer, ROWS, NUM_BITS);\n-    Stopwatch stopwatch = Stopwatch.createUnstarted();\n-    stopwatch.start();\n-    // sequentially unpack 1 integer at a time\n-    for (int startIndex = 0; startIndex < ROWS; startIndex++) {\n-      reader.readInt(startIndex);\n-    }\n-    stopwatch.stop();\n-    System.out.println(\"fast pinot took: \" + stopwatch.elapsed(TimeUnit.MILLISECONDS) + \"ms\");\n-  }\n-\n-  static void readFastPinotFwdIndexSequentialContiguousBulk()\n-      throws IOException {\n-    PinotDataBuffer pinotDataBuffer = PinotDataBuffer.loadBigEndianFile(pinotOutFile);\n-    FixedBitIntReaderWriterV2 reader = new FixedBitIntReaderWriterV2(pinotDataBuffer, ROWS, NUM_BITS);\n-    Stopwatch stopwatch = Stopwatch.createUnstarted();\n-    int[] unpacked = new int[32];\n-    stopwatch.start();\n-    // sequentially unpack 32 integers at a time\n-    for (int startIndex = 0; startIndex < ROWS; startIndex += 32) {\n-      reader.readInt(startIndex, 32, unpacked);\n-      //checkUnpackedValues(startIndex, unpacked);\n-    }\n-    stopwatch.stop();\n-    System.out.println(\"fast pinot took: \" + stopwatch.elapsed(TimeUnit.MILLISECONDS) + \"ms\");\n-  }\n-\n-  private static void checkUnpackedValues(int startIndex, int[] out) {\n-    for (int i = 0; i < 32; i++) {\n-      if (out[i] != rawValues[startIndex + i]) {\n-        throw new IllegalStateException(\"incorrect data: startIndex:\" +startIndex + \" i:\" + i + \" actual:\"+out[i] + \" expected:\"+rawValues[startIndex +i]);\n-      }\n-    }\n-  }\n-\n-  static void readPFORFwdIndex()\n-      throws IOException {\n-    FileChannel fileChannel = new RandomAccessFile(bitPackedFile, \"r\").getChannel();\n-    ByteBuffer buffer =\n-        fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, bitPackedFile.length());\n-    int[] compressed = new int[NUM_BITS];\n-    int[] unpacked = new int[32];\n-    Stopwatch stopwatch = Stopwatch.createUnstarted();\n-    stopwatch.start();\n-    // sequentially unpack 32 integers at a time\n-    for (int i = 0; i < ROWS; i += 32) {\n-      for (int j = 0; j < compressed.length; j++) {\n-        compressed[j] = buffer.getInt();\n-      }\n-      BitPacking.fastunpack(compressed, 0, unpacked, 0, NUM_BITS);\n-      //checkUnpackedValues(i, unpacked);\n-    }\n-    stopwatch.stop();\n-    System.out.println(\"PFOR took: \" + stopwatch.elapsed(TimeUnit.MILLISECONDS)+ \" ms\");\n-  }\n-\n-  public static void main(String[] args)\n-      throws Exception {\n-    System.out.println(\"ROWS = \" + ROWS);\n-    System.out.println(\"NUM_BITS = \" + NUM_BITS);\n-    generateRawFile();\n-    generatePinotFwdIndex();\n-    generatePFORFwdIndex();\n-    //compareFormats();\n-    readRawFile();\n-    readPinotFwdIndexSequentialContiguous();\n-    readFastPinotFwdIndexSequentialContiguous();\n-    readPinotFwdIndexSequentialContiguousBulk();\n-    readFastPinotFwdIndexSequentialContiguousBulk();\n-    readPFORFwdIndex();\n-  }\n-}\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2OTg4Mg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r426969882", "bodyText": "This class needs exhaustive unit tests to ensure all cases are covered.", "author": "mayankshriv", "createdAt": "2020-05-19T00:55:27Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,418 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {", "originalCommit": "a65409f873f3735d34378f4ab3526795f7a919a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NzAyNg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r429077026", "bodyText": "Added several covering all possible cases. Will do another round in a follow-up", "author": "siddharthteotia", "createdAt": "2020-05-22T07:03:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk2OTg4Mg=="}], "type": "inlineReview", "revised_code": {"commit": "f0679eca59cacdd600fce7d43cd95a0e7244ea94", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 5969a12845..b4e5ba624e 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -18,6 +18,7 @@\n  */\n package org.apache.pinot.core.io.util;\n \n+import com.google.common.base.Preconditions;\n import java.io.Closeable;\n import java.io.IOException;\n import org.apache.pinot.core.plan.DocIdSetPlanNode;\n"}}, {"oid": "f0679eca59cacdd600fce7d43cd95a0e7244ea94", "url": "https://github.com/apache/pinot/commit/f0679eca59cacdd600fce7d43cd95a0e7244ea94", "message": "Add unit tests", "committedDate": "2020-05-20T07:29:13Z", "type": "commit"}, {"oid": "3cf936181ec2a69422a355bb328e60f3f4aa1adb", "url": "https://github.com/apache/pinot/commit/3cf936181ec2a69422a355bb328e60f3f4aa1adb", "message": "new", "committedDate": "2020-05-21T06:04:47Z", "type": "commit"}, {"oid": "942ef3ed41afc4b81e3166f80d0ba33c1a27ab78", "url": "https://github.com/apache/pinot/commit/942ef3ed41afc4b81e3166f80d0ba33c1a27ab78", "message": "Improved degree of vectorization and more tests", "committedDate": "2020-05-21T08:26:06Z", "type": "commit"}, {"oid": "45bd3a39fbe6b0a0efeedd5bcf673484fe91ba98", "url": "https://github.com/apache/pinot/commit/45bd3a39fbe6b0a0efeedd5bcf673484fe91ba98", "message": "fix build", "committedDate": "2020-05-21T16:54:18Z", "type": "commit"}, {"oid": "14543bbe7905e74dc2439f06c26ad246c3462fba", "url": "https://github.com/apache/pinot/commit/14543bbe7905e74dc2439f06c26ad246c3462fba", "message": "cleanup", "committedDate": "2020-05-22T07:01:56Z", "type": "commit"}, {"oid": "4798b799496d39b9828fb8a2de0a4bcdba3bdc0c", "url": "https://github.com/apache/pinot/commit/4798b799496d39b9828fb8a2de0a4bcdba3bdc0c", "message": "docs", "committedDate": "2020-05-22T07:18:17Z", "type": "commit"}, {"oid": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "url": "https://github.com/apache/pinot/commit/ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "message": "change file name", "committedDate": "2020-05-22T07:25:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDExNw==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r426940117", "bodyText": "why is this check needed?", "author": "kishoreg", "createdAt": "2020-05-18T23:08:29Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public final class FixedBitIntReaderWriterV2 implements Closeable {\n+  private volatile PinotDataBitSetV2 _dataBitSet;\n+  private final int _numBitsPerValue;\n+\n+  public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n+    Preconditions\n+        .checkState(dataBuffer.size() == (int) (((long) numValues * numBitsPerValue + Byte.SIZE - 1) / Byte.SIZE));\n+    _dataBitSet = PinotDataBitSetV2.createBitSet(dataBuffer, numBitsPerValue);\n+    _numBitsPerValue = numBitsPerValue;\n+  }\n+\n+  /**\n+   * Read dictionaryId for a particular docId\n+   * @param index docId to get the dictionaryId for\n+   * @return dictionaryId\n+   */\n+  public int readInt(int index) {\n+    return _dataBitSet.readInt(index);\n+  }\n+\n+  /**\n+   * Array based API to read dictionaryIds for a contiguous\n+   * range of docIds starting at startDocId for a given length\n+   * @param startDocId docId range start\n+   * @param length length of contiguous docId range\n+   * @param buffer out buffer to read dictionaryIds into\n+   */\n+  public void readInt(int startDocId, int length, int[] buffer) {\n+    _dataBitSet.readInt(startDocId, length, buffer);\n+  }\n+\n+  /**\n+   * Array based API to read dictionaryIds for an array of docIds\n+   * which are monotonically increasing but not necessarily contiguous\n+   * @param docIds array of docIds to read the dictionaryIds for\n+   * @param docIdStartIndex start index in docIds array\n+   * @param docIdLength length to process in docIds array\n+   * @param values out array to store the dictionaryIds into\n+   * @param valuesStartIndex start index in values array\n+   */\n+  public void readValues(int[] docIds, int docIdStartIndex, int docIdLength, int[] values, int valuesStartIndex) {\n+    int docIdEndIndex = docIdStartIndex + docIdLength - 1;\n+    if (shouldBulkRead(docIds, docIdStartIndex, docIdEndIndex)) {\n+      _dataBitSet.readInt(docIds, docIdStartIndex, docIdLength, values, valuesStartIndex);\n+    } else {\n+      for (int i = docIdStartIndex; i <= docIdEndIndex; i++) {\n+        values[valuesStartIndex++] = _dataBitSet.readInt(docIds[i]);\n+      }\n+    }\n+  }\n+\n+  private boolean shouldBulkRead(int[] docIds, int startIndex, int endIndex) {\n+    int numDocsToRead = endIndex - startIndex + 1;\n+    int docIdRange = docIds[endIndex] - docIds[startIndex] + 1;\n+    if (docIdRange > DocIdSetPlanNode.MAX_DOC_PER_CALL) {", "originalCommit": "a65409f873f3735d34378f4ab3526795f7a919a5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDIwNTE4Nw==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430205187", "bodyText": "I think this is coming from the previous commit. The latest version of the PE doesn't have this check.", "author": "siddharthteotia", "createdAt": "2020-05-26T07:21:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDExNw=="}], "type": "inlineReview", "revised_code": {"commit": "f0679eca59cacdd600fce7d43cd95a0e7244ea94", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\nindex d9ceec5525..e66154c067 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n\n@@ -27,13 +27,11 @@ import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n \n public final class FixedBitIntReaderWriterV2 implements Closeable {\n   private volatile PinotDataBitSetV2 _dataBitSet;\n-  private final int _numBitsPerValue;\n \n   public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n     Preconditions\n         .checkState(dataBuffer.size() == (int) (((long) numValues * numBitsPerValue + Byte.SIZE - 1) / Byte.SIZE));\n     _dataBitSet = PinotDataBitSetV2.createBitSet(dataBuffer, numBitsPerValue);\n-    _numBitsPerValue = numBitsPerValue;\n   }\n \n   /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjU4MA==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430592580", "bodyText": "You don't need to make this volatile and set it to null in close(). This issue has been addressed in #4764", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:42:48Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public final class FixedBitIntReaderWriterV2 implements Closeable {\n+  private volatile PinotDataBitSetV2 _dataBitSet;", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNDU4Ng==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430824586", "bodyText": "done", "author": "siddharthteotia", "createdAt": "2020-05-27T02:35:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjU4MA=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\nindex e6a9041430..55c8c942f1 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/FixedBitIntReaderWriterV2.java\n\n@@ -21,12 +21,11 @@ package org.apache.pinot.core.io.util;\n import com.google.common.base.Preconditions;\n import java.io.Closeable;\n import java.io.IOException;\n-import org.apache.pinot.core.plan.DocIdSetPlanNode;\n import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n \n \n public final class FixedBitIntReaderWriterV2 implements Closeable {\n-  private volatile PinotDataBitSetV2 _dataBitSet;\n+  private PinotDataBitSetV2 _dataBitSet;\n \n   public FixedBitIntReaderWriterV2(PinotDataBuffer dataBuffer, int numValues, int numBitsPerValue) {\n     Preconditions\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NDY1Nw==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430594657", "bodyText": "Don't limit it to dictId and docId in the javadoc. The BitSet is a general reader/writer which can be used for different purposes.", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:46:18Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNDU5Nw==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430824597", "bodyText": "done", "author": "siddharthteotia", "createdAt": "2020-05-27T02:35:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NDY1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzgwMA==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430597800", "bodyText": "For performance concern, we can remove the docIdsStartIndex and outpos and always assume they are 0", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:51:40Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient\n+   * because of simplified bitmath.\n+   * @param index docId\n+   * @return unpacked dictId\n+   */\n+  public abstract int readInt(int index);\n+\n+  /**\n+   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * and length. This uses vectorization as much as possible for all the aligned\n+   * reads and also takes care of the small byte-sized window of unaligned read.\n+   * @param startIndex start docId\n+   * @param length length\n+   * @param out out array to store the unpacked dictIds\n+   */\n+  public abstract void readInt(int startIndex, int length, int[] out);\n+\n+  /**\n+   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * contiguous. So there could be gaps in the array:\n+   * e.g: [1, 3, 7, 9, 11, 12]\n+   * The actual read is done by the previous API since that is efficient\n+   * as it exploits contiguity and uses vectorization. However, since\n+   * the out[] array has to be correctly populated with the unpacked dictId\n+   * for each docId, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked dictId into the out array throwing away\n+   * the unnecessary dictIds unpacked as part of contiguous read\n+   * @param docIds docIds array\n+   * @param docIdsStartIndex starting index in the docIds array\n+   * @param length length to read (number of docIds to read in the array)\n+   * @param out out array to store the unpacked dictIds\n+   * @param outpos starting index in the out array\n+   */\n+  public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyODMxMg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430828312", "bodyText": "See the outer API in FixedBitIntReaderWriterV2 that calls this. That API tries to judge sparseness before deciding to do bulk read. The decision is made on a chunk of values at a time and this bulk API is called for each chunk.", "author": "siddharthteotia", "createdAt": "2020-05-27T02:50:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NzgwMA=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5OTQ2Mg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430599462", "bodyText": "This won't work because docIds might not be contiguous and endDocId - startDocId + 1 could be much larger than DocIdSetPlanNode.MAX_DOC_PER_CALL. Also, we should not always do such bulk read, docIds can be very sparse.", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:54:28Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient\n+   * because of simplified bitmath.\n+   * @param index docId\n+   * @return unpacked dictId\n+   */\n+  public abstract int readInt(int index);\n+\n+  /**\n+   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * and length. This uses vectorization as much as possible for all the aligned\n+   * reads and also takes care of the small byte-sized window of unaligned read.\n+   * @param startIndex start docId\n+   * @param length length\n+   * @param out out array to store the unpacked dictIds\n+   */\n+  public abstract void readInt(int startIndex, int length, int[] out);\n+\n+  /**\n+   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * contiguous. So there could be gaps in the array:\n+   * e.g: [1, 3, 7, 9, 11, 12]\n+   * The actual read is done by the previous API since that is efficient\n+   * as it exploits contiguity and uses vectorization. However, since\n+   * the out[] array has to be correctly populated with the unpacked dictId\n+   * for each docId, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked dictId into the out array throwing away\n+   * the unnecessary dictIds unpacked as part of contiguous read\n+   * @param docIds docIds array\n+   * @param docIdsStartIndex starting index in the docIds array\n+   * @param length length to read (number of docIds to read in the array)\n+   * @param out out array to store the unpacked dictIds\n+   * @param outpos starting index in the out array\n+   */\n+  public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n+    int startDocId = docIds[docIdsStartIndex];\n+    int endDocId = docIds[docIdsStartIndex + length - 1];\n+    int[] dictIds = THREAD_LOCAL_DICT_IDS.get();\n+    // do a contiguous bulk read\n+    readInt(startDocId, endDocId - startDocId + 1, dictIds);", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNTAyMg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430825022", "bodyText": "See the outer API in FixedBitIntReaderWriterV2 that calls this. That API tries to judge sparseness before deciding to do bulk read. The decision is made on a chunk of values at a time", "author": "siddharthteotia", "createdAt": "2020-05-27T02:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5OTQ2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMDg0Mg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430600842", "bodyText": "Use long to index the dataBuffer so that we can handle big buffer (> 2G)", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:56:40Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient\n+   * because of simplified bitmath.\n+   * @param index docId\n+   * @return unpacked dictId\n+   */\n+  public abstract int readInt(int index);\n+\n+  /**\n+   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * and length. This uses vectorization as much as possible for all the aligned\n+   * reads and also takes care of the small byte-sized window of unaligned read.\n+   * @param startIndex start docId\n+   * @param length length\n+   * @param out out array to store the unpacked dictIds\n+   */\n+  public abstract void readInt(int startIndex, int length, int[] out);\n+\n+  /**\n+   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * contiguous. So there could be gaps in the array:\n+   * e.g: [1, 3, 7, 9, 11, 12]\n+   * The actual read is done by the previous API since that is efficient\n+   * as it exploits contiguity and uses vectorization. However, since\n+   * the out[] array has to be correctly populated with the unpacked dictId\n+   * for each docId, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked dictId into the out array throwing away\n+   * the unnecessary dictIds unpacked as part of contiguous read\n+   * @param docIds docIds array\n+   * @param docIdsStartIndex starting index in the docIds array\n+   * @param length length to read (number of docIds to read in the array)\n+   * @param out out array to store the unpacked dictIds\n+   * @param outpos starting index in the out array\n+   */\n+  public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n+    int startDocId = docIds[docIdsStartIndex];\n+    int endDocId = docIds[docIdsStartIndex + length - 1];\n+    int[] dictIds = THREAD_LOCAL_DICT_IDS.get();\n+    // do a contiguous bulk read\n+    readInt(startDocId, endDocId - startDocId + 1, dictIds);\n+    out[outpos] = dictIds[0];\n+    // set the unpacked dictId correctly. this is needed since there could\n+    // be gaps and some dictIds may have to be thrown/ignored.\n+    for (int i = 1; i < length; i++) {\n+      out[outpos + i] = dictIds[docIds[docIdsStartIndex + i] - startDocId];\n+    }\n+  }\n+\n+  public static PinotDataBitSetV2 createBitSet(PinotDataBuffer pinotDataBuffer, int numBitsPerValue) {\n+    switch (numBitsPerValue) {\n+      case 2:\n+        return new Bit2Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 4:\n+        return new Bit4Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 8:\n+        return new Bit8Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 16:\n+        return new Bit16Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 32:\n+        return new RawInt(pinotDataBuffer, numBitsPerValue);\n+      default:\n+        throw new UnsupportedOperationException(numBitsPerValue + \"not supported by PinotDataBitSetV2\");\n+    }\n+  }\n+\n+  public static class Bit2Encoded extends PinotDataBitSetV2 {\n+    Bit2Encoded(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      long bitOffset = (long) index * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDgyNzkzMw==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430827933", "bodyText": "done", "author": "siddharthteotia", "createdAt": "2020-05-27T02:48:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMDg0Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMTYzNg==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430601636", "bodyText": "We might also have 0 (all zeros) and 1 (0/1)", "author": "Jackie-Jiang", "createdAt": "2020-05-26T17:57:50Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient\n+   * because of simplified bitmath.\n+   * @param index docId\n+   * @return unpacked dictId\n+   */\n+  public abstract int readInt(int index);\n+\n+  /**\n+   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * and length. This uses vectorization as much as possible for all the aligned\n+   * reads and also takes care of the small byte-sized window of unaligned read.\n+   * @param startIndex start docId\n+   * @param length length\n+   * @param out out array to store the unpacked dictIds\n+   */\n+  public abstract void readInt(int startIndex, int length, int[] out);\n+\n+  /**\n+   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * contiguous. So there could be gaps in the array:\n+   * e.g: [1, 3, 7, 9, 11, 12]\n+   * The actual read is done by the previous API since that is efficient\n+   * as it exploits contiguity and uses vectorization. However, since\n+   * the out[] array has to be correctly populated with the unpacked dictId\n+   * for each docId, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked dictId into the out array throwing away\n+   * the unnecessary dictIds unpacked as part of contiguous read\n+   * @param docIds docIds array\n+   * @param docIdsStartIndex starting index in the docIds array\n+   * @param length length to read (number of docIds to read in the array)\n+   * @param out out array to store the unpacked dictIds\n+   * @param outpos starting index in the out array\n+   */\n+  public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n+    int startDocId = docIds[docIdsStartIndex];\n+    int endDocId = docIds[docIdsStartIndex + length - 1];\n+    int[] dictIds = THREAD_LOCAL_DICT_IDS.get();\n+    // do a contiguous bulk read\n+    readInt(startDocId, endDocId - startDocId + 1, dictIds);\n+    out[outpos] = dictIds[0];\n+    // set the unpacked dictId correctly. this is needed since there could\n+    // be gaps and some dictIds may have to be thrown/ignored.\n+    for (int i = 1; i < length; i++) {\n+      out[outpos + i] = dictIds[docIds[docIdsStartIndex + i] - startDocId];\n+    }\n+  }\n+\n+  public static PinotDataBitSetV2 createBitSet(PinotDataBuffer pinotDataBuffer, int numBitsPerValue) {\n+    switch (numBitsPerValue) {", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMTM2Ng==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r430611366", "bodyText": "Do not close the buffer (see #5400)", "author": "Jackie-Jiang", "createdAt": "2020-05-26T18:12:46Z", "path": "pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java", "diffHunk": "@@ -0,0 +1,516 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.pinot.core.io.util;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import org.apache.pinot.core.plan.DocIdSetPlanNode;\n+import org.apache.pinot.core.segment.memory.PinotDataBuffer;\n+\n+\n+public abstract class PinotDataBitSetV2 implements Closeable {\n+  private static final int BYTE_MASK = 0xFF;\n+  static final int MAX_VALUES_UNPACKED_SINGLE_ALIGNED_READ = 16; // comes from 2-bit encoding\n+\n+  private static final ThreadLocal<int[]> THREAD_LOCAL_DICT_IDS =\n+      ThreadLocal.withInitial(() -> new int[DocIdSetPlanNode.MAX_DOC_PER_CALL]);\n+\n+  protected PinotDataBuffer _dataBuffer;\n+  protected int _numBitsPerValue;\n+\n+  /**\n+   * Unpack single dictId at the given docId. This is efficient\n+   * because of simplified bitmath.\n+   * @param index docId\n+   * @return unpacked dictId\n+   */\n+  public abstract int readInt(int index);\n+\n+  /**\n+   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * and length. This uses vectorization as much as possible for all the aligned\n+   * reads and also takes care of the small byte-sized window of unaligned read.\n+   * @param startIndex start docId\n+   * @param length length\n+   * @param out out array to store the unpacked dictIds\n+   */\n+  public abstract void readInt(int startIndex, int length, int[] out);\n+\n+  /**\n+   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * contiguous. So there could be gaps in the array:\n+   * e.g: [1, 3, 7, 9, 11, 12]\n+   * The actual read is done by the previous API since that is efficient\n+   * as it exploits contiguity and uses vectorization. However, since\n+   * the out[] array has to be correctly populated with the unpacked dictId\n+   * for each docId, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked dictId into the out array throwing away\n+   * the unnecessary dictIds unpacked as part of contiguous read\n+   * @param docIds docIds array\n+   * @param docIdsStartIndex starting index in the docIds array\n+   * @param length length to read (number of docIds to read in the array)\n+   * @param out out array to store the unpacked dictIds\n+   * @param outpos starting index in the out array\n+   */\n+  public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n+    int startDocId = docIds[docIdsStartIndex];\n+    int endDocId = docIds[docIdsStartIndex + length - 1];\n+    int[] dictIds = THREAD_LOCAL_DICT_IDS.get();\n+    // do a contiguous bulk read\n+    readInt(startDocId, endDocId - startDocId + 1, dictIds);\n+    out[outpos] = dictIds[0];\n+    // set the unpacked dictId correctly. this is needed since there could\n+    // be gaps and some dictIds may have to be thrown/ignored.\n+    for (int i = 1; i < length; i++) {\n+      out[outpos + i] = dictIds[docIds[docIdsStartIndex + i] - startDocId];\n+    }\n+  }\n+\n+  public static PinotDataBitSetV2 createBitSet(PinotDataBuffer pinotDataBuffer, int numBitsPerValue) {\n+    switch (numBitsPerValue) {\n+      case 2:\n+        return new Bit2Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 4:\n+        return new Bit4Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 8:\n+        return new Bit8Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 16:\n+        return new Bit16Encoded(pinotDataBuffer, numBitsPerValue);\n+      case 32:\n+        return new RawInt(pinotDataBuffer, numBitsPerValue);\n+      default:\n+        throw new UnsupportedOperationException(numBitsPerValue + \"not supported by PinotDataBitSetV2\");\n+    }\n+  }\n+\n+  public static class Bit2Encoded extends PinotDataBitSetV2 {\n+    Bit2Encoded(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      long bitOffset = (long) index * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      int val = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+      bitOffset = bitOffset & 7;\n+      return  (val >>> (6 - bitOffset)) & 3;\n+    }\n+\n+    @Override\n+    public void readInt(int startIndex, int length, int[] out) {\n+      long bitOffset = (long) startIndex * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      bitOffset = bitOffset & 7;\n+      int packed = 0;\n+      int i = 0;\n+\n+      /*\n+       * Bytes are read as follows to get maximum vectorization\n+       *\n+       * [1 byte] - read from either the 2nd/4th/6th bit to 7th bit to unpack 1/2/3 integers\n+       * [chunks of 4 bytes] - read 4 bytes at a time to unpack 16 integers\n+       * [1 chunk of 2 bytes] - read 2 bytes to unpack 8 integers\n+       * [1 byte] - read the byte to unpack 4 integers\n+       * [1 byte] - unpack 1/2/3 integers from first 2/4/6 bits\n+       */\n+\n+      // unaligned read within a byte\n+      if (bitOffset != 0) {\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        if (bitOffset == 2) {\n+          // unpack 3 integers from bits 2-7\n+          out[0] = (packed >>> 4) & 3;\n+          out[1] = (packed >>> 2) & 3;\n+          out[2] = packed & 3;\n+          i = 3;\n+          length -= 3;\n+        }\n+        else if (bitOffset == 4) {\n+          // unpack 2 integers from bits 4 to 7\n+          out[0] = (packed >>> 2) & 3;\n+          out[1] = packed & 3;\n+          i = 2;\n+          length -= 2;\n+        } else {\n+          // unpack integer from bits 6 to 7\n+          out[0] = packed & 3;\n+          i = 1;\n+          length -= 1;\n+        }\n+        byteOffset++;\n+      }\n+\n+      // aligned reads at 4-byte boundary to unpack 16 integers\n+      while (length >= 16) {\n+        packed = _dataBuffer.getInt(byteOffset);\n+        out[i] = packed >>> 30;\n+        out[i + 1] = (packed >>> 28) & 3;\n+        out[i + 2] = (packed >>> 26) & 3;\n+        out[i + 3] = (packed >>> 24) & 3;\n+        out[i + 4] = (packed >>> 22) & 3;\n+        out[i + 5] = (packed >>> 20) & 3;\n+        out[i + 6] = (packed >>> 18) & 3;\n+        out[i + 7] = (packed >>> 16) & 3;\n+        out[i + 8] = (packed >>> 14) & 3;\n+        out[i + 9] = (packed >>> 12) & 3;\n+        out[i + 10] = (packed >>> 10) & 3;\n+        out[i + 11] = (packed >>> 8) & 3;\n+        out[i + 12] = (packed >>> 6) & 3;\n+        out[i + 13] = (packed >>> 4) & 3;\n+        out[i + 14] = (packed >>> 2) & 3;\n+        out[i + 15] = packed & 3;\n+        length -= 16;\n+        byteOffset += 4;\n+        i += 16;\n+      }\n+\n+      if (length >= 8) {\n+        packed = (int)_dataBuffer.getShort(byteOffset) & 0xffff;\n+        out[i] = (packed >>> 14) & 3;\n+        out[i + 1] = (packed >>> 12) & 3;\n+        out[i + 2] = (packed >>> 10) & 3;\n+        out[i + 3] = (packed >>> 8) & 3;\n+        out[i + 4] = (packed >>> 6) & 3;\n+        out[i + 5] = (packed >>> 4) & 3;\n+        out[i + 6] = (packed >>> 2) & 3;\n+        out[i + 7] = packed & 3;\n+        length -= 8;\n+        byteOffset += 2;\n+        i += 8;\n+      }\n+\n+      // aligned read at byte boundary to unpack 4 integers\n+      if (length >= 4) {\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        out[i] = packed >>> 6;\n+        out[i + 1] = (packed >>> 4) & 3;\n+        out[i + 2] = (packed >>> 2) & 3;\n+        out[i + 3] = packed & 3;\n+        length -= 4;\n+        byteOffset++;\n+        i += 4;\n+      }\n+\n+      // handle spill-over\n+\n+      if (length > 0) {\n+        // unpack from bits 0-1\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        out[i] = packed >>> 6;\n+        length--;\n+      }\n+\n+      if (length > 0) {\n+        // unpack from bits 2-3\n+        out[i + 1] = (packed >>> 4) & 3;\n+        length--;\n+      }\n+\n+      if (length > 0) {\n+        // unpack from bits 4-5\n+        out[i + 2] = (packed >>> 2) & 3;\n+        length--;\n+      }\n+    }\n+  }\n+\n+  public static class Bit4Encoded extends PinotDataBitSetV2 {\n+    Bit4Encoded(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      long bitOffset = (long) index * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      int val = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+      bitOffset = bitOffset & 7;\n+      return (bitOffset == 0) ? val >>> 4 : val & 0xf;\n+    }\n+\n+    @Override\n+    public void readInt(int startIndex, int length, int[] out) {\n+      long bitOffset = (long) startIndex * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      bitOffset = bitOffset & 7;\n+      int packed = 0;\n+      int i = 0;\n+\n+      /*\n+       * Bytes are read as follows to get maximum vectorization\n+       *\n+       * [1 byte] - read from the 4th bit to 7th bit to unpack 1 integer\n+       * [chunks of 4 bytes] - read 4 bytes at a time to unpack 8 integers\n+       * [1 chunk of 2 bytes] - read 2 bytes to unpack 4 integers\n+       * [1 byte] - unpack 1 integer from first 4 bits\n+       */\n+\n+      // unaligned read within a byte from bits 4-7\n+      if (bitOffset != 0) {\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        out[0] = packed & 0xf;\n+        i = 1;\n+        byteOffset++;\n+        length--;\n+      }\n+\n+      // aligned read at 4-byte boundary to unpack 8 integers\n+      while (length >= 8) {\n+        packed = _dataBuffer.getInt(byteOffset);\n+        out[i] = packed >>> 28;\n+        out[i + 1] = (packed >>> 24) & 0xf;\n+        out[i + 2] = (packed >>> 20) & 0xf;\n+        out[i + 3] = (packed >>> 16) & 0xf;\n+        out[i + 4] = (packed >>> 12) & 0xf;\n+        out[i + 5] = (packed >>> 8) & 0xf;\n+        out[i + 6] = (packed >>> 4) & 0xf;\n+        out[i + 7] = packed & 0xf;\n+        length -= 8;\n+        i += 8;\n+        byteOffset += 4;\n+      }\n+\n+      // aligned read at 2-byte boundary to unpack 4 integers\n+      if (length >= 4) {\n+        packed = (int)_dataBuffer.getShort(byteOffset) & 0xffff;\n+        out[i] = (packed >>> 12) & 0xf;\n+        out[i + 1] = (packed >>> 8) & 0xf;\n+        out[i + 2] = (packed >>> 4) & 0xf;\n+        out[i + 3] = packed & 0xf;\n+        length -= 4;\n+        i += 4;\n+        byteOffset += 2;\n+      }\n+\n+      // aligned read at byte boundary to unpack 2 integers\n+      if (length >= 2) {\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        out[i] = packed >>> 4;\n+        out[i + 1] = packed & 0xf;\n+        length -= 2;\n+        i += 2;\n+        byteOffset++;\n+      }\n+\n+      // handle spill over -- unpack from bits 0-3\n+      if (length > 0) {\n+        packed = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        out[i] = packed >>> 4;\n+        length--;\n+      }\n+    }\n+  }\n+\n+  public static class Bit8Encoded extends PinotDataBitSetV2 {\n+    Bit8Encoded(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      long bitOffset = (long) index * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      return ((int)_dataBuffer.getByte(byteOffset)) & 0xff;\n+    }\n+\n+    @Override\n+    public void readInt(int startIndex, int length, int[] out) {\n+      long bitOffset = (long) startIndex * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      int i = 0;\n+      int packed = 0;\n+\n+      /*\n+       * Bytes are read as follows to get maximum vectorization\n+       *\n+       * [chunks of 4 bytes] - read 4 bytes at a time to unpack 4 integers\n+       * [1 chunk of 2 bytes] - read 2 bytes to unpack 4 integers\n+       * [1 byte] - unpack 1 integer from first 4 bits\n+       */\n+\n+      // aligned read at 4-byte boundary to unpack 4 integers\n+      while (length >= 4) {\n+        packed = _dataBuffer.getInt(byteOffset);\n+        out[i] = packed >>> 24;\n+        out[i + 1] = (packed >>> 16) & 0xff;\n+        out[i + 2] = (packed >>> 8) & 0xff;\n+        out[i + 3] = packed & 0xff;\n+        length -= 4;\n+        byteOffset += 4;\n+        i += 4;\n+      }\n+\n+      // aligned read at 2-byte boundary to unpack 2 integers\n+      if (length >= 2) {\n+        packed = (int)_dataBuffer.getShort(byteOffset) & 0xffff;\n+        out[i] = (packed >>> 8) & 0xff;\n+        out[i + 1] = packed & 0xff;\n+        length -= 2;\n+        byteOffset += 2;\n+        i += 2;\n+      }\n+\n+      // handle spill over at byte boundary to unpack 1 integer\n+      if (length > 0) {\n+        out[i] = (int)_dataBuffer.getByte(byteOffset) & 0xff;\n+        length--;\n+      }\n+    }\n+  }\n+\n+  public static class Bit16Encoded extends PinotDataBitSetV2 {\n+    Bit16Encoded(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      long bitOffset = (long) index * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      return ((int)_dataBuffer.getShort(byteOffset)) & 0xffff;\n+    }\n+\n+    @Override\n+    public void readInt(int startIndex, int length, int[] out) {\n+      long bitOffset = (long) startIndex * _numBitsPerValue;\n+      int byteOffset = (int) (bitOffset / Byte.SIZE);\n+      int i = 0;\n+      int packed;\n+\n+      /*\n+       * Bytes are read as follows to get maximum vectorization\n+       *\n+       * [chunks of 4 bytes] - read 4 bytes at a time to unpack 2 integers\n+       * [1 chunk of 2 bytes] - read 2 bytes to unpack 1 integer\n+       */\n+\n+      // aligned reads at 4-byte boundary to unpack 2 integers\n+      while (length >= 2) {\n+        packed = _dataBuffer.getInt(byteOffset);\n+        out[i] = packed >>> 16;\n+        out[i + 1] = packed & 0xffff;\n+        length -= 2;\n+        i += 2;\n+        byteOffset += 4;\n+      }\n+\n+      // handle spill over at 2-byte boundary to unpack 1 integer\n+      if (length > 0) {\n+        out[i] = (int)_dataBuffer.getShort(byteOffset) & 0xffff;\n+        length--;\n+      }\n+    }\n+  }\n+\n+  public static class RawInt extends PinotDataBitSetV2 {\n+    RawInt(PinotDataBuffer dataBuffer, int numBits) {\n+      _dataBuffer = dataBuffer;\n+      _numBitsPerValue = numBits;\n+    }\n+\n+    @Override\n+    public int readInt(int index) {\n+      return _dataBuffer.getInt(index * Integer.BYTES);\n+    }\n+\n+    @Override\n+    public void readInt(int startIndex, int length, int[] out) {\n+      int byteOffset = startIndex * Integer.BYTES;\n+      for (int i = 0; i < length; i++) {\n+        out[i] = _dataBuffer.getInt(byteOffset);\n+        byteOffset += 4;\n+      }\n+    }\n+  }\n+\n+  protected void writeInt(int index, int value) {\n+    long bitOffset = (long) index * _numBitsPerValue;\n+    int byteOffset = (int) (bitOffset / Byte.SIZE);\n+    int bitOffsetInFirstByte = (int) (bitOffset % Byte.SIZE);\n+\n+    int firstByte = _dataBuffer.getByte(byteOffset);\n+\n+    int firstByteMask = BYTE_MASK >>> bitOffsetInFirstByte;\n+    int numBitsLeft = _numBitsPerValue - (Byte.SIZE - bitOffsetInFirstByte);\n+    if (numBitsLeft <= 0) {\n+      // The value is inside the first byte\n+      firstByteMask &= BYTE_MASK << -numBitsLeft;\n+      _dataBuffer.putByte(byteOffset, (byte) ((firstByte & ~firstByteMask) | (value << -numBitsLeft)));\n+    } else {\n+      // The value is in multiple bytes\n+      _dataBuffer\n+          .putByte(byteOffset, (byte) ((firstByte & ~firstByteMask) | ((value >>> numBitsLeft) & firstByteMask)));\n+      while (numBitsLeft > Byte.SIZE) {\n+        numBitsLeft -= Byte.SIZE;\n+        _dataBuffer.putByte(++byteOffset, (byte) (value >> numBitsLeft));\n+      }\n+      int lastByte = _dataBuffer.getByte(++byteOffset);\n+      _dataBuffer.putByte(byteOffset,\n+          (byte) ((lastByte & (BYTE_MASK >>> numBitsLeft)) | (value << (Byte.SIZE - numBitsLeft))));\n+    }\n+  }\n+\n+  public void writeInt(int startIndex, int length, int[] values) {\n+    long startBitOffset = (long) startIndex * _numBitsPerValue;\n+    int byteOffset = (int) (startBitOffset / Byte.SIZE);\n+    int bitOffsetInFirstByte = (int) (startBitOffset % Byte.SIZE);\n+\n+    int firstByte = _dataBuffer.getByte(byteOffset);\n+\n+    for (int i = 0; i < length; i++) {\n+      int value = values[i];\n+      if (bitOffsetInFirstByte == Byte.SIZE) {\n+        bitOffsetInFirstByte = 0;\n+        firstByte = _dataBuffer.getByte(++byteOffset);\n+      }\n+      int firstByteMask = BYTE_MASK >>> bitOffsetInFirstByte;\n+      int numBitsLeft = _numBitsPerValue - (Byte.SIZE - bitOffsetInFirstByte);\n+      if (numBitsLeft <= 0) {\n+        // The value is inside the first byte\n+        firstByteMask &= BYTE_MASK << -numBitsLeft;\n+        firstByte = ((firstByte & ~firstByteMask) | (value << -numBitsLeft));\n+        _dataBuffer.putByte(byteOffset, (byte) firstByte);\n+        bitOffsetInFirstByte = Byte.SIZE + numBitsLeft;\n+      } else {\n+        // The value is in multiple bytes\n+        _dataBuffer\n+            .putByte(byteOffset, (byte) ((firstByte & ~firstByteMask) | ((value >>> numBitsLeft) & firstByteMask)));\n+        while (numBitsLeft > Byte.SIZE) {\n+          numBitsLeft -= Byte.SIZE;\n+          _dataBuffer.putByte(++byteOffset, (byte) (value >> numBitsLeft));\n+        }\n+        int lastByte = _dataBuffer.getByte(++byteOffset);\n+        firstByte = (lastByte & (0xFF >>> numBitsLeft)) | (value << (Byte.SIZE - numBitsLeft));\n+        _dataBuffer.putByte(byteOffset, (byte) firstByte);\n+        bitOffsetInFirstByte = numBitsLeft;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void close()\n+      throws IOException {\n+    _dataBuffer.close();", "originalCommit": "ab19454de0c1c3d3e1fd25e62f682d5cd4f5d210", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE2Mzk3NQ==", "url": "https://github.com/apache/pinot/pull/5409#discussion_r432163975", "bodyText": "done", "author": "siddharthteotia", "createdAt": "2020-05-28T22:43:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMTM2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "7e84e41e7004b4320787011af38e135cd9de3734", "chunk": "diff --git a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\nindex 8ab377a618..978ae386e8 100644\n--- a/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n+++ b/pinot-core/src/main/java/org/apache/pinot/core/io/util/PinotDataBitSetV2.java\n\n@@ -35,37 +35,37 @@ public abstract class PinotDataBitSetV2 implements Closeable {\n   protected int _numBitsPerValue;\n \n   /**\n-   * Unpack single dictId at the given docId. This is efficient\n+   * Decode integers starting at a given index. This is efficient\n    * because of simplified bitmath.\n    * @param index docId\n-   * @return unpacked dictId\n+   * @return unpacked integer\n    */\n-  public abstract int readInt(int index);\n+  public abstract int readInt(long index);\n \n   /**\n-   * Unpack dictIds for a contiguous range of docIds represented by startIndex\n+   * Decode integers for a contiguous range of indexes represented by startIndex\n    * and length. This uses vectorization as much as possible for all the aligned\n    * reads and also takes care of the small byte-sized window of unaligned read.\n    * @param startIndex start docId\n    * @param length length\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the unpacked integers\n    */\n-  public abstract void readInt(int startIndex, int length, int[] out);\n+  public abstract void readInt(long startIndex, int length, int[] out);\n \n   /**\n-   * Unpack dictIds for an array of docIds[] which is not necessarily\n+   * Decode integers for an array of indexes which is not necessarily\n    * contiguous. So there could be gaps in the array:\n    * e.g: [1, 3, 7, 9, 11, 12]\n    * The actual read is done by the previous API since that is efficient\n    * as it exploits contiguity and uses vectorization. However, since\n-   * the out[] array has to be correctly populated with the unpacked dictId\n-   * for each docId, a post-processing step is needed after the bulk contiguous\n-   * read to correctly set the unpacked dictId into the out array throwing away\n-   * the unnecessary dictIds unpacked as part of contiguous read\n-   * @param docIds docIds array\n+   * the out[] array has to be correctly populated with the unpacked integer\n+   * for each index, a post-processing step is needed after the bulk contiguous\n+   * read to correctly set the unpacked integer into the out array throwing away\n+   * the unnecessary values decoded as part of contiguous read\n+   * @param docIds index array\n    * @param docIdsStartIndex starting index in the docIds array\n    * @param length length to read (number of docIds to read in the array)\n-   * @param out out array to store the unpacked dictIds\n+   * @param out out array to store the decoded integers\n    * @param outpos starting index in the out array\n    */\n   public void readInt(int[] docIds, int docIdsStartIndex, int length, int[] out, int outpos) {\n"}}, {"oid": "7e84e41e7004b4320787011af38e135cd9de3734", "url": "https://github.com/apache/pinot/commit/7e84e41e7004b4320787011af38e135cd9de3734", "message": "address review comments and add more benchmarks", "committedDate": "2020-05-28T22:42:41Z", "type": "commit"}]}