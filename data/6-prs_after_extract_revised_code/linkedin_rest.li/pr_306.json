{"pr_number": 306, "pr_title": "Add protobuf stream data decoder", "pr_createdAt": "2020-05-21T23:15:56Z", "pr_url": "https://github.com/linkedin/rest.li/pull/306", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDY5OA==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984698", "bodyText": "final?", "author": "karthikrg", "createdAt": "2020-05-22T00:33:44Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 744140ec0..87a5c7d81 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -49,7 +49,7 @@ import static com.linkedin.data.codec.ProtobufDataCodec.*;\n class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n {\n \n-  private SymbolTable _symbolTable;\n+  private final SymbolTable _symbolTable;\n \n   protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n   {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDg3Ng==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984876", "bodyText": "final", "author": "karthikrg", "createdAt": "2020-05-22T00:34:29Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 744140ec0..87a5c7d81 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -49,7 +49,7 @@ import static com.linkedin.data.codec.ProtobufDataCodec.*;\n class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n {\n \n-  private SymbolTable _symbolTable;\n+  private final SymbolTable _symbolTable;\n \n   protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n   {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NDkzOQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428984939", "bodyText": "final", "author": "karthikrg", "createdAt": "2020-05-22T00:34:44Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;\n+\n+    private Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private TextBuffer _textBuffer;  //buffer to hold parsed string characters.", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 744140ec0..87a5c7d81 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -49,7 +49,7 @@ import static com.linkedin.data.codec.ProtobufDataCodec.*;\n class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n {\n \n-  private SymbolTable _symbolTable;\n+  private final SymbolTable _symbolTable;\n \n   protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n   {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTI0Ng==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985246", "bodyText": "switch instead of if else here?", "author": "karthikrg", "createdAt": "2020-05-22T00:35:55Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,886 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private SymbolTable _symbolTable;\n+\n+    private Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      if (_currentToken == START_OBJECT)", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 744140ec0..87a5c7d81 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -49,7 +49,7 @@ import static com.linkedin.data.codec.ProtobufDataCodec.*;\n class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n {\n \n-  private SymbolTable _symbolTable;\n+  private final SymbolTable _symbolTable;\n \n   protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n   {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTg2OA==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985868", "bodyText": "Does the test pass if you set this to 1? That will be the ultimate litmus test since you would have accounted for all edge cases.", "author": "karthikrg", "createdAt": "2020-05-22T00:38:29Z", "path": "data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.ChunkedByteStringWriter;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.TestUtil;\n+import com.linkedin.data.codec.CodecDataProviders;\n+import com.linkedin.data.codec.ProtobufCodecOptions;\n+import com.linkedin.data.codec.ProtobufDataCodec;\n+import com.linkedin.entitystream.EntityStream;\n+import com.linkedin.entitystream.EntityStreams;\n+import com.linkedin.entitystream.Writer;\n+import java.util.concurrent.ExecutionException;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.*;\n+\n+\n+public class TestProtobufDataDecoder\n+{\n+  @Test(dataProvider = \"protobufCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, boolean enableFixedLengthFloatDoubles)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(false).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+    codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(true).build());\n+    bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+  }\n+\n+  @Test(dataProvider = \"numbersData\", dataProviderClass = CodecDataProviders.class)\n+  public void testNumbers(Object number) throws Exception\n+  {\n+    DataMap dataMap = new DataMap();\n+    dataMap.put(\"number\", number);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+    assertEquals(decode(bytes), dataMap);\n+  }\n+\n+  @Test\n+  public void testIntValues() throws Exception\n+  {\n+    int inc = (Integer.MAX_VALUE - Integer.MAX_VALUE / 100) / 10000;\n+    for (int i = Integer.MAX_VALUE / 100; i <= Integer.MAX_VALUE && i > 0; i += inc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"int\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getInteger(\"int\"), Integer.valueOf(i));\n+    }\n+    for (int i = Integer.MIN_VALUE; i <= Integer.MIN_VALUE / 100 && i < 0; i += inc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"int\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getInteger(\"int\"), Integer.valueOf(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testLongValues() throws Exception\n+  {\n+    long longInc = (Long.MAX_VALUE - Long.MAX_VALUE / 100L) / 10000L;\n+    for (long i = Long.MAX_VALUE / 100L; i <= Long.MAX_VALUE && i > 0; i += longInc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"long\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getLong(\"long\"), Long.valueOf(i));\n+    }\n+    for (long i = Long.MIN_VALUE; i <= Long.MIN_VALUE / 100L && i < 0; i += longInc)\n+    {\n+      DataMap dataMap = new DataMap();\n+      dataMap.put(\"long\", i);\n+      byte[] bytes =\n+          TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+      DataMap decodedMap = (DataMap) decode(bytes);\n+      assertEquals(decodedMap.getLong(\"long\"), Long.valueOf(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testInvalidMap() throws Exception\n+  {\n+    DataList dataList = new DataList();\n+    dataList.add(1);\n+    dataList.add(2);\n+    dataList.add(4);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataList);\n+    decode(bytes);\n+\n+    try\n+    {\n+      decodeMap(bytes);\n+      fail(\"Parsing list as map.\");\n+    }\n+    catch (ExecutionException e)\n+    {\n+      // Expected.\n+    }\n+  }\n+\n+  @Test\n+  public void testInvalidList() throws Exception\n+  {\n+    DataMap dataMap = new DataMap();\n+    dataMap.put(\"key\", true);\n+    byte[] bytes =\n+        TestUtil.dataComplexToBytes(new ProtobufDataCodec(new ProtobufCodecOptions.Builder().build()), dataMap);\n+    decode(bytes);\n+\n+    try\n+    {\n+      decodeList(bytes);\n+      fail(\"Parsing map as list\");\n+    }\n+    catch (ExecutionException e)\n+    {\n+      // Expected.\n+    }\n+  }\n+\n+  private static DataComplex decode(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataComplex> decoder = new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_TOKENS);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static DataMap decodeMap(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataMap> decoder =\n+        new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_OBJECT_TOKEN);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static DataList decodeList(byte[] bytes) throws Exception\n+  {\n+    ProtobufDataDecoder<DataList> decoder =\n+        new ProtobufDataDecoder<>(null, AbstractDataDecoder.START_ARRAY_TOKEN);\n+    return decode(bytes, decoder);\n+  }\n+\n+  private static <T extends DataComplex> T decode(byte[] bytes, ProtobufDataDecoder<T> decoder) throws Exception\n+  {\n+    Writer<ByteString> writer = new ChunkedByteStringWriter(bytes, 3);", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3MDI0MQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r429070241", "bodyText": "will add test for different buffer size", "author": "aman1309", "createdAt": "2020-05-22T06:43:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTg2OA=="}], "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java b/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\nindex aee80c247..d370a6b2c 100644\n--- a/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\n+++ b/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\n\n@@ -41,15 +41,19 @@ public class TestProtobufDataDecoder\n       throws Exception {\n     ProtobufDataCodec codec = new ProtobufDataCodec(\n         new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n-            .setEnableASCIIOnlyStrings(false).build());\n+            .setEnableASCIIOnlyStrings(true).build());\n     byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n-    DataComplex decodedDataComplex = decode(bytes);\n+    DataComplex decodedDataComplex = decode(bytes, 20);\n     assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n-    codec = new ProtobufDataCodec(\n-        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n-            .setEnableASCIIOnlyStrings(true).build());\n-    bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n-    decodedDataComplex = decode(bytes);\n+  }\n+\n+  @Test(dataProvider = \"streamCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, int chunkSize)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableASCIIOnlyStrings(true).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes, chunkSize);\n     assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTk5Nw==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r428985997", "bodyText": "We should take setEnableASCIIOnlyStrings as a dataprovider param", "author": "karthikrg", "createdAt": "2020-05-22T00:39:05Z", "path": "data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.ChunkedByteStringWriter;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.TestUtil;\n+import com.linkedin.data.codec.CodecDataProviders;\n+import com.linkedin.data.codec.ProtobufCodecOptions;\n+import com.linkedin.data.codec.ProtobufDataCodec;\n+import com.linkedin.entitystream.EntityStream;\n+import com.linkedin.entitystream.EntityStreams;\n+import com.linkedin.entitystream.Writer;\n+import java.util.concurrent.ExecutionException;\n+import org.testng.annotations.Test;\n+\n+import static org.testng.Assert.*;\n+\n+\n+public class TestProtobufDataDecoder\n+{\n+  @Test(dataProvider = \"protobufCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, boolean enableFixedLengthFloatDoubles)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(false).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes);\n+    assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n+    codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n+            .setEnableASCIIOnlyStrings(true).build());", "originalCommit": "0ebe473e4165034ace9961425ffd37d760232454", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2OTg3MQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r429069871", "bodyText": "actually will remove test cases for setEnableASCIIOnlyStrings=false as this is deprecated, any both ascii and utf-8 string can be tested without this.", "author": "aman1309", "createdAt": "2020-05-22T06:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk4NTk5Nw=="}], "type": "inlineReview", "revised_code": {"commit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "chunk": "diff --git a/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java b/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\nindex aee80c247..d370a6b2c 100644\n--- a/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\n+++ b/data/src/test/java/com/linkedin/data/codec/entitystream/TestProtobufDataDecoder.java\n\n@@ -41,15 +41,19 @@ public class TestProtobufDataDecoder\n       throws Exception {\n     ProtobufDataCodec codec = new ProtobufDataCodec(\n         new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n-            .setEnableASCIIOnlyStrings(false).build());\n+            .setEnableASCIIOnlyStrings(true).build());\n     byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n-    DataComplex decodedDataComplex = decode(bytes);\n+    DataComplex decodedDataComplex = decode(bytes, 20);\n     assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n-    codec = new ProtobufDataCodec(\n-        new ProtobufCodecOptions.Builder().setEnableFixedLengthFloatDoubles(enableFixedLengthFloatDoubles)\n-            .setEnableASCIIOnlyStrings(true).build());\n-    bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n-    decodedDataComplex = decode(bytes);\n+  }\n+\n+  @Test(dataProvider = \"streamCodecData\", dataProviderClass = CodecDataProviders.class)\n+  public void testDecoder(String testName, DataComplex dataComplex, int chunkSize)\n+      throws Exception {\n+    ProtobufDataCodec codec = new ProtobufDataCodec(\n+        new ProtobufCodecOptions.Builder().setEnableASCIIOnlyStrings(true).build());\n+    byte[] bytes = TestUtil.dataComplexToBytes(codec, dataComplex);\n+    DataComplex decodedDataComplex = decode(bytes, chunkSize);\n     assertEquals(TestUtil.dataComplexToBytes(codec, decodedDataComplex), bytes);\n   }\n \n"}}, {"oid": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "url": "https://github.com/linkedin/rest.li/commit/a02dc6839e3934b6a1b197c499f6e18722b068dd", "message": "Add protobuf stream data decoder", "committedDate": "2020-05-22T08:30:40Z", "type": "forcePushed"}, {"oid": "af27bfb3a8a06952b4dd5c4bca9cc44d8242a9ff", "url": "https://github.com/linkedin/rest.li/commit/af27bfb3a8a06952b4dd5c4bca9cc44d8242a9ff", "message": "Add protobuf stream data decoder", "committedDate": "2020-05-22T10:07:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkzNjcwNQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430936705", "bodyText": "_endOfInput", "author": "karthikbalasub", "createdAt": "2020-05-27T08:14:39Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDQzNg==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944436", "bodyText": "It would be good to add some inline comments in this method.. See my comments below:", "author": "karthikbalasub", "createdAt": "2020-05-27T08:27:24Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDQ4OA==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944488", "bodyText": "Explain why you read the size first for some ordinals.", "author": "karthikbalasub", "createdAt": "2020-05-27T08:27:30Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NDkzMw==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430944933", "bodyText": "This comment doesn't explain clearly what is happening. I see you are trying to see if the current map/list is completed. Doc that.", "author": "karthikbalasub", "createdAt": "2020-05-27T08:28:14Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NTM3MQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430945371", "bodyText": "Why is this not the first check?", "author": "karthikbalasub", "createdAt": "2020-05-27T08:28:58Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTE2MjcwMQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r431162701", "bodyText": "after eof there is still a possibility of END object tokens since protobuf doesn't encode end ordinals/tokens", "author": "aman1309", "createdAt": "2020-05-27T14:11:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NTM3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NjY1MA==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430946650", "bodyText": "Not clear why you had to do this? For raw bytes, you should clear the array when you finished reading the expected number of bytes?", "author": "karthikbalasub", "createdAt": "2020-05-27T08:30:58Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTE2NzE2Mg==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r431167162", "bodyText": "I was using _bytesValue as buffer too, to avoid references long byte arrays but now on 2nd thoughts it won't matter much.\nAdded a byteBuffer separately which can be reset similar to other read pending objects.", "author": "aman1309", "createdAt": "2020-05-27T14:17:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NjY1MA=="}], "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0NzM0Nw==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430947347", "bodyText": "I suggest renaming this to \"checkEndComplexObj\" as you are not really reading from the data.", "author": "karthikbalasub", "createdAt": "2020-05-27T08:32:09Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk0OTkxMw==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430949913", "bodyText": "Comment this is done for reading key,value pairs for each map item", "author": "karthikbalasub", "createdAt": "2020-05-27T08:36:21Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      switch (_currentToken)\n+      {\n+        case START_OBJECT:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue << 1;", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk3NDQwMg==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430974402", "bodyText": "rename this to _textBufferPos", "author": "karthikbalasub", "createdAt": "2020-05-27T09:15:24Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk3OTI2OQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r430979269", "bodyText": "Add doc here, explain the different scenarios:\n\nnew string, fully readable\nnew string, partially readable\nreading remaining part of a string.\nand include details of the different variables used for each scenario. For example _intValue represents the remaining chars to read for scenario 3.", "author": "karthikbalasub", "createdAt": "2020-05-27T09:23:05Z", "path": "data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java", "diffHunk": "@@ -0,0 +1,891 @@\n+/*\n+   Copyright (c) 2020 LinkedIn Corp.\n+\n+   Licensed under the Apache License, Version 2.0 (the \"License\");\n+   you may not use this file except in compliance with the License.\n+   You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+ */\n+\n+package com.linkedin.data.codec.entitystream;\n+\n+import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n+import com.linkedin.data.ByteString;\n+import com.linkedin.data.DataComplex;\n+import com.linkedin.data.DataList;\n+import com.linkedin.data.DataMap;\n+import com.linkedin.data.DataMapBuilder;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n+import com.linkedin.data.codec.DataDecodingException;\n+import com.linkedin.data.codec.symbol.EmptySymbolTable;\n+import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.protobuf.ProtoReader;\n+import com.linkedin.data.protobuf.ProtoWriter;\n+import com.linkedin.data.protobuf.TextBuffer;\n+import com.linkedin.data.protobuf.Utf8Utils;\n+import java.io.IOException;\n+import java.util.ArrayDeque;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+\n+import static com.linkedin.data.parser.NonBlockingDataParser.Token.*;\n+import static com.linkedin.data.codec.ProtobufDataCodec.*;\n+\n+/**\n+ * A ProtoBuf format decoder for a {@link DataComplex} object, reading from an\n+ * {@link com.linkedin.entitystream.EntityStream} of ByteString.\n+ * The implementation is backed by a non blocking {@link ProtobufStreamDataParser}. Because the raw bytes are\n+ * pushed to the decoder, it keeps the partially built data structure in a stack.\n+ *\n+ * @author amgupta1\n+ */\n+class ProtobufDataDecoder<T extends DataComplex> extends AbstractDataDecoder<T>\n+{\n+\n+  private final SymbolTable _symbolTable;\n+\n+  protected ProtobufDataDecoder(SymbolTable symbolTable, EnumSet<NonBlockingDataParser.Token> expectedFirstToken)\n+  {\n+    super(expectedFirstToken);\n+    _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+  }\n+\n+  @Override\n+  protected NonBlockingDataParser createDataParser() throws IOException\n+  {\n+    return new ProtobufStreamDataParser(_symbolTable);\n+  }\n+\n+  @Override\n+  protected DataComplex createDataObject(NonBlockingDataParser parser)\n+  {\n+    return new DataMap(DataMapBuilder.getOptimumHashMapCapacityFromSize(parser.getComplexObjSize()));\n+  }\n+\n+  @Override\n+  protected DataComplex createDataList(NonBlockingDataParser parser)\n+  {\n+    return new DataList(parser.getComplexObjSize());\n+  }\n+\n+  class ProtobufStreamDataParser implements NonBlockingDataParser\n+  {\n+    private final SymbolTable _symbolTable;\n+\n+    private final Deque<Integer> _complexObjTokenSizeStack = new ArrayDeque<>();\n+    private int _currComplexObjTokenSize = -1;\n+\n+    private byte[] _input;  //holds feed input bytes\n+    private int _limit;\n+    private int _pos;\n+\n+    private boolean _eofInput;  //no more inputs can be feed if this is set to true\n+\n+    private final TextBuffer _textBuffer;  //buffer to hold parsed string characters.\n+    private int _bufferPos = -1;  //signify no. of chars in text buffers as buffer is reused to avoid thrashing\n+\n+    private int _pendingCharUtfRep;  // no. of bytes used by Utf-8 multi-byte representation of pending char\n+    private int _pendingIntShifts = -1;  // remaining bits/bytes for int32/64\n+    private long _pendingInt64;\n+    private int _pendingInt32;\n+\n+    // Stores current token returned from #nextToken else Token#NOT_AVAILABLE\n+    private Token _currentToken;\n+    private byte _currentOrdinal = -1;\n+\n+    //Below value variables hold parsed value for current token returned from #nextToken\n+    private byte[] _bytesValue;\n+    private String _stringValue;\n+    private int _intValue;\n+    private long _longValue;\n+\n+    ProtobufStreamDataParser(SymbolTable symbolTable)\n+    {\n+      _symbolTable = symbolTable == null ? EmptySymbolTable.SHARED : symbolTable;\n+      _textBuffer = new TextBuffer(ProtoReader.DEFAULT_TEXT_BUFFER_SIZE);\n+    }\n+\n+    @Override\n+    public void feedInput(byte[] data, int offset, int len) throws IOException\n+    {\n+      if (data == null || data.length < offset + len)\n+      {\n+        throw new IllegalArgumentException(\"Bad arguments\");\n+      }\n+\n+      if (_pos >= _limit && !_eofInput)\n+      {\n+        _pos = offset;\n+        _limit = offset + len;\n+        _input = data;\n+      }\n+      else\n+      {\n+        throw new IOException(\"Invalid state: Parser cannot accept more data\");\n+      }\n+    }\n+\n+    @Override\n+    public void endOfInput()\n+    {\n+      _eofInput = true;\n+    }\n+\n+    @Override\n+    public Token nextToken() throws IOException\n+    {\n+      // First: regardless of where we really are, need at least one more byte;\n+      // can simplify some of the checks by short-circuiting right away\n+      if (_pos >= _limit) {\n+        Token endComplexObjToken = readEndComplexObj();\n+        if (endComplexObjToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(endComplexObjToken);\n+        }\n+        if (_eofInput) {\n+          return EOF_INPUT;\n+        }\n+        return NOT_AVAILABLE;\n+      }\n+      if (_currentToken != NOT_AVAILABLE)\n+      {\n+        _currentToken = readEndComplexObj();\n+        if (_currentToken != NOT_AVAILABLE)\n+        {\n+          return finishToken(_currentToken);\n+        }\n+        _currentOrdinal = _input[_pos++];\n+        //release bytes array if previous token was Token#RAW_BYTES\n+        _bytesValue = null;\n+      }\n+      Token currToken;\n+      switch (_currentOrdinal)\n+      {\n+        case MAP_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_OBJECT;\n+          }\n+          break;\n+        case LIST_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = START_ARRAY;\n+          }\n+          break;\n+        case ASCII_STRING_LITERAL_ORDINAL:\n+          currToken = readASCIIString();\n+          break;\n+        case STRING_LITERAL_ORDINAL:\n+          currToken = readString();\n+          break;\n+        case STRING_REFERENCE_ORDINAL:\n+          currToken = readStringReference();\n+          break;\n+        case INTEGER_ORDINAL:\n+          currToken = readInt32();\n+          break;\n+        case LONG_ORDINAL:\n+          currToken = readInt64();\n+          break;\n+        case FLOAT_ORDINAL:\n+          currToken = readInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case FIXED_FLOAT_ORDINAL:\n+          currToken = readFixedInt32();\n+          if (currToken == INTEGER)\n+          {\n+            currToken = FLOAT;\n+          }\n+          break;\n+        case DOUBLE_ORDINAL:\n+          currToken = readInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case FIXED_DOUBLE_ORDINAL:\n+          currToken = readFixedInt64();\n+          if (currToken == LONG)\n+          {\n+            currToken = DOUBLE;\n+          }\n+          break;\n+        case BOOLEAN_TRUE_ORDINAL:\n+          currToken = BOOL_TRUE;\n+          break;\n+        case BOOLEAN_FALSE_ORDINAL:\n+          currToken = BOOL_FALSE;\n+          break;\n+        case RAW_BYTES_ORDINAL:\n+          currToken = readByteArray();\n+          break;\n+        case NULL_ORDINAL:\n+          currToken = NULL;\n+          break;\n+        default: throw new DataDecodingException(\"Unknown ordinal: \" + _currentOrdinal);\n+      }\n+      return finishToken(currToken);\n+    }\n+\n+    private Token readEndComplexObj()\n+    {\n+      if(_currComplexObjTokenSize == 0)\n+      {\n+        if (!_complexObjTokenSizeStack.isEmpty())\n+        {\n+          _currComplexObjTokenSize = _complexObjTokenSizeStack.pop();\n+        }\n+        return isCurrList() ? END_ARRAY : END_OBJECT;\n+      }\n+      return NOT_AVAILABLE;\n+    }\n+\n+    private Token readStringReference() throws IOException\n+    {\n+      Token refToken = readInt32();\n+      if (refToken == NOT_AVAILABLE)\n+      {\n+        return NOT_AVAILABLE;\n+      }\n+      if ((_stringValue = _symbolTable.getSymbolName(_intValue)) == null)\n+      {\n+        throw new DataDecodingException(\"Error decoding string reference\");\n+      }\n+      return STRING;\n+    }\n+\n+    private Token finishToken(Token token)\n+    {\n+      _currentToken = token;\n+      switch (_currentToken)\n+      {\n+        case START_OBJECT:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue << 1;\n+          break;\n+        case START_ARRAY:\n+          if (_currComplexObjTokenSize > 0)\n+          {\n+            _complexObjTokenSizeStack.push(_currComplexObjTokenSize);\n+          }\n+          _currComplexObjTokenSize = _intValue;\n+          break;\n+        case NOT_AVAILABLE:\n+          break;\n+        default:\n+          _currComplexObjTokenSize--;\n+      }\n+      return _currentToken;\n+    }\n+\n+    @Override\n+    public int getComplexObjSize()\n+    {\n+      return _currentToken == START_OBJECT || _currentToken == START_ARRAY ? _intValue : -1;\n+    }\n+\n+    @Override\n+    public String getString() throws IOException\n+    {\n+      if (_currentToken != STRING)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: String value is not available\");\n+      }\n+      return _stringValue;\n+    }\n+\n+    @Override\n+    public ByteString getRawBytes() throws IOException {\n+      if (_currentToken != RAW_BYTES)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return ByteString.unsafeWrap(_bytesValue);\n+    }\n+\n+    @Override\n+    public int getIntValue() throws IOException\n+    {\n+      if (_currentToken != INTEGER)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: int value is not available\");\n+      }\n+      return _intValue;\n+    }\n+\n+    @Override\n+    public long getLongValue() throws IOException\n+    {\n+      if (_currentToken != LONG)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return _longValue;\n+    }\n+\n+    @Override\n+    public float getFloatValue() throws IOException\n+    {\n+      if (_currentToken != FLOAT)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return Float.intBitsToFloat(_intValue);\n+    }\n+\n+    @Override\n+    public double getDoubleValue() throws IOException\n+    {\n+      if (_currentToken != DOUBLE)\n+      {\n+        throw new DataDecodingException(\"Unexpected call: Raw bytes value is not available\");\n+      }\n+      return Double.longBitsToDouble(_longValue);\n+    }\n+\n+    /*\n+    * Non blocking ProtoReader Implementation\n+    */\n+\n+    private Token readASCIIString() throws IOException {", "originalCommit": "a02dc6839e3934b6a1b197c499f6e18722b068dd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTIwNDYzMQ==", "url": "https://github.com/linkedin/rest.li/pull/306#discussion_r431204631", "bodyText": "added comments for 3 cases.\ncommented on _intValue variable, also using local variable as remainingSize to signify the same.", "author": "aman1309", "createdAt": "2020-05-27T14:57:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk3OTI2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "1a5aac743af2d7104422fb19b75686b0427f5135", "chunk": "diff --git a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\nindex 87a5c7d81..e926d2ed8 100644\n--- a/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n+++ b/data/src/main/java/com/linkedin/data/codec/entitystream/ProtobufDataDecoder.java\n\n@@ -16,16 +16,15 @@\n \n package com.linkedin.data.codec.entitystream;\n \n-import com.fasterxml.jackson.dataformat.smile.SmileFactory;\n import com.linkedin.data.ByteString;\n import com.linkedin.data.DataComplex;\n import com.linkedin.data.DataList;\n import com.linkedin.data.DataMap;\n import com.linkedin.data.DataMapBuilder;\n-import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.codec.DataDecodingException;\n import com.linkedin.data.codec.symbol.EmptySymbolTable;\n import com.linkedin.data.codec.symbol.SymbolTable;\n+import com.linkedin.data.parser.NonBlockingDataParser;\n import com.linkedin.data.protobuf.ProtoReader;\n import com.linkedin.data.protobuf.ProtoWriter;\n import com.linkedin.data.protobuf.TextBuffer;\n"}}, {"oid": "1a5aac743af2d7104422fb19b75686b0427f5135", "url": "https://github.com/linkedin/rest.li/commit/1a5aac743af2d7104422fb19b75686b0427f5135", "message": "address review comments", "committedDate": "2020-05-27T16:07:35Z", "type": "commit"}, {"oid": "1a5aac743af2d7104422fb19b75686b0427f5135", "url": "https://github.com/linkedin/rest.li/commit/1a5aac743af2d7104422fb19b75686b0427f5135", "message": "address review comments", "committedDate": "2020-05-27T16:07:35Z", "type": "forcePushed"}]}