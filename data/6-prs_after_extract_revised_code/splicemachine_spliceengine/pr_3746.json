{"pr_number": 3746, "pr_title": "DB-9781: create missing index for system table", "pr_createdAt": "2020-07-01T17:14:09Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/3746", "timeline": [{"oid": "c6eacfc47ee0d8f2f3940bb08b36268f9bc8a228", "url": "https://github.com/splicemachine/spliceengine/commit/c6eacfc47ee0d8f2f3940bb08b36268f9bc8a228", "message": "DB-9781: create missing index for system table", "committedDate": "2020-07-01T16:21:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAzMzU2Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3746#discussion_r449033566", "bodyText": "Is this related to this PR?", "author": "dgomezferro", "createdAt": "2020-07-02T14:15:17Z", "path": "hbase_sql/src/main/java/com/splicemachine/hbase/HBaseRegionLoads.java", "diffHunk": "@@ -234,6 +234,11 @@ private static String tableForRegion(String regionName){\n     public Collection<PartitionLoad> tableLoad(String tableName, boolean refresh){\n         if (refresh) {\n             Map<String, Map<String, PartitionLoad>> loads = cache.get();\n+            if (loads == null) {", "originalCommit": "c6eacfc47ee0d8f2f3940bb08b36268f9bc8a228", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA4Mzg3Mg==", "url": "https://github.com/splicemachine/spliceengine/pull/3746#discussion_r449083872", "bodyText": "Somewhat. Erin hit this problem when she tried to fix a table", "author": "jyuanca", "createdAt": "2020-07-02T15:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAzMzU2Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAzNjM1OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3746#discussion_r449036359", "bodyText": "This is not used", "author": "dgomezferro", "createdAt": "2020-07-02T14:19:20Z", "path": "hbase_sql/src/test/java/com/splicemachine/hbase/CheckTableIT.java", "diffHunk": "@@ -186,6 +192,72 @@ public static void dropTables() throws Exception {\n         spliceClassWatcher.execute(\"drop table CHECKTABLEIT2.F\");\n     }\n \n+    @Test\n+    public void testSystemTable() throws Exception {\n+        // delete one row from SYSCONGLOMERATES_INDEX2\n+        ResultSet rs = spliceClassWatcher.executeQuery(\"select rowid from sys.sysconglomerates --splice-properties index=SYSCONGLOMERATES_INDEX2\\n\" +\n+                \"where conglomeratename='GI'\");\n+        rs.next();\n+        String rowid = rs.getString(1);\n+        rs = spliceClassWatcher.executeQuery(\"select conglomeratenumber from sys.sysconglomerates where conglomeratename='SYSCONGLOMERATES_INDEX2'\");\n+        rs.next();\n+        long index2 = rs.getLong(1);\n+        rs.close();\n+        spliceClassWatcher.execute(String.format(\"call syscs_util.syscs_dictionary_delete(%d, '%s')\",\n+               index2, rowid));\n+\n+        // delete one row from SYSCONGLOMERATES_INDEX1\n+        rs = spliceClassWatcher.executeQuery(\"select conglomerateid from sys.sysconglomerates where conglomeratename='GI'\");\n+        rs.next();\n+        String conglomerateId = rs.getString(1);\n+        rs.close();\n+\n+        rs = spliceClassWatcher.executeQuery(String.format(\"select rowid from sys.sysconglomerates --splice-properties index=SYSCONGLOMERATES_INDEX1\\n\" +\n+                \"where conglomerateid='%s'\", conglomerateId));\n+        rs.next();\n+        rowid = rs.getString(1);\n+        rs.close();\n+\n+        rs = spliceClassWatcher.executeQuery(\"select conglomeratenumber from sys.sysconglomerates where conglomeratename='SYSCONGLOMERATES_INDEX1'\");\n+        rs.next();\n+        long index1 = rs.getLong(1);\n+        rs.close();\n+\n+        spliceClassWatcher.execute(String.format(\"call syscs_util.syscs_dictionary_delete(%d, '%s')\",\n+                index1, rowid));\n+\n+        // Repair missing indexes\n+        spliceClassWatcher.execute(String.format(\"call syscs_util.fix_table('SYS', 'SYSCONGLOMERATES', null, '%s/fix-conglomerates.out')\", getResourceDirectory()));\n+        String select =\n+                \"SELECT \\\"message\\\" \" +\n+                        \"from new com.splicemachine.derby.vti.SpliceFileVTI(\" +\n+                        \"'%s',NULL,'|',NULL,'HH:mm:ss','yyyy-MM-dd','yyyy-MM-dd HH:mm:ss','true','UTF-8' ) \" +\n+                        \"AS messages (\\\"message\\\" varchar(200)) order by 1\";\n+        rs = spliceClassWatcher.executeQuery(format(select, String.format(\"%s/fix-conglomerates.out\", getResourceDirectory())));\n+        String s = TestUtils.FormattedResult.ResultFactory.toStringUnsorted(rs);\n+        rs.close();\n+\n+        rs = spliceClassWatcher.executeQuery(\"select rowid from sys.sysconglomerates --splice-properties index=null\\n\" +\n+                \"where conglomeratename='GI'\");\n+        rs.next();\n+        rowid = rs.getString(1);\n+\n+        String expected = String.format(\"message                                   |\\n\" +", "originalCommit": "c6eacfc47ee0d8f2f3940bb08b36268f9bc8a228", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTEyMjUxNg==", "url": "https://github.com/splicemachine/spliceengine/pull/3746#discussion_r449122516", "bodyText": "good catch. I forgot to check results", "author": "jyuanca", "createdAt": "2020-07-02T16:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAzNjM1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "bef0c8bb5a1835f6838398ced6c47ed967d51cbe", "chunk": "diff --git a/hbase_sql/src/test/java/com/splicemachine/hbase/CheckTableIT.java b/hbase_sql/src/test/java/com/splicemachine/hbase/CheckTableIT.java\nindex 5fed8e13a4..0f8b0ae29a 100644\n--- a/hbase_sql/src/test/java/com/splicemachine/hbase/CheckTableIT.java\n+++ b/hbase_sql/src/test/java/com/splicemachine/hbase/CheckTableIT.java\n\n@@ -251,6 +251,7 @@ public class CheckTableIT extends SpliceUnitTest {\n                 \"                         SYSCONGLOMERATES_INDEX1:                           |\\n\" +\n                 \"                         SYSCONGLOMERATES_INDEX2:                           |\", rowid, rowid);\n \n+        Assert.assertEquals(s, expected, s);\n         // Check the table again\n         rs = spliceClassWatcher.executeQuery(String.format(\"call syscs_util.check_table('SYS', 'SYSCONGLOMERATES', null, 2, '%s/fix-conglomerates.out')\", getResourceDirectory()));\n         rs.next();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0MTk4NQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3746#discussion_r449041985", "bodyText": "This doesn't make much sense to me, CheckTableJob is already running in the OlapServer, as if it was going to be running in Spark. But here we are choosing whether to run it in Spark or the RS. If we choose RS then as far as I can see it's going to run on the OlapServer instead. That decision should have been made in SpliceAdmin.checkIndexesInDetail() when submitting the checkTableJob to the OlapServer", "author": "dgomezferro", "createdAt": "2020-07-02T14:27:05Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/impl/storage/CheckTableJob.java", "diffHunk": "@@ -100,17 +99,21 @@ public Void call() throws Exception {\n \n         String table = Long.toString(heapConglomId);\n         Collection<PartitionLoad> partitionLoadCollection = EngineDriver.driver().partitionLoadWatcher().tableLoad(table, true);\n+\n         boolean distributed = false;\n-        for (PartitionLoad load: partitionLoadCollection) {\n-            if (load.getMemStoreSize() > 1*MB || load.getStorefileSize() > 1*MB)\n-                distributed = true;\n+        if (request.useSpark == null) {\n+            for (PartitionLoad load : partitionLoadCollection) {\n+                if (load.getMemStoreSize() > 1 * MB || load.getStorefileSize() > 1 * MB)\n+                    distributed = true;\n+            }\n+        } else {\n+            distributed = request.useSpark;\n         }\n         DataSetProcessor dsp = null;\n         if (distributed) {\n             SpliceLogUtils.info(LOG, \"Run check_table on spark\");\n             dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n-        }\n-        else {\n+        } else {\n             SpliceLogUtils.info(LOG, \"Run check_table on region server\");\n             dsp = EngineDriver.driver().processorFactory().localProcessor(null, null);\n         }", "originalCommit": "c6eacfc47ee0d8f2f3940bb08b36268f9bc8a228", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d86e2319ad9bb8f521c0983d554c71f656a89a8b", "chunk": "diff --git a/splice_machine/src/main/java/com/splicemachine/derby/impl/storage/CheckTableJob.java b/splice_machine/src/main/java/com/splicemachine/derby/impl/storage/CheckTableJob.java\nindex d4b5adc19f..39644900c4 100644\n--- a/splice_machine/src/main/java/com/splicemachine/derby/impl/storage/CheckTableJob.java\n+++ b/splice_machine/src/main/java/com/splicemachine/derby/impl/storage/CheckTableJob.java\n\n@@ -65,96 +31,29 @@ import java.util.concurrent.Callable;\n  */\n public class CheckTableJob implements Callable<Void> {\n \n-    private static Logger LOG=Logger.getLogger(CheckTableJob.class);\n-\n     private final OlapStatus jobStatus;\n     private final DistributedCheckTableJob request;\n-    private String tableName;\n-    private String schemaName;\n-    private Activation activation;\n-    private TxnView txn;\n-    private DataDictionary dd;\n-    private List<DDLMessage.TentativeIndex> tentativeIndexList;\n     private long heapConglomId;\n     private TableDescriptor td;\n-    private ConglomerateDescriptorList cdList;\n-    private DDLMessage.Table table;\n-    private LanguageConnectionContext lcc;\n-    private String tableVersion;\n-    private Map<Long, LeadingIndexColumnInfo> leadingIndexColumnInfoMap;\n \n     public CheckTableJob(DistributedCheckTableJob request,OlapStatus jobStatus) {\n         this.jobStatus = jobStatus;\n         this.request = request;\n     }\n \n-    private static int MB = 1024*1024;\n     @Override\n     public Void call() throws Exception {\n+        init();\n+\n         if(!jobStatus.markRunning()){\n             //the client has already cancelled us or has died before we could get started, so stop now\n             return null;\n         }\n-        init();\n-\n-        String table = Long.toString(heapConglomId);\n-        Collection<PartitionLoad> partitionLoadCollection = EngineDriver.driver().partitionLoadWatcher().tableLoad(table, true);\n-\n-        boolean distributed = false;\n-        if (request.useSpark == null) {\n-            for (PartitionLoad load : partitionLoadCollection) {\n-                if (load.getMemStoreSize() > 1 * MB || load.getStorefileSize() > 1 * MB)\n-                    distributed = true;\n-            }\n-        } else {\n-            distributed = request.useSpark;\n-        }\n-        DataSetProcessor dsp = null;\n-        if (distributed) {\n-            SpliceLogUtils.info(LOG, \"Run check_table on spark\");\n-            dsp = EngineDriver.driver().processorFactory().distributedProcessor();\n-        } else {\n-            SpliceLogUtils.info(LOG, \"Run check_table on region server\");\n-            dsp = EngineDriver.driver().processorFactory().localProcessor(null, null);\n-        }\n-\n-        dsp.setSchedulerPool(\"admin\");\n-        dsp.setJobGroup(request.jobGroup, \"\");\n \n         CheckTableResult checkTableResult = new CheckTableResult();\n-        Map<String, List<String>> errors = new TreeMap<>();\n-\n-        int[] baseColumnMap = getBaseColumnMap(tentativeIndexList);\n-        DataSet<ExecRow> tableDataSet = getTableDataSet(dsp, heapConglomId, tentativeIndexList, baseColumnMap);\n-        ExecRow key = getTableKeyExecRow(heapConglomId);\n-        KeyHashDecoder tableKeyDecoder = getKeyDecoder(key, null);\n-        TableChecker tableChecker = dsp.getTableChecker(schemaName, tableName, tableDataSet,\n-                tableKeyDecoder, key, request.txn, request.fix, baseColumnMap, request.isSystemTable);\n-\n-        // Check each index\n-        for(DDLMessage.TentativeIndex tentativeIndex : tentativeIndexList) {\n-            DDLMessage.Index index = tentativeIndex.getIndex();\n-            long indexConglom = index.getConglomerate();\n-            String indexName = SpliceTableAdmin.getIndexName(cdList, index.getConglomerate());\n-            PairDataSet<String, Tuple2<byte[], ExecRow>> indexDataSet = getIndexDataSet(dsp, indexConglom, index.getUnique());\n-\n-            LeadingIndexColumnInfo leadingIndexColumnInfo = null;\n-            if (index.getExcludeDefaults() || index.getExcludeNulls()) {\n-                long conglomerate = index.getConglomerate();\n-                leadingIndexColumnInfo = leadingIndexColumnInfoMap.get(conglomerate);\n-            }\n-            if (!distributed) {\n-                // Create a new dataset for table if it is not checked on spark, because the dataset is essentially an\n-                // iterator. Each time the table is checked, the iterator is consumed.\n-                tableDataSet = getTableDataSet(dsp, heapConglomId, tentativeIndexList, baseColumnMap);\n-                tableChecker.setTableDataSet(tableDataSet);\n-            }\n-            List<String> messages = tableChecker.checkIndex(indexDataSet, indexName, leadingIndexColumnInfo,\n-                    index.getConglomerate(), tentativeIndex);\n-            if (messages.size() > 0) {\n-                errors.put(indexName, messages);\n-            }\n-        }\n+        Map<String, List<String>> errors = CheckTableUtils.checkTable(request.schemaName, request.tableName, td, request.tentativeIndexList,\n+                heapConglomId, true, request.fix, request.isSystemTable, request.txn,\n+                request.ah.getActivation(), request.jobGroup);\n \n         if (errors.size() > 0) {\n             checkTableResult.setResults(errors);\n"}}, {"oid": "bef0c8bb5a1835f6838398ced6c47ed967d51cbe", "url": "https://github.com/splicemachine/spliceengine/commit/bef0c8bb5a1835f6838398ced6c47ed967d51cbe", "message": "address review comments", "committedDate": "2020-07-02T16:05:59Z", "type": "commit"}, {"oid": "d86e2319ad9bb8f521c0983d554c71f656a89a8b", "url": "https://github.com/splicemachine/spliceengine/commit/d86e2319ad9bb8f521c0983d554c71f656a89a8b", "message": "refactor", "committedDate": "2020-07-03T00:35:04Z", "type": "commit"}]}