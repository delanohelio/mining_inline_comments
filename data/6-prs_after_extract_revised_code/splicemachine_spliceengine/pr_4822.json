{"pr_number": 4822, "pr_title": "DB-10728 Fix maxExecutorCores calculation on clusters.", "pr_createdAt": "2020-12-10T07:40:31Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/4822", "timeline": [{"oid": "61e1f19c1795edf66de01e5a7b4cd50b1a00a758", "url": "https://github.com/splicemachine/spliceengine/commit/61e1f19c1795edf66de01e5a7b4cd50b1a00a758", "message": "DB-10728 Fix maxExecutorCores calculation on clusters.", "committedDate": "2020-12-10T07:36:04Z", "type": "commit"}, {"oid": "f2d97817d4f512072883c924bcf9402baa303107", "url": "https://github.com/splicemachine/spliceengine/commit/f2d97817d4f512072883c924bcf9402baa303107", "message": "DB-10728 Remove dependency on cedarsoftware DeepEquals.", "committedDate": "2020-12-16T02:17:58Z", "type": "commit"}, {"oid": "3302cbdeffe8e07ad432887b51e7fecc1d78ff10", "url": "https://github.com/splicemachine/spliceengine/commit/3302cbdeffe8e07ad432887b51e7fecc1d78ff10", "message": "DB-10728 Fix Spotbugs", "committedDate": "2020-12-16T05:19:13Z", "type": "commit"}, {"oid": "94546d92d1d082d0b604bf430476c1f0955863db", "url": "https://github.com/splicemachine/spliceengine/commit/94546d92d1d082d0b604bf430476c1f0955863db", "message": "DB-10728 Fix Spotbugs, take 2.", "committedDate": "2020-12-16T05:24:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545844884", "bodyText": "I think this is not used, remove if so", "author": "dgomezferro", "createdAt": "2020-12-18T13:54:50Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -68,6 +71,8 @@\n     // numNodes is written to zookeeper by OlapServerMaster, so we have\n     // to wait until the Olap Server comes up before getting numNodes from zookeeper.\n     private static int MAX_EXECUTOR_CORES = -1;\n+    // A value to use until the Olap Server comes up.\n+    private static int DEFAULT_MAX_EXECUTOR_CORES = 8;\n     private static int numSparkNodes = -1;", "originalCommit": "94546d92d1d082d0b604bf430476c1f0955863db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg5ODg5OQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545898899", "bodyText": "This is needed.  I ran into problems if I made getMaxExecutorCores non-retryable.  If we call it now before ZK data is there we can use the default.", "author": "msirek", "createdAt": "2020-12-18T15:12:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwNzcyNw==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545907727", "bodyText": "I meant the numSparkNodes field, I believe it's only written to but never used. Sorry for not being clear, I can see how my message was confusing!", "author": "dgomezferro", "createdAt": "2020-12-18T15:26:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkzMTQ5MA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545931490", "bodyText": "I see.  It might actually help for debugging to have a static field that tells us how many spark nodes there are (or how many we think there are), so if you don't mind I'd like to leave it in for now.", "author": "msirek", "createdAt": "2020-12-18T16:05:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk0OTgxMA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545949810", "bodyText": "Ok", "author": "dgomezferro", "createdAt": "2020-12-18T16:35:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NDg4NA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NTc3MA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545845770", "bodyText": "I'd rather have this function in OlapServerMaster or a class logically close to the OlapServer, I think this class shouldn't have that much knowledge about YARN. Ideally we should just read the number of executor cores from Zookeeper.", "author": "dgomezferro", "createdAt": "2020-12-18T13:56:21Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -265,21 +270,38 @@ public int getMaxExecutorCores() {\n         if (MAX_EXECUTOR_CORES != -1)\n             return MAX_EXECUTOR_CORES;\n         synchronized (HEngineSqlEnv.class) {\n-            int sparkNodes = getNumSparkNodes();\n+            byte [] sparkYarnConfigBytes = getSparkYarnConfigBytes();\n+            if (sparkYarnConfigBytes == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n+            SparkYarnConfiguration conf = null;\n+            try {\n+                ByteArrayInputStream bis = new ByteArrayInputStream(sparkYarnConfigBytes);\n+                ObjectInput in = new ObjectInputStream(bis);\n+                conf = (SparkYarnConfiguration) in.readObject();\n+            }\n+            catch (Exception e) {\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+            }\n+            if (conf == null)\n+                return DEFAULT_MAX_EXECUTOR_CORES;\n+\n             int maxExecutorCores =\n-              calculateMaxExecutorCores(HConfiguration.unwrapDelegate().get(\"yarn.nodemanager.resource.memory-mb\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.enabled\"),\n-                                        getProperty(\"splice.spark.executor.instances\"),\n-                                        getProperty(\"splice.spark.executor.cores\"),\n-                                        getProperty(\"splice.spark.executor.memory\"),\n-                                        getProperty(\"splice.spark.dynamicAllocation.maxExecutors\"),\n-                                        getProperty(\"splice.spark.executor.memoryOverhead\"),\n-                                        getProperty(\"splice.spark.yarn.executor.memoryOverhead\"),\n-                                        sparkNodes > 0 ? sparkNodes : 1);\n-            if (sparkNodes > 0) {\n-                numSparkNodes = sparkNodes;\n+              calculateMaxExecutorCores(conf.getYarnNodemanagerResourceMemoryMB(),", "originalCommit": "94546d92d1d082d0b604bf430476c1f0955863db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg5OTIwOQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545899209", "bodyText": "OK, I moved the calculation to OlapServerMaster.", "author": "msirek", "createdAt": "2020-12-18T15:12:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NTc3MA=="}], "type": "inlineReview", "revised_code": {"commit": "40e50e5b26610891d22c088f3114ef9233533034", "chunk": "diff --git a/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java b/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\nindex a0cef6f32f..d5550a42cc 100644\n--- a/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\n+++ b/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\n\n@@ -193,111 +186,29 @@ public class HEngineSqlEnv extends EngineSqlEnvironment{\n     }\n \n \n-    /**\n-     * Parse a Spark or Hadoop size parameter value, that may use b, k, m, g, t, p\n-     * to represent bytes, kilobytes, megabytes, gigabytes, terabytes or petabytes respectively,\n-     * and return back the corresponding number of bytes.  Valid suffixes can also end\n-     * with a 'b' : kb, mb, gb, tb, pb.\n-     * @param sizeString the parameter value string to parse\n-     * @param defaultValue the default value of the parameter if an invalid\n-     *                     <code>sizeString</code> was passed.\n-     * @return The value in bytes of <code>sizeString</code>, or <code>defaultValue</code>\n-     *         if a <code>sizeString</code> was passed that could not be parsed.\n-     */\n-    public static long parseSizeString(String sizeString, long defaultValue, String defaultSuffix) {\n-        long retVal = defaultValue;\n-        Pattern sizePattern = Pattern.compile(\"(^[\\\\d.]+)([bkmgtp]$)\", Pattern.CASE_INSENSITIVE);\n-        Pattern sizePattern2 = Pattern.compile(\"(^[\\\\d.]+)([kmgtp][b]$)\", Pattern.CASE_INSENSITIVE);\n-        sizeString = sizeString.trim();\n-\n-        // Add a default suffix if none is specified.\n-        if (sizeString.matches(\"^.*\\\\d$\"))\n-            sizeString = sizeString + defaultSuffix;\n-\n-        Matcher matcher1 = sizePattern.matcher(sizeString);\n-        Matcher matcher2 = sizePattern2.matcher(sizeString);\n-        Map<String, Integer> suffixes = new HashMap<>();\n-        suffixes.put(\"b\", 0);\n-        suffixes.put(\"k\", 1);\n-        suffixes.put(\"m\", 2);\n-        suffixes.put(\"g\", 3);\n-        suffixes.put(\"t\", 4);\n-        suffixes.put(\"p\", 5);\n-        suffixes.put(\"kb\", 1);\n-        suffixes.put(\"mb\", 2);\n-        suffixes.put(\"gb\", 3);\n-        suffixes.put(\"tb\", 4);\n-        suffixes.put(\"pb\", 5);\n-\n-        boolean found1 = matcher1.find();\n-        boolean found2 = matcher2.find();\n-        Matcher matcher = found1 ? matcher1 : matcher2;\n-\n-        if (found1 || found2) {\n-            BigInteger value;\n-            String digits = matcher.group(1);\n-            try {\n-              value = new BigInteger(digits);\n-            }\n-            catch (NumberFormatException e) {\n-              return defaultValue;\n-            }\n-            int power = suffixes.get(matcher.group(2).toLowerCase());\n-            BigInteger multiplicand = BigInteger.valueOf(1024).pow(power);\n-            value = value.multiply(multiplicand);\n-            if (value.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0)\n-              return Long.MAX_VALUE;\n-            if (value.compareTo(BigInteger.valueOf(0)) < 0)\n-              return defaultValue;\n-\n-            retVal = value.longValue();\n-        }\n-        else {\n-            try {\n-                retVal = Long.parseLong(sizeString);\n-            }\n-            catch (NumberFormatException e) {\n-                return defaultValue;\n-            }\n-            if (retVal < 1)\n-                retVal = defaultValue;\n-        }\n-        return retVal;\n-    }\n-\n     @Override\n     public int getMaxExecutorCores() {\n         if (MAX_EXECUTOR_CORES != -1)\n             return MAX_EXECUTOR_CORES;\n         synchronized (HEngineSqlEnv.class) {\n-            byte [] sparkYarnConfigBytes = getSparkYarnConfigBytes();\n-            if (sparkYarnConfigBytes == null)\n+            byte [] maxExecutorCoresBytes = getMaxExecutorCoresBytes();\n+            if (maxExecutorCoresBytes == null)\n                 return DEFAULT_MAX_EXECUTOR_CORES;\n \n-            SparkYarnConfiguration conf = null;\n+            int maxExecutorCores;\n+            int numNodes;\n             try {\n-                ByteArrayInputStream bis = new ByteArrayInputStream(sparkYarnConfigBytes);\n+                ByteArrayInputStream bis = new ByteArrayInputStream(maxExecutorCoresBytes);\n                 ObjectInput in = new ObjectInputStream(bis);\n-                conf = (SparkYarnConfiguration) in.readObject();\n+                maxExecutorCores = in.readInt();\n+                numNodes = in.readInt();\n             }\n             catch (Exception e) {\n                 return DEFAULT_MAX_EXECUTOR_CORES;\n             }\n-            if (conf == null)\n-                return DEFAULT_MAX_EXECUTOR_CORES;\n \n-            int maxExecutorCores =\n-              calculateMaxExecutorCores(conf.getYarnNodemanagerResourceMemoryMB(),\n-                                        conf.getDynamicAllocationEnabled(),\n-                                        conf.getExecutorInstances(),\n-                                        conf.getExecutorCores(),\n-                                        conf.getExecutorMemory(),\n-                                        conf.getDynamicAllocationMaxExecutors(),\n-                                        conf.getExecutorMemoryOverhead(),\n-                                        conf.getYarnExecutorMemoryOverhead(),\n-                                        conf.getNumNodes());\n-            if (conf.getNumNodes() > 0) {\n-                numSparkNodes = conf.getNumNodes();\n+            if (numNodes > 0 && maxExecutorCores > 0) {\n+                numSparkNodes = numNodes;\n                 MAX_EXECUTOR_CORES = maxExecutorCores;\n             }\n             else\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545846017", "bodyText": "If it's not a lot of work make this a Protobuf message for future extendability.", "author": "dgomezferro", "createdAt": "2020-12-18T13:56:50Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java", "diffHunk": "@@ -78,22 +83,22 @@\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkNumNodesZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_NUM_NODES_PATH;\n+    private static final String sparkYarnConfigZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n \n \n-    // Find the number of nodes on which Spark executors can be run.\n-    private static int getNumSparkNodes() {\n-        int numNodes = -1;\n+    // Find the Spark and YARN configuration from Zookeeper.\n+    private static byte [] getSparkYarnConfigBytes() {\n+        byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkNumNodesZkPath);\n+            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);", "originalCommit": "94546d92d1d082d0b604bf430476c1f0955863db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwMDcyNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545900724", "bodyText": "This is an ephemeral ZK field, so we don't need to be backwards compatible.  It goes away every time zookeeper reboots.  Would it matter to use protobuf?", "author": "msirek", "createdAt": "2020-12-18T15:15:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkxMDU1NA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545910554", "bodyText": "I created https://splicemachine.atlassian.net/browse/DB-11068 to change all messages in OlapServerMaster. It could matter for online upgrade.", "author": "dgomezferro", "createdAt": "2020-12-18T15:30:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE0NTg0NQ==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r546145845", "bodyText": "OK, we can handle it through the Jira.", "author": "msirek", "createdAt": "2020-12-18T23:33:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjAxNw=="}], "type": "inlineReview", "revised_code": {"commit": "40e50e5b26610891d22c088f3114ef9233533034", "chunk": "diff --git a/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java b/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\nindex a0cef6f32f..d5550a42cc 100644\n--- a/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\n+++ b/hbase_sql/src/main/java/com/splicemachine/derby/lifecycle/HEngineSqlEnv.java\n\n@@ -83,20 +76,20 @@ public class HEngineSqlEnv extends EngineSqlEnvironment{\n     private OlapClient olapClient;\n     private OperationManager operationManager;\n     private ZkServiceDiscovery serviceDiscovery;\n-    private static final String sparkYarnConfigZkPath =\n-            HConfiguration.getConfiguration().getSpliceRootPath() + SPARK_YARN_CONFIG_PATH;\n+    private static final String maxExecutorCoresZkPath =\n+            HConfiguration.getConfiguration().getSpliceRootPath() + HBaseConfiguration.MAX_EXECUTOR_CORES;\n \n \n-    // Find the Spark and YARN configuration from Zookeeper.\n-    private static byte [] getSparkYarnConfigBytes() {\n+    // Find maxExecutorCores from Zookeeper.\n+    private static byte [] getMaxExecutorCoresBytes() {\n         byte [] returnData = null;\n         try {\n-            byte [] data = ZkUtils.getData(sparkYarnConfigZkPath);\n+            byte [] data = ZkUtils.getData(maxExecutorCoresZkPath);\n             if (data != null && data.length > 0)\n                 returnData = data;\n         }\n         catch (Exception | java.lang.AssertionError e) {\n-            LOG.warn(\"Unable to find the SparkYarnConfiguration in zookeeper.\");\n+            LOG.warn(\"Unable to find maxExecutorCores in zookeeper.\");\n         }\n         return returnData;\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjUxNA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545846514", "bodyText": "If we compute the number of cores in OlapServerMaster (or close by) I don't think we need this class anymore", "author": "dgomezferro", "createdAt": "2020-12-18T13:57:42Z", "path": "hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n+ *\n+ * This file is part of Splice Machine.\n+ * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n+ * GNU Affero General Public License as published by the Free Software Foundation, either\n+ * version 3, or (at your option) any later version.\n+ * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n+ * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n+ * See the GNU Affero General Public License for more details.\n+ * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n+ * If not, see <http://www.gnu.org/licenses/>.\n+ */\n+\n+package com.splicemachine.access;\n+\n+import java.io.*;\n+\n+/**\n+ * A serializable class for storing YARN and Spark properties in Zookeeper.\n+ *\n+ */\n+public class SparkYarnConfiguration implements Externalizable {", "originalCommit": "94546d92d1d082d0b604bf430476c1f0955863db", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTkwMDgzOA==", "url": "https://github.com/splicemachine/spliceengine/pull/4822#discussion_r545900838", "bodyText": "Yes, it is removed.", "author": "msirek", "createdAt": "2020-12-18T15:15:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg0NjUxNA=="}], "type": "inlineReview", "revised_code": {"commit": "40e50e5b26610891d22c088f3114ef9233533034", "chunk": "diff --git a/hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java b/hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java\ndeleted file mode 100644\nindex 8acc83ba21..0000000000\n--- a/hbase_storage/src/main/java/com/splicemachine/access/SparkYarnConfiguration.java\n+++ /dev/null\n\n@@ -1,213 +0,0 @@\n-/*\n- * Copyright (c) 2012 - 2020 Splice Machine, Inc.\n- *\n- * This file is part of Splice Machine.\n- * Splice Machine is free software: you can redistribute it and/or modify it under the terms of the\n- * GNU Affero General Public License as published by the Free Software Foundation, either\n- * version 3, or (at your option) any later version.\n- * Splice Machine is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;\n- * without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n- * See the GNU Affero General Public License for more details.\n- * You should have received a copy of the GNU Affero General Public License along with Splice Machine.\n- * If not, see <http://www.gnu.org/licenses/>.\n- */\n-\n-package com.splicemachine.access;\n-\n-import java.io.*;\n-\n-/**\n- * A serializable class for storing YARN and Spark properties in Zookeeper.\n- *\n- */\n-public class SparkYarnConfiguration implements Externalizable {\n-\n-    public SparkYarnConfiguration() {}\n-\n-    private int numNodes;\n-\n-    // Yarn properties\n-    private String yarnNodemanagerResourceMemoryMB;\n-\n-    // Spark Properties\n-    private String dynamicAllocationEnabled;\n-    private String executorInstances;\n-    private String executorCores;\n-    private String executorMemory;\n-    private String dynamicAllocationMaxExecutors;\n-    private String executorMemoryOverhead;\n-    private String yarnExecutorMemoryOverhead;\n-\n-    public boolean equals(Object other) {\n-        if (this == other) {\n-            return true;\n-        }\n-        if (other instanceof SparkYarnConfiguration)\n-            return this.equals((SparkYarnConfiguration)other);\n-\n-        return false;\n-    }\n-\n-    public boolean equals(SparkYarnConfiguration other) {\n-        if (numNodes != other.getNumNodes())\n-            return false;\n-        if (!stringEquals(yarnNodemanagerResourceMemoryMB, other.getYarnNodemanagerResourceMemoryMB()))\n-            return false;\n-        if (!stringEquals(dynamicAllocationEnabled, other.getDynamicAllocationEnabled()))\n-            return false;\n-        if (!stringEquals(executorInstances, other.getExecutorInstances()))\n-            return false;\n-        if (!stringEquals(executorCores, other.getExecutorCores()))\n-            return false;\n-        if (!stringEquals(executorMemory, other.getExecutorMemory()))\n-            return false;\n-        if (!stringEquals(dynamicAllocationMaxExecutors, other.getDynamicAllocationMaxExecutors()))\n-            return false;\n-        if (!stringEquals(executorMemoryOverhead, other.getExecutorMemoryOverhead()))\n-            return false;\n-        if (!stringEquals(yarnExecutorMemoryOverhead, other.getYarnExecutorMemoryOverhead()))\n-            return false;\n-\n-        return true;\n-    }\n-\n-    public int hashCode() {\n-        int hc = numNodes;\n-        hc = hc*31 + (yarnNodemanagerResourceMemoryMB == null ? 0 : yarnNodemanagerResourceMemoryMB.hashCode());\n-        hc = hc*31 + (dynamicAllocationEnabled == null ? 0 : dynamicAllocationEnabled.hashCode());\n-        hc = hc*31 + (executorInstances == null ? 0 : executorInstances.hashCode());\n-        hc = hc*31 + (executorCores == null ? 0 : executorCores.hashCode());\n-        hc = hc*31 + (executorMemory == null ? 0 : executorMemory.hashCode());\n-        hc = hc*31 + (dynamicAllocationMaxExecutors == null ? 0 : dynamicAllocationMaxExecutors.hashCode());\n-        hc = hc*31 + (executorMemoryOverhead == null ? 0 : executorMemoryOverhead.hashCode());\n-        hc = hc*31 + (yarnExecutorMemoryOverhead == null ? 0 : yarnExecutorMemoryOverhead.hashCode());\n-        return hc;\n-    }\n-\n-    private boolean stringEquals(String string1, String string2) {\n-        if (string1 == null)\n-            return string2 == null;\n-        else if (string2 == null)\n-            return false;\n-        return string1.equals(string2);\n-    }\n-\n-    @Override\n-    public void writeExternal(ObjectOutput out) throws IOException {\n-        out.writeInt(numNodes);\n-\n-        writeNullableString(yarnNodemanagerResourceMemoryMB, out);\n-\n-        writeNullableString(dynamicAllocationEnabled, out);\n-        writeNullableString(executorInstances, out);\n-        writeNullableString(executorCores, out);\n-        writeNullableString(executorMemory, out);\n-        writeNullableString(dynamicAllocationMaxExecutors, out);\n-        writeNullableString(executorMemoryOverhead, out);\n-        writeNullableString(yarnExecutorMemoryOverhead, out);\n-\n-    }\n-\n-    @Override\n-    public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {\n-        numNodes = in.readInt();\n-\n-        yarnNodemanagerResourceMemoryMB = readNullableString(in);\n-\n-        dynamicAllocationEnabled = readNullableString(in);\n-        executorInstances = readNullableString(in);\n-        executorCores = readNullableString(in);\n-        executorMemory = readNullableString(in);\n-        dynamicAllocationMaxExecutors = readNullableString(in);\n-        executorMemoryOverhead = readNullableString(in);\n-        yarnExecutorMemoryOverhead = readNullableString(in);\n-    }\n-\n-    private static void writeNullableString(String value, ObjectOutput out) throws IOException {\n-        if (value != null) {\n-            out.writeBoolean(true);\n-            out.writeUTF(value);\n-        } else {\n-            out.writeBoolean(false);\n-        }\n-    }\n-\n-    private static String readNullableString(ObjectInput in) throws IOException{\n-        if(in.readBoolean())\n-            return in.readUTF();\n-        return null;\n-    }\n-\n-    public int getNumNodes() {\n-        return numNodes;\n-    }\n-\n-    public void setNumNodes(int numNodes) {\n-        this.numNodes = numNodes;\n-    }\n-\n-    public String getYarnNodemanagerResourceMemoryMB() {\n-        return yarnNodemanagerResourceMemoryMB;\n-    }\n-\n-    public void setYarnNodemanagerResourceMemoryMB(String yarnNodemanagerResourceMemoryMB) {\n-        this.yarnNodemanagerResourceMemoryMB = yarnNodemanagerResourceMemoryMB;\n-    }\n-\n-    public String getDynamicAllocationEnabled() {\n-        return dynamicAllocationEnabled;\n-    }\n-\n-    public void setDynamicAllocationEnabled(String dynamicAllocationEnabled) {\n-        this.dynamicAllocationEnabled = dynamicAllocationEnabled;\n-    }\n-\n-    public String getExecutorInstances() {\n-        return executorInstances;\n-    }\n-\n-    public void setExecutorInstances(String executorInstances) {\n-        this.executorInstances = executorInstances;\n-    }\n-\n-    public String getExecutorCores() {\n-        return executorCores;\n-    }\n-\n-    public void setExecutorCores(String executorCores) {\n-        this.executorCores = executorCores;\n-    }\n-\n-    public String getExecutorMemory() {\n-        return executorMemory;\n-    }\n-\n-    public void setExecutorMemory(String executorMemory) {\n-        this.executorMemory = executorMemory;\n-    }\n-\n-    public String getDynamicAllocationMaxExecutors() {\n-        return dynamicAllocationMaxExecutors;\n-    }\n-\n-    public void setDynamicAllocationMaxExecutors(String dynamicAllocationMaxExecutors) {\n-        this.dynamicAllocationMaxExecutors = dynamicAllocationMaxExecutors;\n-    }\n-\n-    public String getExecutorMemoryOverhead() {\n-        return executorMemoryOverhead;\n-    }\n-\n-    public void setExecutorMemoryOverhead(String executorMemoryOverhead) {\n-        this.executorMemoryOverhead = executorMemoryOverhead;\n-    }\n-\n-    public String getYarnExecutorMemoryOverhead() {\n-        return yarnExecutorMemoryOverhead;\n-    }\n-\n-    public void setYarnExecutorMemoryOverhead(String yarnExecutorMemoryOverhead) {\n-        this.yarnExecutorMemoryOverhead = yarnExecutorMemoryOverhead;\n-    }\n-\n-}\n"}}, {"oid": "40e50e5b26610891d22c088f3114ef9233533034", "url": "https://github.com/splicemachine/spliceengine/commit/40e50e5b26610891d22c088f3114ef9233533034", "message": "DB-10728 Move maxExecutorCores calculation to OlapServerMaster.", "committedDate": "2020-12-18T15:05:48Z", "type": "commit"}, {"oid": "fbed83f6313b513ba011bff6db7430134e3fe574", "url": "https://github.com/splicemachine/spliceengine/commit/fbed83f6313b513ba011bff6db7430134e3fe574", "message": "Merge branch 'master' into DB-10728", "committedDate": "2020-12-18T16:29:07Z", "type": "commit"}]}