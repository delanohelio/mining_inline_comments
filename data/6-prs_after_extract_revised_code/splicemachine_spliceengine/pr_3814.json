{"pr_number": 3814, "pr_title": "DB-9835 reduce time spent in listdirs for isEmptyDirectory", "pr_createdAt": "2020-07-13T14:03:04Z", "pr_url": "https://github.com/splicemachine/spliceengine/pull/3814", "timeline": [{"oid": "7853370922dfd405eb68c9cc22361621adb1b05b", "url": "https://github.com/splicemachine/spliceengine/commit/7853370922dfd405eb68c9cc22361621adb1b05b", "message": "DB-9835 avoid to check for empty directory if spark does it", "committedDate": "2020-07-13T15:24:33Z", "type": "forcePushed"}, {"oid": "6a7837e608f6d4085b8a9735fff2bd287bf698f1", "url": "https://github.com/splicemachine/spliceengine/commit/6a7837e608f6d4085b8a9735fff2bd287bf698f1", "message": "DB-9835 avoid to check for empty directory if spark does it", "committedDate": "2020-07-16T13:17:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456329902", "bodyText": "So if I understand correctly, you removed the redundant check for directory existence and you move it to exception handling logic during the actual read, right?", "author": "hatyo", "createdAt": "2020-07-17T09:28:54Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -343,23 +343,17 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                           double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n+            ExternalTableUtils.preSortColumns(tableSchema.fields(), partitionColumnMap);\n \n             try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;", "originalCommit": "6a7837e608f6d4085b8a9735fff2bd287bf698f1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMzU2OA==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456333568", "bodyText": "yes. spark is anyhow checking first if the directory exists. so if the directory would exist, the previous version would do this check twice, which is costing us some performance on EVERY read (depending on ping ~300ms). of course we want to be good with error messages, so in the new version, if the query fails, we can do the check again to have a good error output.", "author": "martinrupp", "createdAt": "2020-07-17T09:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQ0MTA5Ng==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456441096", "bodyText": "That's awesome!", "author": "hatyo", "createdAt": "2020-07-17T13:27:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMyOTkwMg=="}], "type": "inlineReview", "revised_code": {"commit": "3c92628b17b99780dbc9afd3a53aa8830520f617", "chunk": "diff --git a/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java b/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\nindex e5ba214dda..8767d47b94 100644\n--- a/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\n+++ b/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\n\n@@ -353,23 +353,16 @@ public class SparkDataSetProcessor implements DistributedDataSetProcessor, Seria\n             } catch (Exception e) {\n                 return handleExceptionSparkRead(e, location, false);\n             }\n-            ExternalTableUtils.sortColumns(table.schema().fields(), partitionColumnMap);\n-            table = processExternalDataset(execRow, table, baseColumnMap, qualifiers, probeValue);\n \n-            if (useSample) {\n-                return new NativeSparkDataSet(table\n-                        .sample(false, sampleFraction), context);\n-            } else {\n-                return new NativeSparkDataSet(table, context);\n-            }\n+            ExternalTableUtils.sortColumns(table.schema().fields(), partitionColumnMap);\n+            return externalTablesPostProcess(baseColumnMap, context, qualifiers, probeValue,\n+                    execRow, useSample, sampleFraction, table);\n         } catch (Exception e) {\n             throw StandardException.newException(\n                     SQLState.EXTERNAL_TABLES_READ_FAILURE, e, e.getMessage());\n         }\n     }\n \n-\n-\n     @Override\n     public <V> DataSet<V> readAvroFile(StructType tableSchema, int[] baseColumnMap, int[] partitionColumnMap,\n                                        String location, OperationContext context, Qualifier[][] qualifiers,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMDQyMw==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456330423", "bodyText": "uhm ... do you want to keep this comment? :)", "author": "hatyo", "createdAt": "2020-07-17T09:29:48Z", "path": "hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java", "diffHunk": "@@ -383,33 +377,34 @@ public Partitioner getPartitioner(DataSet<ExecRow> dataSet, ExecRow template, in\n                                        boolean useSample, double sampleFraction) throws StandardException {\n         try {\n             Dataset<Row> table = null;\n-            try {\n-                DataSet<V> empty_ds = checkExistingOrEmpty( location, context );\n-                if( empty_ds != null ) return empty_ds;\n-\n-                StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n+            StructType copy = new StructType(Arrays.copyOf(tableSchema.fields(), tableSchema.fields().length));\n \n-                // Infer schema from external files\\\n-                StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            // Infer schema from external files\n+            // todo: this is slow on bigger directories, as it's calling getExternalFileSchema,\n+            // which will do a spark.read() before doing the spark.read() here ...\n+            StructType dataSchema = ExternalTableUtils.getDataSchema(this, tableSchema, partitionColumnMap, location, \"a\");\n+            if(dataSchema == null)\n+                return getEmpty(RDDName.EMPTY_DATA_SET.displayName(), context);\n \n+            try {\n                 SparkSession spark = SpliceSpark.getSession();\n                 // Creates a DataFrame from a specified file\n                 table = spark.read().schema(dataSchema).format(\"com.databricks.spark.avro\").load(location);\n+            } catch (Exception e) {\n+                return handleExceptionSparkRead(e, location, false);\n+            }\n \n-                int i = 0;\n-                for (StructField sf : copy.fields()) {\n-                    if (sf.dataType().sameType(DataTypes.DateType)) {\n-                        String colName = table.schema().fields()[i].name();\n-                        table = table.withColumn(colName, table.col(colName).cast(DataTypes.DateType));\n-                    }\n-                    i++;\n+            // todo: find out what this is doing and comment it", "originalCommit": "6a7837e608f6d4085b8a9735fff2bd287bf698f1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzNTk5MQ==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r456335991", "bodyText": "It seems like Dates are not correctly handled by the AVRO Dataframe and returned as something else. We need to document this behavior, have a function doing this.", "author": "martinrupp", "createdAt": "2020-07-17T09:41:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjMzMDQyMw=="}], "type": "inlineReview", "revised_code": {"commit": "3c92628b17b99780dbc9afd3a53aa8830520f617", "chunk": "diff --git a/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java b/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\nindex e5ba214dda..8767d47b94 100644\n--- a/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\n+++ b/hbase_sql/src/main/java/com/splicemachine/derby/stream/spark/SparkDataSetProcessor.java\n\n@@ -353,23 +353,16 @@ public class SparkDataSetProcessor implements DistributedDataSetProcessor, Seria\n             } catch (Exception e) {\n                 return handleExceptionSparkRead(e, location, false);\n             }\n-            ExternalTableUtils.sortColumns(table.schema().fields(), partitionColumnMap);\n-            table = processExternalDataset(execRow, table, baseColumnMap, qualifiers, probeValue);\n \n-            if (useSample) {\n-                return new NativeSparkDataSet(table\n-                        .sample(false, sampleFraction), context);\n-            } else {\n-                return new NativeSparkDataSet(table, context);\n-            }\n+            ExternalTableUtils.sortColumns(table.schema().fields(), partitionColumnMap);\n+            return externalTablesPostProcess(baseColumnMap, context, qualifiers, probeValue,\n+                    execRow, useSample, sampleFraction, table);\n         } catch (Exception e) {\n             throw StandardException.newException(\n                     SQLState.EXTERNAL_TABLES_READ_FAILURE, e, e.getMessage());\n         }\n     }\n \n-\n-\n     @Override\n     public <V> DataSet<V> readAvroFile(StructType tableSchema, int[] baseColumnMap, int[] partitionColumnMap,\n                                        String location, OperationContext context, Qualifier[][] qualifiers,\n"}}, {"oid": "3c92628b17b99780dbc9afd3a53aa8830520f617", "url": "https://github.com/splicemachine/spliceengine/commit/3c92628b17b99780dbc9afd3a53aa8830520f617", "message": "DB-9835 address code review", "committedDate": "2020-07-19T18:45:13Z", "type": "forcePushed"}, {"oid": "93aa90215de7944604ae5795cda632d14cdd6f97", "url": "https://github.com/splicemachine/spliceengine/commit/93aa90215de7944604ae5795cda632d14cdd6f97", "message": "DB-9571 adding assertion for external tables running in control", "committedDate": "2020-07-24T08:40:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459966547", "bodyText": "Is supportAvroDateTypeColumns() function effectively deleted? And, it seems castDateTypeInAvroDataSet() is used only once. Is it expected to be used in other places, too?", "author": "ascend1", "createdAt": "2020-07-24T10:11:07Z", "path": "splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java", "diffHunk": "@@ -64,15 +66,18 @@ public static StructType supportAvroDateType(StructType schema, String storedAs)\n         return schema;\n     }\n \n-    public static void supportAvroDateTypeColumns(ExecRow execRow) throws StandardException {\n-        for(int i=0; i < execRow.size(); i++){\n-            if (execRow.getColumn(i + 1).getTypeName().equals(\"DATE\")) {\n-                execRow.setColumn(i + 1, new SQLVarchar());\n+    public static Dataset<Row> castDateTypeInAvroDataSet(Dataset<Row> dataset, StructType tableSchema) {\n+        int i = 0;\n+        for (StructField sf : tableSchema.fields()) {\n+            if (sf.dataType().sameType(DataTypes.DateType)) {\n+                String colName = dataset.schema().fields()[i].name();\n+                dataset = dataset.withColumn(colName, dataset.col(colName).cast(DataTypes.DateType));\n             }\n+            i++;\n         }\n+        return dataset;\n     }", "originalCommit": "93aa90215de7944604ae5795cda632d14cdd6f97", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2OTY3Nw==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459969677", "bodyText": "yes supportAvroDateTypeColumns wasn't used anywhere, so i deleted it. yes castDateTypeInAvroDataSet is only used in one place, but i put it next to supportAvroDateType, because these two functions are related. That's why it's in ExternalTableUtils.", "author": "martinrupp", "createdAt": "2020-07-24T10:19:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk3MDY5Mw==", "url": "https://github.com/splicemachine/spliceengine/pull/3814#discussion_r459970693", "bodyText": "OK, that makes sense. Thanks!", "author": "ascend1", "createdAt": "2020-07-24T10:21:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk2NjU0Nw=="}], "type": "inlineReview", "revised_code": {"commit": "fe55d57bfeafcaabdb731c1a9c93d33db6ff17f8", "chunk": "diff --git a/splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java b/splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java\nindex c8f614f602..6af47688fe 100644\n--- a/splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java\n+++ b/splice_machine/src/main/java/com/splicemachine/derby/stream/utils/ExternalTableUtils.java\n\n@@ -66,18 +64,15 @@ public class ExternalTableUtils {\n         return schema;\n     }\n \n-    public static Dataset<Row> castDateTypeInAvroDataSet(Dataset<Row> dataset, StructType tableSchema) {\n-        int i = 0;\n-        for (StructField sf : tableSchema.fields()) {\n-            if (sf.dataType().sameType(DataTypes.DateType)) {\n-                String colName = dataset.schema().fields()[i].name();\n-                dataset = dataset.withColumn(colName, dataset.col(colName).cast(DataTypes.DateType));\n+    public static void supportAvroDateTypeColumns(ExecRow execRow) throws StandardException {\n+        for(int i=0; i < execRow.size(); i++){\n+            if (execRow.getColumn(i + 1).getTypeName().equals(\"DATE\")) {\n+                execRow.setColumn(i + 1, new SQLVarchar());\n             }\n-            i++;\n         }\n-        return dataset;\n     }\n \n+\n     public static StructType getSchema(Activation activation, long conglomerateId) throws StandardException {\n \n         boolean prepared = false;\n"}}, {"oid": "fe55d57bfeafcaabdb731c1a9c93d33db6ff17f8", "url": "https://github.com/splicemachine/spliceengine/commit/fe55d57bfeafcaabdb731c1a9c93d33db6ff17f8", "message": "DB-9835 dont use recursive listdir for isEmptyDirectory if FileSystem doesnt implement efficiently\n\nseems like cluster is not using PrestoS3Filesystem, so we rather dont use rec listdir for isEmptyDirectory", "committedDate": "2020-07-26T20:52:41Z", "type": "commit"}, {"oid": "2f6abf2c86bcc98332c362d68812c53e648d0ab3", "url": "https://github.com/splicemachine/spliceengine/commit/2f6abf2c86bcc98332c362d68812c53e648d0ab3", "message": "DB-9835 avoid to check for empty directory if spark does it", "committedDate": "2020-07-26T20:52:41Z", "type": "commit"}, {"oid": "4160bf6d558b9db6a636e7a1306b9f8917037509", "url": "https://github.com/splicemachine/spliceengine/commit/4160bf6d558b9db6a636e7a1306b9f8917037509", "message": "DB-9835 address code review", "committedDate": "2020-07-26T20:52:41Z", "type": "commit"}, {"oid": "a9cb98e7862b8b3d887dafff1789e50f39a9a4af", "url": "https://github.com/splicemachine/spliceengine/commit/a9cb98e7862b8b3d887dafff1789e50f39a9a4af", "message": "DB-9571 adding assertion for external tables running in control", "committedDate": "2020-07-26T20:52:41Z", "type": "commit"}, {"oid": "a9cb98e7862b8b3d887dafff1789e50f39a9a4af", "url": "https://github.com/splicemachine/spliceengine/commit/a9cb98e7862b8b3d887dafff1789e50f39a9a4af", "message": "DB-9571 adding assertion for external tables running in control", "committedDate": "2020-07-26T20:52:41Z", "type": "forcePushed"}]}