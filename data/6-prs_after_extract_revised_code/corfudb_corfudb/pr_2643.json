{"pr_number": 2643, "pr_title": "Config the message size for both snapshot full sync and delta sync.", "pr_createdAt": "2020-07-20T20:59:24Z", "pr_url": "https://github.com/CorfuDB/CorfuDB/pull/2643", "timeline": [{"oid": "ac05eedb97815a166243f74c532ac24acfc5e11a", "url": "https://github.com/CorfuDB/CorfuDB/commit/ac05eedb97815a166243f74c532ac24acfc5e11a", "message": "Make snapshot message size configurable.\n   * Make the message size configurable.\n   * While reading a SMR entry, setup its size.\n   * While processing an opaque entry, get its size.\n   * Send snapshot message with the maxMessageSize.", "committedDate": "2020-07-20T20:24:28Z", "type": "commit"}, {"oid": "18ed759aaf2de8917a1942c49171ca75628cf5e3", "url": "https://github.com/CorfuDB/CorfuDB/commit/18ed759aaf2de8917a1942c49171ca75628cf5e3", "message": "Some cleanup:\n   * The function getSerializedSize works only for an opaque smr entry.\n   * While passing buffer, truncate it to the proper size.", "committedDate": "2020-07-20T20:26:23Z", "type": "commit"}, {"oid": "cad850979100fa3d5b2b949aeebf820e32e3aac7", "url": "https://github.com/CorfuDB/CorfuDB/commit/cad850979100fa3d5b2b949aeebf820e32e3aac7", "message": "Support the configuration for LogEntryDataMessage size.\n  * Add options for input of data message size.\n  * Add unit tests to verify the data message size is correctly set.", "committedDate": "2020-07-20T20:26:50Z", "type": "commit"}, {"oid": "fdc9adda2a903080450bd42f219075d9f92e158b", "url": "https://github.com/CorfuDB/CorfuDB/commit/fdc9adda2a903080450bd42f219075d9f92e158b", "message": "Rebase and merge the change.", "committedDate": "2020-07-20T20:57:25Z", "type": "commit"}, {"oid": "6a2d16f5a512623999593234d0b9d1ddf9d8a482", "url": "https://github.com/CorfuDB/CorfuDB/commit/6a2d16f5a512623999593234d0b9d1ddf9d8a482", "message": "Some cleanup.", "committedDate": "2020-07-20T22:00:57Z", "type": "commit"}, {"oid": "9d1117d719dd92f90b0c17948a051bf1521d89f6", "url": "https://github.com/CorfuDB/CorfuDB/commit/9d1117d719dd92f90b0c17948a051bf1521d89f6", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-20T22:43:56Z", "type": "commit"}, {"oid": "99e78c613f007d81a6b4d3d64b516da21e504e13", "url": "https://github.com/CorfuDB/CorfuDB/commit/99e78c613f007d81a6b4d3d64b516da21e504e13", "message": "Fix issues with SMREntry size.", "committedDate": "2020-07-20T23:40:13Z", "type": "commit"}, {"oid": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "url": "https://github.com/CorfuDB/CorfuDB/commit/15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "message": "Change some logging level from info to debug.", "committedDate": "2020-07-21T00:52:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMxODQ2MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458318461", "bodyText": "if this is only related to Log Replication, can we rename it to getLogReplication....()?", "author": "pankti-m", "createdAt": "2020-07-21T18:54:51Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**\n+     * Get the max number of messages can be sent over per batch.\n+     * @return\n+     */\n+    public int getMaxNumMsgPerBatch() {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\nindex 131b9850e0d..0a4566122da 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n\n@@ -223,7 +223,7 @@ public class ServerContext implements AutoCloseable {\n      * Get the max number of messages can be sent over per batch.\n      * @return\n      */\n-    public int getMaxNumMsgPerBatch() {\n+    public int getLogReplicationMaxNumMsgPerBatch() {\n         String val = getServerConfig(String.class, \"--snapshot-batch\");\n         return val == null ? DEFAULT_MAX_NUM_MSG_PER_BATCH : Integer.parseInt(val);\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMDAyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458320029", "bodyText": "can we rename to DEFAULT_TIMEOUT_MS?", "author": "pankti-m", "createdAt": "2020-07-21T18:57:35Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT = 5000;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\nindex 2cf68e1eba5..6dabb784ebc 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n\n@@ -14,13 +14,13 @@ import java.util.Set;\n public class LogReplicationConfig {\n \n     // Log Replication message timeout time in milliseconds.\n-    public static final int DEFAULT_TIMEOUT = 5000;\n+    public static final int DEFAULT_TIMEOUT_MS = 5000;\n \n     // Log Replication default max number of message generated at the active cluster for each batch.\n     public static final int DEFAULT_MAX_NUM_MSG_PER_BATCH = 10;\n \n     // Log Replication default max data message size is 64MB.\n-    public static final int DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE = (64 << 20);\n+    public static final int MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED = (64 << 20);\n \n     /**\n      * percentage of log data per log replication message\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMTg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458321888", "bodyText": "can we rename this to max_message_per_snapshot or snapshot_batch_num_messages?", "author": "pankti-m", "createdAt": "2020-07-21T19:00:55Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -44,7 +44,8 @@\n                     + \"\\n\"\n                     + \"Usage:\\n\"\n                     + \"\\tlog_replication_server (-l <path>|-m) [-nsN] [-a <address>|-q <interface-name>] \"\n-                    + \"[--snapshot-batch=<batch-size>]\"\n+                    + \"[--snapshot-batch=<batch-size>] \"", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\nindex c46ae4400c1..0612a02fe8f 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\n\n@@ -44,7 +44,7 @@ public class CorfuInterClusterReplicationServer implements Runnable {\n                     + \"\\n\"\n                     + \"Usage:\\n\"\n                     + \"\\tlog_replication_server (-l <path>|-m) [-nsN] [-a <address>|-q <interface-name>] \"\n-                    + \"[--snapshot-batch=<batch-size>] \"\n+                    + \"[--max-num-snapshot-msg-per-batch=<batch-size>] \"\n                     + \"[--max-data-message-size=<msg-size>] \"\n                     + \"[--lock-lease=<lease-duration>]\"\n                     + \"[-c <ratio>] [-d <level>] [-p <seconds>] \"\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjI3OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322279", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:01:42Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java", "diffHunk": "@@ -171,14 +172,20 @@\n                     + \" --metrics-port=<metrics_port>                                            \"\n                     + \"              Metrics provider server port [default: 9999].\\n             \"\n                     + \" --snapshot-batch=<batch-size>                                            \"\n-                    + \"              Snapshot (Full) Sync batch size (number of entries)\\n       \"\n+                    + \"              Snapshot (Full) Sync batch size.\\n                          \"\n+                    + \"              The max number of messages per batch)\\n                      \"\n+                    + \"                                                                          \"\n+                    + \" --max-data-message-size=<msg-size>                                       \"\n+                    + \"              The max size of replication data message in bytes.\\n   \"\n+                    + \"                                                                          \"\n                     + \" --lock-lease=<lease-duration>                                            \"\n                     + \"              Lock lease duration in seconds\\n                            \"\n                     + \" -h, --help                                                               \"\n                     + \"              Show this screen\\n\"\n                     + \" --version                                                                \"\n                     + \"              Show version\\n\";\n \n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\nindex c46ae4400c1..0612a02fe8f 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/CorfuInterClusterReplicationServer.java\n\n@@ -185,7 +185,6 @@ public class CorfuInterClusterReplicationServer implements Runnable {\n                     + \" --version                                                                \"\n                     + \"              Show version\\n\";\n \n-\n     // Active Corfu Server Node.\n     private volatile CorfuInterClusterReplicationServerNode activeServer;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjkyNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458322927", "bodyText": "why did we reduce it?", "author": "pankti-m", "createdAt": "2020-07-21T19:02:53Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 1;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NTIxMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458465211", "bodyText": "As we increase the size of the message to 64MB, there is no point to have a big buffer.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMjkyNw=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java\nindex 105563bf334..d624f082712 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java\n\n@@ -45,7 +45,7 @@ public final class DefaultClusterConfig {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 1;\n+    private static int logSenderBufferSize = 2;\n \n     @Getter\n     private static int logSenderRetryCount = 5;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyMzkzMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458323932", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:04:38Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,25 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex afb931f89a3..77b1d10c914 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -77,7 +77,6 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n     }\n \n     public void processBuffer() {\n-\n         /**\n          *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n          *  skip processing and remove it from buffer.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyODMzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458328335", "bodyText": "the comment says timestamp but we are checking on version.  Is it intended?", "author": "pankti-m", "createdAt": "2020-07-21T19:12:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMjM5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459202399", "bodyText": "For opaque entry, the version is the timestamp.", "author": "xiaoqin2012", "createdAt": "2020-07-23T03:58:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyODMzNQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyOTUwOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458329508", "bodyText": "will opaque entry list contain all streams to be replicated, even if they do not have any data?", "author": "pankti-m", "createdAt": "2020-07-21T19:15:02Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntryWriter.java", "diffHunk": "@@ -98,21 +93,37 @@ void processMsg(LogReplicationEntry txMessage) {\n \n         lastMsgTs = Math.max(persistLogTS, lastMsgTs);\n \n+\n+        // If this entry's max timestamp is not bigger than the persistLogTs, skip the whole message.\n         if (topologyConfigId != persistSiteConfigID || ts != persistSnapStart || ts != persistSnapDone ||\n-                txMessage.getMetadata().getPreviousTimestamp() != persistLogTS) {\n+                entryTS <= persistLogTS) {\n             log.warn(\"Skip write this msg {} as its timestamp is later than the persisted one \" +\n                     txMessage.getMetadata() +  \" persisteSiteConfig \" + persistSiteConfigID + \" persistSnapStart \" + persistSnapStart +\n                     \" persistSnapDone \" + persistSnapDone + \" persistLogTs \" + persistLogTS);\n             return;\n         }\n \n+        // Skip Opaque entries with timestamp that are not larger than persistedTs\n+        OpaqueEntry[] newOpaqueEntryList = opaqueEntryList.stream().filter(x->x.getVersion() > persistLogTS).toArray(OpaqueEntry[]::new);\n+\n+        // Check that all opaque entries contain the correct streams\n+        for (OpaqueEntry opaqueEntry : newOpaqueEntryList) {\n+            if (!streamMap.keySet().containsAll(opaqueEntry.getEntries().keySet())) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NjIzMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458466232", "bodyText": "For tx opqaque entry is a SMRentry that contains all data changed in one transaction.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:35:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMyOTUwOA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMDkxNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458330916", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:17:36Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -131,24 +133,25 @@ public LogReplicationSinkManager(String localCorfuEndpoint, LogReplicationConfig\n          */\n         this.rxState = RxState.LOG_ENTRY_SYNC;\n         this.config = config;\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\nindex 6761740b9a6..1b2a61d1255 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\n\n@@ -133,7 +132,6 @@ public class LogReplicationSinkManager implements DataReceiver {\n          */\n         this.rxState = RxState.LOG_ENTRY_SYNC;\n         this.config = config;\n-\n         init();\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMTExMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458331111", "bodyText": "nit - extra newline.  Also, since this is measuring time, can we rename to something like default_ack_delay_ms or something else?", "author": "pankti-m", "createdAt": "2020-07-21T19:18:00Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java", "diffHunk": "@@ -36,10 +36,12 @@\n      */\n     private static final String config_file = \"/config/corfu/corfu_replication_config.properties\";\n \n+    private final int DEFAULT_ACK_CNT = 1;\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NjY3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458466677", "bodyText": "This is the count of messages. As the message becomes bigger now, so we send an ACK for each message.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:37:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzMTExMQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\nindex 6761740b9a6..1b2a61d1255 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogReplicationSinkManager.java\n\n@@ -37,7 +37,6 @@ public class LogReplicationSinkManager implements DataReceiver {\n     private static final String config_file = \"/config/corfu/corfu_replication_config.properties\";\n \n     private final int DEFAULT_ACK_CNT = 1;\n-\n     /*\n      * how long in milliseconds a ACK sent back to sender\n      */\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNTYxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458335610", "bodyText": "sequencer -> sequence number", "author": "pankti-m", "createdAt": "2020-07-21T19:26:33Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\nindex a84d7672895..b6edb30e42e 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\n\n@@ -178,7 +178,7 @@ public class StreamsSnapshotWriter implements SnapshotWriter {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n+            log.error(\"Received {} Expecting snapshot message sequencer number {} != recvSeq {} or wrong message type {} expecting {}\",\n                     message.getMetadata(), message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n                     message.getMetadata().getMessageMetadataType(), MessageType.SNAPSHOT_MESSAGE);\n             throw new ReplicationWriterException(\"Message is out of order or wrong type\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODMzNzA5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458337099", "bodyText": "we expect the opaqueEntry size == 1.  Why is the iteration needed?", "author": "pankti-m", "createdAt": "2020-07-21T19:29:19Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java", "diffHunk": "@@ -178,23 +178,28 @@ public void apply(LogReplicationEntry message) {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n-                    message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n+            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n+                    message.getMetadata(), message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n                     message.getMetadata().getMessageMetadataType(), MessageType.SNAPSHOT_MESSAGE);\n             throw new ReplicationWriterException(\"Message is out of order or wrong type\");\n         }\n \n-        byte[] payload = message.getPayload();\n-        OpaqueEntry opaqueEntry = OpaqueEntry.deserialize(Unpooled.wrappedBuffer(payload));\n-\n-        if (opaqueEntry.getEntries().keySet().size() != 1) {\n-            log.error(\"The opaqueEntry has more than one entry {}\", opaqueEntry);\n+        // For snapshot message, it has only one opaque entry.\n+        if (message.getOpaqueEntryList().size() > 1) {\n+            log.error(\" Get {} instead of one opaque entry in Snapshot Message\", message.getOpaqueEntryList().size());\n             return;\n         }\n \n-        UUID uuid = opaqueEntry.getEntries().keySet().stream().findFirst().get();\n-        processOpaqueEntry(opaqueEntry.getEntries().get(uuid), message.getMetadata().getSnapshotSyncSeqNum(), uuidMap.get(uuid));\n-        recvSeq++;\n+        for (OpaqueEntry opaqueEntry : message.getOpaqueEntryList()) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\nindex a84d7672895..b6edb30e42e 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/StreamsSnapshotWriter.java\n\n@@ -178,7 +178,7 @@ public class StreamsSnapshotWriter implements SnapshotWriter {\n \n         if (message.getMetadata().getSnapshotSyncSeqNum() != recvSeq ||\n                 message.getMetadata().getMessageMetadataType() != MessageType.SNAPSHOT_MESSAGE) {\n-            log.error(\"Received {} Expecting sequencer {} != recvSeq {} or wrong message type {} expecting {}\",\n+            log.error(\"Received {} Expecting snapshot message sequencer number {} != recvSeq {} or wrong message type {} expecting {}\",\n                     message.getMetadata(), message.getMetadata().getSnapshotSyncSeqNum(), recvSeq,\n                     message.getMetadata().getMessageMetadataType(), MessageType.SNAPSHOT_MESSAGE);\n             throw new ReplicationWriterException(\"Message is out of order or wrong type\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDQ4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344484", "bodyText": "static import of a method is in general not a recommended practice..", "author": "pankti-m", "createdAt": "2020-07-21T19:43:55Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -14,11 +13,16 @@\n import org.corfudb.runtime.view.stream.OpaqueStream;\n \n import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.ArrayList;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n import java.util.UUID;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE;\n+import static org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader.calculateSize;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex 10c8b836ce2..f950229e8ad 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -20,8 +19,7 @@ import java.util.List;\n import java.util.Set;\n import java.util.UUID;\n \n-import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE;\n-import static org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader.calculateSize;\n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;\n \n @Slf4j\n @NotThreadSafe\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NDgxOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458344818", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:44:27Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex 10c8b836ce2..f950229e8ad 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -56,13 +54,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n \n     private OpaqueEntry lastOpaqueEntry = null;\n \n-\n     private boolean hasNoiseData = false;\n \n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n-\n         this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n \n         Set<String> streams = config.getStreamsToReplicate();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTQzNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345434", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:45:33Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex 10c8b836ce2..f950229e8ad 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -56,13 +54,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n \n     private OpaqueEntry lastOpaqueEntry = null;\n \n-\n     private boolean hasNoiseData = false;\n \n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n-\n         this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n \n         Set<String> streams = config.getStreamsToReplicate();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NTczNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458345734", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T19:46:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +36,59 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex 10c8b836ce2..f950229e8ad 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -56,13 +54,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n \n     private OpaqueEntry lastOpaqueEntry = null;\n \n-\n     private boolean hasNoiseData = false;\n \n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n-\n         this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n \n         Set<String> streams = config.getStreamsToReplicate();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0NjM2Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458346366", "bodyText": "nit - can we remove the extra newlines?", "author": "pankti-m", "createdAt": "2020-07-21T19:47:23Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -89,34 +107,88 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n                     entry.getEntries().keySet(), streamUUIDs);\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        hasNoiseData = true;\n+        return false;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+    private int calculateOpaqueEntrySize(OpaqueEntry opaqueEntry) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex 10c8b836ce2..f950229e8ad 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -102,15 +99,37 @@ public class StreamsLogEntryReader implements LogEntryReader {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n \n         hasNoiseData = true;\n         return false;\n     }\n \n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n+\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+\n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MDE1NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458360154", "bodyText": "since this is used by both LogEntryReader and SnapshotReader, can we move it to a common place?", "author": "pankti-m", "createdAt": "2020-07-21T20:13:43Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0ODU2OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459648568", "bodyText": "Move to an utility class.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:32:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MDE1NA=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 6a9e7d32627..8ce650a0caf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -106,21 +111,6 @@ public class StreamsSnapshotReader implements SnapshotReader {\n         return txMsg;\n     }\n \n-    /**\n-     * Given a list of SMREntries, calculate the total sizeInBytes.\n-     * @param smrEntries\n-     * @return\n-     */\n-    public static int calculateSize(List<SMREntry> smrEntries) {\n-        int size = 0;\n-        for (SMREntry entry : smrEntries) {\n-            size += entry.getSerializedSize();\n-        }\n-\n-        log.trace(\"current entry sizeInBytes {}\", size);\n-        return size;\n-    }\n-\n     /**\n      * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MTA4MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458361081", "bodyText": "why do we check for both default size and user-configured size?  It should be either one, right?", "author": "pankti-m", "createdAt": "2020-07-21T20:15:22Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTY0NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469644", "bodyText": "If an entry's size is bigger than the user's setup but is smaller than the default that we can handle, we want that the log replication still make progress, but log a warning. But if the entry's size is bigger than we can handle, we will fail.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MTA4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 6a9e7d32627..8ce650a0caf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -106,21 +111,6 @@ public class StreamsSnapshotReader implements SnapshotReader {\n         return txMsg;\n     }\n \n-    /**\n-     * Given a list of SMREntries, calculate the total sizeInBytes.\n-     * @param smrEntries\n-     * @return\n-     */\n-    public static int calculateSize(List<SMREntry> smrEntries) {\n-        int size = 0;\n-        for (SMREntry entry : smrEntries) {\n-            size += entry.getSerializedSize();\n-        }\n-\n-        log.trace(\"current entry sizeInBytes {}\", size);\n-        return size;\n-    }\n-\n     /**\n      * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjE4NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362185", "bodyText": "this is not an error.  the entry will get sent in the next batch.  Can we change to debug or trace?", "author": "pankti-m", "createdAt": "2020-07-21T20:17:28Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTgxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469812", "bodyText": "This is an error and will cause the log replication transport layer failure.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjE4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 6a9e7d32627..8ce650a0caf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -106,21 +111,6 @@ public class StreamsSnapshotReader implements SnapshotReader {\n         return txMsg;\n     }\n \n-    /**\n-     * Given a list of SMREntries, calculate the total sizeInBytes.\n-     * @param smrEntries\n-     * @return\n-     */\n-    public static int calculateSize(List<SMREntry> smrEntries) {\n-        int size = 0;\n-        for (SMREntry entry : smrEntries) {\n-            size += entry.getSerializedSize();\n-        }\n-\n-        log.trace(\"current entry sizeInBytes {}\", size);\n-        return size;\n-    }\n-\n     /**\n      * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458362371", "bodyText": "debug or trace?", "author": "pankti-m", "createdAt": "2020-07-21T20:17:50Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +62,116 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Given a list of SMREntries, calculate the total sizeInBytes.\n+     * @param smrEntries\n+     * @return\n+     */\n+    public static int calculateSize(List<SMREntry> smrEntries) {\n+        int size = 0;\n+        for (SMREntry entry : smrEntries) {\n+            size += entry.getSerializedSize();\n+        }\n+\n+        log.trace(\"current entry sizeInBytes {}\", size);\n+        return size;\n+    }\n+\n+    /**\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, DEFAULT_LOG_REPLICATION_DATA_MSG_SIZE);\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2OTkxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458469915", "bodyText": "This is a warning.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:48:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MDA5Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458470093", "bodyText": "As I rebased, this is what Anny and I discussed from the previous PR.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MjM3MQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 6a9e7d32627..8ce650a0caf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -106,21 +111,6 @@ public class StreamsSnapshotReader implements SnapshotReader {\n         return txMsg;\n     }\n \n-    /**\n-     * Given a list of SMREntries, calculate the total sizeInBytes.\n-     * @param smrEntries\n-     * @return\n-     */\n-    public static int calculateSize(List<SMREntry> smrEntries) {\n-        int size = 0;\n-        for (SMREntry entry : smrEntries) {\n-            size += entry.getSerializedSize();\n-        }\n-\n-        log.trace(\"current entry sizeInBytes {}\", size);\n-        return size;\n-    }\n-\n     /**\n      * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MzEyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458363125", "bodyText": "can it be private?", "author": "pankti-m", "createdAt": "2020-07-21T20:19:16Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -223,4 +281,21 @@ public void setTopologyConfigId(long topologyConfigId) {\n         this.topologyConfigId = topologyConfigId;\n     }\n \n+    /**\n+     * Record a list of SMR entries\n+     */\n+    static class SMREntryList {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0ODk1Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459648953", "bodyText": "made the change.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM2MzEyNQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 6a9e7d32627..8ce650a0caf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -284,7 +276,7 @@ public class StreamsSnapshotReader implements SnapshotReader {\n     /**\n      * Record a list of SMR entries\n      */\n-    static class SMREntryList {\n+    static private class SMREntryList {\n \n         // The total sizeInBytes of smrEntries in bytes.\n         @Getter\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjEwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422105", "bodyText": "can we change the method name to something like calculateOpaqueSMRSerializedSize or something of that kind?", "author": "pankti-m", "createdAt": "2020-07-21T22:20:08Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\nindex ae6595d57c5..c41be84ed75 100644\n--- a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n+++ b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n\n@@ -154,7 +153,7 @@ public class SMREntry extends LogEntry implements ISMRConsumable {\n      * Calculate an Opaque SMR entry's serialized size.\n      * @throws IllegalAccessException\n      */\n-    private int calculateSerializedSize() {\n+    private int calculateOpaqueSMREntrySerializedSize() {\n         if (!opaque) {\n             log.error(\"This operation only supported for an opaque SMR entry\");\n             return 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyMjg5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458422896", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T22:22:03Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +146,42 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateSerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized int getSerializedSize() {\n+        if (serializedSize == null) {\n+            serializedSize = calculateSerializedSize();\n+        }\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\nindex ae6595d57c5..c41be84ed75 100644\n--- a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n+++ b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n\n@@ -154,7 +153,7 @@ public class SMREntry extends LogEntry implements ISMRConsumable {\n      * Calculate an Opaque SMR entry's serialized size.\n      * @throws IllegalAccessException\n      */\n-    private int calculateSerializedSize() {\n+    private int calculateOpaqueSMREntrySerializedSize() {\n         if (!opaque) {\n             log.error(\"This operation only supported for an opaque SMR entry\");\n             return 0;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQyNDEzMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458424131", "bodyText": "nit - extra newline", "author": "pankti-m", "createdAt": "2020-07-21T22:25:04Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -190,6 +238,8 @@ public void serialize(ByteBuf b) {\n                     b.writeInt(length);\n                     b.writerIndex(lengthIndex + length + 4);\n                 });\n+", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\nindex ae6595d57c5..c41be84ed75 100644\n--- a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n+++ b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n\n@@ -238,7 +236,6 @@ public class SMREntry extends LogEntry implements ISMRConsumable {\n                     b.writeInt(length);\n                     b.writerIndex(lengthIndex + length + 4);\n                 });\n-\n         serializedSize = b.writerIndex() - startWriterIndex;\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432289", "bodyText": "can we import logReplicationConfig instead?", "author": "pankti-m", "createdAt": "2020-07-21T22:45:53Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n+            TrimmedException {\n+        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n+        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n+        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n+\n+        LogReplicationEntry entry = null;\n+\n+        do {\n+            entry = reader.read(UUID.randomUUID());\n+\n+            if (entry != null) {\n+                msgQ.add(entry);\n+            }\n+\n+            System.out.println(\" msgQ size \" + msgQ.size());\n+\n+        } while (entry != null);\n+    }\n+\n+    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n+        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjY4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432688", "bodyText": "is this a test?  what does it test?", "author": "pankti-m", "createdAt": "2020-07-21T22:47:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTI2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471265", "bodyText": "Same as the above.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:53:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjI4OQ=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java b/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java\ndeleted file mode 100644\nindex 310325e62b5..00000000000\n--- a/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java\n+++ /dev/null\n\n@@ -1,295 +0,0 @@\n-package org.corfudb.infrastructure.logreplication;\n-\n-import org.corfudb.infrastructure.logreplication.replication.receive.LogEntryWriter;\n-import org.corfudb.infrastructure.logreplication.replication.receive.LogReplicationMetadataManager;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.LogEntryReader;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.SnapshotReadMessage;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader;\n-import org.corfudb.integration.ReplicationReaderWriterIT;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsLogEntryReader;\n-import org.corfudb.protocols.logprotocol.OpaqueEntry;\n-import org.corfudb.protocols.logprotocol.SMREntry;\n-import org.corfudb.protocols.wireprotocol.ILogData;\n-import org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry;\n-import org.corfudb.runtime.CorfuRuntime;\n-import org.corfudb.runtime.CorfuStoreMetadata;\n-import org.corfudb.runtime.collections.CorfuStore;\n-import org.corfudb.runtime.collections.CorfuTable;\n-import org.corfudb.runtime.collections.Query;\n-import org.corfudb.runtime.collections.Table;\n-import org.corfudb.runtime.collections.TableOptions;\n-import org.corfudb.runtime.collections.TxBuilder;\n-import org.corfudb.runtime.exceptions.TrimmedException;\n-import org.corfudb.runtime.view.AbstractViewTest;\n-import org.corfudb.runtime.view.Address;\n-import org.corfudb.runtime.view.ObjectsView;\n-import org.corfudb.runtime.view.StreamOptions;\n-import org.corfudb.runtime.view.stream.IStreamView;\n-import org.corfudb.runtime.view.stream.OpaqueStream;\n-import org.corfudb.test.SampleSchema.Uuid;\n-import org.junit.Test;\n-\n-import java.lang.reflect.InvocationTargetException;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Set;\n-import java.util.UUID;\n-import java.util.stream.Stream;\n-\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.generateTransactions;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.openStreams;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.printTails;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.verifyData;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.verifyNoData;\n-\n-public class ReplicationReaderWriterTest extends AbstractViewTest {\n-    static public final String PRIMARY_SITE_ID = \"Cluster-Paris\";\n-\n-    static private final int START_VAL = 1;\n-    static final int NUM_KEYS = 4;\n-\n-    // Enforce to read each entry for each message\n-    // each log entry size is 62, there are 2 log entry per dataMsg\n-    // each snapshot entry is 33, there are 4 snapshot entry per dataMsg\n-    static public final int MAX_MSG_SIZE = 160;\n-    static private final int snapshotEntryPerMsg = MAX_MSG_SIZE / 33;\n-    static private final int txEntryPerMsg = MAX_MSG_SIZE / 62;\n-\n-    static public final int BATCH_SIZE = 2;\n-\n-    static private final int NUM_TRANSACTIONS = 20;\n-\n-    CorfuRuntime srcDataRuntime = null;\n-    CorfuRuntime dstDataRuntime = null;\n-    CorfuRuntime readerRuntime = null;\n-    CorfuRuntime writerRuntime = null;\n-\n-    HashMap<String, CorfuTable<Long, Long>> srcTables = new HashMap<>();\n-    HashMap<String, CorfuTable<Long, Long>> dstTables = new HashMap<>();\n-    LogEntryReader logEntryReader;\n-    LogEntryWriter logEntryWriter;\n-\n-    /*\n-     * the in-memory data for corfu tables for verification.\n-     */\n-    HashMap<String, HashMap<Long, Long>> hashMap = new HashMap<String, HashMap<Long, Long>>();\n-\n-    /*\n-     * store message generated by stream snapshot log reader and will play it at the writer side.\n-     */\n-    List<LogReplicationEntry> msgQ = new ArrayList<LogReplicationEntry>();\n-\n-    public void setup() {\n-        srcDataRuntime = getDefaultRuntime().connect();\n-        srcDataRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        dstDataRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        readerRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        writerRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-\n-        UUID uuid = UUID.randomUUID();\n-        LogReplicationConfig config = new LogReplicationConfig(hashMap.keySet());\n-        LogReplicationMetadataManager logReplicationMetadataManager = new LogReplicationMetadataManager(readerRuntime, 0, uuid.toString());\n-        logEntryReader = new StreamsLogEntryReader(readerRuntime, config);\n-        logEntryWriter = new LogEntryWriter(writerRuntime, config, logReplicationMetadataManager);\n-    }\n-\n-    @Test\n-    public void testLogEntryReplication() {\n-        setup();\n-\n-        openStreams(srcTables, srcDataRuntime);\n-        generateTransactions(srcTables, hashMap, NUM_TRANSACTIONS, srcDataRuntime, START_VAL);\n-        printTails(\"after writing data to src tables\", srcDataRuntime, dstDataRuntime);\n-\n-        readLogEntryMsgs(msgQ, srcTables.keySet(), readerRuntime);\n-        assertThat(msgQ.size()).isEqualTo(NUM_TRANSACTIONS/txEntryPerMsg);\n-\n-        writeLogEntryMsgs(msgQ, srcTables.keySet(), writerRuntime);\n-        printTails(\"after playing message at dst\", srcDataRuntime, dstDataRuntime);\n-        openStreams(dstTables, dstDataRuntime);\n-\n-        verifyData(\"after writing log entry at dst\", dstTables, hashMap);\n-    }\n-\n-    private void readMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n-        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n-        StreamsSnapshotReader reader = new StreamsSnapshotReader(rt, config);\n-\n-        reader.reset(rt.getAddressSpaceView().getLogTail());\n-        while (true) {\n-            SnapshotReadMessage snapshotReadMessage = reader.read(UUID.randomUUID());\n-            msgQ.addAll(snapshotReadMessage.getMessages());\n-            if (snapshotReadMessage.isEndRead()) {\n-                break;\n-            }\n-        }\n-    }\n-\n-    @Test\n-    public void testSnapshotReplication() {\n-        setup();\n-        openStreams(srcTables, srcDataRuntime);\n-\n-        generateTransactions(srcTables, hashMap, NUM_TRANSACTIONS, srcDataRuntime, START_VAL);\n-        printTails(\"after writing data to src tables\", srcDataRuntime, dstDataRuntime);\n-\n-        readMsgs(msgQ, hashMap.keySet(), readerRuntime);\n-        assertThat(msgQ.size()).isEqualTo(NUM_TRANSACTIONS*srcTables.size()/snapshotEntryPerMsg);\n-\n-        //call clear table\n-        for (String name : srcTables.keySet()) {\n-            CorfuTable<Long, Long> table = srcTables.get(name);\n-            table.clear();\n-        }\n-\n-        verifyNoData(srcTables);\n-\n-        ReplicationReaderWriterIT.writeSnapLogMsgs(msgQ, srcTables.keySet(), writerRuntime);\n-\n-        //verify data with hashtable\n-        openStreams(dstTables, dstDataRuntime);\n-        verifyData(\"after writing log entry at dst\", dstTables, hashMap);\n-    }\n-\n-    /**\n-     * Test the TxBuilder logUpdate API work properly.\n-     * It first populate tableA with some data. Then read tableA with stream API,\n-     * then apply the smrEntries to tableB with logUpdate API.\n-     * Verify that tableB contains all the keys that A has.\n-     *\n-     * @throws NoSuchMethodException\n-     * @throws IllegalAccessException\n-     * @throws InvocationTargetException\n-     */\n-    @Test\n-    public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessException, InvocationTargetException {\n-        String namespace = \"default_namespace\";\n-        String tableAName = \"tableA\";\n-        String tableBName = \"tableB\";\n-        String tableCName = \"tableC\";\n-\n-        //start runtime 1, populate some data for table A, table C\n-        CorfuRuntime runtime1 = getDefaultRuntime().setTransactionLogging(true).connect();\n-        CorfuStore corfuStore1 = new CorfuStore(runtime1);\n-\n-        Table<Uuid, Uuid, Uuid> tableA = corfuStore1.openTable(namespace, tableAName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-\n-        Table<Uuid, Uuid, Uuid> tableC = corfuStore1.openTable(namespace, tableCName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        UUID uuidA = CorfuRuntime.getStreamID(tableA.getFullyQualifiedTableName());\n-\n-        //update tableA\n-        for (int i = 0; i < NUM_KEYS; i ++) {\n-            UUID uuid = UUID.randomUUID();\n-            Uuid key = Uuid.newBuilder()\n-                    .setMsb(uuid.getMostSignificantBits()).setLsb(uuid.getLeastSignificantBits())\n-                    .build();\n-            corfuStore1.tx(namespace).update(tableAName, key, key, key).commit();\n-        }\n-\n-        //start runtime 2, open A, B as a stream and C as an UFO\n-        CorfuRuntime runtime2 = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        CorfuStore corfuStore2 = new CorfuStore(runtime2);\n-        Table<Uuid, Uuid, Uuid> tableC2 = corfuStore2.openTable(namespace, tableCName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        StreamOptions options = StreamOptions.builder()\n-                .ignoreTrimmed(false)\n-                .cacheEntries(false)\n-                .build();\n-\n-        Stream streamA = (new OpaqueStream(runtime2, runtime2.getStreamsView().\n-                get(uuidA, options))).streamUpTo(runtime2.getAddressSpaceView().getLogTail());\n-\n-        IStreamView txStream = runtime2.getStreamsView()\n-                .getUnsafe(ObjectsView.TRANSACTION_STREAM_ID, StreamOptions.builder()\n-                        .cacheEntries(false)\n-                        .build());\n-        long tail = runtime2.getAddressSpaceView().getLogTail();\n-\n-        Iterator<OpaqueEntry> iterator = streamA.iterator();\n-\n-        Table<Uuid, Uuid, Uuid> tableB = corfuStore1.openTable(namespace, tableBName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        UUID uuidB = CorfuRuntime.getStreamID(tableB.getFullyQualifiedTableName());\n-\n-        while (iterator.hasNext()) {\n-            CorfuStoreMetadata.Timestamp timestamp = corfuStore2.getTimestamp();\n-            TxBuilder txBuilder = corfuStore2.tx(namespace);\n-\n-            //runtime2.getObjectsView().TXBegin();\n-\n-            UUID uuid = UUID.randomUUID();\n-            Uuid key = Uuid.newBuilder()\n-                    .setMsb(uuid.getMostSignificantBits()).setLsb(uuid.getLeastSignificantBits())\n-                    .build();\n-            txBuilder.update(tableCName, key, key, key);\n-            OpaqueEntry opaqueEntry = iterator.next();\n-            for( SMREntry smrEntry : opaqueEntry.getEntries().get(uuidA)) {\n-                    txBuilder.logUpdate(CorfuRuntime.getStreamID(tableB.getFullyQualifiedTableName()), smrEntry);\n-            }\n-            txBuilder.commit(timestamp);\n-        }\n-\n-\n-        //verify data at B and C with runtime 1\n-        txStream.seek(tail);\n-        Iterator<ILogData> iterator1 = txStream.streamUpTo(runtime2.getAddressSpaceView().getLogTail()).iterator();\n-        while(iterator1.hasNext()) {\n-            ILogData data = iterator1.next();\n-            data.getStreams().contains(uuidB);\n-        }\n-        System.out.print(\"\\nstreamBTail \" + runtime2.getAddressSpaceView().getAllTails().getStreamTails().get(uuidB));\n-\n-\n-        Query q = corfuStore1.query(namespace);\n-        Set<Uuid> aSet = q.keySet(tableAName, null);\n-        Set<Uuid> bSet = q.keySet(tableBName, null);\n-\n-        System.out.print(\"\\naSet \" + aSet + \"\\n\\nbSet \" + bSet);\n-        assertThat(bSet.containsAll(aSet)).isTrue();\n-        assertThat(aSet.containsAll(bSet)).isTrue();\n-    }\n-\n-    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n-            TrimmedException {\n-        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n-        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n-        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n-\n-        LogReplicationEntry entry = null;\n-\n-        do {\n-            entry = reader.read(UUID.randomUUID());\n-\n-            if (entry != null) {\n-                msgQ.add(entry);\n-            }\n-\n-            System.out.println(\" msgQ size \" + msgQ.size());\n-\n-        } while (entry != null);\n-    }\n-\n-    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n-        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);\n-        LogReplicationMetadataManager logReplicationMetadataManager = new LogReplicationMetadataManager(rt, 0, PRIMARY_SITE_ID);\n-        LogEntryWriter writer = new LogEntryWriter(rt, config, logReplicationMetadataManager);\n-\n-        if (msgQ.isEmpty()) {\n-            System.out.println(\"msgQ is empty\");\n-        }\n-\n-        for (LogReplicationEntry msg : msgQ) {\n-            writer.apply(msg);\n-        }\n-    }\n-\n-}\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjU2NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458432564", "bodyText": "is this a test?  what does it test?", "author": "pankti-m", "createdAt": "2020-07-21T22:46:44Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java", "diffHunk": "@@ -247,4 +257,39 @@ public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessEx\n         assertThat(bSet.containsAll(aSet)).isTrue();\n         assertThat(aSet.containsAll(bSet)).isTrue();\n     }\n+\n+    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTE2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471169", "bodyText": "This is a function used by this test. This test the reader/writer directly not through the transport layer.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMjU2NA=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java b/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java\ndeleted file mode 100644\nindex 310325e62b5..00000000000\n--- a/test/src/test/java/org/corfudb/infrastructure/logreplication/ReplicationReaderWriterTest.java\n+++ /dev/null\n\n@@ -1,295 +0,0 @@\n-package org.corfudb.infrastructure.logreplication;\n-\n-import org.corfudb.infrastructure.logreplication.replication.receive.LogEntryWriter;\n-import org.corfudb.infrastructure.logreplication.replication.receive.LogReplicationMetadataManager;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.LogEntryReader;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.SnapshotReadMessage;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsSnapshotReader;\n-import org.corfudb.integration.ReplicationReaderWriterIT;\n-import org.corfudb.infrastructure.logreplication.replication.send.logreader.StreamsLogEntryReader;\n-import org.corfudb.protocols.logprotocol.OpaqueEntry;\n-import org.corfudb.protocols.logprotocol.SMREntry;\n-import org.corfudb.protocols.wireprotocol.ILogData;\n-import org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry;\n-import org.corfudb.runtime.CorfuRuntime;\n-import org.corfudb.runtime.CorfuStoreMetadata;\n-import org.corfudb.runtime.collections.CorfuStore;\n-import org.corfudb.runtime.collections.CorfuTable;\n-import org.corfudb.runtime.collections.Query;\n-import org.corfudb.runtime.collections.Table;\n-import org.corfudb.runtime.collections.TableOptions;\n-import org.corfudb.runtime.collections.TxBuilder;\n-import org.corfudb.runtime.exceptions.TrimmedException;\n-import org.corfudb.runtime.view.AbstractViewTest;\n-import org.corfudb.runtime.view.Address;\n-import org.corfudb.runtime.view.ObjectsView;\n-import org.corfudb.runtime.view.StreamOptions;\n-import org.corfudb.runtime.view.stream.IStreamView;\n-import org.corfudb.runtime.view.stream.OpaqueStream;\n-import org.corfudb.test.SampleSchema.Uuid;\n-import org.junit.Test;\n-\n-import java.lang.reflect.InvocationTargetException;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.List;\n-import java.util.Set;\n-import java.util.UUID;\n-import java.util.stream.Stream;\n-\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.generateTransactions;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.openStreams;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.printTails;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.verifyData;\n-import static org.corfudb.integration.ReplicationReaderWriterIT.verifyNoData;\n-\n-public class ReplicationReaderWriterTest extends AbstractViewTest {\n-    static public final String PRIMARY_SITE_ID = \"Cluster-Paris\";\n-\n-    static private final int START_VAL = 1;\n-    static final int NUM_KEYS = 4;\n-\n-    // Enforce to read each entry for each message\n-    // each log entry size is 62, there are 2 log entry per dataMsg\n-    // each snapshot entry is 33, there are 4 snapshot entry per dataMsg\n-    static public final int MAX_MSG_SIZE = 160;\n-    static private final int snapshotEntryPerMsg = MAX_MSG_SIZE / 33;\n-    static private final int txEntryPerMsg = MAX_MSG_SIZE / 62;\n-\n-    static public final int BATCH_SIZE = 2;\n-\n-    static private final int NUM_TRANSACTIONS = 20;\n-\n-    CorfuRuntime srcDataRuntime = null;\n-    CorfuRuntime dstDataRuntime = null;\n-    CorfuRuntime readerRuntime = null;\n-    CorfuRuntime writerRuntime = null;\n-\n-    HashMap<String, CorfuTable<Long, Long>> srcTables = new HashMap<>();\n-    HashMap<String, CorfuTable<Long, Long>> dstTables = new HashMap<>();\n-    LogEntryReader logEntryReader;\n-    LogEntryWriter logEntryWriter;\n-\n-    /*\n-     * the in-memory data for corfu tables for verification.\n-     */\n-    HashMap<String, HashMap<Long, Long>> hashMap = new HashMap<String, HashMap<Long, Long>>();\n-\n-    /*\n-     * store message generated by stream snapshot log reader and will play it at the writer side.\n-     */\n-    List<LogReplicationEntry> msgQ = new ArrayList<LogReplicationEntry>();\n-\n-    public void setup() {\n-        srcDataRuntime = getDefaultRuntime().connect();\n-        srcDataRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        dstDataRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        readerRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        writerRuntime = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-\n-        UUID uuid = UUID.randomUUID();\n-        LogReplicationConfig config = new LogReplicationConfig(hashMap.keySet());\n-        LogReplicationMetadataManager logReplicationMetadataManager = new LogReplicationMetadataManager(readerRuntime, 0, uuid.toString());\n-        logEntryReader = new StreamsLogEntryReader(readerRuntime, config);\n-        logEntryWriter = new LogEntryWriter(writerRuntime, config, logReplicationMetadataManager);\n-    }\n-\n-    @Test\n-    public void testLogEntryReplication() {\n-        setup();\n-\n-        openStreams(srcTables, srcDataRuntime);\n-        generateTransactions(srcTables, hashMap, NUM_TRANSACTIONS, srcDataRuntime, START_VAL);\n-        printTails(\"after writing data to src tables\", srcDataRuntime, dstDataRuntime);\n-\n-        readLogEntryMsgs(msgQ, srcTables.keySet(), readerRuntime);\n-        assertThat(msgQ.size()).isEqualTo(NUM_TRANSACTIONS/txEntryPerMsg);\n-\n-        writeLogEntryMsgs(msgQ, srcTables.keySet(), writerRuntime);\n-        printTails(\"after playing message at dst\", srcDataRuntime, dstDataRuntime);\n-        openStreams(dstTables, dstDataRuntime);\n-\n-        verifyData(\"after writing log entry at dst\", dstTables, hashMap);\n-    }\n-\n-    private void readMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n-        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n-        StreamsSnapshotReader reader = new StreamsSnapshotReader(rt, config);\n-\n-        reader.reset(rt.getAddressSpaceView().getLogTail());\n-        while (true) {\n-            SnapshotReadMessage snapshotReadMessage = reader.read(UUID.randomUUID());\n-            msgQ.addAll(snapshotReadMessage.getMessages());\n-            if (snapshotReadMessage.isEndRead()) {\n-                break;\n-            }\n-        }\n-    }\n-\n-    @Test\n-    public void testSnapshotReplication() {\n-        setup();\n-        openStreams(srcTables, srcDataRuntime);\n-\n-        generateTransactions(srcTables, hashMap, NUM_TRANSACTIONS, srcDataRuntime, START_VAL);\n-        printTails(\"after writing data to src tables\", srcDataRuntime, dstDataRuntime);\n-\n-        readMsgs(msgQ, hashMap.keySet(), readerRuntime);\n-        assertThat(msgQ.size()).isEqualTo(NUM_TRANSACTIONS*srcTables.size()/snapshotEntryPerMsg);\n-\n-        //call clear table\n-        for (String name : srcTables.keySet()) {\n-            CorfuTable<Long, Long> table = srcTables.get(name);\n-            table.clear();\n-        }\n-\n-        verifyNoData(srcTables);\n-\n-        ReplicationReaderWriterIT.writeSnapLogMsgs(msgQ, srcTables.keySet(), writerRuntime);\n-\n-        //verify data with hashtable\n-        openStreams(dstTables, dstDataRuntime);\n-        verifyData(\"after writing log entry at dst\", dstTables, hashMap);\n-    }\n-\n-    /**\n-     * Test the TxBuilder logUpdate API work properly.\n-     * It first populate tableA with some data. Then read tableA with stream API,\n-     * then apply the smrEntries to tableB with logUpdate API.\n-     * Verify that tableB contains all the keys that A has.\n-     *\n-     * @throws NoSuchMethodException\n-     * @throws IllegalAccessException\n-     * @throws InvocationTargetException\n-     */\n-    @Test\n-    public void testUFOWithLogUpdate() throws NoSuchMethodException, IllegalAccessException, InvocationTargetException {\n-        String namespace = \"default_namespace\";\n-        String tableAName = \"tableA\";\n-        String tableBName = \"tableB\";\n-        String tableCName = \"tableC\";\n-\n-        //start runtime 1, populate some data for table A, table C\n-        CorfuRuntime runtime1 = getDefaultRuntime().setTransactionLogging(true).connect();\n-        CorfuStore corfuStore1 = new CorfuStore(runtime1);\n-\n-        Table<Uuid, Uuid, Uuid> tableA = corfuStore1.openTable(namespace, tableAName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-\n-        Table<Uuid, Uuid, Uuid> tableC = corfuStore1.openTable(namespace, tableCName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        UUID uuidA = CorfuRuntime.getStreamID(tableA.getFullyQualifiedTableName());\n-\n-        //update tableA\n-        for (int i = 0; i < NUM_KEYS; i ++) {\n-            UUID uuid = UUID.randomUUID();\n-            Uuid key = Uuid.newBuilder()\n-                    .setMsb(uuid.getMostSignificantBits()).setLsb(uuid.getLeastSignificantBits())\n-                    .build();\n-            corfuStore1.tx(namespace).update(tableAName, key, key, key).commit();\n-        }\n-\n-        //start runtime 2, open A, B as a stream and C as an UFO\n-        CorfuRuntime runtime2 = getNewRuntime(getDefaultNode()).setTransactionLogging(true).connect();\n-        CorfuStore corfuStore2 = new CorfuStore(runtime2);\n-        Table<Uuid, Uuid, Uuid> tableC2 = corfuStore2.openTable(namespace, tableCName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        StreamOptions options = StreamOptions.builder()\n-                .ignoreTrimmed(false)\n-                .cacheEntries(false)\n-                .build();\n-\n-        Stream streamA = (new OpaqueStream(runtime2, runtime2.getStreamsView().\n-                get(uuidA, options))).streamUpTo(runtime2.getAddressSpaceView().getLogTail());\n-\n-        IStreamView txStream = runtime2.getStreamsView()\n-                .getUnsafe(ObjectsView.TRANSACTION_STREAM_ID, StreamOptions.builder()\n-                        .cacheEntries(false)\n-                        .build());\n-        long tail = runtime2.getAddressSpaceView().getLogTail();\n-\n-        Iterator<OpaqueEntry> iterator = streamA.iterator();\n-\n-        Table<Uuid, Uuid, Uuid> tableB = corfuStore1.openTable(namespace, tableBName,\n-                Uuid.class, Uuid.class, Uuid.class, TableOptions.builder().build());\n-\n-        UUID uuidB = CorfuRuntime.getStreamID(tableB.getFullyQualifiedTableName());\n-\n-        while (iterator.hasNext()) {\n-            CorfuStoreMetadata.Timestamp timestamp = corfuStore2.getTimestamp();\n-            TxBuilder txBuilder = corfuStore2.tx(namespace);\n-\n-            //runtime2.getObjectsView().TXBegin();\n-\n-            UUID uuid = UUID.randomUUID();\n-            Uuid key = Uuid.newBuilder()\n-                    .setMsb(uuid.getMostSignificantBits()).setLsb(uuid.getLeastSignificantBits())\n-                    .build();\n-            txBuilder.update(tableCName, key, key, key);\n-            OpaqueEntry opaqueEntry = iterator.next();\n-            for( SMREntry smrEntry : opaqueEntry.getEntries().get(uuidA)) {\n-                    txBuilder.logUpdate(CorfuRuntime.getStreamID(tableB.getFullyQualifiedTableName()), smrEntry);\n-            }\n-            txBuilder.commit(timestamp);\n-        }\n-\n-\n-        //verify data at B and C with runtime 1\n-        txStream.seek(tail);\n-        Iterator<ILogData> iterator1 = txStream.streamUpTo(runtime2.getAddressSpaceView().getLogTail()).iterator();\n-        while(iterator1.hasNext()) {\n-            ILogData data = iterator1.next();\n-            data.getStreams().contains(uuidB);\n-        }\n-        System.out.print(\"\\nstreamBTail \" + runtime2.getAddressSpaceView().getAllTails().getStreamTails().get(uuidB));\n-\n-\n-        Query q = corfuStore1.query(namespace);\n-        Set<Uuid> aSet = q.keySet(tableAName, null);\n-        Set<Uuid> bSet = q.keySet(tableBName, null);\n-\n-        System.out.print(\"\\naSet \" + aSet + \"\\n\\nbSet \" + bSet);\n-        assertThat(bSet.containsAll(aSet)).isTrue();\n-        assertThat(aSet.containsAll(bSet)).isTrue();\n-    }\n-\n-    public static void readLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) throws\n-            TrimmedException {\n-        LogReplicationConfig config = new LogReplicationConfig(streams, BATCH_SIZE, MAX_MSG_SIZE);\n-        StreamsLogEntryReader reader = new StreamsLogEntryReader(rt, config);\n-        reader.setGlobalBaseSnapshot(Address.NON_ADDRESS, Address.NON_ADDRESS);\n-\n-        LogReplicationEntry entry = null;\n-\n-        do {\n-            entry = reader.read(UUID.randomUUID());\n-\n-            if (entry != null) {\n-                msgQ.add(entry);\n-            }\n-\n-            System.out.println(\" msgQ size \" + msgQ.size());\n-\n-        } while (entry != null);\n-    }\n-\n-    public static void writeLogEntryMsgs(List<LogReplicationEntry> msgQ, Set<String> streams, CorfuRuntime rt) {\n-        org.corfudb.infrastructure.logreplication.LogReplicationConfig config = new LogReplicationConfig(streams);\n-        LogReplicationMetadataManager logReplicationMetadataManager = new LogReplicationMetadataManager(rt, 0, PRIMARY_SITE_ID);\n-        LogEntryWriter writer = new LogEntryWriter(rt, config, logReplicationMetadataManager);\n-\n-        if (msgQ.isEmpty()) {\n-            System.out.println(\"msgQ is empty\");\n-        }\n-\n-        for (LogReplicationEntry msg : msgQ) {\n-            writer.apply(msg);\n-        }\n-    }\n-\n-}\n\\ No newline at end of file\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433056", "bodyText": "how was this number derived?", "author": "pankti-m", "createdAt": "2020-07-21T22:48:04Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private static final int SHUTDOWN_RETRIES = 10;\n     private static final long SHUTDOWN_RETRY_WAIT = 500;\n \n+    private static final int MSG_SIZE = 131072;", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MTQ1OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458471458", "bodyText": "This is set for the test case. Not too big as then all entries will be sent over in one message.", "author": "xiaoqin2012", "createdAt": "2020-07-22T00:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzIyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223229", "bodyText": "Can we add this comment in the code so it is easy to follow?", "author": "annym", "createdAt": "2020-07-23T05:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzA1Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/AbstractIT.java b/test/src/test/java/org/corfudb/integration/AbstractIT.java\nindex bdc0624fa75..e654c688312 100644\n--- a/test/src/test/java/org/corfudb/integration/AbstractIT.java\n+++ b/test/src/test/java/org/corfudb/integration/AbstractIT.java\n\n@@ -60,6 +60,9 @@ public class AbstractIT extends AbstractCorfuTest {\n     private static final int SHUTDOWN_RETRIES = 10;\n     private static final long SHUTDOWN_RETRY_WAIT = 500;\n \n+    // Config the msg size for log replication data\n+    // sent from active cluster to the standby cluster.\n+    // We set it as 128KB to make multiple messages during the tests.\n     private static final int MSG_SIZE = 131072;\n \n     public CorfuRuntime runtime;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzUxMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458433510", "bodyText": "remove", "author": "pankti-m", "createdAt": "2020-07-21T22:49:24Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -556,7 +564,7 @@ public String getOptionsString() {\n          * @throws IOException\n          */\n         public Process runServer() throws IOException {\n-            final String serverConsoleLogPath = CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";\n+            final String serverConsoleLogPath = \"/Users/maxi/Projects/tmp/test.result\"; //CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MTEzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459651137", "bodyText": "Done.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:37:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzMzUxMA=="}], "type": "inlineReview", "revised_code": {"commit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/AbstractIT.java b/test/src/test/java/org/corfudb/integration/AbstractIT.java\nindex bdc0624fa75..25d4583efb5 100644\n--- a/test/src/test/java/org/corfudb/integration/AbstractIT.java\n+++ b/test/src/test/java/org/corfudb/integration/AbstractIT.java\n\n@@ -564,7 +564,7 @@ public class AbstractIT extends AbstractCorfuTest {\n          * @throws IOException\n          */\n         public Process runServer() throws IOException {\n-            final String serverConsoleLogPath = \"/Users/maxi/Projects/tmp/test.result\"; //CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";\n+            final String serverConsoleLogPath = CORFU_LOG_PATH + File.separator + host + \"_\" + port + \"_consolelog\";\n \n             File logPath = new File(getCorfuServerLogPath(host, port));\n             if (!logPath.exists()) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNzg0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458437843", "bodyText": "what is the purpose of this method?", "author": "pankti-m", "createdAt": "2020-07-21T23:02:03Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzNjQwMA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459636400", "bodyText": "Wait replication data reach at the standby cluster.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:11:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQzNzg0Mw=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex 60be6b3d911..1d581d81dbc 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -324,6 +323,11 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n             }\n     }\n \n+    /**\n+     * Wait replication data reach at the standby cluster.\n+     * @param tables\n+     * @param hashMap\n+     */\n     void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {\n         for (String name : hashMap.keySet()) {\n             CorfuTable<Long, Long> table = tables.get(name);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MTkyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458441928", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:14:42Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex 60be6b3d911..ad8e222cd20 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -1098,8 +1100,8 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n         generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);\n     }\n \n-        // startCrossTx indicates if we start with a transaction across Tables\n-    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys, int startValue) throws Exception {\n+    // startCrossTx indicates if we start with a transaction across Tables\n+    private void writeCrossTableTransactions(Set<String> crossTableTransactions, boolean startCrossTx) throws Exception {\n         // Setup two separate Corfu Servers: source (primary) and destination (standby)\n         setupEnv();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MjAyNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442025", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:14:57Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1042,9 +1094,12 @@ public void testLogEntrySyncLargeTables() throws Exception {\n \n     /* ********************** AUXILIARY METHODS ********************** */\n \n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys) throws Exception {\n+        generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);\n+    }\n \n-    // startCrossTx indicates if we start with a transaction across Tables\n-    private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boolean startCrossTx) throws Exception {\n+        // startCrossTx indicates if we start with a transaction across Tables\n+    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys, int startValue) throws Exception {", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex 60be6b3d911..ad8e222cd20 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -1098,8 +1100,8 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n         generateTxCrossTables(crossTableTransactions, startCrossTx, numKeys, 0);\n     }\n \n-        // startCrossTx indicates if we start with a transaction across Tables\n-    private void generateTxCrossTables(Set<String> crossTableTransactions, boolean startCrossTx, int numKeys, int startValue) throws Exception {\n+    // startCrossTx indicates if we start with a transaction across Tables\n+    private void writeCrossTableTransactions(Set<String> crossTableTransactions, boolean startCrossTx) throws Exception {\n         // Setup two separate Corfu Servers: source (primary) and destination (standby)\n         setupEnv();\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0Mjg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r458442888", "bodyText": "numKeys -> NUM_KEYS", "author": "pankti-m", "createdAt": "2020-07-21T23:17:40Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -1054,20 +1109,20 @@ private void testSnapshotSyncCrossTables(Set<String> crossTableTransactions, boo\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n         if (startCrossTx) {\n-            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, startValue);\n         }\n \n         // Write data to t0\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t1\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n \n         // Write data to t2\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, numKeys, srcDataRuntime, 0);\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n-        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS*2);\n+        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, numKeys*2);", "originalCommit": "15327c2ef047d773ad33fcbbde2424d3fdbac9ee", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "558d9bc783fa25db0791b58efbb4e33da951a052", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex 60be6b3d911..516df4f0602 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -1109,20 +1084,20 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n         if (startCrossTx) {\n-            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, startValue);\n+            generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n         }\n \n         // Write data to t0\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t0), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n \n         // Write data to t1\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, numKeys, srcDataRuntime, numKeys);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t1), srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS);\n \n         // Write data to t2\n-        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, numKeys, srcDataRuntime, 0);\n+        generateTransactionsCrossTables(srcCorfuTables, Collections.singleton(t2), srcDataForVerification, NUM_KEYS, srcDataRuntime, 0);\n \n         // Write data across to tables specified in crossTableTransactions in transaction\n-        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, numKeys, srcDataRuntime, numKeys*2);\n+        generateTransactionsCrossTables(srcCorfuTables, crossTableTransactions, srcDataForVerification, NUM_KEYS, srcDataRuntime, NUM_KEYS*2);\n \n         // Verify data just written against in-memory copy\n         verifyData(srcCorfuTables, srcDataForVerification);\n"}}, {"oid": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "url": "https://github.com/CorfuDB/CorfuDB/commit/49dc824b6fca4e7bc9c5639308f6db6440016cc1", "message": "Address comments and fix tests.", "committedDate": "2020-07-22T23:39:13Z", "type": "commit"}, {"oid": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "url": "https://github.com/CorfuDB/CorfuDB/commit/ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-23T04:26:00Z", "type": "commit"}, {"oid": "558d9bc783fa25db0791b58efbb4e33da951a052", "url": "https://github.com/CorfuDB/CorfuDB/commit/558d9bc783fa25db0791b58efbb4e33da951a052", "message": "Fix compiling issues.", "committedDate": "2020-07-23T05:06:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2Mzg3OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459163878", "bodyText": "Thinking on the readers. I'm not sure the right words, because even I was confused the first time I reviewed cause I understood that if it is set to 5 we send 5 messages embedded in a single message (a batch of 5). But, we use it is to send 5 messages and yield the thread, but 5 messages and not 1 fat one. Maybe it's more accurate to say: \"Get the max number of messages sent per Log Replication Runtime in between FSM worker thread yield\" (IDK just giving ideas, something in that line)", "author": "annym", "createdAt": "2020-07-23T00:58:29Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -216,16 +219,30 @@ public String getPluginConfigFilePath() {\n         return pluginConfigFilePath == null ? PLUGIN_CONFIG_FILE_PATH : pluginConfigFilePath;\n     }\n \n-    public int getSnapshotSyncBatchSize() {\n-        Integer snapshotSyncBatchSize = getServerConfig(Integer.class, \"--snapshot-batch\");\n-        return snapshotSyncBatchSize == null ? SnapshotSender.DEFAULT_SNAPSHOT_BATCH_SIZE : snapshotSyncBatchSize;\n+    /**", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzNzkxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459637912", "bodyText": "You gave the word earlier. If you want to change, you can change it in your PR.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:13:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2Mzg3OA=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\nindex 0a4566122da..135c0035808 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n\n@@ -240,7 +240,7 @@ public class ServerContext implements AutoCloseable {\n      */\n     public int getLogReplicationMaxDataMessageSize() {\n         String val = getServerConfig(String.class, \"--max-data-message-size\");\n-        return val == null ? MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED : Integer.parseInt(val);\n+        return val == null ? MAX_DATA_MSG_SIZE_SUPPORTED : Integer.parseInt(val);\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDA0Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164047", "bodyText": "Can we shorten it? Since it already has \"LogReplication\" from LogReplicationConfig. --> maybe MAX_DATA_MSG_SIZE", "author": "annym", "createdAt": "2020-07-23T00:59:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java", "diffHunk": "@@ -44,6 +43,8 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\nindex 0a4566122da..135c0035808 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/ServerContext.java\n\n@@ -43,7 +43,7 @@ import java.util.concurrent.TimeUnit;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n-import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;\n+import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.MAX_DATA_MSG_SIZE_SUPPORTED;\n import static org.corfudb.infrastructure.logreplication.LogReplicationConfig.DEFAULT_MAX_NUM_MSG_PER_BATCH;\n import static org.corfudb.util.MetricsUtils.isMetricsReportingSetUp;\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NDc2NQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459164765", "bodyText": "message -> messages", "author": "annym", "createdAt": "2020-07-23T01:02:08Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -13,15 +13,40 @@\n @Data\n public class LogReplicationConfig {\n \n+    // Log Replication message timeout time in milliseconds.\n+    public static final int DEFAULT_TIMEOUT_MS = 5000;\n+\n+    // Log Replication default max number of message generated at the active cluster for each batch.", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\nindex 6dabb784ebc..39ab304c210 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n\n@@ -16,11 +16,11 @@ public class LogReplicationConfig {\n     // Log Replication message timeout time in milliseconds.\n     public static final int DEFAULT_TIMEOUT_MS = 5000;\n \n-    // Log Replication default max number of message generated at the active cluster for each batch.\n+    // Log Replication default max number of messages generated at the active cluster for each batch.\n     public static final int DEFAULT_MAX_NUM_MSG_PER_BATCH = 10;\n \n     // Log Replication default max data message size is 64MB.\n-    public static final int MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED = (64 << 20);\n+    public static final int MAX_DATA_MSG_SIZE_SUPPORTED = (64 << 20);\n \n     /**\n      * percentage of log data per log replication message\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459170886", "bodyText": "We can simplify with:\nthis(streamsToReplicate, DEFAULT_MAX_NUM_MSG_PER_BATCH, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);", "author": "annym", "createdAt": "2020-07-23T01:27:11Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java", "diffHunk": "@@ -30,16 +55,21 @@\n      */\n     public LogReplicationConfig(Set<String> streamsToReplicate) {\n         this.streamsToReplicate = streamsToReplicate;\n+        this.maxNumMsgPerBatch = DEFAULT_MAX_NUM_MSG_PER_BATCH;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzgxMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459653812", "bodyText": "It is harder for later code review. It is easier logic to init all things together in one place instead of jumping code around for simple logic.", "author": "xiaoqin2012", "createdAt": "2020-07-23T18:42:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4NTkxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459685915", "bodyText": "it is actually a good practice, to avoid code duplication, and having all inits in one single place or when a change comes, you'll have to change in several places (prone to errors):\nhttps://stackoverflow.com/questions/7577627/purpose-of-constructor-chaining", "author": "annym", "createdAt": "2020-07-23T19:42:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MDg4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\nindex 6dabb784ebc..39ab304c210 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/LogReplicationConfig.java\n\n@@ -54,10 +54,7 @@ public class LogReplicationConfig {\n      * @param streamsToReplicate Unique identifiers for all streams to be replicated across sites.\n      */\n     public LogReplicationConfig(Set<String> streamsToReplicate) {\n-        this.streamsToReplicate = streamsToReplicate;\n-        this.maxNumMsgPerBatch = DEFAULT_MAX_NUM_MSG_PER_BATCH;\n-        this.maxMsgSize = MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED;\n-        this.maxDataSizePerMsg = maxMsgSize * DATA_FRACTION_PER_MSG / 100;\n+        this(streamsToReplicate, DEFAULT_MAX_NUM_MSG_PER_BATCH, MAX_DATA_MSG_SIZE_SUPPORTED);\n     }\n \n     /**\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTE1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171156", "bodyText": "What is this buffer?", "author": "annym", "createdAt": "2020-07-23T01:28:35Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/infrastructure/plugins/DefaultClusterConfig.java", "diffHunk": "@@ -45,7 +45,7 @@ private DefaultClusterConfig() {\n     private static String standbyLogReplicationPort = \"9020\";\n \n     @Getter\n-    private static int logSenderBufferSize = 20;\n+    private static int logSenderBufferSize = 2;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY3MTkzMw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459671933", "bodyText": "At both the sender and receiver we have buffers for messages. As the message is big, the standard double buffering may be enough.", "author": "xiaoqin2012", "createdAt": "2020-07-23T19:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTE1Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTMxOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171318", "bodyText": "Can we add a comment. Not sure what this means.", "author": "annym", "createdAt": "2020-07-23T01:29:19Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/fsm/TestLogEntryReader.java", "diffHunk": "@@ -26,4 +26,9 @@ public void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp) {\n     public void setTopologyConfigId(long siteConfigID) {\n \n     }\n+\n+    @Override\n+    public boolean hasNoiseData() {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE3MTkyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459171929", "bodyText": "Shouldn't we remove metadata.getTimestamp?", "author": "annym", "createdAt": "2020-07-23T01:32:06Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex 77b1d10c914..c783748d5cf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -84,8 +84,7 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n          */\n         for (LogReplicationEntry entry : buffer.values()) {\n             LogReplicationEntryMetadata metadata = entry.getMetadata();\n-            if (metadata.getTimestamp() < lastProcessedSeq) {\n-                //remove it\n+            if (metadata.getTimestamp() <= lastProcessedSeq) {\n                 buffer.remove(metadata.getPreviousTimestamp());\n             } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n                 sinkManager.processMessage(entry);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTU2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459181569", "bodyText": "remove lastProcessedSeq or metadata.getTimestamp? as lastProcessedSeq is the one that has already been processed and we just applied entry.", "author": "annym", "createdAt": "2020-07-23T02:16:46Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it\n+                buffer.remove(metadata.getPreviousTimestamp());\n+            } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n+                sinkManager.processMessage(entry);\n+                ackCnt++;\n+                buffer.remove(lastProcessedSeq);", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc0OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459714748", "bodyText": "For each message, it has the previous time stamp and the currentTimestamp( the max ts of all transactions in this message). So if all transactions in one message have been processed, we will remove it. If it has transactions that haven't been processed, we will still buffer it and process it later.\nThis happens because the possible leadership change and the two leaders at the active cluster may packaging the messages differently as the polling transaction log speed  are different at the different nodes.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:39:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MTU2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex 77b1d10c914..c783748d5cf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -84,8 +84,7 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n          */\n         for (LogReplicationEntry entry : buffer.values()) {\n             LogReplicationEntryMetadata metadata = entry.getMetadata();\n-            if (metadata.getTimestamp() < lastProcessedSeq) {\n-                //remove it\n+            if (metadata.getTimestamp() <= lastProcessedSeq) {\n                 buffer.remove(metadata.getPreviousTimestamp());\n             } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n                 sinkManager.processMessage(entry);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MjA4Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182086", "bodyText": "This comment might be redundant.", "author": "annym", "createdAt": "2020-07-23T02:19:01Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {\n+                //remove it", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex 77b1d10c914..c783748d5cf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -84,8 +84,7 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n          */\n         for (LogReplicationEntry entry : buffer.values()) {\n             LogReplicationEntryMetadata metadata = entry.getMetadata();\n-            if (metadata.getTimestamp() < lastProcessedSeq) {\n-                //remove it\n+            if (metadata.getTimestamp() <= lastProcessedSeq) {\n                 buffer.remove(metadata.getPreviousTimestamp());\n             } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n                 sinkManager.processMessage(entry);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4Mjk1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459182951", "bodyText": "<= , right? cause if the same last processed entry was resent we can remove or we'll have a leak.", "author": "annym", "createdAt": "2020-07-23T02:22:28Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {\n+        /**\n+         *  For each message in the  buffer, if its timestamp is smaller than last processed log entry's timestamp,\n+         *  skip processing and remove it from buffer.\n+         *  If its preTs and currentTs is overlapping with the last processed log entry's timestamp, process it.\n+         */\n+        for (LogReplicationEntry entry : buffer.values()) {\n+            LogReplicationEntryMetadata metadata = entry.getMetadata();\n+            if (metadata.getTimestamp() < lastProcessedSeq) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex 77b1d10c914..c783748d5cf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -84,8 +84,7 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n          */\n         for (LogReplicationEntry entry : buffer.values()) {\n             LogReplicationEntryMetadata metadata = entry.getMetadata();\n-            if (metadata.getTimestamp() < lastProcessedSeq) {\n-                //remove it\n+            if (metadata.getTimestamp() <= lastProcessedSeq) {\n                 buffer.remove(metadata.getPreviousTimestamp());\n             } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n                 sinkManager.processMessage(entry);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459183539", "bodyText": "What happens if there are remaining messages in the queue which are skipped because they were not the subsequent sequence numbers and no further messages are received? would we ever apply them? Let me illustrate it. For instance:\nUpdates to Stream A: 0, 1, 2, 5, 7, 8\nlastProcessedSeqNum = 2\nLet's say the buffer looks like this (as some messages were resent and they are out of order):\n0, 2, 1, 0, 7, 8, 5\nWe ignore 0, 2, 1, 0... We leave 7 in the buffer but do not process, same with 8... Then we process 5. But now 7 and 8 are left in the queue. And if no more data is received for an hour, 7 and 8 won't be applied, as this is kicked off as part of receiving data.", "author": "annym", "createdAt": "2020-07-23T02:25:25Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java", "diffHunk": "@@ -74,4 +75,24 @@ public boolean verifyMessageType(LogReplicationEntry entry) {\n \n         return true;\n     }\n+\n+    public void processBuffer() {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4NTk5Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459185992", "bodyText": "mmm I kept looking and I see we have a processBuffer in SinkBufferManager, which would take care of out of order messages, but one question, in that method we get the lastProcessedSeqNum, isn't that the one that we already applied, and that should not be available anymore in the buffer? Thus, we are not really processing the unordered until a new message is received?", "author": "annym", "createdAt": "2020-07-23T02:37:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNjk5Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459716993", "bodyText": "In your case, when it receives 5, it will process 5 and right after process it:\n\nit will update the lastSeqNum\nit will look at the buffer to see if there are any messages are in order to be processed.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:43:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMjY0Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459722642", "bodyText": "the map is indexed according to the preTs/preSeq number.\nFor your case the map:\nafter process message 2, the lastTs = 2\nfor message 7, the preTs is 5\nfor message 8, the preTs is 7\nfor message 5, the preTs is 2: we will process it and update the lastTs as 5, use 5 as the index to lookup and find message 7.", "author": "xiaoqin2012", "createdAt": "2020-07-23T20:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4MzUzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\nindex 77b1d10c914..c783748d5cf 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/LogEntrySinkBufferManager.java\n\n@@ -84,8 +84,7 @@ public class LogEntrySinkBufferManager extends SinkBufferManager {\n          */\n         for (LogReplicationEntry entry : buffer.values()) {\n             LogReplicationEntryMetadata metadata = entry.getMetadata();\n-            if (metadata.getTimestamp() < lastProcessedSeq) {\n-                //remove it\n+            if (metadata.getTimestamp() <= lastProcessedSeq) {\n                 buffer.remove(metadata.getPreviousTimestamp());\n             } else if (metadata.getPreviousTimestamp() <= lastProcessedSeq && metadata.getTimestamp() > lastProcessedSeq) {\n                 sinkManager.processMessage(entry);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE4ODgzNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459188837", "bodyText": "haven't -> haven't been applied", "author": "annym", "createdAt": "2020-07-23T02:51:15Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\nindex 7ca9a3150a9..3acf4e8c408 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\n\n@@ -136,12 +120,12 @@ public abstract class SinkBufferManager {\n     public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage) {\n \n         if (verifyMessageType(dataMessage) == false)\n-           return null;\n+            return null;\n \n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        // This message contains entries that haven't applied yet\n+        // This message contains entries that haven't been applied yet\n         if (preTs <= lastProcessedSeq && currentTs > lastProcessedSeq) {\n             sinkManager.processMessage(dataMessage);\n             ackCnt++;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5Mjk5Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459192997", "bodyText": "Can this happen because a LogEntryMessage might have numerous delta's inside? and it picks the min of all as the previous and the max of all as the timestamp?", "author": "annym", "createdAt": "2020-07-23T03:11:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java", "diffHunk": "@@ -141,7 +141,8 @@ public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage)\n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        if (preTs == lastProcessedSeq) {\n+        // This message contains entries that haven't applied yet\n+        if (preTs <= lastProcessedSeq && currentTs > lastProcessedSeq) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\nindex 7ca9a3150a9..3acf4e8c408 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/receive/SinkBufferManager.java\n\n@@ -136,12 +120,12 @@ public abstract class SinkBufferManager {\n     public LogReplicationEntry processMsgAndBuffer(LogReplicationEntry dataMessage) {\n \n         if (verifyMessageType(dataMessage) == false)\n-           return null;\n+            return null;\n \n         long preTs = getPreSeq(dataMessage);\n         long currentTs = getCurrentSeq(dataMessage);\n \n-        // This message contains entries that haven't applied yet\n+        // This message contains entries that haven't been applied yet\n         if (preTs <= lastProcessedSeq && currentTs > lastProcessedSeq) {\n             sinkManager.processMessage(dataMessage);\n             ackCnt++;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NjE0Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459196143", "bodyText": "Can you please add a description to this.", "author": "annym", "createdAt": "2020-07-23T03:26:45Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java", "diffHunk": "@@ -23,4 +23,6 @@\n     void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp);\n \n     void setTopologyConfigId(long topologyConfigId);\n+", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java\nindex db92d9f728c..7857e177d57 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/LogEntryReader.java\n\n@@ -22,7 +22,13 @@ public interface LogEntryReader {\n \n     void reset(long lastSentBaseSnapshotTimestamp, long lastAckedTimestamp);\n \n+    // Set current topologyConfigId that will be used to construct messages.\n     void setTopologyConfigId(long topologyConfigId);\n \n+    // If the transaction log contains both replicated streams and other streams,\n+    // we treat it as nosieData.\n+    // If the log data size is bigger than the max msg size supported, we set hasNoiseData too.\n+    // When haNoiseData, a log replication exception will be thrown and triggers\n+    // to stop the log replication state machine.\n     boolean hasNoiseData();\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5NzEwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459197102", "bodyText": "This class can be removed right?", "author": "annym", "createdAt": "2020-07-23T03:31:16Z", "path": "test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java", "diffHunk": "@@ -0,0 +1,4 @@\n+package org.corfudb.integration;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java b/test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java\ndeleted file mode 100644\nindex 151f9428fee..00000000000\n--- a/test/src/test/java/org/corfudb/integration/ReplicationReaderWriterWithUFOIT.java\n+++ /dev/null\n\n@@ -1,4 +0,0 @@\n-package org.corfudb.integration;\n-\n-public class ReplicationReaderWriterWithUFOIT {\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTAwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199005", "bodyText": "curious why did we remove this assertion? it should still hold valid.", "author": "annym", "createdAt": "2020-07-23T03:40:48Z", "path": "test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java", "diffHunk": "@@ -349,8 +349,6 @@ public void testSnapshotSyncStreamImplementation() throws Exception {\n \n         Queue<LogReplicationEntry> listenerQueue = ((TestDataSender) dataSender).getEntryQueue();\n \n-        assertThat(LARGE_NUM_ENTRIES/ StreamsSnapshotReader.MAX_NUM_SMR_ENTRY).isLessThanOrEqualTo(listenerQueue.size());", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjE3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459726177", "bodyText": "Now we are using the size not the number of entries.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTAwNQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java b/test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java\nindex 137beef9b44..cbde7845bcb 100644\n--- a/test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java\n+++ b/test/src/test/java/org/corfudb/infrastructure/logreplication/LogReplicationFSMTest.java\n\n@@ -342,7 +344,8 @@ public class LogReplicationFSMTest extends AbstractViewTest implements Observer\n \n         // Block until the snapshot sync completes and next transition occurs.\n         while (fsm.getState().getType() != LogReplicationStateType.IN_LOG_ENTRY_SYNC) {\n-            //\n+            sleep(100);\n+            log.info(\"stateType {} expected type {}\", fsm.getState().getType(), LogReplicationStateType.IN_LOG_ENTRY_SYNC);\n         }\n \n         assertThat(fsm.getState().getType()).isEqualTo(LogReplicationStateType.IN_LOG_ENTRY_SYNC);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5OTA2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459199069", "bodyText": "we can remove this commented code.", "author": "annym", "createdAt": "2020-07-23T03:41:15Z", "path": "runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java", "diffHunk": "@@ -135,6 +145,41 @@ void deserializeBuffer(ByteBuf b, CorfuRuntime rt) {\n             b.skipBytes(len);\n         }\n         SMRArguments = arguments;\n+        serializedSize = b.readerIndex() - readIndex + 1;\n+    }\n+\n+\n+    /**\n+     * Calculate an Opaque SMR entry's serialized size.\n+     * @throws IllegalAccessException\n+     */\n+    private int calculateOpaqueSMREntrySerializedSize() {\n+        if (!opaque) {\n+            log.error(\"This operation only supported for an opaque SMR entry\");\n+            return 0;\n+        }\n+\n+        int size = 0;\n+\n+        for (Object smrArg : SMRArguments) {\n+            size += ((byte[])smrArg).length;\n+        }\n+\n+        size += (SMRMethod.length() * Character.BYTES);\n+        size += Integer.BYTES;\n+\n+        return size;\n+    }\n+\n+    /**\n+     * The serialized size of an opaque SMR entry.\n+     * @return\n+     */\n+    public synchronized Integer getSerializedSize() {\n+        //if (serializedSize == null) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\nindex c41be84ed75..47f5c6c320b 100644\n--- a/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n+++ b/runtime/src/main/java/org/corfudb/protocols/logprotocol/SMREntry.java\n\n@@ -171,17 +171,6 @@ public class SMREntry extends LogEntry implements ISMRConsumable {\n         return size;\n     }\n \n-    /**\n-     * The serialized size of an opaque SMR entry.\n-     * @return\n-     */\n-    public synchronized Integer getSerializedSize() {\n-        //if (serializedSize == null) {\n-        //    serializedSize = calculateOpaqueSMREntrySerializedSize();\n-       // }\n-        return serializedSize;\n-    }\n-\n     /**\n      * Given a buffer with the logreader index pointing to a serialized SMREntry, this method will\n      * seek the buffer's logreader index to the end of the entry.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMDQ0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459200446", "bodyText": "Shouldn't this be LogEntryReader as it could have any implementation? like the test one we use in some test.", "author": "annym", "createdAt": "2020-07-23T03:48:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java", "diffHunk": "@@ -32,7 +33,7 @@\n     /*\n      * Implementation of Log Entry Reader. Default implementation reads at the stream layer.\n      */\n-    private LogEntryReader logEntryReader;\n+    private StreamsLogEntryReader logEntryReader;", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java\nindex 2466a6a61c6..10dab4e47af 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/LogEntrySender.java\n\n@@ -33,7 +33,7 @@ public class LogEntrySender {\n     /*\n      * Implementation of Log Entry Reader. Default implementation reads at the stream layer.\n      */\n-    private StreamsLogEntryReader logEntryReader;\n+    private LogEntryReader logEntryReader;\n \n    /*\n     * Implementation of buffering messages and sending/resending messages\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMTk2Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459201963", "bodyText": "A bit confusing, maybe -> Max number of messages sent in burst during a snapshot cycle. (or something in that line)", "author": "annym", "createdAt": "2020-07-23T03:56:04Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java", "diffHunk": "@@ -39,15 +42,14 @@\n @Slf4j\n public class SnapshotSender {\n \n-    public static int DEFAULT_SNAPSHOT_BATCH_SIZE = 100;\n-    public static final int DEFAULT_TIMEOUT = 5000;\n-\n     private CorfuRuntime runtime;\n     private SnapshotReader snapshotReader;\n     private SenderBufferManager dataSenderBufferManager;\n     private LogReplicationFSM fsm;\n     private long baseSnapshotTimestamp;\n-    private final int snapshotSyncBatchSize;\n+\n+    // The max number of message can be sent over per cycle run during snapshot full sync state.", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java\nindex 190347d0550..5df0b67b12f 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/SnapshotSender.java\n\n@@ -48,7 +48,7 @@ public class SnapshotSender {\n     private LogReplicationFSM fsm;\n     private long baseSnapshotTimestamp;\n \n-    // The max number of message can be sent over per cycle run during snapshot full sync state.\n+    // The max number of message can be sent over in burst for a snapshot cycle.\n     private final int maxNumSnapshotMsgPerBatch;\n \n     // This flag will indicate the start of a snapshot sync, so start snapshot marker is sent once.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzcwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203702", "bodyText": "entry is never used.", "author": "annym", "createdAt": "2020-07-23T04:04:48Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzgzNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203835", "bodyText": "please add access modifier, private. As this is always a static analysis error.", "author": "annym", "createdAt": "2020-07-23T04:05:15Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwMzg5Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459203896", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T04:05:38Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -72,7 +72,7 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+    private LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n         // Set the last timestamp as the max timestamp\n         currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n         LogReplicationEntry txMessage = new LogReplicationEntry(MSG_TYPE, topologyConfigId, logEntryRequestId,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTQyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205422", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T04:13:43Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -32,45 +34,58 @@\n     private Set<UUID> streamUUIDs;\n \n     // the opaquestream wrapper for the transaction stream.\n-    private TxOpaqueStream txStream;\n-   \n+    private TxOpaqueStream txOpaqueStream;\n \n     // the base snapshot the log entry logreader starts to poll transaction logs\n     private long globalBaseSnapshot;\n+\n     // timestamp of the transaction log that is the previous message\n     private long preMsgTs;\n+\n     // the timestamp of the transaction log that is the current message\n     private long currentMsgTs;\n+\n     // the sequence number of the message based on the globalBaseSnapshot\n     private long sequence;\n \n     private long topologyConfigId;\n \n+    private final int maxDataSizePerMsg;\n+\n+    private OpaqueEntry lastOpaqueEntry = null;\n+\n+    private boolean hasNoiseData = false;\n+\n     public StreamsLogEntryReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n+\n         Set<String> streams = config.getStreamsToReplicate();\n+\n         streamUUIDs = new HashSet<>();\n         for (String s : streams) {\n             streamUUIDs.add(CorfuRuntime.getStreamID(s));\n         }\n \n         //create an opaque stream for transaction stream\n-        txStream = new TxOpaqueStream(rt);\n+        txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessage(OpaqueEntry entry, UUID logEntryRequestId) {\n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, entry);\n-        currentMsgTs = entry.getVersion();\n+    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+        // Set the last timestamp as the max timestamp\n+        currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n         LogReplicationEntry txMessage = new LogReplicationEntry(MSG_TYPE, topologyConfigId, logEntryRequestId,\n-                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, buf.array());\n+                currentMsgTs, preMsgTs, globalBaseSnapshot, sequence, opaqueEntryList);\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        return  txMessage;\n+        log.trace(\"Generate a log entry message {} with {} transactions \", txMessage.getMetadata(), opaqueEntryList.size());\n+        return txMessage;\n     }\n \n-    boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n+\n+    // Check if it has the correct streams.\n+    boolean shouldProcess(OpaqueEntry entry) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -72,7 +72,7 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         txOpaqueStream = new TxOpaqueStream(rt);\n     }\n \n-    LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n+    private LogReplicationEntry generateMessageWithOpaqueEntryList(List<OpaqueEntry> opaqueEntryList, UUID logEntryRequestId) {\n         // Set the last timestamp as the max timestamp\n         currentMsgTs = opaqueEntryList.get(opaqueEntryList.size() - 1).getVersion();\n         LogReplicationEntry txMessage = new LogReplicationEntry(MSG_TYPE, topologyConfigId, logEntryRequestId,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNTcyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459205728", "bodyText": "maybe rename to checkValidSize?", "author": "annym", "createdAt": "2020-07-23T04:15:13Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206762", "bodyText": "No need to break as it is already in the while condition, or if we want to break before the while loop remove from the loop.", "author": "annym", "createdAt": "2020-07-23T04:19:56Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {", "originalCommit": "49dc824b6fca4e7bc9c5639308f6db6440016cc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjkwOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459206909", "bodyText": "I think it's better in the while loop.\nwhile(currentMsgSize < maxDataSizePerMsg && !hasNoiseData && txOpaqueStream.hasNext()) ... it's ok if we read a next (when there is no, as that method returns null if !hasNext)\nBut if you want to take it out of the while and leave it here, we might simplify for readiness:\nif (hasNoiseData || !txOpaqueStream.hasNext()) {\nbreak;\n}", "author": "annym", "createdAt": "2020-07-23T04:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MjIxNw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460182217", "bodyText": "Consider that the max_msg_size is 100 and there are only two entrys. entry0 with size  90, entry1 with size 20\nafter processing the first entry, the while is correct, then we set  lastEntry = entry2.\nbut entry2 could not go with the first msg.\nwhile constructing the second message, if there are no new entrys, the hasNext gives false, but we need to add entry2 first. So the logic is check the lastEntry first. then calling the next.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:11:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwNjc2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDM5OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214399", "bodyText": "why do we set hasNoiseData to true, the fact that the size of the entry is greater than the boundary does not have implications on this, right?", "author": "annym", "createdAt": "2020-07-23T04:56:54Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5MjYxNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460192614", "bodyText": "Changed it.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:32:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDM5OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDcxNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214715", "bodyText": "this is repeated. Same statement as before.", "author": "annym", "createdAt": "2020-07-23T04:58:39Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNTY5OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459825698", "bodyText": "I think this was accidentally copied. Same as in L.113.", "author": "annym", "createdAt": "2020-07-24T02:33:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDcxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNDk1Mw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459214953", "bodyText": "simplify:\nreturn !(currentEntrySize + CurrentMsgSize > maxDataSizePerMsg);", "author": "annym", "createdAt": "2020-07-23T04:59:34Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215426", "bodyText": "what happens if there is noisy data but the list is not empty? we should still abort, right?", "author": "annym", "createdAt": "2020-07-23T05:01:58Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNTk1MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459825951", "bodyText": "??", "author": "annym", "createdAt": "2020-07-24T02:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NzY4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460177688", "bodyText": "For example, we have log0, log1, log2, log3. All logs from 0 to 2 are good, but log3 is bad. Should we transfer log0, log1, log2, first, then abort when process log3?", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:02:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIwNTM0MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460205340", "bodyText": "Well from early discussions with Medhavi my understanding is that it's just an invalid state, so we can directly abort without processing anything.", "author": "annym", "createdAt": "2020-07-24T17:57:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTQyNg=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTc0MQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459215741", "bodyText": "It's cleaner if generateMessageWithOpaqueEntryList accepts the empty list and returns a null. Less if conditions.", "author": "annym", "createdAt": "2020-07-23T05:03:06Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java", "diffHunk": "@@ -84,39 +99,100 @@ boolean shouldProcess(OpaqueEntry entry) throws ReplicationReaderException {\n             return false;\n         }\n \n-        //If the entry's stream set contains both interested streams and other streams, it is not\n-        //the expected behavior\n+        // If the entry's stream set contains both interested streams and other streams, it is not\n+        // the expected behavior\n         log.error(\"There are noisy streams {} in the entry, expected streams set {}\",\n-                    entry.getEntries().keySet(), streamUUIDs);\n+                entry.getEntries().keySet(), streamUUIDs);\n+\n+        hasNoiseData = true;\n+        return false;\n+    }\n+\n+    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+        // For interested entry, if its size is too big we should skip and report error\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n+                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+            hasNoiseData = true;\n+            return false;\n+        }\n+\n+        if (currentEntrySize > maxDataSizePerMsg) {\n+            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                    currentEntrySize, maxDataSizePerMsg);\n+        }\n \n-        throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+        // Skip append this entry, will process it for the next message;\n+        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg) {\n+            return false;\n+        }\n+\n+        return true;\n     }\n \n     public void setGlobalBaseSnapshot(long snapshot, long ackTimestamp) {\n         globalBaseSnapshot = snapshot;\n         preMsgTs = Math.max(snapshot, ackTimestamp);\n         log.info(\"snapshot {} ackTimestamp {} preMsgTs {} seek {}\", snapshot, ackTimestamp, preMsgTs, preMsgTs + 1);\n-        txStream.seek(preMsgTs + 1);\n+        txOpaqueStream.seek(preMsgTs + 1);\n         sequence = 0;\n     }\n \n+\n     @Override\n     public LogReplicationEntry read(UUID logEntryRequestId) throws TrimmedException, IllegalTransactionStreamsException {\n+        List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n+        int currentEntrySize = 0;\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (txStream.hasNext()) {\n-                OpaqueEntry opaqueEntry = txStream.next();\n-                if (!shouldProcess(opaqueEntry)) {\n-                    continue;\n+            while (currentMsgSize < maxDataSizePerMsg && !hasNoiseData) {\n+\n+                if (lastOpaqueEntry != null && shouldProcess(lastOpaqueEntry)) {\n+\n+                    // If the currentEntry is too big to append the current message, will skip it and\n+                    // append it to the next message as the first entry.\n+                    currentEntrySize = ReaderUtility.calculateOpaqueEntrySize(lastOpaqueEntry);\n+\n+                    if (!checkSizeOK(lastOpaqueEntry, currentMsgSize, currentEntrySize)) {\n+                        break;\n+                    }\n+\n+                    // Add the lastOpaqueEntry to the current message.\n+                    opaqueEntryList.add(lastOpaqueEntry);\n+                    currentMsgSize += currentEntrySize;\n+                    lastOpaqueEntry = null;\n+                }\n+\n+                if (hasNoiseData) {\n+                    break;\n                 }\n-                LogReplicationEntry txMessage = generateMessage(opaqueEntry, logEntryRequestId);\n-                return txMessage;\n+\n+                if (!txOpaqueStream.hasNext()) {\n+                    break;\n+                }\n+\n+                lastOpaqueEntry = txOpaqueStream.next();\n             }\n+\n+            log.trace(\"Generate LogEntryDataMessage size {} with {} entries for maxDataSizePerMsg {}. lastEnry size {}\",\n+                    currentMsgSize, opaqueEntryList.size(), maxDataSizePerMsg, lastOpaqueEntry == null? 0 : currentEntrySize);\n+\n+            if (opaqueEntryList.size() == 0 && hasNoiseData) {\n+                throw new IllegalTransactionStreamsException(\"There are noisy streams in the transaction log entry\");\n+            }\n+\n+            if (opaqueEntryList.size() == 0) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\nindex f950229e8ad..f1db89ff72b 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsLogEntryReader.java\n\n@@ -108,11 +108,11 @@ public class StreamsLogEntryReader implements LogEntryReader {\n         return false;\n     }\n \n-    boolean checkSizeOK(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n+    private boolean checkValidSize(OpaqueEntry entry, int currentMsgSize, int currentEntrySize) {\n         // For interested entry, if its size is too big we should skip and report error\n         if (currentEntrySize > maxDataSizePerMsg) {\n             log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\",\n-                    currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                    currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n             hasNoiseData = true;\n             return false;\n         }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNjAxNA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459216014", "bodyText": "can this logger be removed?", "author": "annym", "createdAt": "2020-07-23T05:04:24Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 8ce650a0caf..cb4332c4008 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -127,8 +127,8 @@ public class StreamsSnapshotReader implements SnapshotReader {\n                     if (smrEntries != null) {\n                         int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n \n-                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n-                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                        if (currentEntrySize > MAX_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n                             throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n                         } else if (currentEntrySize > maxDataSizePerMsg) {\n                             observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNzEyOA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459217128", "bodyText": "L 151 -157 can be simplified:\nif (!stream.iterator.hasNext()) {\nbreak;\n}\nlastEntry = (OpaqueEntry) stream.iterator.next();", "author": "annym", "createdAt": "2020-07-23T05:09:31Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {\n+                            break;\n+                        }\n+\n+                        smrList.addAll(smrEntries);\n+                        currentMsgSize += currentEntrySize;\n+                        stream.maxVersion = Math.max(stream.maxVersion, lastEntry.getVersion());\n+                    }\n+                    lastEntry = null;\n+                }\n+\n+                if (stream.iterator.hasNext()) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 8ce650a0caf..cb4332c4008 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -127,8 +127,8 @@ public class StreamsSnapshotReader implements SnapshotReader {\n                     if (smrEntries != null) {\n                         int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n \n-                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n-                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                        if (currentEntrySize > MAX_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n                             throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n                         } else if (currentEntrySize > maxDataSizePerMsg) {\n                             observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODUyOQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218529", "bodyText": "I'm confused, why do we have this same comparison of a single currentEntrySize twice? shouldn't it only compare with maxDataSizePerMsg, as this is the user configured (which could default to MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED)", "author": "annym", "createdAt": "2020-07-23T05:15:40Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5NzA3Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460197077", "bodyText": "User can setup a different one. Supported is the one we can support. The user setup one is the one the user thinks it gives the best performance. Like 64MB is the max we can support. But in reality maybe 2M gives the best performance. But it depends the real hardware config that the users are aware of or have done experiments with.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:41:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODUyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 8ce650a0caf..cb4332c4008 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -127,8 +127,8 @@ public class StreamsSnapshotReader implements SnapshotReader {\n                     if (smrEntries != null) {\n                         int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n \n-                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n-                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                        if (currentEntrySize > MAX_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n                             throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n                         } else if (currentEntrySize > maxDataSizePerMsg) {\n                             observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODk2OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459218969", "bodyText": "why do we need currentMsgSize != 0? If we already checked that currentEntrySize is bigger previously and we throw an exception.", "author": "annym", "createdAt": "2020-07-23T05:17:45Z", "path": "infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java", "diffHunk": "@@ -56,68 +67,103 @@\n     public StreamsSnapshotReader(CorfuRuntime runtime, LogReplicationConfig config) {\n         this.rt = runtime;\n         this.rt.parseConfigurationString(runtime.getLayoutServers().get(0)).connect();\n+        this.maxDataSizePerMsg = config.getMaxDataSizePerMsg();\n         streams = config.getStreamsToReplicate();\n+        log.debug(\"The maxDataSizePerMsg {} \", maxDataSizePerMsg);\n     }\n \n     /**\n      * Given a streamID and list of smrEntries, generate an OpaqueEntry\n      * @param streamID\n-     * @param smrEntries\n+     * @param entryList\n      * @return\n      */\n-    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID, List smrEntries) {\n+    private OpaqueEntry generateOpaqueEntry(long version, UUID streamID,  SMREntryList entryList) {\n         Map<UUID, List<SMREntry>> map = new HashMap<>();\n-        map.put(streamID, smrEntries);\n+        map.put(streamID, entryList.getSmrEntries());\n         return new OpaqueEntry(version, map);\n     }\n \n     /**\n      * Given a list of entries with the same stream, will generate an OpaqueEntry and\n      * use the opaque entry to generate a TxMessage.\n      * @param stream\n-     * @param entries\n+     * @param entryList\n      * @return\n      */\n-    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, List<SMREntry> entries, UUID snapshotRequestId) {\n+    private LogReplicationEntry generateMessage(OpaqueStreamIterator stream, SMREntryList entryList, UUID snapshotRequestId) {\n         currentMsgTs = stream.maxVersion;\n-        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entries);\n+        OpaqueEntry opaqueEntry = generateOpaqueEntry(currentMsgTs, stream.uuid, entryList);\n         if (!stream.iterator.hasNext()) {\n             //mark the end of the current stream.\n             currentMsgTs = snapshotTimestamp;\n         }\n \n-        ByteBuf buf = Unpooled.buffer();\n-        OpaqueEntry.serialize(buf, opaqueEntry);\n+        LogReplicationEntry txMsg = new LogReplicationEntry(MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n+                preMsgTs, snapshotTimestamp, sequence, opaqueEntry);\n \n-        org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry txMsg = new org.corfudb.protocols.wireprotocol.logreplication.LogReplicationEntry\n-                (MessageType.SNAPSHOT_MESSAGE, topologyConfigId, snapshotRequestId, currentMsgTs,\n-                preMsgTs, snapshotTimestamp, sequence, buf.array());\n         preMsgTs = currentMsgTs;\n         sequence++;\n-        log.debug(\"Generate TxMsg {}\", txMsg.getMetadata());\n+\n+        log.trace(\"txMsg {} deepsize sizeInBytes {} entryList.sizeInByres {}  with numEntries {} deepSize sizeInBytes {}\",\n+                txMsg.getMetadata(), MetricsUtils.sizeOf.deepSizeOf(txMsg), entryList.getSizeInBytes(), entryList.getSmrEntries().size(), MetricsUtils.sizeOf.deepSizeOf(entryList.smrEntries));\n+\n         return txMsg;\n     }\n \n     /**\n-     * Read numEntries from the current stream.\n+     * Read log data from the current stream until the sum of all SMR entries's sizeInBytes reaches the maxDataSizePerMsg.\n      * @param stream\n-     * @param numEntries\n      * @return\n      */\n-    private List<SMREntry> next(OpaqueStreamIterator stream, int numEntries) {\n-        //if it is the end of the stream, set an end of stream mark, the current\n-        List<SMREntry> list = new ArrayList<>();\n+    private SMREntryList next(OpaqueStreamIterator stream) {\n+        List<SMREntry> smrList = new ArrayList<>();\n+        int currentMsgSize = 0;\n+\n         try {\n-            while (stream.iterator.hasNext() && list.size() < numEntries) {\n-                OpaqueEntry entry = (OpaqueEntry) stream.iterator.next();\n-                stream.maxVersion = Math.max(stream.maxVersion, entry.getVersion());\n-                list.addAll(entry.getEntries().get(stream.uuid));\n+            while (currentMsgSize < maxDataSizePerMsg) {\n+                if (lastEntry != null) {\n+                    List<SMREntry> smrEntries = lastEntry.getEntries().get(stream.uuid);\n+                    if (smrEntries != null) {\n+                        int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n+\n+                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                            throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n+                        } else if (currentEntrySize > maxDataSizePerMsg) {\n+                            observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n+                            log.warn(\"The current entry size {} is bigger than the configured maxDataSizePerMsg {}\",\n+                                    currentEntrySize, maxDataSizePerMsg);\n+                        }\n+\n+                        // Skip append this entry in this message. Will process it first at the next round.\n+                        if (currentEntrySize + currentMsgSize > maxDataSizePerMsg && currentMsgSize != 0) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5Nzg0Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460197846", "bodyText": "If it is the only entry and its size is bigger than the configured but smaller than the max_supported, we still continue the replication , right? I thought we have discussed this before.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:42:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxODk2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\nindex 8ce650a0caf..cb4332c4008 100644\n--- a/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n+++ b/infrastructure/src/main/java/org/corfudb/infrastructure/logreplication/replication/send/logreader/StreamsSnapshotReader.java\n\n@@ -127,8 +127,8 @@ public class StreamsSnapshotReader implements SnapshotReader {\n                     if (smrEntries != null) {\n                         int currentEntrySize = ReaderUtility.calculateSize(smrEntries);\n \n-                        if (currentEntrySize > MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED) {\n-                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_LOG_REPLICATION_DATA_MSG_SIZE_SUPPORTED);\n+                        if (currentEntrySize > MAX_DATA_MSG_SIZE_SUPPORTED) {\n+                            log.error(\"The current entry size {} is bigger than the maxDataSizePerMsg {} supported\", currentEntrySize, MAX_DATA_MSG_SIZE_SUPPORTED);\n                             throw new IllegalSnapshotEntrySizeException(\" The snapshot entry is bigger than the system supported\");\n                         } else if (currentEntrySize > maxDataSizePerMsg) {\n                             observeBiggerMsg.setValue(observeBiggerMsg.getValue()+1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMjQ4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459222484", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T05:32:52Z", "path": "runtime/src/main/java/org/corfudb/protocols/wireprotocol/logreplication/LogReplicationEntry.java", "diffHunk": "@@ -13,41 +21,102 @@\n  *\n  * @author annym\n  */\n+@Slf4j\n @Data\n public class LogReplicationEntry implements ICorfuPayload<LogReplicationEntry> {\n \n     private LogReplicationEntryMetadata metadata;\n \n-    private byte[] payload;\n+    private List<OpaqueEntry> opaqueEntryList = new ArrayList<>();\n \n-    public LogReplicationEntry(LogReplicationEntryMetadata metadata, byte[] payload) {\n-        this.payload = payload;\n+\n+    // Only used by test cases\n+    @VisibleForTesting\n+    byte[] payload;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzMxMQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223311", "bodyText": "Camel case please.", "author": "annym", "createdAt": "2020-07-23T05:36:15Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -313,6 +315,7 @@ public static Process runReplicationServer(int port, String pluginConfigFilePath\n                 .setHost(DEFAULT_HOST)\n                 .setPort(port)\n                 .setPluginConfigFilePath(pluginConfigFilePath)\n+                .setMsg_size(MSG_SIZE)", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMzY4NA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459223684", "bodyText": "Java uses CamelCase.", "author": "annym", "createdAt": "2020-07-23T05:37:36Z", "path": "test/src/test/java/org/corfudb/integration/AbstractIT.java", "diffHunk": "@@ -507,6 +510,7 @@ public Process runServer() throws IOException {\n         private String compressionCodec = null;\n         private String pluginConfigFilePath = null;\n         private String logPath = null;\n+        private int msg_size = 0;", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDM0OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224348", "bodyText": "private", "author": "annym", "createdAt": "2020-07-23T05:40:18Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -311,6 +324,16 @@ void verifyTables(HashMap<String, CorfuTable<Long, Long>> tables0, HashMap<Strin\n             }\n     }\n \n+    void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex ad8e222cd20..1d581d81dbc 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -324,6 +323,11 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n             }\n     }\n \n+    /**\n+     * Wait replication data reach at the standby cluster.\n+     * @param tables\n+     * @param hashMap\n+     */\n     void waitData(HashMap<String, CorfuTable<Long, Long>> tables, HashMap<String, HashMap<Long, Long>> hashMap) {\n         for (String name : hashMap.keySet()) {\n             CorfuTable<Long, Long> table = tables.get(name);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDYwNQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459224605", "bodyText": "Why did this change?", "author": "annym", "createdAt": "2020-07-23T05:41:17Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -695,7 +742,7 @@ public void testLogEntrySyncValidCrossTablesWithTriggerTimeout() throws Exceptio\n         expectedAckMessages =  NUM_KEYS*WRITE_CYCLES;\n \n         testConfig.clear().setDropMessageLevel(2);\n-        startLogEntrySync(crossTables, WAIT.ON_ERROR);\n+        startLogEntrySync(crossTables, WAIT.ON_TIMEOUT_ERROR);", "originalCommit": "ef54576388fe5f0e24a6a63a3b8de1b2cf83aced", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE5OTU4OQ==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r460199589", "bodyText": "It is much clear about what each test is wait on.", "author": "xiaoqin2012", "createdAt": "2020-07-24T17:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDYwNQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNTU1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459225556", "bodyText": "why did this change? there is a method that takes only one condition.", "author": "annym", "createdAt": "2020-07-23T05:45:18Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMDI4Nw==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459730287", "bodyText": "I want to put all things clear for each test case instead of using the wrapper, otherwise, while debugging, I need to jump around to see what are the real arguments passed.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:10:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNTU1Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226180", "bodyText": "The intention of the observables and the WAIT.ON_ACK is to avoid while loops that are just stuck there until a condition is met. Can't we set the expected number of ACKS by knowing the size of what we wrote and how many messages will be sent, hence how many calks we will receive back?", "author": "annym", "createdAt": "2020-07-23T05:48:01Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -811,16 +832,22 @@ public void testLogEntrySyncValidCrossTablesWithWritingAtSrc() throws Exception\n         testConfig.clear();\n         testConfig.setWritingSrc(true);\n         testConfig.setDeleteOP(true);\n-\n         testConfig.setWaitOn(WAIT.ON_ACK);\n-        startLogEntrySync(crossTables, WAIT.ON_ACK, false);\n+\n+        HashSet<WAIT> waitHashSet = new HashSet<>();\n+        waitHashSet.add(WAIT.ON_ACK);\n+        startLogEntrySync(crossTables, waitHashSet, true);\n \n         expectedAckTimestamp = Long.MAX_VALUE;\n-        // Verify Data on Destination site\n-        System.out.println(\"****** Verify Data on Destination\");\n+\n         // Because t2 is not specified as a replicated table, we should not see it on the destination\n         srcDataForVerification.get(t2).clear();\n \n+        // Verify Data on Destination site\n+        System.out.println(\"****** Wait Data on Destination\");\n+        waitData(dstCorfuTables, srcDataForVerification);", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjI4Mg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226282", "bodyText": "All you need is to set expectedAckMessages to the correct value.", "author": "annym", "createdAt": "2020-07-23T05:48:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMTI1Ng==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459731256", "bodyText": "It is not accurate right. Even we know the value size, but not the MSR entry size in advance.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:13:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjE4MA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjMyMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226322", "bodyText": "required?", "author": "annym", "createdAt": "2020-07-23T05:48:44Z", "path": "test/src/test/java/org/corfudb/integration/LogReplicationIT.java", "diffHunk": "@@ -971,6 +998,8 @@ public void testLogEntrySyncWithTrim() throws Exception {\n         // Setup Environment: two corfu servers (source & destination)\n         setupEnv();\n \n+        log.info(\"Have setutEnv Done\");", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTczMTcwMg==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459731702", "bodyText": "When the test goes wrong, we know the process and where it stuck.", "author": "xiaoqin2012", "createdAt": "2020-07-23T21:14:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjMyMg=="}], "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\nindex 516df4f0602..1d581d81dbc 100644\n--- a/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n+++ b/test/src/test/java/org/corfudb/integration/LogReplicationIT.java\n\n@@ -998,7 +1003,7 @@ public class LogReplicationIT extends AbstractIT implements Observer {\n         // Setup Environment: two corfu servers (source & destination)\n         setupEnv();\n \n-        log.info(\"Have setutEnv Done\");\n+        log.trace(\"Have setutEnv Done\");\n \n         // Open One Stream\n         openStreams(srcCorfuTables, srcDataRuntime, 1);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNjg4OA==", "url": "https://github.com/CorfuDB/CorfuDB/pull/2643#discussion_r459226888", "bodyText": "droppingNum (not starting with caps)", "author": "annym", "createdAt": "2020-07-23T05:51:10Z", "path": "test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java", "diffHunk": "@@ -50,7 +50,9 @@\n \n     final static int DROP_INCREMENT = 4;\n \n-    private int firstDrop = DROP_INCREMENT;\n+    private int DroppingNum = 2;", "originalCommit": "558d9bc783fa25db0791b58efbb4e33da951a052", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "ccaa7a42499718ded47259cfc73b1c68572706a7", "chunk": "diff --git a/test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java b/test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java\nindex 83b21abbab2..f7a1ec85e78 100644\n--- a/test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java\n+++ b/test/src/test/java/org/corfudb/integration/SourceForwardingDataSender.java\n\n@@ -50,7 +50,7 @@ public class SourceForwardingDataSender implements DataSender {\n \n     final static int DROP_INCREMENT = 4;\n \n-    private int DroppingNum = 2;\n+    private int droppingNum = 2;\n \n     private int msgCnt = 0;\n \n"}}, {"oid": "ccaa7a42499718ded47259cfc73b1c68572706a7", "url": "https://github.com/CorfuDB/CorfuDB/commit/ccaa7a42499718ded47259cfc73b1c68572706a7", "message": "Address all the comments.", "committedDate": "2020-07-23T23:06:10Z", "type": "commit"}, {"oid": "83232ab1f7264f15b0deb2923ae3956b71348901", "url": "https://github.com/CorfuDB/CorfuDB/commit/83232ab1f7264f15b0deb2923ae3956b71348901", "message": "Fix compiling erros.", "committedDate": "2020-07-23T23:19:19Z", "type": "commit"}, {"oid": "11d8e5b78240c2f2468d656cf48ffc8c680aba86", "url": "https://github.com/CorfuDB/CorfuDB/commit/11d8e5b78240c2f2468d656cf48ffc8c680aba86", "message": "Merge branch 'log-replication-master' into xq/0720_msg_size_01", "committedDate": "2020-07-24T16:56:51Z", "type": "commit"}]}