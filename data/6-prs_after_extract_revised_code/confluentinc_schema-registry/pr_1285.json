{"pr_number": 1285, "pr_title": "First cut at Protobuf support", "pr_createdAt": "2020-01-08T22:13:13Z", "pr_url": "https://github.com/confluentinc/schema-registry/pull/1285", "timeline": [{"oid": "c4960cf65ed6978b184d6dfd991238a991727532", "url": "https://github.com/confluentinc/schema-registry/commit/c4960cf65ed6978b184d6dfd991238a991727532", "message": "First cut at Protobuf support", "committedDate": "2020-01-08T22:12:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTU5Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515596", "bodyText": "Do these type casts need to be checked before being executed ?", "author": "dragosvictor", "createdAt": "2020-01-09T00:52:56Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjA3ODQ4Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366078487", "bodyText": "The casts are executed in a try-block that catches ClassCastException and throws a DataException.", "author": "rayokota", "createdAt": "2020-01-13T23:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTU5Ng=="}], "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTY5NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515695", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:53:24Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTcxNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515716", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:53:31Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTc5OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515799", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:53:50Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTgyMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515820", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:53:55Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTg0OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515848", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:54:01Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTg5Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515892", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:54:11Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNTkzNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364515934", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:54:25Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNjAzMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364516030", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:54:55Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNjA2MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364516060", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:55:03Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDUxNjA4Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r364516086", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-09T00:55:10Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,952 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = newMessageBuilder(protobufSchema, scopedMapName);\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;", "originalCommit": "c4960cf65ed6978b184d6dfd991238a991727532", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 30d907527..ed4b22318 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -253,7 +253,6 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            // TODO check this\n             throw new IllegalArgumentException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n"}}, {"oid": "7d82d44a8ff3508b5964c7f955eabf55e7063ad2", "url": "https://github.com/confluentinc/schema-registry/commit/7d82d44a8ff3508b5964c7f955eabf55e7063ad2", "message": "Add junit dep", "committedDate": "2020-01-09T02:37:12Z", "type": "commit"}, {"oid": "09f23b9dcc661fbef3eb1afb5e934dfa08c4473e", "url": "https://github.com/confluentinc/schema-registry/commit/09f23b9dcc661fbef3eb1afb5e934dfa08c4473e", "message": "Revert \"Add junit dep\"\n\nThis reverts commit 7d82d44a8ff3508b5964c7f955eabf55e7063ad2.", "committedDate": "2020-01-09T03:51:07Z", "type": "commit"}, {"oid": "e40e212d840dade56fecab4319558d457cdd20d5", "url": "https://github.com/confluentinc/schema-registry/commit/e40e212d840dade56fecab4319558d457cdd20d5", "message": "Bump maven-jar-plugin version", "committedDate": "2020-01-09T04:51:39Z", "type": "commit"}, {"oid": "eb4cb0d385bd470cc86872d2dfef150921221c77", "url": "https://github.com/confluentinc/schema-registry/commit/eb4cb0d385bd470cc86872d2dfef150921221c77", "message": "Merge branch 'master' into add-protobuf-support", "committedDate": "2020-01-09T20:13:13Z", "type": "commit"}, {"oid": "71ad8fa9f9bedd9b942e7a83e8b01f08b503eeaf", "url": "https://github.com/confluentinc/schema-registry/commit/71ad8fa9f9bedd9b942e7a83e8b01f08b503eeaf", "message": "Instantiate Protobuf Provider in SR", "committedDate": "2020-01-09T20:33:53Z", "type": "commit"}, {"oid": "10ebda9daefc30a31e27cf69e294576679610782", "url": "https://github.com/confluentinc/schema-registry/commit/10ebda9daefc30a31e27cf69e294576679610782", "message": "Minor pom cleanup", "committedDate": "2020-01-09T20:48:23Z", "type": "commit"}, {"oid": "77eb162e9852ce25566610ad9bc6d9d97fdeb512", "url": "https://github.com/confluentinc/schema-registry/commit/77eb162e9852ce25566610ad9bc6d9d97fdeb512", "message": "Fix protoc mvn refs", "committedDate": "2020-01-09T21:21:41Z", "type": "commit"}, {"oid": "8aa55244ff5b751b0a7346bf213cae97b7f36d2b", "url": "https://github.com/confluentinc/schema-registry/commit/8aa55244ff5b751b0a7346bf213cae97b7f36d2b", "message": "Clean up provider pom", "committedDate": "2020-01-09T22:09:13Z", "type": "commit"}, {"oid": "e7784e1a93ee1ed66d4783145ddd6332bee0c23b", "url": "https://github.com/confluentinc/schema-registry/commit/e7784e1a93ee1ed66d4783145ddd6332bee0c23b", "message": "Upgrade protoc maven plugin", "committedDate": "2020-01-09T22:35:05Z", "type": "commit"}, {"oid": "9ca59f9e1d55d5d719af9d3b80a6fe1696d62954", "url": "https://github.com/confluentinc/schema-registry/commit/9ca59f9e1d55d5d719af9d3b80a6fe1696d62954", "message": "Set addSources to none", "committedDate": "2020-01-09T22:41:17Z", "type": "commit"}, {"oid": "07d93d5cd6b39d4fb173caf77102659c791ec0e4", "url": "https://github.com/confluentinc/schema-registry/commit/07d93d5cd6b39d4fb173caf77102659c791ec0e4", "message": "More pom cleanup", "committedDate": "2020-01-09T22:47:28Z", "type": "commit"}, {"oid": "b4f77af90401771548cce49a1b61b80cd7d31726", "url": "https://github.com/confluentinc/schema-registry/commit/b4f77af90401771548cce49a1b61b80cd7d31726", "message": "More pom cleanup", "committedDate": "2020-01-09T22:51:26Z", "type": "commit"}, {"oid": "f24a96ae81b26634bb9f87ec6d74ee9d77d0a521", "url": "https://github.com/confluentinc/schema-registry/commit/f24a96ae81b26634bb9f87ec6d74ee9d77d0a521", "message": "Minor fix to protobuf provider registration", "committedDate": "2020-01-09T23:20:54Z", "type": "commit"}, {"oid": "0826faf7c51a99485d6af4e512cc7f75583bf033", "url": "https://github.com/confluentinc/schema-registry/commit/0826faf7c51a99485d6af4e512cc7f75583bf033", "message": "Add serde for Protobuf\n\nAlso fix some use of generics among serializers/deserializers.", "committedDate": "2020-01-10T21:41:53Z", "type": "commit"}, {"oid": "a4c590a34c97c7e8087afee60d8f05cb484636de", "url": "https://github.com/confluentinc/schema-registry/commit/a4c590a34c97c7e8087afee60d8f05cb484636de", "message": "Add ReferenceSubjectNameStrategy for Protobuf serializer\n\nAlso minor fixes for Protobuf deserializer", "committedDate": "2020-01-13T22:44:58Z", "type": "commit"}, {"oid": "ac646f1222968cf85f39dd26049031d81a70d156", "url": "https://github.com/confluentinc/schema-registry/commit/ac646f1222968cf85f39dd26049031d81a70d156", "message": "Fix Copyright date", "committedDate": "2020-01-13T23:13:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwMjIzNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366102234", "bodyText": "Do we need to check for overflow values in the 8/16 bit cases ?", "author": "dragosvictor", "createdAt": "2020-01-14T00:38:27Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,951 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }", "originalCommit": "0826faf7c51a99485d6af4e512cc7f75583bf033", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwMzI0MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366103240", "bodyText": "Connect supports int8/int16/int32, but Protobuf only supports int32", "author": "rayokota", "createdAt": "2020-01-14T00:42:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjEwMjIzNA=="}], "type": "inlineReview", "revised_code": {"commit": "a7b4a2e96a8e234f60df7fec668d8cc47ee542b7", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex ed4b22318..2dbdc1a07 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -215,7 +215,10 @@ public class ProtobufData {\n           String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n           List<Message> newMapValue = new ArrayList<>();\n           for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n-            DynamicMessage.Builder mapBuilder = newMessageBuilder(protobufSchema, scopedMapName);\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n             Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n             final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n             final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n"}}, {"oid": "880a4dc6d44a4cd15e3f612140922c0492547b37", "url": "https://github.com/confluentinc/schema-registry/commit/880a4dc6d44a4cd15e3f612140922c0492547b37", "message": "Enable Protobuf for maven plugin", "committedDate": "2020-01-14T21:27:04Z", "type": "commit"}, {"oid": "95acc0b56c502f032393a1faf2dcd61289fa5533", "url": "https://github.com/confluentinc/schema-registry/commit/95acc0b56c502f032393a1faf2dcd61289fa5533", "message": "Remove extra import", "committedDate": "2020-01-14T21:32:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NzY5Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366657697", "bodyText": "Thought: Is there a strategy that includes the Provider name as part of the subject? e.g. org.example.User-protobuf?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:29:09Z", "path": "avro-serializer/src/main/java/io/confluent/kafka/serializers/subject/DefaultReferenceSubjectNameStrategy.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Copyright 2018 Confluent Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package io.confluent.kafka.serializers.subject;\n+\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.serializers.subject.strategy.ReferenceSubjectNameStrategy;\n+\n+/**\n+ * Default {@link io.confluent.kafka.serializers.subject.strategy.ReferenceSubjectNameStrategy}:\n+ * simply uses the reference name as the subject name.\n+ */\n+public class DefaultReferenceSubjectNameStrategy implements ReferenceSubjectNameStrategy {", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MzI5NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366673294", "bodyText": "The schemaType (AVRO, PROTOBUF, JSONSCHEMA) is available in the schema.", "author": "rayokota", "createdAt": "2020-01-15T02:44:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1NzY5Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1ODA1MQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366658051", "bodyText": "Preference on Arrays.asList?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:30:48Z", "path": "maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java", "diffHunk": "@@ -80,4 +80,11 @@ protected SchemaRegistryClient client() {\n       }\n     }).collect(Collectors.toList());\n   }\n+\n+  private List<SchemaProvider> defaultSchemaProviders() {\n+    List<SchemaProvider> providers = new ArrayList<>();\n+    providers.add(new AvroSchemaProvider());\n+    providers.add(new ProtobufSchemaProvider());\n+    return providers;", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY4MTEwMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366681103", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T03:25:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1ODA1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "chunk": "diff --git a/maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java b/maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java\nindex bc567545a..ee307f534 100644\n--- a/maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java\n+++ b/maven-plugin/src/main/java/io/confluent/kafka/schemaregistry/maven/SchemaRegistryMojo.java\n\n@@ -82,9 +83,8 @@ public abstract class SchemaRegistryMojo extends AbstractMojo {\n   }\n \n   private List<SchemaProvider> defaultSchemaProviders() {\n-    List<SchemaProvider> providers = new ArrayList<>();\n-    providers.add(new AvroSchemaProvider());\n-    providers.add(new ProtobufSchemaProvider());\n-    return providers;\n+    return Arrays.asList(\n+        new AvroSchemaProvider(), new ProtobufSchemaProvider()\n+    );\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1OTEzMg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366659132", "bodyText": "nit: DateTimeFormatter is more modern", "author": "OneCricketeer", "createdAt": "2020-01-15T01:36:01Z", "path": "protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java", "diffHunk": "@@ -0,0 +1,885 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.BoolValue;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.BytesValue;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.DoubleValue;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.FloatValue;\n+import com.google.protobuf.Int32Value;\n+import com.google.protobuf.Int64Value;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.StringValue;\n+import com.google.protobuf.Timestamp;\n+import com.google.protobuf.util.Timestamps;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.errors.DataException;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7053c34aa87e8aaf892c2abee663c504888ef63", "chunk": "diff --git a/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java b/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\nindex 938f8a749..e0998805c 100644\n--- a/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\n+++ b/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\n\n@@ -184,6 +184,16 @@ public class ProtobufDataTest {\n     return complexTypeBuilder;\n   }\n \n+  private SchemaBuilder getInnerMessageSchemaBuilder() {\n+    final SchemaBuilder innerMessageBuilder = SchemaBuilder.struct();\n+    innerMessageBuilder.name(\"InnerMessage\");\n+    innerMessageBuilder.field(\n+        \"id\",\n+      SchemaBuilder.string().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    return innerMessageBuilder;\n+  }\n+\n   private Schema getExpectedNestedTestProtoSchema() {\n     final SchemaBuilder builder = SchemaBuilder.struct();\n     builder.name(\"NestedMessage\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY1OTQwMg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366659402", "bodyText": "nit: LocalDate is more modern", "author": "OneCricketeer", "createdAt": "2020-01-15T01:37:11Z", "path": "protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java", "diffHunk": "@@ -0,0 +1,885 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.BoolValue;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.BytesValue;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.DoubleValue;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.FloatValue;\n+import com.google.protobuf.Int32Value;\n+import com.google.protobuf.Int64Value;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.StringValue;\n+import com.google.protobuf.Timestamp;\n+import com.google.protobuf.util.Timestamps;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.errors.DataException;\n+import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.nio.ByteBuffer;\n+import java.text.ParseException;\n+import java.text.SimpleDateFormat;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto.NestedMessage;\n+import io.confluent.kafka.serializers.protobuf.test.SInt32ValueOuterClass;\n+import io.confluent.kafka.serializers.protobuf.test.SInt64ValueOuterClass;\n+import io.confluent.kafka.serializers.protobuf.test.TimestampValueOuterClass;\n+import io.confluent.kafka.serializers.protobuf.test.TimestampValueOuterClass.TimestampValue;\n+import io.confluent.kafka.serializers.protobuf.test.UInt32ValueOuterClass;\n+\n+import static io.confluent.connect.protobuf.ProtobufData.PROTOBUF_TYPE_TAG;\n+import static io.confluent.connect.protobuf.ProtobufData.PROTOBUF_TYPE_UNION_PREFIX;\n+import static io.confluent.kafka.serializers.protobuf.test.TimestampValueOuterClass.TimestampValue.newBuilder;\n+import static org.apache.kafka.connect.data.Schema.OPTIONAL_INT16_SCHEMA;\n+import static org.apache.kafka.connect.data.Schema.OPTIONAL_INT8_SCHEMA;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+\n+public class ProtobufDataTest {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufDataTest.class);\n+\n+  private static Schema OPTIONAL_INT32_SCHEMA = SchemaBuilder.int32()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_INT64_SCHEMA = SchemaBuilder.int64()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_FLOAT32_SCHEMA = SchemaBuilder.float32()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_FLOAT64_SCHEMA = SchemaBuilder.float64()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_BOOLEAN_SCHEMA = SchemaBuilder.bool()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_STRING_SCHEMA = SchemaBuilder.string()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+  private static Schema OPTIONAL_BYTES_SCHEMA = SchemaBuilder.bytes()\n+      .optional()\n+      .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+      .build();\n+\n+  private static final String VALUE_FIELD_NAME = \"value\";\n+\n+  private SchemaAndValue getExpectedSchemaAndValue(\n+      Schema fieldSchema,\n+      Message message,\n+      Object value\n+  ) {\n+    final SchemaBuilder schemaBuilder = SchemaBuilder.struct();\n+    schemaBuilder.name(message.getDescriptorForType().getName());\n+    schemaBuilder.field(VALUE_FIELD_NAME, fieldSchema);\n+    final Schema schema = schemaBuilder.build();\n+    Struct expectedResult = null;\n+    if (!message.equals(message.getDefaultInstanceForType())) {\n+      expectedResult = new Struct(schema);\n+      expectedResult.put(VALUE_FIELD_NAME, value);\n+    }\n+    return new SchemaAndValue(schema, expectedResult);\n+  }\n+\n+  private StringValue createStringValueMessage(String messageText) {\n+    StringValue.Builder builder = StringValue.newBuilder();\n+    builder.setValue(messageText);\n+    return builder.build();\n+  }\n+\n+  private NestedMessage createNestedTestProtoStringUserId() throws ParseException {\n+    return createNestedTestProto(NestedTestProto.UserId.newBuilder()\n+        .setKafkaUserId(\"my_user\")\n+        .build());\n+  }\n+\n+  private NestedMessage createNestedTestProtoIntUserId() throws ParseException {\n+    return createNestedTestProto(NestedTestProto.UserId.newBuilder().setOtherUserId(5).build());\n+  }\n+\n+  private NestedMessage createNestedTestProto(NestedTestProto.UserId id) throws ParseException {\n+    NestedMessage.Builder message = NestedMessage.newBuilder();\n+    message.setUserId(id);\n+    message.setIsActive(true);\n+    message.addExperimentsActive(\"first experiment\");\n+    message.addExperimentsActive(\"second experiment\");\n+    message.setStatus(NestedTestProto.Status.INACTIVE);\n+\n+    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy/MM/dd\");\n+    java.util.Date date = sdf.parse(\"2017/09/18\");\n+    Timestamp timestamp = Timestamps.fromMillis(date.getTime());\n+    message.setUpdatedAt(timestamp);\n+    message.putMapType(\"Hello\", \"World\");\n+    return message.build();\n+  }\n+\n+  private Schema getExpectedNestedTestProtoSchemaStringUserId() {\n+    return getExpectedNestedTestProtoSchema();\n+  }\n+\n+  private Schema getExpectedNestedTestProtoSchemaIntUserId() {\n+    return getExpectedNestedTestProtoSchema();\n+  }\n+\n+  private SchemaBuilder getComplexTypeSchemaBuilder() {\n+    final SchemaBuilder complexTypeBuilder = SchemaBuilder.struct();\n+    complexTypeBuilder.name(\"ComplexType\");\n+    final SchemaBuilder someValBuilder = SchemaBuilder.struct();\n+    someValBuilder.name(PROTOBUF_TYPE_UNION_PREFIX + \"some_val\");\n+    someValBuilder.field(\n+        \"one_id\",\n+        SchemaBuilder.string().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    someValBuilder.field(\n+        \"other_id\",\n+        SchemaBuilder.int32().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(2)).build()\n+    );\n+    complexTypeBuilder.field(\"some_val_0\", someValBuilder.build());\n+    complexTypeBuilder.field(\n+        \"is_active\",\n+        SchemaBuilder.bool().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(3)).build()\n+    );\n+    return complexTypeBuilder;\n+  }\n+\n+  private Schema getExpectedNestedTestProtoSchema() {\n+    final SchemaBuilder builder = SchemaBuilder.struct();\n+    builder.name(\"NestedMessage\");\n+    final SchemaBuilder userIdBuilder = SchemaBuilder.struct();\n+    userIdBuilder.name(\"UserId\");\n+    final SchemaBuilder idBuilder = SchemaBuilder.struct();\n+    idBuilder.name(PROTOBUF_TYPE_UNION_PREFIX + \"user_id\");\n+    idBuilder.field(\n+        \"kafka_user_id\",\n+        SchemaBuilder.string().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    idBuilder.field(\n+        \"other_user_id\",\n+        SchemaBuilder.int32().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(2)).build()\n+    );\n+    final SchemaBuilder messageIdBuilder = SchemaBuilder.struct();\n+    messageIdBuilder.name(\"MessageId\");\n+    messageIdBuilder.field(\n+        \"id\",\n+        SchemaBuilder.string().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    idBuilder.field(\n+        \"another_id\",\n+        messageIdBuilder.optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(3)).build()\n+    );\n+    userIdBuilder.field(\"user_id_0\", idBuilder.build());\n+    builder.field(\n+        \"user_id\",\n+        userIdBuilder.optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    builder.field(\n+        \"is_active\",\n+        SchemaBuilder.bool().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(2)).build()\n+    );\n+    builder.field(\n+        \"experiments_active\",\n+        SchemaBuilder.array(SchemaBuilder.string().optional().build())\n+            .optional()\n+            .parameter(PROTOBUF_TYPE_TAG, String.valueOf(3))\n+            .build()\n+    );\n+    builder.field(\n+        \"updated_at\",\n+        org.apache.kafka.connect.data.Timestamp.builder()\n+            .optional()\n+            .parameter(PROTOBUF_TYPE_TAG, String.valueOf(4))\n+            .build()\n+    );\n+    builder.field(\"status\",\n+        SchemaBuilder.int32()\n+            .name(\"Status\")\n+            .optional()\n+            .parameter(PROTOBUF_TYPE_TAG, String.valueOf(5))\n+            .parameter(ProtobufData.PROTOBUF_TYPE_ENUM, \"Status\")\n+            .parameter(ProtobufData.PROTOBUF_TYPE_ENUM_PREFIX + \"ACTIVE\", \"0\")\n+            .parameter(ProtobufData.PROTOBUF_TYPE_ENUM_PREFIX + \"INACTIVE\", \"1\")\n+            .build()\n+    );\n+    builder.field(\n+        \"complex_type\",\n+        getComplexTypeSchemaBuilder().optional()\n+            .parameter(PROTOBUF_TYPE_TAG, String.valueOf(6))\n+            .build()\n+    );\n+    builder.field(\"map_type\",\n+        SchemaBuilder.map(\n+            OPTIONAL_STRING_SCHEMA,\n+            SchemaBuilder.string()\n+                .optional()\n+                .parameter(PROTOBUF_TYPE_TAG, String.valueOf(2))\n+                .build()\n+        ).name(\"map_type\").optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(7)).build()\n+    );\n+    return builder.build();\n+  }\n+\n+  private Map<String, String> getTestKeyValueMap() {\n+    Map<String, String> result = new HashMap<>();\n+    result.put(\"Hello\", \"World\");\n+    return result;\n+  }\n+\n+  private Struct getExpectedNestedProtoResultStringUserId() throws ParseException {\n+    Schema schema = getExpectedNestedTestProtoSchemaStringUserId();\n+    Struct result = new Struct(schema.schema());\n+    Struct userId = new Struct(schema.field(\"user_id\").schema());\n+    Struct union = new Struct(schema.field(\"user_id\").schema().field(\"user_id_0\").schema());\n+    union.put(\"kafka_user_id\", \"my_user\");\n+    userId.put(\"user_id_0\", union);\n+    result.put(\"user_id\", userId);\n+    result.put(\"is_active\", true);\n+\n+    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy/MM/dd\");\n+    java.util.Date date = sdf.parse(\"2017/09/18\");\n+    result.put(\"updated_at\", date);\n+\n+    List<String> experiments = new ArrayList<>();\n+    experiments.add(\"first experiment\");\n+    experiments.add(\"second experiment\");\n+    result.put(\"experiments_active\", experiments);\n+\n+    result.put(\"status\", 1); // INACTIVE\n+    result.put(\"map_type\", getTestKeyValueMap());\n+    return result;\n+  }\n+\n+  private Struct getExpectedNestedTestProtoResultIntUserId() throws ParseException {\n+    Schema schema = getExpectedNestedTestProtoSchemaIntUserId();\n+    Struct result = new Struct(schema.schema());\n+    Struct userId = new Struct(schema.field(\"user_id\").schema());\n+    Struct union = new Struct(schema.field(\"user_id\").schema().field(\"user_id_0\").schema());\n+    union.put(\"other_user_id\", 5);\n+    userId.put(\"user_id_0\", union);\n+    result.put(\"user_id\", userId);\n+    result.put(\"is_active\", true);\n+\n+    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy/MM/dd\");\n+    java.util.Date date = sdf.parse(\"2017/09/18\");\n+    result.put(\"updated_at\", date);\n+\n+    List<String> experiments = new ArrayList<>();\n+    experiments.add(\"first experiment\");\n+    experiments.add(\"second experiment\");\n+    result.put(\"experiments_active\", experiments);\n+\n+    result.put(\"status\", 1); // INACTIVE\n+    result.put(\"map_type\", getTestKeyValueMap());\n+    return result;\n+  }\n+\n+  private NestedTestProto.ComplexType createProtoDefaultOneOf() throws ParseException {\n+    NestedTestProto.ComplexType.Builder complexTypeBuilder =\n+        NestedTestProto.ComplexType.newBuilder();\n+    complexTypeBuilder.setOtherId(0);\n+    return complexTypeBuilder.build();\n+  }\n+\n+  private NestedTestProto.ComplexType createProtoMultipleSetOneOf() throws ParseException {\n+    NestedTestProto.ComplexType.Builder complexTypeBuilder =\n+        NestedTestProto.ComplexType.newBuilder();\n+    complexTypeBuilder.setOneId(\"asdf\");\n+    complexTypeBuilder.setOtherId(0);\n+    return complexTypeBuilder.build();\n+  }\n+\n+  private Struct getExpectedComplexTypeProtoWithDefaultOneOf() {\n+    Schema schema = getComplexTypeSchemaBuilder().build();\n+    Struct result = new Struct(schema.schema());\n+    Struct union = new Struct(schema.field(\"some_val_0\").schema());\n+    union.put(\"other_id\", 0);\n+    result.put(\"some_val_0\", union);\n+    result.put(\"is_active\", false);\n+    return result;\n+  }\n+\n+  private void assertSchemasEqual(Schema expectedSchema, Schema actualSchema) {\n+    assertEquals(expectedSchema.isOptional(), actualSchema.isOptional());\n+    assertEquals(expectedSchema.version(), actualSchema.version());\n+    assertEquals(expectedSchema.name(), actualSchema.name());\n+    assertEquals(expectedSchema.doc(), actualSchema.doc());\n+    assertEquals(expectedSchema.type(), actualSchema.type());\n+    assertEquals(expectedSchema.defaultValue(), actualSchema.defaultValue());\n+    assertEquals(expectedSchema.parameters(), actualSchema.parameters());\n+\n+    if (expectedSchema.type() == Schema.Type.STRUCT) {\n+      assertEquals(expectedSchema.fields().size(), actualSchema.fields().size());\n+      for (int i = 0; i < expectedSchema.fields().size(); ++i) {\n+        Field expectedField = expectedSchema.fields().get(i);\n+        Field actualField = actualSchema.field(expectedField.name());\n+        assertSchemasEqual(expectedField.schema(), actualField.schema());\n+      }\n+    } else if (expectedSchema.type() == Schema.Type.ARRAY) {\n+      assertSchemasEqual(expectedSchema.valueSchema(), actualSchema.valueSchema());\n+    } else if (expectedSchema.type() == Schema.Type.MAP) {\n+      assertSchemasEqual(expectedSchema.keySchema(), actualSchema.keySchema());\n+      assertSchemasEqual(expectedSchema.valueSchema(), actualSchema.valueSchema());\n+    }\n+  }\n+\n+  private SchemaAndValue getSchemaAndValue(Message message) throws Exception {\n+    ProtobufSchema protobufSchema = new ProtobufSchema(message.getDescriptorForType());\n+    DynamicMessage dynamicMessage = DynamicMessage.parseFrom(\n+        protobufSchema.toDescriptor(),\n+        message.toByteArray()\n+    );\n+    ProtobufData protobufData = new ProtobufData();\n+    return protobufData.toConnectData(protobufSchema, dynamicMessage);\n+  }\n+\n+  @Test\n+  public void testToConnectDataWithNestedProtobufMessageAndStringUserId() throws Exception {\n+    NestedMessage message = createNestedTestProtoStringUserId();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    Schema expectedSchema = getExpectedNestedTestProtoSchemaStringUserId();\n+    assertSchemasEqual(expectedSchema, result.schema());\n+    Struct expected = getExpectedNestedProtoResultStringUserId();\n+    assertEquals(expected, result.value());\n+  }\n+\n+  @Test\n+  public void testToConnectDataWithNestedProtobufMessageAndIntUserId() throws Exception {\n+    NestedMessage message = createNestedTestProtoIntUserId();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    Schema expectedSchema = getExpectedNestedTestProtoSchemaIntUserId();\n+    assertSchemasEqual(expectedSchema, result.schema());\n+    Struct expected = getExpectedNestedTestProtoResultIntUserId();\n+    assertSchemasEqual(expected.schema(), ((Struct) result.value()).schema());\n+    assertEquals(expected.schema(), ((Struct) result.value()).schema());\n+    assertEquals(expected, result.value());\n+  }\n+\n+  @Test\n+  public void testToConnectDataDefaultOneOf() throws Exception {\n+    Schema schema = getComplexTypeSchemaBuilder().build();\n+    NestedTestProto.ComplexType message = createProtoDefaultOneOf();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertSchemasEqual(schema, result.schema());\n+    assertEquals(new SchemaAndValue(schema, getExpectedComplexTypeProtoWithDefaultOneOf()), result);\n+  }\n+\n+  @Test\n+  public void testToConnectDataDefaultOneOfCannotHaveTwoOneOfsSet() throws Exception {\n+    Schema schema = getComplexTypeSchemaBuilder().build();\n+    NestedTestProto.ComplexType message = createProtoMultipleSetOneOf();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertSchemasEqual(schema, result.schema());\n+    assertEquals(new SchemaAndValue(schema, getExpectedComplexTypeProtoWithDefaultOneOf()), result);\n+  }\n+\n+  // Data Conversion tests\n+  @Test\n+  public void testToConnectNull() {\n+    ProtobufData protobufData = new ProtobufData();\n+    Schema schema = OPTIONAL_BOOLEAN_SCHEMA.schema();\n+    assertNull(protobufData.toConnectData(schema, null));\n+  }\n+\n+  @Test\n+  public void testToConnectBoolean() throws Exception {\n+    Boolean expectedValue = true;\n+    BoolValue.Builder builder = BoolValue.newBuilder();\n+    builder.setValue(expectedValue);\n+    BoolValue message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(\n+        getExpectedSchemaAndValue(OPTIONAL_BOOLEAN_SCHEMA, message, expectedValue),\n+        result\n+    );\n+  }\n+\n+  @Test\n+  public void testToConnectInt32() throws Exception {\n+    Integer expectedValue = 12;\n+    Int32Value.Builder builder = Int32Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    Int32Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_INT32_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectInt32With0() throws Exception {\n+    Integer expectedValue = 0;\n+    Int32Value.Builder builder = Int32Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    Int32Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_INT32_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectInt32WithSint32() throws Exception {\n+    int expectedValue = 12;\n+    SInt32ValueOuterClass.SInt32Value.Builder builder =\n+        SInt32ValueOuterClass.SInt32Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    SInt32ValueOuterClass.SInt32Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_INT32_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectInt32WithUInt32() throws Exception {\n+    final Long UNSIGNED_RESULT = 4294967295L;\n+    Integer expectedValue = -1;\n+    UInt32ValueOuterClass.UInt32Value.Builder builder =\n+        UInt32ValueOuterClass.UInt32Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    UInt32ValueOuterClass.UInt32Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(\n+        getExpectedSchemaAndValue(OPTIONAL_INT64_SCHEMA, message, UNSIGNED_RESULT),\n+        result\n+    );\n+  }\n+\n+  @Test\n+  public void testToConnectInt64() throws Exception {\n+    Long expectedValue = 12L;\n+    Int64Value.Builder builder = Int64Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    Int64Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_INT64_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectSInt64() throws Exception {\n+    Long expectedValue = 12L;\n+    SInt64ValueOuterClass.SInt64Value.Builder builder =\n+        SInt64ValueOuterClass.SInt64Value.newBuilder();\n+    builder.setValue(expectedValue);\n+    SInt64ValueOuterClass.SInt64Value message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_INT64_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectFloat32() throws Exception {\n+    Float expectedValue = 12.f;\n+    FloatValue.Builder builder = FloatValue.newBuilder();\n+    builder.setValue(expectedValue);\n+    FloatValue message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(\n+        getExpectedSchemaAndValue(OPTIONAL_FLOAT32_SCHEMA, message, expectedValue),\n+        result\n+    );\n+  }\n+\n+  @Test\n+  public void testToConnectFloat64() throws Exception {\n+    Double expectedValue = 12.0;\n+    DoubleValue.Builder builder = DoubleValue.newBuilder();\n+    builder.setValue(expectedValue);\n+    DoubleValue message = builder.build();\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(\n+        getExpectedSchemaAndValue(OPTIONAL_FLOAT64_SCHEMA, message, expectedValue),\n+        result\n+    );\n+  }\n+\n+  @Test\n+  public void testToConnectString() throws Exception {\n+    String expectedValue = \"Hello\";\n+    StringValue message = createStringValueMessage(expectedValue);\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_STRING_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectEmptyString() throws Exception {\n+    String expectedValue = \"\";\n+    StringValue message = createStringValueMessage(expectedValue);\n+    SchemaAndValue result = getSchemaAndValue(message);\n+    assertEquals(getExpectedSchemaAndValue(OPTIONAL_STRING_SCHEMA, message, expectedValue), result);\n+  }\n+\n+  @Test\n+  public void testToConnectTimestamp() throws Exception {\n+    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy/MM/dd\");\n+    java.util.Date expectedValue = sdf.parse(\"2017/12/31\");\n+\n+    Timestamp timestamp = Timestamps.fromMillis(expectedValue.getTime());\n+    TimestampValueOuterClass.TimestampValue.Builder builder = newBuilder();\n+    builder.setValue(timestamp);\n+    TimestampValueOuterClass.TimestampValue message = builder.build();\n+\n+    SchemaAndValue result = getSchemaAndValue(message);\n+\n+    Schema timestampSchema = org.apache.kafka.connect.data.Timestamp.builder()\n+        .optional()\n+        .parameter(PROTOBUF_TYPE_TAG, String.valueOf(1))\n+        .build();\n+    assertEquals(getExpectedSchemaAndValue(timestampSchema, message, expectedValue), result);\n+  }\n+\n+  @Test(expected = DataException.class)\n+  public void testToConnectSchemaMismatchArray() {\n+    ProtobufData protobufData = new ProtobufData();\n+    Schema schema = SchemaBuilder.array(OPTIONAL_STRING_SCHEMA).build();\n+    protobufData.toConnectData(schema, Arrays.asList(1, 2, 3));\n+  }\n+\n+  private byte[] getMessageBytes(Schema schema, Object value) throws Exception {\n+    Schema structSchema = SchemaBuilder.struct().field(VALUE_FIELD_NAME, schema).build();\n+    Struct struct = new Struct(structSchema.schema());\n+    struct.put(VALUE_FIELD_NAME, value);\n+    SchemaAndValue schemaAndValue = new SchemaAndValue(structSchema, struct);\n+    return getMessageBytes(schemaAndValue);\n+  }\n+\n+  private byte[] getMessageBytes(SchemaAndValue schemaAndValue) throws Exception {\n+    ProtobufData protobufData = new ProtobufData();\n+    ProtobufSchemaAndValue protobufSchemaAndValue = protobufData.fromConnectData(schemaAndValue);\n+    DynamicMessage dynamicMessage = (DynamicMessage) protobufSchemaAndValue.getValue();\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+    dynamicMessage.writeTo(baos);\n+    return baos.toByteArray();\n+  }\n+\n+  @Test\n+  public void testFromConnectDataWithNestedProtobufMessageAndStringUserId() throws Exception {\n+    NestedMessage nestedMessage = createNestedTestProtoStringUserId();\n+    SchemaAndValue schemaAndValue = getSchemaAndValue(nestedMessage);\n+    byte[] messageBytes = getMessageBytes(schemaAndValue);\n+    Message message = NestedTestProto.NestedMessage.parseFrom(messageBytes);\n+\n+    assertArrayEquals(messageBytes, message.toByteArray());\n+  }\n+\n+  @Test\n+  public void testFromConnectDataWithNestedProtobufMessageAndIntUserId() throws Exception {\n+    NestedMessage nestedMessage = createNestedTestProtoIntUserId();\n+    SchemaAndValue schemaAndValue = getSchemaAndValue(nestedMessage);\n+    byte[] messageBytes = getMessageBytes(schemaAndValue);\n+    Message message = NestedTestProto.NestedMessage.parseFrom(messageBytes);\n+\n+    assertArrayEquals(messageBytes, message.toByteArray());\n+  }\n+\n+  @Test\n+  public void testFromConnectComplex() {\n+    Schema schema = SchemaBuilder.struct()\n+        .field(\"int8\", SchemaBuilder.int8().defaultValue((byte) 2).doc(\"int8 field\").build())\n+        .field(\"int16\", Schema.INT16_SCHEMA)\n+        .field(\"int32\", Schema.INT32_SCHEMA)\n+        .field(\"int64\", Schema.INT64_SCHEMA)\n+        .field(\"float32\", Schema.FLOAT32_SCHEMA)\n+        .field(\"float64\", Schema.FLOAT64_SCHEMA)\n+        .field(\"boolean\", Schema.BOOLEAN_SCHEMA)\n+        .field(\"string\", Schema.STRING_SCHEMA)\n+        .field(\"bytes\", Schema.BYTES_SCHEMA)\n+        .field(\"array\", SchemaBuilder.array(Schema.STRING_SCHEMA).build())\n+        .field(\"map\", SchemaBuilder.map(Schema.STRING_SCHEMA, Schema.INT32_SCHEMA).build())\n+        .field(\"mapNonStringKeys\",\n+            SchemaBuilder.map(Schema.INT32_SCHEMA, Schema.INT32_SCHEMA).build()\n+        )\n+        .build();\n+    Struct struct = new Struct(schema).put(\"int8\", (byte) 12)\n+        .put(\"int16\", (short) 12)\n+        .put(\"int32\", 12)\n+        .put(\"int64\", 12L)\n+        .put(\"float32\", 12.2f)\n+        .put(\"float64\", 12.2)\n+        .put(\"boolean\", true)\n+        .put(\"string\", \"foo\")\n+        .put(\"bytes\", ByteBuffer.wrap(\"foo\".getBytes()))\n+        .put(\"array\", Arrays.asList(\"a\", \"b\", \"c\"))\n+        .put(\"map\", Collections.singletonMap(\"field\", 1))\n+        .put(\"mapNonStringKeys\", Collections.singletonMap(1, 1));\n+\n+    ProtobufSchemaAndValue convertedRecord = new ProtobufData().fromConnectData(schema, struct);\n+    ProtobufSchema protobufSchema = convertedRecord.getSchema();\n+    DynamicMessage message = (DynamicMessage) convertedRecord.getValue();\n+\n+    MessageElement messageElem = (MessageElement) protobufSchema.rawSchema().getTypes().get(0);\n+    FieldElement fieldElem = messageElem.getFields().get(0);\n+    assertEquals(\"int8\", fieldElem.getName());\n+    assertEquals(\"int32\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(1);\n+    assertEquals(\"int16\", fieldElem.getName());\n+    assertEquals(\"int32\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(2);\n+    assertEquals(\"int32\", fieldElem.getName());\n+    assertEquals(\"int32\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(3);\n+    assertEquals(\"int64\", fieldElem.getName());\n+    assertEquals(\"int64\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(4);\n+    assertEquals(\"float32\", fieldElem.getName());\n+    assertEquals(\"float\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(5);\n+    assertEquals(\"float64\", fieldElem.getName());\n+    assertEquals(\"double\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(6);\n+    assertEquals(\"boolean\", fieldElem.getName());\n+    assertEquals(\"bool\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(7);\n+    assertEquals(\"string\", fieldElem.getName());\n+    assertEquals(\"string\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(8);\n+    assertEquals(\"bytes\", fieldElem.getName());\n+    assertEquals(\"bytes\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(9);\n+    assertEquals(\"array\", fieldElem.getName());\n+    assertEquals(\"string\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(10);\n+    assertEquals(\"map\", fieldElem.getName());\n+    assertEquals(\"ConnectDefault1.ConnectDefault2Entry\", fieldElem.getType());\n+    fieldElem = messageElem.getFields().get(11);\n+    assertEquals(\"mapNonStringKeys\", fieldElem.getName());\n+    assertEquals(\"ConnectDefault1.ConnectDefault3Entry\", fieldElem.getType());\n+\n+    Descriptors.FieldDescriptor fieldDescriptor = message.getDescriptorForType()\n+        .findFieldByName(\"int8\");\n+    assertEquals(12, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"int16\");\n+    assertEquals(12, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"int32\");\n+    assertEquals(12, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"int64\");\n+    assertEquals(12L, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"float32\");\n+    assertEquals(12.2f, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"float64\");\n+    assertEquals(12.2, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"boolean\");\n+    assertEquals(true, message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"string\");\n+    assertEquals(\"foo\", message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"bytes\");\n+    assertEquals(ByteString.copyFrom(\"foo\".getBytes()), message.getField(fieldDescriptor));\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"array\");\n+    assertEquals(Arrays.asList(\"a\", \"b\", \"c\"), message.getField(fieldDescriptor));\n+\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"map\");\n+    Object value = message.getField(fieldDescriptor);\n+    DynamicMessage dynamicMessage = ((List<DynamicMessage>) value).get(0);\n+    fieldDescriptor = dynamicMessage.getDescriptorForType().findFieldByName(\"key\");\n+    assertEquals(\"field\", dynamicMessage.getField(fieldDescriptor));\n+    fieldDescriptor = dynamicMessage.getDescriptorForType().findFieldByName(\"value\");\n+    assertEquals(1, dynamicMessage.getField(fieldDescriptor));\n+\n+    fieldDescriptor = message.getDescriptorForType().findFieldByName(\"mapNonStringKeys\");\n+    value = message.getField(fieldDescriptor);\n+    dynamicMessage = ((List<DynamicMessage>) value).get(0);\n+    fieldDescriptor = dynamicMessage.getDescriptorForType().findFieldByName(\"key\");\n+    assertEquals(1, dynamicMessage.getField(fieldDescriptor));\n+    fieldDescriptor = dynamicMessage.getDescriptorForType().findFieldByName(\"value\");\n+    assertEquals(1, dynamicMessage.getField(fieldDescriptor));\n+  }\n+\n+\n+  @Test\n+  public void testFromConnectInt8() throws Exception {\n+    Byte value = 15;\n+    byte[] messageBytes = getMessageBytes(OPTIONAL_INT8_SCHEMA, value);\n+    Message message = Int32Value.parseFrom(messageBytes);\n+\n+    assertEquals(1, message.getAllFields().size());\n+\n+    Descriptors.FieldDescriptor fieldDescriptor = message.getDescriptorForType()\n+        .findFieldByName(VALUE_FIELD_NAME);\n+    // Value converted to int\n+    assertEquals(value.intValue(), message.getField(fieldDescriptor));\n+  }\n+\n+  @Test\n+  public void testFromConnectInt16() throws Exception {\n+    Short value = 15;\n+    byte[] messageBytes = getMessageBytes(OPTIONAL_INT16_SCHEMA, value);\n+    Message message = Int32Value.parseFrom(messageBytes);\n+\n+    assertEquals(1, message.getAllFields().size());\n+\n+    Descriptors.FieldDescriptor fieldDescriptor = message.getDescriptorForType()\n+        .findFieldByName(VALUE_FIELD_NAME);\n+    // Value converted to int\n+    assertEquals(value.intValue(), message.getField(fieldDescriptor));\n+  }\n+\n+  @Test\n+  public void testFromConnectInt32() throws Exception {\n+    Integer value = 15;\n+    byte[] messageBytes = getMessageBytes(OPTIONAL_INT32_SCHEMA, value);\n+    Message message = Int32Value.parseFrom(messageBytes);\n+\n+    assertEquals(1, message.getAllFields().size());\n+\n+    Descriptors.FieldDescriptor fieldDescriptor = message.getDescriptorForType()\n+        .findFieldByName(VALUE_FIELD_NAME);\n+    assertEquals(value, message.getField(fieldDescriptor));\n+  }\n+\n+  @Test\n+  public void testFromConnectInt64() throws Exception {\n+    Long value = 15L;\n+    byte[] messageBytes = getMessageBytes(OPTIONAL_INT64_SCHEMA, value);\n+    Message message = Int64Value.parseFrom(messageBytes);\n+\n+    assertEquals(1, message.getAllFields().size());\n+\n+    Descriptors.FieldDescriptor fieldDescriptor = message.getDescriptorForType()\n+        .findFieldByName(VALUE_FIELD_NAME);\n+    assertEquals(value, message.getField(fieldDescriptor));\n+  }\n+\n+  @Test\n+  public void testFromConnectTimestamp() throws Exception {\n+    SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy/MM/dd\");\n+    java.util.Date date = sdf.parse(\"2017/09/18\");", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "b7053c34aa87e8aaf892c2abee663c504888ef63", "chunk": "diff --git a/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java b/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\nindex 938f8a749..e0998805c 100644\n--- a/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\n+++ b/protobuf-converter/src/test/java/io/confluent/connect/protobuf/ProtobufDataTest.java\n\n@@ -184,6 +184,16 @@ public class ProtobufDataTest {\n     return complexTypeBuilder;\n   }\n \n+  private SchemaBuilder getInnerMessageSchemaBuilder() {\n+    final SchemaBuilder innerMessageBuilder = SchemaBuilder.struct();\n+    innerMessageBuilder.name(\"InnerMessage\");\n+    innerMessageBuilder.field(\n+        \"id\",\n+      SchemaBuilder.string().optional().parameter(PROTOBUF_TYPE_TAG, String.valueOf(1)).build()\n+    );\n+    return innerMessageBuilder;\n+  }\n+\n   private Schema getExpectedNestedTestProtoSchema() {\n     final SchemaBuilder builder = SchemaBuilder.struct();\n     builder.name(\"NestedMessage\");\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MDQ2Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366660462", "bodyText": "Maybe store these strings in resources?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:42:03Z", "path": "protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaTest.java", "diffHunk": "@@ -0,0 +1,370 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.JsonNodeFactory;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ProtobufSchemaTest {\n+\n+  private static ObjectMapper objectMapper = new ObjectMapper();\n+\n+  private static final String recordSchemaString = \"syntax = \\\"proto3\\\";\\n\"", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4NDA5Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368184096", "bodyText": "+1", "author": "dragosvictor", "createdAt": "2020-01-18T00:05:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MDQ2Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTIzNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366661236", "bodyText": "Should JsonFormat.printer() be externalized?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:45:37Z", "path": "protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageFormatter.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.protobuf;\n+\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.JsonFormat;\n+import kafka.common.MessageFormatter;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.apache.kafka.common.serialization.Deserializer;\n+\n+import java.io.IOException;\n+import java.io.PrintStream;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufDeserializer;\n+\n+/**\n+ * Example\n+ * To use ProtobufMessageFormatter, first make sure that Zookeeper, Kafka and schema registry\n+ * server are\n+ * all started. Second, make sure the jar for ProtobufMessageFormatter and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-consumer.sh. Then run the following command.\n+ *\n+ * <p>1. To read only the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.ProtobufMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081\n+ *\n+ * <p>2. To read both the key and the value of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.ProtobufMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true\n+ *\n+ * <p>3. To read the key, value, and timestamp of the messages in JSON\n+ * bin/kafka-console-consumer.sh --consumer.config config/consumer.properties --topic t1 \\\n+ * --zookeeper localhost:2181 --formatter io.confluent.kafka.formatter.ProtobufMessageFormatter \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property print.key=true \\\n+ * --property print.timestamp=true\n+ */\n+public class ProtobufMessageFormatter extends AbstractKafkaProtobufDeserializer\n+    implements MessageFormatter {\n+\n+  private static final byte[] NULL_BYTES = \"null\".getBytes(StandardCharsets.UTF_8);\n+  private boolean printKey = false;\n+  private boolean printTimestamp = false;\n+  private boolean printIds = false;\n+  private boolean printKeyId = false;\n+  private boolean printValueId = false;\n+  private byte[] keySeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] lineSeparator = \"\\n\".getBytes(StandardCharsets.UTF_8);\n+  private byte[] idSeparator = \"\\t\".getBytes(StandardCharsets.UTF_8);\n+  private Deserializer keyDeserializer;\n+\n+  /**\n+   * Constructor needed by kafka console consumer.\n+   */\n+  public ProtobufMessageFormatter() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  ProtobufMessageFormatter(\n+      SchemaRegistryClient schemaRegistryClient, Deserializer keyDeserializer\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keyDeserializer = keyDeserializer;\n+  }\n+\n+  @Override\n+  public void init(Properties props) {\n+    if (props == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+    schemaRegistry = createSchemaRegistry(url, originals);\n+\n+    if (props.containsKey(\"print.timestamp\")) {\n+      printTimestamp = props.getProperty(\"print.timestamp\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"print.key\")) {\n+      printKey = props.getProperty(\"print.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"line.separator\")) {\n+      lineSeparator = props.getProperty(\"line.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+    if (props.containsKey(\"key.deserializer\")) {\n+      try {\n+        keyDeserializer = (Deserializer) Class.forName((String) props.get(\"key.deserializer\"))\n+            .newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key deserializer\", e);\n+      }\n+    }\n+    if (props.containsKey(\"print.schema.ids\")) {\n+      printIds = props.getProperty(\"print.schema.ids\").trim().toLowerCase().equals(\"true\");\n+      if (printIds) {\n+        printValueId = true;\n+        if (keyDeserializer == null\n+            || keyDeserializer instanceof AbstractKafkaProtobufDeserializer) {\n+          printKeyId = true;\n+        }\n+      }\n+    }\n+    if (props.containsKey(\"schema.id.separator\")) {\n+      idSeparator = props.getProperty(\"schema.id.separator\").getBytes(StandardCharsets.UTF_8);\n+    }\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public void writeTo(ConsumerRecord<byte[], byte[]> consumerRecord, PrintStream output) {\n+    if (printTimestamp) {\n+      try {\n+        TimestampType timestampType = consumerRecord.timestampType();\n+        if (timestampType != TimestampType.NO_TIMESTAMP_TYPE) {\n+          output.write(String.format(\"%s:%d\", timestampType, consumerRecord.timestamp())\n+              .getBytes(StandardCharsets.UTF_8));\n+        } else {\n+          output.write(\"NO_TIMESTAMP\".getBytes(StandardCharsets.UTF_8));\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the timestamp\", ioe);\n+      }\n+    }\n+    if (printKey) {\n+      try {\n+        if (keyDeserializer != null) {\n+          Object deserializedKey = consumerRecord.key() == null\n+                                   ? null\n+                                   : keyDeserializer.deserialize(null, consumerRecord.key());\n+          output.write(deserializedKey != null ? deserializedKey.toString()\n+              .getBytes(StandardCharsets.UTF_8) : NULL_BYTES);\n+        } else {\n+          writeTo(consumerRecord.key(), output);\n+        }\n+        if (printKeyId) {\n+          output.write(idSeparator);\n+          int schemaId = schemaIdFor(consumerRecord.key());\n+          output.print(schemaId);\n+        }\n+        output.write(keySeparator);\n+      } catch (IOException ioe) {\n+        throw new SerializationException(\"Error while formatting the key\", ioe);\n+      }\n+    }\n+    try {\n+      writeTo(consumerRecord.value(), output);\n+      if (printValueId) {\n+        output.write(idSeparator);\n+        int schemaId = schemaIdFor(consumerRecord.value());\n+        output.print(schemaId);\n+      }\n+      output.write(lineSeparator);\n+    } catch (IOException ioe) {\n+      throw new SerializationException(\"Error while formatting the value\", ioe);\n+    }\n+  }\n+\n+  private void writeTo(byte[] data, PrintStream output) throws IOException {\n+    Message object = (Message) deserialize(data);\n+    try {\n+      output.print(JsonFormat.printer().print(object));", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "chunk": "diff --git a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageFormatter.java b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageFormatter.java\nindex 6f16f4734..e1a12671e 100644\n--- a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageFormatter.java\n+++ b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageFormatter.java\n\n@@ -203,7 +203,7 @@ public class ProtobufMessageFormatter extends AbstractKafkaProtobufDeserializer\n   }\n \n   private void writeTo(byte[] data, PrintStream output) throws IOException {\n-    Message object = (Message) deserialize(data);\n+    Message object = deserialize(data);\n     try {\n       output.print(JsonFormat.printer().print(object));\n     } catch (InvalidProtocolBufferException e) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTM1MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366661350", "bodyText": "Comments should be updated with protobuf value schemas", "author": "OneCricketeer", "createdAt": "2020-01-15T01:46:11Z", "path": "protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import com.google.protobuf.util.JsonFormat;\n+import kafka.common.KafkaException;\n+import kafka.common.MessageReader;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+\n+/**\n+ * Example\n+ * To use ProtobufMessageReader, first make sure that Zookeeper, Kafka and schema registry server\n+ * are\n+ * all started. Second, make sure the jar for ProtobufMessageReader and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-producer.sh. Then run the following\n+ * command.\n+ *\n+ * <p>1. Send Protobuf string as value. (make sure there is no space in the schema string)\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"string\"}'", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3MzQ1Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366673452", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T02:45:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTM1MA=="}], "type": "inlineReview", "revised_code": {"commit": "cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "chunk": "diff --git a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\nindex 7fcdfb397..4118e48f3 100644\n--- a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\n+++ b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\n\n@@ -50,37 +50,14 @@ import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n  * in the classpath of kafka-console-producer.sh. Then run the following\n  * command.\n  *\n- * <p>1. Send Protobuf string as value. (make sure there is no space in the schema string)\n+ * <p>Send Protobuf record as value.\n  * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n  * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n  * --property schema.registry.url=http://localhost:8081 \\\n- * --property value.schema='{\"type\":\"string\"}'\n- *\n- * <p>In the shell, type in the following.\n- * \"a\"\n- * \"b\"\n- *\n- * <p>2. Send Protobuf record as value.\n- * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n- * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n- * --property schema.registry.url=http://localhost:8081 \\\n- * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n- * \"type\":\"string\"}]}'\n+ * --property value.schema='syntax = \"proto3\"; message MyRecord { string f1 = 1; }'\n  *\n  * <p>In the shell, type in the following.\n  * {\"f1\": \"value1\"}\n- *\n- * <p>3. Send Protobuf string as key and Protobuf record as value.\n- * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n- * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n- * --property schema.registry.url=http://localhost:8081 \\\n- * --property parse.key=true \\\n- * --property key.schema='{\"type\":\"string\"}' \\\n- * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n- * \"type\":\"string\"}]}'\n- *\n- * <p>In the shell, type in the following.\n- * \"key1\" \\t {\"f1\": \"value1\"}\n  */\n public class ProtobufMessageReader extends AbstractKafkaProtobufSerializer\n     implements MessageReader {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTYxNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366661614", "bodyText": "Should JsonFormat.parser() be externalized?", "author": "OneCricketeer", "createdAt": "2020-01-15T01:47:32Z", "path": "protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import com.google.protobuf.util.JsonFormat;\n+import kafka.common.KafkaException;\n+import kafka.common.MessageReader;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+\n+/**\n+ * Example\n+ * To use ProtobufMessageReader, first make sure that Zookeeper, Kafka and schema registry server\n+ * are\n+ * all started. Second, make sure the jar for ProtobufMessageReader and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-producer.sh. Then run the following\n+ * command.\n+ *\n+ * <p>1. Send Protobuf string as value. (make sure there is no space in the schema string)\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"string\"}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * \"a\"\n+ * \"b\"\n+ *\n+ * <p>2. Send Protobuf record as value.\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n+ * \"type\":\"string\"}]}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * {\"f1\": \"value1\"}\n+ *\n+ * <p>3. Send Protobuf string as key and Protobuf record as value.\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property parse.key=true \\\n+ * --property key.schema='{\"type\":\"string\"}' \\\n+ * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n+ * \"type\":\"string\"}]}'\n+ *\n+ * <p>In the shell, type in the following.\n+ * \"key1\" \\t {\"f1\": \"value1\"}\n+ */\n+public class ProtobufMessageReader extends AbstractKafkaProtobufSerializer\n+    implements MessageReader {\n+\n+  private String topic = null;\n+  private BufferedReader reader = null;\n+  private Boolean parseKey = false;\n+  private String keySeparator = \"\\t\";\n+  private boolean ignoreError = false;\n+  private ProtobufSchema keySchema = null;\n+  private ProtobufSchema valueSchema = null;\n+  private String keySubject = null;\n+  private String valueSubject = null;\n+  private Serializer keySerializer;\n+\n+  /**\n+   * Constructor needed by kafka console producer.\n+   */\n+  public ProtobufMessageReader() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  ProtobufMessageReader(\n+      SchemaRegistryClient schemaRegistryClient,\n+      ProtobufSchema keySchema,\n+      ProtobufSchema valueSchema,\n+      String topic,\n+      boolean parseKey,\n+      BufferedReader reader,\n+      boolean autoRegister\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keySchema = keySchema;\n+    this.valueSchema = valueSchema;\n+    this.topic = topic;\n+    this.keySubject = topic + \"-key\";\n+    this.valueSubject = topic + \"-value\";\n+    this.parseKey = parseKey;\n+    this.reader = reader;\n+    this.autoRegisterSchema = autoRegister;\n+  }\n+\n+  @Override\n+  public void init(java.io.InputStream inputStream, Properties props) {\n+    topic = props.getProperty(\"topic\");\n+    if (props.containsKey(\"parse.key\")) {\n+      parseKey = props.getProperty(\"parse.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\");\n+    }\n+    if (props.containsKey(\"ignore.error\")) {\n+      ignoreError = props.getProperty(\"ignore.error\").trim().toLowerCase().equals(\"true\");\n+    }\n+    reader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+\n+    schemaRegistry = new CachedSchemaRegistryClient(Collections.singletonList(url),\n+        AbstractKafkaSchemaSerDeConfig.MAX_SCHEMAS_PER_SUBJECT_DEFAULT,\n+        Collections.singletonList(new ProtobufSchemaProvider()),\n+        originals\n+    );\n+    if (!props.containsKey(\"value.schema\")) {\n+      throw new ConfigException(\"Must provide the Protobuf schema string in value.schema\");\n+    }\n+    String valueSchemaString = props.getProperty(\"value.schema\");\n+    valueSchema = new ProtobufSchema(valueSchemaString);\n+\n+    keySerializer = getKeySerializer(props);\n+\n+    if (needsKeySchema()) {\n+      if (!props.containsKey(\"key.schema\")) {\n+        throw new ConfigException(\"Must provide the Protobuf schema string in key.schema\");\n+      }\n+      String keySchemaString = props.getProperty(\"key.schema\");\n+      keySchema = new ProtobufSchema(keySchemaString);\n+    }\n+    keySubject = topic + \"-key\";\n+    valueSubject = topic + \"-value\";\n+    if (props.containsKey(\"auto.register\")) {\n+      this.autoRegisterSchema = Boolean.parseBoolean(props.getProperty(\"auto.register\").trim());\n+    } else {\n+      this.autoRegisterSchema = true;\n+    }\n+  }\n+\n+  private Serializer getKeySerializer(Properties props) throws ConfigException {\n+    if (props.containsKey(\"key.serializer\")) {\n+      try {\n+        return (Serializer) Class.forName((String) props.get(\"key.serializer\")).newInstance();\n+      } catch (Exception e) {\n+        throw new ConfigException(\"Error initializing Key serializer\", e);\n+      }\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private boolean needsKeySchema() {\n+    return parseKey && keySerializer == null;\n+  }\n+\n+  private Map<String, Object> getPropertiesMap(Properties props) {\n+    Map<String, Object> originals = new HashMap<>();\n+    for (final String name : props.stringPropertyNames()) {\n+      originals.put(name, props.getProperty(name));\n+    }\n+    return originals;\n+  }\n+\n+  @Override\n+  public ProducerRecord<byte[], byte[]> readMessage() {\n+    try {\n+      String line = reader.readLine();\n+      if (line == null) {\n+        return null;\n+      }\n+      if (!parseKey) {\n+        DynamicMessage value = jsonToProtobuf(line, valueSchema);\n+        byte[] serializedValue = serializeImpl(valueSubject, topic, false, value, valueSchema);\n+        return new ProducerRecord<>(topic, serializedValue);\n+      } else {\n+        int keyIndex = line.indexOf(keySeparator);\n+        if (keyIndex < 0) {\n+          if (ignoreError) {\n+            DynamicMessage value = jsonToProtobuf(line, valueSchema);\n+            byte[] serializedValue = serializeImpl(valueSubject, topic, false, value, valueSchema);\n+            return new ProducerRecord<>(topic, serializedValue);\n+          } else {\n+            throw new KafkaException(\"No key found in line \" + line);\n+          }\n+        } else {\n+          String keyString = line.substring(0, keyIndex);\n+          String valueString = (keyIndex + keySeparator.length() > line.length())\n+                               ? \"\"\n+                               : line.substring(keyIndex + keySeparator.length());\n+          byte[] serializedKey;\n+          if (keySerializer != null) {\n+            serializedKey = keySerializer.serialize(topic, keyString);\n+          } else {\n+            DynamicMessage key = jsonToProtobuf(keyString, keySchema);\n+            serializedKey = serializeImpl(keySubject, topic, true, key, keySchema);\n+          }\n+          DynamicMessage value = jsonToProtobuf(valueString, valueSchema);\n+          byte[] serializedValue = serializeImpl(valueSubject, topic, false, value, valueSchema);\n+          return new ProducerRecord<>(topic, serializedKey, serializedValue);\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new KafkaException(\"Error reading from input\", e);\n+    }\n+  }\n+\n+  private DynamicMessage jsonToProtobuf(String jsonString, ProtobufSchema schema) {\n+    try {\n+      DynamicMessage.Builder message = schema.newMessageBuilder();\n+      JsonFormat.parser().merge(jsonString, message);", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "chunk": "diff --git a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\nindex 7fcdfb397..4118e48f3 100644\n--- a/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\n+++ b/protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java\n\n@@ -50,37 +50,14 @@ import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n  * in the classpath of kafka-console-producer.sh. Then run the following\n  * command.\n  *\n- * <p>1. Send Protobuf string as value. (make sure there is no space in the schema string)\n+ * <p>Send Protobuf record as value.\n  * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n  * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n  * --property schema.registry.url=http://localhost:8081 \\\n- * --property value.schema='{\"type\":\"string\"}'\n- *\n- * <p>In the shell, type in the following.\n- * \"a\"\n- * \"b\"\n- *\n- * <p>2. Send Protobuf record as value.\n- * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n- * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n- * --property schema.registry.url=http://localhost:8081 \\\n- * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n- * \"type\":\"string\"}]}'\n+ * --property value.schema='syntax = \"proto3\"; message MyRecord { string f1 = 1; }'\n  *\n  * <p>In the shell, type in the following.\n  * {\"f1\": \"value1\"}\n- *\n- * <p>3. Send Protobuf string as key and Protobuf record as value.\n- * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n- * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n- * --property schema.registry.url=http://localhost:8081 \\\n- * --property parse.key=true \\\n- * --property key.schema='{\"type\":\"string\"}' \\\n- * --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"f1\",\n- * \"type\":\"string\"}]}'\n- *\n- * <p>In the shell, type in the following.\n- * \"key1\" \\t {\"f1\": \"value1\"}\n  */\n public class ProtobufMessageReader extends AbstractKafkaProtobufSerializer\n     implements MessageReader {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTgxNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366661814", "bodyText": "I feel like this method could be placed in a more centralized location, if it doesn't already exist in some fashion", "author": "OneCricketeer", "createdAt": "2020-01-15T01:48:26Z", "path": "protobuf-serializer/src/main/java/io/confluent/kafka/serializers/protobuf/AbstractKafkaProtobufDeserializer.java", "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.serializers.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.MessageLite;\n+import kafka.utils.VerifiableProperties;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.lang.reflect.Method;\n+import java.nio.ByteBuffer;\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaUtils;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDe;\n+\n+public abstract class AbstractKafkaProtobufDeserializer<T extends MessageLite>\n+    extends AbstractKafkaSchemaSerDe {\n+  protected Class<T> specificProtobufClass;\n+  protected Method parseMethod;\n+\n+  /**\n+   * Sets properties for this deserializer without overriding the schema registry client itself.\n+   * Useful for testing, where a mock client is injected.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  protected void configure(KafkaProtobufDeserializerConfig config) {\n+    configureClientProperties(config, new ProtobufSchemaProvider());\n+    try {\n+      this.specificProtobufClass = (Class<T>) config.getClass(\n+          KafkaProtobufDeserializerConfig.SPECIFIC_PROTOBUF_CLASS_CONFIG);\n+      if (specificProtobufClass != null && !specificProtobufClass.equals(Object.class)) {\n+        this.parseMethod = specificProtobufClass.getDeclaredMethod(\"parseFrom\", ByteBuffer.class);\n+      }\n+    } catch (Exception e) {\n+      throw new ConfigException(\"Proto class \"\n+          + specificProtobufClass.getCanonicalName()\n+          + \" is not a valid proto3 message class\", e);\n+    }\n+  }\n+\n+  protected KafkaProtobufDeserializerConfig deserializerConfig(Map<String, ?> props) {\n+    try {\n+      return new KafkaProtobufDeserializerConfig(props);\n+    } catch (io.confluent.common.config.ConfigException e) {\n+      throw new ConfigException(e.getMessage());\n+    }\n+  }\n+\n+  protected KafkaProtobufDeserializerConfig deserializerConfig(VerifiableProperties props) {\n+    return new KafkaProtobufDeserializerConfig(props.props());\n+  }\n+\n+  private ByteBuffer getByteBuffer(byte[] payload) {", "originalCommit": "95acc0b56c502f032393a1faf2dcd61289fa5533", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY3NTUzNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r366675536", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-15T02:55:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MTgxNA=="}], "type": "inlineReview", "revised_code": {"commit": "cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "chunk": "diff --git a/protobuf-serializer/src/main/java/io/confluent/kafka/serializers/protobuf/AbstractKafkaProtobufDeserializer.java b/protobuf-serializer/src/main/java/io/confluent/kafka/serializers/protobuf/AbstractKafkaProtobufDeserializer.java\nindex 3144fd8ec..ee0d747d7 100644\n--- a/protobuf-serializer/src/main/java/io/confluent/kafka/serializers/protobuf/AbstractKafkaProtobufDeserializer.java\n+++ b/protobuf-serializer/src/main/java/io/confluent/kafka/serializers/protobuf/AbstractKafkaProtobufDeserializer.java\n\n@@ -71,14 +71,6 @@ public abstract class AbstractKafkaProtobufDeserializer<T extends MessageLite>\n     return new KafkaProtobufDeserializerConfig(props.props());\n   }\n \n-  private ByteBuffer getByteBuffer(byte[] payload) {\n-    ByteBuffer buffer = ByteBuffer.wrap(payload);\n-    if (buffer.get() != MAGIC_BYTE) {\n-      throw new SerializationException(\"Unknown magic byte!\");\n-    }\n-    return buffer;\n-  }\n-\n   /**\n    * Deserializes the payload without including schema information for primitive types, maps, and\n    * arrays. Just the resulting deserialized object is returned.\n"}}, {"oid": "cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "url": "https://github.com/confluentinc/schema-registry/commit/cc863ecda7a0c38c59bbff26d1b79320d0e37cc3", "message": "Incorporate some review feedback", "committedDate": "2020-01-15T03:46:41Z", "type": "commit"}, {"oid": "6f764e916987b4471c30bbf9a88c7e1c837789d8", "url": "https://github.com/confluentinc/schema-registry/commit/6f764e916987b4471c30bbf9a88c7e1c837789d8", "message": "Minor refactoring of default providers", "committedDate": "2020-01-15T04:03:44Z", "type": "commit"}, {"oid": "39ca7c2b02d55c247931447f4bfab4c97a10bf6c", "url": "https://github.com/confluentinc/schema-registry/commit/39ca7c2b02d55c247931447f4bfab4c97a10bf6c", "message": "Merge branch 'master' into add-protobuf-support", "committedDate": "2020-01-15T20:54:22Z", "type": "commit"}, {"oid": "b71612d2a3ac6a127e4f15491b5026b06e7ee493", "url": "https://github.com/confluentinc/schema-registry/commit/b71612d2a3ac6a127e4f15491b5026b06e7ee493", "message": "Refactor common serializer classes\n\nAlso minor refactoring in KafkaSchemaRegistry ctor", "committedDate": "2020-01-15T21:10:55Z", "type": "commit"}, {"oid": "2bb5314d4b1fac8eb1f1cf569d2b041a94d5a647", "url": "https://github.com/confluentinc/schema-registry/commit/2bb5314d4b1fac8eb1f1cf569d2b041a94d5a647", "message": "Add configs for specific Protobuf class for key and value\n\nAlso preserve various Java options in Protobuf", "committedDate": "2020-01-16T23:36:42Z", "type": "commit"}, {"oid": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "url": "https://github.com/confluentinc/schema-registry/commit/7a07245b91b214abccf660c3ebea92d2ea0c68e9", "message": "Clean up generic usage\n\nUse Message to be more restrictive so we can depend on descriptors", "committedDate": "2020-01-17T03:29:45Z", "type": "commit"}, {"oid": "b7053c34aa87e8aaf892c2abee663c504888ef63", "url": "https://github.com/confluentinc/schema-registry/commit/b7053c34aa87e8aaf892c2abee663c504888ef63", "message": "Add inner message test", "committedDate": "2020-01-17T21:01:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODA5MTgwNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368091804", "bodyText": "Is this going to output useful debugging information if one of the resolved references fails to parse ? Right now we name and shame the 'schemaString', but it might not always be the culprit.\nNit: 'Could'.", "author": "dragosvictor", "createdAt": "2020-01-17T19:10:12Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODA5NTYzOA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368095638", "bodyText": "Nit: This is redundant.", "author": "dragosvictor", "createdAt": "2020-01-17T19:19:26Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODA5OTUxNw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368099517", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T19:28:26Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEwMTE2NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368101164", "bodyText": "Question: do we risk an infinite recursion here ?", "author": "dragosvictor", "createdAt": "2020-01-17T19:32:09Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE2NjkxMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369166913", "bodyText": "No the descriptors don't have cycles.", "author": "rayokota", "createdAt": "2020-01-21T18:24:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEwMTE2NA=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEzMzQ5Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368133497", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T20:57:58Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEzMzU0Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368133547", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T20:58:05Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEzMzYxNw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368133617", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T20:58:15Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (fd.getOptions().hasPacked()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\"packed\", kind, fd.getOptions().getPacked(), false);\n+      options.add(option);\n+    }\n+    if (fd.toProto().hasJsonName()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"json_name\",\n+          kind,\n+          fd.toProto().getJsonName(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    String defaultValue = fd.hasDefaultValue() && fd.getDefaultValue() != null\n+                          ? fd.getDefaultValue().toString()\n+                          : null;\n+    // NOTE: skip some options\n+    return new FieldElement(location,\n+        inOneof ? null : label(fd),\n+        dataType(fd),\n+        name,\n+        defaultValue,\n+        fd.getNumber(),\n+        \"\",\n+        options.build()\n+    );\n+  }\n+\n+  private Field.Label label(FieldDescriptor fd) {\n+    FileDescriptor.Syntax syntax = fd.getFile().getSyntax();\n+    boolean isProto3 = syntax == FileDescriptor.Syntax.PROTO3;\n+    switch (fd.toProto().getLabel()) {\n+      case LABEL_REQUIRED:\n+        return isProto3 ? null : Field.Label.REQUIRED;\n+      case LABEL_OPTIONAL:\n+        return isProto3 ? null : Field.Label.OPTIONAL;\n+      case LABEL_REPEATED:\n+        return Field.Label.REPEATED;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported label\");\n+    }\n+  }\n+\n+  private String dataType(FieldDescriptor field) {\n+    FieldDescriptor.Type type = field.getType();\n+    switch (type) {\n+      case MESSAGE:\n+        return field.getMessageType().getFullName();\n+      case ENUM:\n+        return field.getEnumType().getFullName();\n+      default:\n+        return type.name().toLowerCase();\n+    }\n+  }\n+\n+  public Descriptor toDescriptor() {\n+    return toDescriptor(name());\n+  }\n+\n+  public Descriptor toDescriptor(String name) {\n+    return toDynamicSchema().getMessageDescriptor(name);\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder() {\n+    return newMessageBuilder(name());\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder(String name) {\n+    return toDynamicSchema().newMessageBuilder(name);\n+  }\n+\n+  private MessageElement firstMessage() {\n+    for (TypeElement typeElement : schemaObj.getTypes()) {\n+      if (typeElement instanceof MessageElement) {\n+        return (MessageElement) typeElement;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Protobuf schema definition \"\n+        + \" contains no message type definitions\");\n+  }\n+\n+  @VisibleForTesting\n+  protected DynamicSchema toDynamicSchema() {\n+    if (dynamicSchema == null) {\n+      dynamicSchema = toDynamicSchema(DEFAULT_NAME, schemaObj, dependencies);\n+    }\n+    return dynamicSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private static DynamicSchema toDynamicSchema(\n+      String name, ProtoFileElement rootElem, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    log.trace(\"*** toDynamicSchema: \" + rootElem.toSchema());", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEzMzY1MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368133650", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T20:58:22Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (fd.getOptions().hasPacked()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\"packed\", kind, fd.getOptions().getPacked(), false);\n+      options.add(option);\n+    }\n+    if (fd.toProto().hasJsonName()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"json_name\",\n+          kind,\n+          fd.toProto().getJsonName(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    String defaultValue = fd.hasDefaultValue() && fd.getDefaultValue() != null\n+                          ? fd.getDefaultValue().toString()\n+                          : null;\n+    // NOTE: skip some options\n+    return new FieldElement(location,\n+        inOneof ? null : label(fd),\n+        dataType(fd),\n+        name,\n+        defaultValue,\n+        fd.getNumber(),\n+        \"\",\n+        options.build()\n+    );\n+  }\n+\n+  private Field.Label label(FieldDescriptor fd) {\n+    FileDescriptor.Syntax syntax = fd.getFile().getSyntax();\n+    boolean isProto3 = syntax == FileDescriptor.Syntax.PROTO3;\n+    switch (fd.toProto().getLabel()) {\n+      case LABEL_REQUIRED:\n+        return isProto3 ? null : Field.Label.REQUIRED;\n+      case LABEL_OPTIONAL:\n+        return isProto3 ? null : Field.Label.OPTIONAL;\n+      case LABEL_REPEATED:\n+        return Field.Label.REPEATED;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported label\");\n+    }\n+  }\n+\n+  private String dataType(FieldDescriptor field) {\n+    FieldDescriptor.Type type = field.getType();\n+    switch (type) {\n+      case MESSAGE:\n+        return field.getMessageType().getFullName();\n+      case ENUM:\n+        return field.getEnumType().getFullName();\n+      default:\n+        return type.name().toLowerCase();\n+    }\n+  }\n+\n+  public Descriptor toDescriptor() {\n+    return toDescriptor(name());\n+  }\n+\n+  public Descriptor toDescriptor(String name) {\n+    return toDynamicSchema().getMessageDescriptor(name);\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder() {\n+    return newMessageBuilder(name());\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder(String name) {\n+    return toDynamicSchema().newMessageBuilder(name);\n+  }\n+\n+  private MessageElement firstMessage() {\n+    for (TypeElement typeElement : schemaObj.getTypes()) {\n+      if (typeElement instanceof MessageElement) {\n+        return (MessageElement) typeElement;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Protobuf schema definition \"\n+        + \" contains no message type definitions\");\n+  }\n+\n+  @VisibleForTesting\n+  protected DynamicSchema toDynamicSchema() {\n+    if (dynamicSchema == null) {\n+      dynamicSchema = toDynamicSchema(DEFAULT_NAME, schemaObj, dependencies);\n+    }\n+    return dynamicSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private static DynamicSchema toDynamicSchema(\n+      String name, ProtoFileElement rootElem, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    log.trace(\"*** toDynamicSchema: \" + rootElem.toSchema());\n+    DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+    try {\n+      if (rootElem.getPackageName() != null) {\n+        schema.setPackage(rootElem.getPackageName());\n+      }\n+      for (TypeElement typeElem : rootElem.getTypes()) {\n+        if (typeElem instanceof MessageElement) {\n+          MessageDefinition message = toDynamicMessage(schema, (MessageElement) typeElem);\n+          schema.addMessageDefinition(message);\n+        } else if (typeElem instanceof EnumElement) {\n+          EnumDefinition enumer = toDynamicEnum((EnumElement) typeElem);\n+          schema.addEnumDefinition(enumer);\n+        }\n+      }\n+      for (String ref : rootElem.getImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      for (String ref : rootElem.getPublicImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addPublicDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      String javaPackageName = findOption(\"java_package\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaPackageName != null) {\n+        schema.setJavaPackage(javaPackageName);\n+      }\n+      String javaOuterClassname = findOption(\"java_outer_classname\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaOuterClassname != null) {\n+        schema.setJavaOuterClassname(javaOuterClassname);\n+      }\n+      Boolean javaMultipleFiles = findOption(\"java_multiple_files\", rootElem.getOptions())\n+          .map(o -> Boolean.valueOf(o.getValue().toString())).orElse(null);\n+      if (javaMultipleFiles != null) {\n+        schema.setJavaMultipleFiles(javaMultipleFiles);\n+      }\n+      schema.setName(name);\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private static MessageDefinition toDynamicMessage(\n+      DynamicSchema.Builder schema,\n+      MessageElement messageElem\n+  ) {\n+    log.trace(\"*** message: \" + messageElem.getName());", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODEzODYxMQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368138611", "bodyText": "Nit: Can we parameterize this log message ?", "author": "dragosvictor", "createdAt": "2020-01-17T21:13:00Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE0MTA0MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368141040", "bodyText": "Nit: extra space in message.", "author": "dragosvictor", "createdAt": "2020-01-17T21:20:34Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (fd.getOptions().hasPacked()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\"packed\", kind, fd.getOptions().getPacked(), false);\n+      options.add(option);\n+    }\n+    if (fd.toProto().hasJsonName()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"json_name\",\n+          kind,\n+          fd.toProto().getJsonName(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    String defaultValue = fd.hasDefaultValue() && fd.getDefaultValue() != null\n+                          ? fd.getDefaultValue().toString()\n+                          : null;\n+    // NOTE: skip some options\n+    return new FieldElement(location,\n+        inOneof ? null : label(fd),\n+        dataType(fd),\n+        name,\n+        defaultValue,\n+        fd.getNumber(),\n+        \"\",\n+        options.build()\n+    );\n+  }\n+\n+  private Field.Label label(FieldDescriptor fd) {\n+    FileDescriptor.Syntax syntax = fd.getFile().getSyntax();\n+    boolean isProto3 = syntax == FileDescriptor.Syntax.PROTO3;\n+    switch (fd.toProto().getLabel()) {\n+      case LABEL_REQUIRED:\n+        return isProto3 ? null : Field.Label.REQUIRED;\n+      case LABEL_OPTIONAL:\n+        return isProto3 ? null : Field.Label.OPTIONAL;\n+      case LABEL_REPEATED:\n+        return Field.Label.REPEATED;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported label\");\n+    }\n+  }\n+\n+  private String dataType(FieldDescriptor field) {\n+    FieldDescriptor.Type type = field.getType();\n+    switch (type) {\n+      case MESSAGE:\n+        return field.getMessageType().getFullName();\n+      case ENUM:\n+        return field.getEnumType().getFullName();\n+      default:\n+        return type.name().toLowerCase();\n+    }\n+  }\n+\n+  public Descriptor toDescriptor() {\n+    return toDescriptor(name());\n+  }\n+\n+  public Descriptor toDescriptor(String name) {\n+    return toDynamicSchema().getMessageDescriptor(name);\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder() {\n+    return newMessageBuilder(name());\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder(String name) {\n+    return toDynamicSchema().newMessageBuilder(name);\n+  }\n+\n+  private MessageElement firstMessage() {\n+    for (TypeElement typeElement : schemaObj.getTypes()) {\n+      if (typeElement instanceof MessageElement) {\n+        return (MessageElement) typeElement;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Protobuf schema definition \"", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3Nzk0NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368177944", "bodyText": "Question: can this ever return null ?", "author": "dragosvictor", "createdAt": "2020-01-17T23:33:27Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (fd.getOptions().hasPacked()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\"packed\", kind, fd.getOptions().getPacked(), false);\n+      options.add(option);\n+    }\n+    if (fd.toProto().hasJsonName()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"json_name\",\n+          kind,\n+          fd.toProto().getJsonName(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    String defaultValue = fd.hasDefaultValue() && fd.getDefaultValue() != null\n+                          ? fd.getDefaultValue().toString()\n+                          : null;\n+    // NOTE: skip some options\n+    return new FieldElement(location,\n+        inOneof ? null : label(fd),\n+        dataType(fd),\n+        name,\n+        defaultValue,\n+        fd.getNumber(),\n+        \"\",\n+        options.build()\n+    );\n+  }\n+\n+  private Field.Label label(FieldDescriptor fd) {\n+    FileDescriptor.Syntax syntax = fd.getFile().getSyntax();\n+    boolean isProto3 = syntax == FileDescriptor.Syntax.PROTO3;\n+    switch (fd.toProto().getLabel()) {\n+      case LABEL_REQUIRED:\n+        return isProto3 ? null : Field.Label.REQUIRED;\n+      case LABEL_OPTIONAL:\n+        return isProto3 ? null : Field.Label.OPTIONAL;\n+      case LABEL_REPEATED:\n+        return Field.Label.REPEATED;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported label\");\n+    }\n+  }\n+\n+  private String dataType(FieldDescriptor field) {\n+    FieldDescriptor.Type type = field.getType();\n+    switch (type) {\n+      case MESSAGE:\n+        return field.getMessageType().getFullName();\n+      case ENUM:\n+        return field.getEnumType().getFullName();\n+      default:\n+        return type.name().toLowerCase();\n+    }\n+  }\n+\n+  public Descriptor toDescriptor() {\n+    return toDescriptor(name());\n+  }\n+\n+  public Descriptor toDescriptor(String name) {\n+    return toDynamicSchema().getMessageDescriptor(name);\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder() {\n+    return newMessageBuilder(name());\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder(String name) {\n+    return toDynamicSchema().newMessageBuilder(name);\n+  }\n+\n+  private MessageElement firstMessage() {\n+    for (TypeElement typeElement : schemaObj.getTypes()) {\n+      if (typeElement instanceof MessageElement) {\n+        return (MessageElement) typeElement;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Protobuf schema definition \"\n+        + \" contains no message type definitions\");\n+  }\n+\n+  @VisibleForTesting\n+  protected DynamicSchema toDynamicSchema() {\n+    if (dynamicSchema == null) {\n+      dynamicSchema = toDynamicSchema(DEFAULT_NAME, schemaObj, dependencies);\n+    }\n+    return dynamicSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private static DynamicSchema toDynamicSchema(\n+      String name, ProtoFileElement rootElem, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    log.trace(\"*** toDynamicSchema: \" + rootElem.toSchema());\n+    DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+    try {\n+      if (rootElem.getPackageName() != null) {\n+        schema.setPackage(rootElem.getPackageName());\n+      }\n+      for (TypeElement typeElem : rootElem.getTypes()) {\n+        if (typeElem instanceof MessageElement) {\n+          MessageDefinition message = toDynamicMessage(schema, (MessageElement) typeElem);\n+          schema.addMessageDefinition(message);\n+        } else if (typeElem instanceof EnumElement) {\n+          EnumDefinition enumer = toDynamicEnum((EnumElement) typeElem);\n+          schema.addEnumDefinition(enumer);\n+        }\n+      }\n+      for (String ref : rootElem.getImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      for (String ref : rootElem.getPublicImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addPublicDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      String javaPackageName = findOption(\"java_package\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaPackageName != null) {\n+        schema.setJavaPackage(javaPackageName);\n+      }\n+      String javaOuterClassname = findOption(\"java_outer_classname\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaOuterClassname != null) {\n+        schema.setJavaOuterClassname(javaOuterClassname);\n+      }\n+      Boolean javaMultipleFiles = findOption(\"java_multiple_files\", rootElem.getOptions())\n+          .map(o -> Boolean.valueOf(o.getValue().toString())).orElse(null);\n+      if (javaMultipleFiles != null) {\n+        schema.setJavaMultipleFiles(javaMultipleFiles);\n+      }\n+      schema.setName(name);\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private static MessageDefinition toDynamicMessage(\n+      DynamicSchema.Builder schema,\n+      MessageElement messageElem\n+  ) {\n+    log.trace(\"*** message: \" + messageElem.getName());\n+    MessageDefinition.Builder message = MessageDefinition.newBuilder(messageElem.getName());\n+    Set<String> added = new HashSet<>();\n+    for (OneOfElement oneof : messageElem.getOneOfs()) {\n+      MessageDefinition.OneofBuilder oneofBuilder = message.addOneof(oneof.getName());\n+      for (FieldElement field : oneof.getFields()) {\n+        String defaultVal = field.getDefaultValue();\n+        String jsonName = findOption(\"json_name\", field.getOptions()).map(o -> o.getValue()\n+            .toString()).orElse(null);\n+        oneofBuilder.addField(\n+            field.getType(),\n+            field.getName(),\n+            field.getTag(),\n+            defaultVal,\n+            jsonName\n+        );\n+        added.add(field.getName());\n+      }\n+    }\n+    for (FieldElement field : messageElem.getFields()) {\n+      if (added.contains(field.getName())) {\n+        continue;\n+      }\n+      String label = field.getLabel() != null ? field.getLabel().toString().toLowerCase() : null;\n+      String fieldType = field.getType();\n+      String defaultVal = field.getDefaultValue();\n+      String jsonName = findOption(\"json_name\", field.getOptions()).map(o -> o.getValue()\n+          .toString()).orElse(null);\n+      Boolean isPacked = findOption(\n+          \"packed\",\n+          field.getOptions()\n+      ).map(o -> Boolean.valueOf(o.getValue().toString())).orElse(null);\n+      ProtoType protoType = ProtoType.get(fieldType);\n+      // Map fields are only permitted in messages\n+      if (protoType.isMap()) {\n+        label = \"repeated\";\n+        fieldType = toMapEntry(field.getName());\n+        MessageDefinition.Builder mapMessage = MessageDefinition.newBuilder(fieldType);\n+        mapMessage.setMapEntry(true);\n+        mapMessage.addField(null, protoType.keyType().simpleName(), KEY_FIELD, 1, null);\n+        mapMessage.addField(null, protoType.valueType().simpleName(), VALUE_FIELD, 2, null);\n+        schema.addMessageDefinition(mapMessage.build());\n+      }\n+      message.addField(\n+          label,\n+          fieldType,\n+          field.getName(),\n+          field.getTag(),\n+          defaultVal,\n+          jsonName,\n+          isPacked\n+      );\n+    }\n+    for (TypeElement type : messageElem.getNestedTypes()) {\n+      if (type instanceof MessageElement) {\n+        message.addMessageDefinition(toDynamicMessage(schema, (MessageElement) type));\n+      } else if (type instanceof EnumElement) {\n+        message.addEnumDefinition(toDynamicEnum((EnumElement) type));\n+      }\n+    }\n+    for (ReservedElement reserved : messageElem.getReserveds()) {\n+      for (Object elem : reserved.getValues()) {\n+        if (elem instanceof String) {\n+          message.addReservedName((String) elem);\n+        } else if (elem instanceof Integer) {\n+          int tag = (Integer) elem;\n+          message.addReservedRange(tag, tag);\n+        } else if (elem instanceof Range) {\n+          Range<Integer> range = (Range<Integer>) elem;\n+          message.addReservedRange(range.lowerEndpoint(), range.upperEndpoint());\n+        } else {\n+          throw new IllegalStateException(\"Unsupported reserved type: \" + elem.getClass()\n+              .getName());\n+        }\n+      }\n+    }\n+    return message.build();\n+  }\n+\n+  private static Optional<OptionElement> findOption(String name, List<OptionElement> options) {\n+    return options.stream().filter(o -> o.getName().equals(name)).findFirst();\n+  }", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE2NzY0MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369167640", "bodyText": "No, it will always be a non-null Optional", "author": "rayokota", "createdAt": "2020-01-21T18:25:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3Nzk0NA=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3OTcwNw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368179707", "bodyText": "Should we log all the incompatible changes ? If not, can we stop at the first one, instead of computing all of them ?", "author": "dragosvictor", "createdAt": "2020-01-17T23:42:23Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java", "diffHunk": "@@ -0,0 +1,707 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Range;\n+import com.google.protobuf.DescriptorProtos.DescriptorProto.ReservedRange;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.FileDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.squareup.wire.schema.Field;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.ProtoFile;\n+import com.squareup.wire.schema.ProtoType;\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+import com.squareup.wire.schema.internal.parser.OptionElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import com.squareup.wire.schema.internal.parser.ReservedElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.Difference;\n+import io.confluent.kafka.schemaregistry.protobuf.diff.SchemaDiff;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+\n+import static com.google.common.base.CaseFormat.LOWER_UNDERSCORE;\n+import static com.google.common.base.CaseFormat.UPPER_CAMEL;\n+\n+public class ProtobufSchema implements ParsedSchema {\n+\n+  private static final Logger log = LoggerFactory.getLogger(ProtobufSchema.class);\n+\n+  public static final String TYPE = \"PROTOBUF\";\n+\n+  public static final String DEFAULT_NAME = \"default\";\n+  public static final String MAP_ENTRY_SUFFIX = \"Entry\";  // Suffix used by protoc\n+  public static final String KEY_FIELD = \"key\";\n+  public static final String VALUE_FIELD = \"value\";\n+\n+  public static final Location DEFAULT_LOCATION = Location.get(DEFAULT_NAME);\n+\n+  private final ProtoFileElement schemaObj;\n+\n+  private final Integer version;\n+\n+  private final List<SchemaReference> references;\n+\n+  private final Map<String, ProtoFileElement> dependencies;\n+\n+  private transient DynamicSchema dynamicSchema;\n+\n+  public ProtobufSchema(String schemaString) {\n+    this(schemaString, Collections.emptyList(), Collections.emptyMap(), null);\n+  }\n+\n+  public ProtobufSchema(\n+      String schemaString,\n+      List<SchemaReference> references,\n+      Map<String, String> resolvedReferences,\n+      Integer version\n+  ) {\n+    try {\n+      this.schemaObj = ProtoParser.parse(DEFAULT_LOCATION, schemaString);\n+      this.version = version;\n+      this.references = references;\n+      this.dependencies = resolvedReferences.entrySet()\n+          .stream()\n+          .collect(Collectors.toMap(\n+              Map.Entry::getKey,\n+              e -> ProtoParser.parse(Location.get(e.getKey()), e.getValue())\n+          ));\n+    } catch (IllegalStateException e) {\n+      log.error(\"Cound not parse Protobuf schema \" + schemaString, e);\n+      throw e;\n+    }\n+  }\n+\n+  public ProtobufSchema(\n+      ProtoFileElement protoFileElement,\n+      List<SchemaReference> references,\n+      Map<String, ProtoFileElement> dependencies\n+  ) {\n+    this.schemaObj = protoFileElement;\n+    this.version = null;\n+    this.references = references;\n+    this.dependencies = dependencies;\n+  }\n+\n+  public ProtobufSchema(Descriptor descriptor) {\n+    this(descriptor.getFile(), descriptor.getName());\n+  }\n+\n+  public ProtobufSchema(\n+      FileDescriptor descriptor, String messageName\n+  ) {\n+    Map<String, ProtoFileElement> dependencies = new HashMap<>();\n+    this.schemaObj = toProtoFile(descriptor, messageName, dependencies);\n+    this.version = null;\n+    this.references = Collections.emptyList();\n+    this.dependencies = dependencies;\n+  }\n+\n+  private ProtoFileElement toProtoFile(\n+      FileDescriptor file, String messageName, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    final Location location = Location.get(file.getFullName());\n+    String packageName = file.getPackage();\n+    // Don't set empty package name\n+    if (\"\".equals(packageName)) {\n+      packageName = null;\n+    }\n+\n+    ProtoFile.Syntax syntax = null;\n+    switch (file.getSyntax()) {\n+      case PROTO2:\n+        syntax = ProtoFile.Syntax.PROTO_2;\n+        break;\n+      case PROTO3:\n+        syntax = ProtoFile.Syntax.PROTO_3;\n+        break;\n+      case UNKNOWN:\n+      default:\n+        syntax = null;\n+        break;\n+    }\n+    ImmutableList.Builder<TypeElement> types = ImmutableList.builder();\n+    for (Descriptor md : reorderMessageTypes(file.getMessageTypes(), messageName)) {\n+      MessageElement message = toMessage(md);\n+      types.add(message);\n+    }\n+    for (EnumDescriptor ed : file.getEnumTypes()) {\n+      EnumElement enumer = toEnum(ed);\n+      types.add(enumer);\n+    }\n+    ImmutableList.Builder<String> imports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      imports.add(depName);\n+    }\n+    ImmutableList.Builder<String> publicImports = ImmutableList.builder();\n+    for (FileDescriptor dependency : file.getPublicDependencies()) {\n+      String depName = dependency.getName();\n+      dependencies.put(depName, toProtoFile(dependency, depName, dependencies));\n+      publicImports.add(depName);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (file.getOptions().hasJavaPackage()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_package\",\n+          kind,\n+          file.getOptions().getJavaPackage(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaOuterClassname()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"java_outer_classname\",\n+          kind,\n+          file.getOptions().getJavaOuterClassname(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    if (file.getOptions().hasJavaMultipleFiles()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"java_multiple_files\",\n+          kind,\n+          file.getOptions().getJavaMultipleFiles(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip services, extensions, some options\n+    return new ProtoFileElement(location,\n+        packageName,\n+        syntax,\n+        imports.build(),\n+        publicImports.build(),\n+        types.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList(),\n+        options.build()\n+    );\n+  }\n+\n+  private List<Descriptor> reorderMessageTypes(List<Descriptor> types, String messageName) {\n+    List<Descriptor> result = new ArrayList<>(types.size());\n+    for (Descriptor type : types) {\n+      if (type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    for (Descriptor type : types) {\n+      if (!type.getName().equals(messageName)) {\n+        result.add(type);\n+      }\n+    }\n+    return result;\n+  }\n+\n+  private MessageElement toMessage(Descriptor descriptor) {\n+    Location location = Location.get(descriptor.getFile().getFullName());\n+    String name = descriptor.getName();\n+    log.trace(\"*** msg name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    ImmutableList.Builder<OneOfElement> oneofs = ImmutableList.builder();\n+    ImmutableList.Builder<TypeElement> nested = ImmutableList.builder();\n+    ImmutableList.Builder<ReservedElement> reserved = ImmutableList.builder();\n+    for (OneofDescriptor od : descriptor.getOneofs()) {\n+      OneOfElement oneof = toOneof(od);\n+      oneofs.add(oneof);\n+    }\n+    for (FieldDescriptor fd : descriptor.getFields()) {\n+      if (fd.getContainingOneof() == null) {\n+        FieldElement field = toField(fd, false);\n+        fields.add(field);\n+      }\n+    }\n+    for (Descriptor nestedDesc : descriptor.getNestedTypes()) {\n+      MessageElement nestedMessage = toMessage(nestedDesc);\n+      nested.add(nestedMessage);\n+    }\n+    for (EnumDescriptor nestedDesc : descriptor.getEnumTypes()) {\n+      EnumElement nestedEnum = toEnum(nestedDesc);\n+      nested.add(nestedEnum);\n+    }\n+    for (ReservedRange range : descriptor.toProto().getReservedRangeList()) {\n+      ReservedElement reservedElem = toReserved(location, range);\n+      reserved.add(reservedElem);\n+    }\n+    for (String reservedName : descriptor.toProto().getReservedNameList()) {\n+      ReservedElement reservedElem = new ReservedElement(\n+          location,\n+          \"\",\n+          Collections.singletonList(reservedName)\n+      );\n+      reserved.add(reservedElem);\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (descriptor.getOptions().hasMapEntry()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"map_entry\",\n+          kind,\n+          descriptor.getOptions().getMapEntry(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options, extensions, groups\n+    return new MessageElement(location,\n+        name,\n+        \"\",\n+        nested.build(),\n+        options.build(),\n+        reserved.build(),\n+        fields.build(),\n+        oneofs.build(),\n+        Collections.emptyList(),\n+        Collections.emptyList()\n+    );\n+  }\n+\n+  private ReservedElement toReserved(Location location, ReservedRange range) {\n+    List<Object> values = new ArrayList<>();\n+    int start = range.getStart();\n+    int end = range.getEnd();\n+    values.add(start == end ? start : Range.closed(start, end));\n+    return new ReservedElement(location, \"\", values);\n+  }\n+\n+  private OneOfElement toOneof(OneofDescriptor od) {\n+    String name = od.getName();\n+    log.trace(\"*** oneof name: \" + name);\n+    ImmutableList.Builder<FieldElement> fields = ImmutableList.builder();\n+    for (FieldDescriptor fd : od.getFields()) {\n+      FieldElement field = toField(fd, true);\n+      fields.add(field);\n+    }\n+    // NOTE: skip groups\n+    return new OneOfElement(name, \"\", fields.build(), Collections.emptyList());\n+  }\n+\n+  private EnumElement toEnum(EnumDescriptor ed) {\n+    Location location = Location.get(ed.getFile().getFullName());\n+    String name = ed.getName();\n+    log.trace(\"*** enum name: \" + name);\n+    ImmutableList.Builder<EnumConstantElement> constants = ImmutableList.builder();\n+    for (EnumValueDescriptor ev : ed.getValues()) {\n+      // NOTE: skip options\n+      constants.add(new EnumConstantElement(\n+          location,\n+          ev.getName(),\n+          ev.getNumber(),\n+          \"\",\n+          Collections.emptyList()\n+      ));\n+    }\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (ed.getOptions().hasAllowAlias()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\n+          \"allow_alias\",\n+          kind,\n+          ed.getOptions().getAllowAlias(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    // NOTE: skip some options\n+    return new EnumElement(location, name, \"\", options.build(), constants.build());\n+  }\n+\n+  private FieldElement toField(FieldDescriptor fd, boolean inOneof) {\n+    final Location location = Location.get(fd.getFile().getFullName());\n+    String name = fd.getName();\n+    log.trace(\"*** field name: \" + name);\n+    ImmutableList.Builder<OptionElement> options = ImmutableList.builder();\n+    if (fd.getOptions().hasPacked()) {\n+      OptionElement.Kind kind = OptionElement.Kind.BOOLEAN;\n+      OptionElement option = new OptionElement(\"packed\", kind, fd.getOptions().getPacked(), false);\n+      options.add(option);\n+    }\n+    if (fd.toProto().hasJsonName()) {\n+      OptionElement.Kind kind = OptionElement.Kind.STRING;\n+      OptionElement option = new OptionElement(\n+          \"json_name\",\n+          kind,\n+          fd.toProto().getJsonName(),\n+          false\n+      );\n+      options.add(option);\n+    }\n+    String defaultValue = fd.hasDefaultValue() && fd.getDefaultValue() != null\n+                          ? fd.getDefaultValue().toString()\n+                          : null;\n+    // NOTE: skip some options\n+    return new FieldElement(location,\n+        inOneof ? null : label(fd),\n+        dataType(fd),\n+        name,\n+        defaultValue,\n+        fd.getNumber(),\n+        \"\",\n+        options.build()\n+    );\n+  }\n+\n+  private Field.Label label(FieldDescriptor fd) {\n+    FileDescriptor.Syntax syntax = fd.getFile().getSyntax();\n+    boolean isProto3 = syntax == FileDescriptor.Syntax.PROTO3;\n+    switch (fd.toProto().getLabel()) {\n+      case LABEL_REQUIRED:\n+        return isProto3 ? null : Field.Label.REQUIRED;\n+      case LABEL_OPTIONAL:\n+        return isProto3 ? null : Field.Label.OPTIONAL;\n+      case LABEL_REPEATED:\n+        return Field.Label.REPEATED;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported label\");\n+    }\n+  }\n+\n+  private String dataType(FieldDescriptor field) {\n+    FieldDescriptor.Type type = field.getType();\n+    switch (type) {\n+      case MESSAGE:\n+        return field.getMessageType().getFullName();\n+      case ENUM:\n+        return field.getEnumType().getFullName();\n+      default:\n+        return type.name().toLowerCase();\n+    }\n+  }\n+\n+  public Descriptor toDescriptor() {\n+    return toDescriptor(name());\n+  }\n+\n+  public Descriptor toDescriptor(String name) {\n+    return toDynamicSchema().getMessageDescriptor(name);\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder() {\n+    return newMessageBuilder(name());\n+  }\n+\n+  public DynamicMessage.Builder newMessageBuilder(String name) {\n+    return toDynamicSchema().newMessageBuilder(name);\n+  }\n+\n+  private MessageElement firstMessage() {\n+    for (TypeElement typeElement : schemaObj.getTypes()) {\n+      if (typeElement instanceof MessageElement) {\n+        return (MessageElement) typeElement;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Protobuf schema definition \"\n+        + \" contains no message type definitions\");\n+  }\n+\n+  @VisibleForTesting\n+  protected DynamicSchema toDynamicSchema() {\n+    if (dynamicSchema == null) {\n+      dynamicSchema = toDynamicSchema(DEFAULT_NAME, schemaObj, dependencies);\n+    }\n+    return dynamicSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private static DynamicSchema toDynamicSchema(\n+      String name, ProtoFileElement rootElem, Map<String, ProtoFileElement> dependencies\n+  ) {\n+    log.trace(\"*** toDynamicSchema: \" + rootElem.toSchema());\n+    DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+    try {\n+      if (rootElem.getPackageName() != null) {\n+        schema.setPackage(rootElem.getPackageName());\n+      }\n+      for (TypeElement typeElem : rootElem.getTypes()) {\n+        if (typeElem instanceof MessageElement) {\n+          MessageDefinition message = toDynamicMessage(schema, (MessageElement) typeElem);\n+          schema.addMessageDefinition(message);\n+        } else if (typeElem instanceof EnumElement) {\n+          EnumDefinition enumer = toDynamicEnum((EnumElement) typeElem);\n+          schema.addEnumDefinition(enumer);\n+        }\n+      }\n+      for (String ref : rootElem.getImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      for (String ref : rootElem.getPublicImports()) {\n+        ProtoFileElement dep = dependencies.get(ref);\n+        if (dep != null) {\n+          schema.addPublicDependency(ref);\n+          schema.addSchema(toDynamicSchema(ref, dep, dependencies));\n+        }\n+      }\n+      String javaPackageName = findOption(\"java_package\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaPackageName != null) {\n+        schema.setJavaPackage(javaPackageName);\n+      }\n+      String javaOuterClassname = findOption(\"java_outer_classname\", rootElem.getOptions())\n+          .map(o -> o.getValue().toString()).orElse(null);\n+      if (javaOuterClassname != null) {\n+        schema.setJavaOuterClassname(javaOuterClassname);\n+      }\n+      Boolean javaMultipleFiles = findOption(\"java_multiple_files\", rootElem.getOptions())\n+          .map(o -> Boolean.valueOf(o.getValue().toString())).orElse(null);\n+      if (javaMultipleFiles != null) {\n+        schema.setJavaMultipleFiles(javaMultipleFiles);\n+      }\n+      schema.setName(name);\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private static MessageDefinition toDynamicMessage(\n+      DynamicSchema.Builder schema,\n+      MessageElement messageElem\n+  ) {\n+    log.trace(\"*** message: \" + messageElem.getName());\n+    MessageDefinition.Builder message = MessageDefinition.newBuilder(messageElem.getName());\n+    Set<String> added = new HashSet<>();\n+    for (OneOfElement oneof : messageElem.getOneOfs()) {\n+      MessageDefinition.OneofBuilder oneofBuilder = message.addOneof(oneof.getName());\n+      for (FieldElement field : oneof.getFields()) {\n+        String defaultVal = field.getDefaultValue();\n+        String jsonName = findOption(\"json_name\", field.getOptions()).map(o -> o.getValue()\n+            .toString()).orElse(null);\n+        oneofBuilder.addField(\n+            field.getType(),\n+            field.getName(),\n+            field.getTag(),\n+            defaultVal,\n+            jsonName\n+        );\n+        added.add(field.getName());\n+      }\n+    }\n+    for (FieldElement field : messageElem.getFields()) {\n+      if (added.contains(field.getName())) {\n+        continue;\n+      }\n+      String label = field.getLabel() != null ? field.getLabel().toString().toLowerCase() : null;\n+      String fieldType = field.getType();\n+      String defaultVal = field.getDefaultValue();\n+      String jsonName = findOption(\"json_name\", field.getOptions()).map(o -> o.getValue()\n+          .toString()).orElse(null);\n+      Boolean isPacked = findOption(\n+          \"packed\",\n+          field.getOptions()\n+      ).map(o -> Boolean.valueOf(o.getValue().toString())).orElse(null);\n+      ProtoType protoType = ProtoType.get(fieldType);\n+      // Map fields are only permitted in messages\n+      if (protoType.isMap()) {\n+        label = \"repeated\";\n+        fieldType = toMapEntry(field.getName());\n+        MessageDefinition.Builder mapMessage = MessageDefinition.newBuilder(fieldType);\n+        mapMessage.setMapEntry(true);\n+        mapMessage.addField(null, protoType.keyType().simpleName(), KEY_FIELD, 1, null);\n+        mapMessage.addField(null, protoType.valueType().simpleName(), VALUE_FIELD, 2, null);\n+        schema.addMessageDefinition(mapMessage.build());\n+      }\n+      message.addField(\n+          label,\n+          fieldType,\n+          field.getName(),\n+          field.getTag(),\n+          defaultVal,\n+          jsonName,\n+          isPacked\n+      );\n+    }\n+    for (TypeElement type : messageElem.getNestedTypes()) {\n+      if (type instanceof MessageElement) {\n+        message.addMessageDefinition(toDynamicMessage(schema, (MessageElement) type));\n+      } else if (type instanceof EnumElement) {\n+        message.addEnumDefinition(toDynamicEnum((EnumElement) type));\n+      }\n+    }\n+    for (ReservedElement reserved : messageElem.getReserveds()) {\n+      for (Object elem : reserved.getValues()) {\n+        if (elem instanceof String) {\n+          message.addReservedName((String) elem);\n+        } else if (elem instanceof Integer) {\n+          int tag = (Integer) elem;\n+          message.addReservedRange(tag, tag);\n+        } else if (elem instanceof Range) {\n+          Range<Integer> range = (Range<Integer>) elem;\n+          message.addReservedRange(range.lowerEndpoint(), range.upperEndpoint());\n+        } else {\n+          throw new IllegalStateException(\"Unsupported reserved type: \" + elem.getClass()\n+              .getName());\n+        }\n+      }\n+    }\n+    return message.build();\n+  }\n+\n+  private static Optional<OptionElement> findOption(String name, List<OptionElement> options) {\n+    return options.stream().filter(o -> o.getName().equals(name)).findFirst();\n+  }\n+\n+  private static EnumDefinition toDynamicEnum(EnumElement enumElem) {\n+    Boolean allowAlias = findOption(\"allow_alias\", enumElem.getOptions()).map(o -> Boolean.valueOf(o\n+        .getValue()\n+        .toString())).orElse(null);\n+    EnumDefinition.Builder enumer = EnumDefinition.newBuilder(enumElem.getName(), allowAlias);\n+    for (EnumConstantElement constant : enumElem.getConstants()) {\n+      enumer.addValue(constant.getName(), constant.getTag());\n+    }\n+    return enumer.build();\n+  }\n+\n+  public ProtoFileElement rawSchema() {\n+    return schemaObj;\n+  }\n+\n+  @Override\n+  public String schemaType() {\n+    return TYPE;\n+  }\n+\n+  @Override\n+  public String name() {\n+    return firstMessage().getName();\n+  }\n+\n+  @Override\n+  public String canonicalString() {\n+    // Remove full-line comments, such as the location\n+    return schemaObj.toSchema().replaceAll(\"^//.*?\\\\n\", \"\");\n+  }\n+\n+  public Integer version() {\n+    return version;\n+  }\n+\n+  @Override\n+  public List<SchemaReference> references() {\n+    return references;\n+  }\n+\n+  public Map<String, String> resolvedReferences() {\n+    return dependencies.entrySet()\n+        .stream()\n+        .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().toSchema()));\n+  }\n+\n+  public Map<String, ProtoFileElement> dependencies() {\n+    return dependencies;\n+  }\n+\n+  @Override\n+  public boolean isBackwardCompatible(ParsedSchema previousSchema) {\n+    if (!schemaType().equals(previousSchema.schemaType())) {\n+      return false;\n+    }\n+    final List<Difference> differences = SchemaDiff.compare(\n+        ((ProtobufSchema) previousSchema).schemaObj,\n+        schemaObj\n+    );\n+    final List<Difference> incompatibleDiffs = differences.stream()\n+        .filter(diff -> !SchemaDiff.COMPATIBLE_CHANGES.contains(diff.getType()))\n+        .collect(Collectors.toList());\n+    boolean isCompatible = incompatibleDiffs.isEmpty();\n+    if (!isCompatible) {\n+      log.warn(\"Found incompatible change: {}\", incompatibleDiffs.get(0));\n+    }\n+    return isCompatible;", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE2OTI2NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369169264", "bodyText": "I'll log the remaining at debug level.", "author": "rayokota", "createdAt": "2020-01-21T18:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE3OTcwNw=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\nindex 776494886..0368ebece 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchema.java\n\n@@ -84,6 +84,8 @@ public class ProtobufSchema implements ParsedSchema {\n \n   private final Integer version;\n \n+  private final String name;\n+\n   private final List<SchemaReference> references;\n \n   private final Map<String, ProtoFileElement> dependencies;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4Mzk3OA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368183978", "bodyText": "Can we skip the null and instanceof checks here ? I couldn't find any instances where they would help. Basically refactor the method so it accepts a non-null Message parameter.", "author": "dragosvictor", "createdAt": "2020-01-18T00:05:14Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.JsonFormat;\n+\n+import java.io.IOException;\n+import java.io.StringWriter;\n+import java.nio.charset.StandardCharsets;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+\n+public class ProtobufSchemaUtils {\n+\n+  private static final ObjectMapper jsonMapper = new ObjectMapper();\n+\n+  public static ProtobufSchema copyOf(ProtobufSchema schema) {\n+    return new ProtobufSchema(schema.canonicalString(),\n+        schema.references(),\n+        schema.resolvedReferences(),\n+        schema.version()\n+    );\n+  }\n+\n+  public static ProtobufSchema getSchema(Object object) {\n+    if (object == null) {\n+      return null;\n+    } else if (object instanceof Message) {\n+      Message message = (Message) object;\n+      Descriptors.Descriptor desc = message.getDescriptorForType();\n+      return new ProtobufSchema(desc);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported type of class \" + object.getClass()\n+          .getName());\n+    }", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MDM5OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369170399", "bodyText": "I'll change to take type Message", "author": "rayokota", "createdAt": "2020-01-21T18:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4Mzk3OA=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\nindex bc40f0b39..881d085aa 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n\n@@ -37,7 +37,8 @@ public class ProtobufSchemaUtils {\n     return new ProtobufSchema(schema.canonicalString(),\n         schema.references(),\n         schema.resolvedReferences(),\n-        schema.version()\n+        schema.version(),\n+        schema.name()\n     );\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4NDQ0NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368184445", "bodyText": "Similar. Since the method is static, can we refactor it to take a ProtobufSchema object instead of ParsedSchema ?", "author": "dragosvictor", "createdAt": "2020-01-18T00:07:46Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.JsonFormat;\n+\n+import java.io.IOException;\n+import java.io.StringWriter;\n+import java.nio.charset.StandardCharsets;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+\n+public class ProtobufSchemaUtils {\n+\n+  private static final ObjectMapper jsonMapper = new ObjectMapper();\n+\n+  public static ProtobufSchema copyOf(ProtobufSchema schema) {\n+    return new ProtobufSchema(schema.canonicalString(),\n+        schema.references(),\n+        schema.resolvedReferences(),\n+        schema.version()\n+    );\n+  }\n+\n+  public static ProtobufSchema getSchema(Object object) {\n+    if (object == null) {\n+      return null;\n+    } else if (object instanceof Message) {\n+      Message message = (Message) object;\n+      Descriptors.Descriptor desc = message.getDescriptorForType();\n+      return new ProtobufSchema(desc);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported type of class \" + object.getClass()\n+          .getName());\n+    }\n+  }\n+\n+  public static Object toObject(JsonNode value, ParsedSchema parsedSchema) throws IOException {\n+    ProtobufSchema schema = (ProtobufSchema) parsedSchema;", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MTI0OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369171249", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-21T18:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4NDQ0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\nindex bc40f0b39..881d085aa 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n\n@@ -37,7 +37,8 @@ public class ProtobufSchemaUtils {\n     return new ProtobufSchema(schema.canonicalString(),\n         schema.references(),\n         schema.resolvedReferences(),\n-        schema.version()\n+        schema.version(),\n+        schema.name()\n     );\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4NDg0NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r368184844", "bodyText": "Similar. Can we enforce the value to be a Message ?", "author": "dragosvictor", "createdAt": "2020-01-18T00:10:09Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.JsonFormat;\n+\n+import java.io.IOException;\n+import java.io.StringWriter;\n+import java.nio.charset.StandardCharsets;\n+\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+\n+public class ProtobufSchemaUtils {\n+\n+  private static final ObjectMapper jsonMapper = new ObjectMapper();\n+\n+  public static ProtobufSchema copyOf(ProtobufSchema schema) {\n+    return new ProtobufSchema(schema.canonicalString(),\n+        schema.references(),\n+        schema.resolvedReferences(),\n+        schema.version()\n+    );\n+  }\n+\n+  public static ProtobufSchema getSchema(Object object) {\n+    if (object == null) {\n+      return null;\n+    } else if (object instanceof Message) {\n+      Message message = (Message) object;\n+      Descriptors.Descriptor desc = message.getDescriptorForType();\n+      return new ProtobufSchema(desc);\n+    } else {\n+      throw new IllegalArgumentException(\"Unsupported type of class \" + object.getClass()\n+          .getName());\n+    }\n+  }\n+\n+  public static Object toObject(JsonNode value, ParsedSchema parsedSchema) throws IOException {\n+    ProtobufSchema schema = (ProtobufSchema) parsedSchema;\n+    StringWriter out = new StringWriter();\n+    jsonMapper.writeValue(out, value);\n+    String jsonString = out.toString();\n+    DynamicMessage.Builder message = schema.newMessageBuilder();\n+    JsonFormat.parser().merge(jsonString, message);\n+    return message.build();\n+  }\n+\n+  public static byte[] toJson(Object value) throws IOException {\n+    if (value == null) {\n+      return null;\n+    } else if (value instanceof Message) {", "originalCommit": "7a07245b91b214abccf660c3ebea92d2ea0c68e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTE3MTIyNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369171226", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-21T18:33:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE4NDg0NA=="}], "type": "inlineReview", "revised_code": {"commit": "040c4bf2ea2158e5c6527c87600e19344950297f", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\nindex bc40f0b39..881d085aa 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/ProtobufSchemaUtils.java\n\n@@ -37,7 +37,8 @@ public class ProtobufSchemaUtils {\n     return new ProtobufSchema(schema.canonicalString(),\n         schema.references(),\n         schema.resolvedReferences(),\n-        schema.version()\n+        schema.version(),\n+        schema.name()\n     );\n   }\n \n"}}, {"oid": "040c4bf2ea2158e5c6527c87600e19344950297f", "url": "https://github.com/confluentinc/schema-registry/commit/040c4bf2ea2158e5c6527c87600e19344950297f", "message": "Serialize message indexes", "committedDate": "2020-01-18T00:58:35Z", "type": "commit"}, {"oid": "a7b4a2e96a8e234f60df7fec668d8cc47ee542b7", "url": "https://github.com/confluentinc/schema-registry/commit/a7b4a2e96a8e234f60df7fec668d8cc47ee542b7", "message": "Improve null handling", "committedDate": "2020-01-18T21:27:16Z", "type": "commit"}, {"oid": "9a35dc580d31b77f5f502aeaf20471dd3c133027", "url": "https://github.com/confluentinc/schema-registry/commit/9a35dc580d31b77f5f502aeaf20471dd3c133027", "message": "Incorporate review feedback", "committedDate": "2020-01-21T18:39:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI2NzIwNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369267204", "bodyText": "Could we use Set's instead of Map's here ? The mapped value is never referenced. Even better if we can drop the use of allTags and updateByTag altogether.", "author": "dragosvictor", "createdAt": "2020-01-21T22:02:35Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/EnumSchemaDiff.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf.diff;\n+\n+import com.squareup.wire.schema.internal.parser.EnumConstantElement;\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_REMOVED;\n+\n+public class EnumSchemaDiff {\n+  static void compare(final Context ctx, final EnumElement original, final EnumElement update) {\n+    Map<Integer, EnumConstantElement> originalByTag = new HashMap<>();\n+    for (EnumConstantElement enumer : original.getConstants()) {\n+      originalByTag.put(enumer.getTag(), enumer);\n+    }\n+    Map<Integer, EnumConstantElement> updateByTag = new HashMap<>();\n+    for (EnumConstantElement enumer : update.getConstants()) {\n+      updateByTag.put(enumer.getTag(), enumer);\n+    }\n+    Set<Integer> allTags = new HashSet<>(originalByTag.keySet());\n+    allTags.addAll(updateByTag.keySet());\n+\n+    for (Integer tag : allTags) {\n+      try (Context.PathScope pathScope = ctx.enterPath(tag.toString())) {\n+        if (!updateByTag.containsKey(tag)) {\n+          ctx.addDifference(ENUM_CONST_REMOVED);\n+        } else if (!originalByTag.containsKey(tag)) {\n+          ctx.addDifference(ENUM_CONST_ADDED);\n+        }\n+      }\n+    }", "originalCommit": "9a35dc580d31b77f5f502aeaf20471dd3c133027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0NTA2Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370345066", "bodyText": "I added an additional check to compare the values", "author": "rayokota", "createdAt": "2020-01-23T20:45:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI2NzIwNA=="}], "type": "inlineReview", "revised_code": {"commit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/EnumSchemaDiff.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/EnumSchemaDiff.java\nindex fb3a82545..55c78d4ff 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/EnumSchemaDiff.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/EnumSchemaDiff.java\n\n@@ -25,6 +25,7 @@ import java.util.Map;\n import java.util.Set;\n \n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_CHANGED;\n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_REMOVED;\n \n public class EnumSchemaDiff {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI3MDkxMg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369270912", "bodyText": "Similarly, can we drop updateByTag and allTags ?", "author": "dragosvictor", "createdAt": "2020-01-21T22:11:47Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/OneOfDiff.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf.diff;\n+\n+import com.squareup.wire.schema.internal.parser.FieldElement;\n+import com.squareup.wire.schema.internal.parser.OneOfElement;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ONEOF_FIELD_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ONEOF_FIELD_REMOVED;\n+\n+public class OneOfDiff {\n+  static void compare(final Context ctx, final OneOfElement original, final OneOfElement update) {\n+    Map<Integer, FieldElement> originalByTag = new HashMap<>();\n+    for (FieldElement field : original.getFields()) {\n+      originalByTag.put(field.getTag(), field);\n+    }\n+    Map<Integer, FieldElement> updateByTag = new HashMap<>();\n+    for (FieldElement field : update.getFields()) {\n+      updateByTag.put(field.getTag(), field);\n+    }\n+\n+    Set<Integer> allTags = new HashSet<>(originalByTag.keySet());\n+    allTags.addAll(updateByTag.keySet());\n+    for (Integer tag : allTags) {\n+      try (Context.PathScope pathScope = ctx.enterPath(tag.toString())) {\n+        if (!updateByTag.containsKey(tag)) {\n+          ctx.addDifference(ONEOF_FIELD_REMOVED);\n+        } else if (!originalByTag.containsKey(tag)) {\n+          ctx.addDifference(ONEOF_FIELD_ADDED);\n+        } else {\n+          FieldSchemaDiff.compare(ctx, originalByTag.get(tag), updateByTag.get(tag));\n+        }\n+      }\n+    }", "originalCommit": "9a35dc580d31b77f5f502aeaf20471dd3c133027", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/OneOfDiff.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/OneOfDiff.java\nindex 6711d5054..f6f28b5d0 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/OneOfDiff.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/OneOfDiff.java\n\n@@ -42,12 +42,14 @@ public class OneOfDiff {\n     allTags.addAll(updateByTag.keySet());\n     for (Integer tag : allTags) {\n       try (Context.PathScope pathScope = ctx.enterPath(tag.toString())) {\n-        if (!updateByTag.containsKey(tag)) {\n+        FieldElement originalField = originalByTag.get(tag);\n+        FieldElement updateField = updateByTag.get(tag);\n+        if (updateField == null) {\n           ctx.addDifference(ONEOF_FIELD_REMOVED);\n-        } else if (!originalByTag.containsKey(tag)) {\n+        } else if (originalField == null) {\n           ctx.addDifference(ONEOF_FIELD_ADDED);\n         } else {\n-          FieldSchemaDiff.compare(ctx, originalByTag.get(tag), updateByTag.get(tag));\n+          FieldSchemaDiff.compare(ctx, originalField, updateField);\n         }\n       }\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI3NDA0NA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369274044", "bodyText": "Nit: this could be factored out.", "author": "dragosvictor", "createdAt": "2020-01-21T22:19:19Z", "path": "protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiff.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf.diff;\n+\n+import com.squareup.wire.schema.internal.parser.EnumElement;\n+import com.squareup.wire.schema.internal.parser.MessageElement;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.TypeElement;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_REMOVED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_REMOVED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.FIELD_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.FIELD_NAME_CHANGED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.FIELD_REMOVED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.MESSAGE_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.MESSAGE_REMOVED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ONEOF_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ONEOF_FIELD_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ONEOF_REMOVED;\n+\n+public class SchemaDiff {\n+  public static final Set<Difference.Type> COMPATIBLE_CHANGES;\n+\n+  static {\n+    Set<Difference.Type> changes = new HashSet<>();\n+\n+    changes.add(MESSAGE_ADDED);\n+    changes.add(MESSAGE_REMOVED);\n+    changes.add(ENUM_ADDED);\n+    changes.add(ENUM_REMOVED);\n+    changes.add(ENUM_CONST_ADDED);\n+    changes.add(ENUM_CONST_REMOVED);\n+    changes.add(FIELD_ADDED);\n+    changes.add(FIELD_REMOVED);\n+    changes.add(FIELD_NAME_CHANGED);\n+    changes.add(ONEOF_ADDED);\n+    changes.add(ONEOF_REMOVED);\n+    changes.add(ONEOF_FIELD_ADDED);\n+\n+    COMPATIBLE_CHANGES = Collections.unmodifiableSet(changes);\n+  }\n+\n+  public static List<Difference> compare(\n+      final ProtoFileElement original,\n+      final ProtoFileElement update\n+  ) {\n+    final Context ctx = new Context(COMPATIBLE_CHANGES);\n+    compare(ctx, original, update);\n+    return ctx.getDifferences();\n+  }\n+\n+  @SuppressWarnings(\"ConstantConditions\")\n+  static void compare(final Context ctx, ProtoFileElement original, ProtoFileElement update) {\n+    collectEnums(ctx, original.getTypes(), true);\n+    collectEnums(ctx, update.getTypes(), false);\n+    compareTypeElements(ctx, original.getTypes(), update.getTypes());\n+  }\n+\n+  private static void collectEnums(\n+      final Context ctx,\n+      final List<TypeElement> types,\n+      boolean isOriginal\n+  ) {\n+    for (TypeElement typeElement : types) {\n+      if (typeElement instanceof MessageElement) {\n+        MessageElement messageElement = (MessageElement) typeElement;\n+        collectEnums(ctx, messageElement.getNestedTypes(), isOriginal);\n+      } else if (typeElement instanceof EnumElement) {\n+        ctx.addEnum(typeElement.getName(), isOriginal);\n+      }\n+    }\n+  }\n+\n+  public static void compareTypeElements(\n+      final Context ctx, final List<TypeElement> original, final List<TypeElement> update\n+  ) {\n+    Map<String, MessageElement> originalMessages = new HashMap<>();\n+    Map<String, MessageElement> updateMessages = new HashMap<>();\n+    Map<String, EnumElement> originalEnums = new HashMap<>();\n+    Map<String, EnumElement> updateEnums = new HashMap<>();\n+    for (TypeElement typeElement : original) {\n+      if (typeElement instanceof MessageElement) {\n+        MessageElement messageElement = (MessageElement) typeElement;\n+        originalMessages.put(messageElement.getName(), messageElement);\n+      } else if (typeElement instanceof EnumElement) {\n+        EnumElement enumElement = (EnumElement) typeElement;\n+        originalEnums.put(enumElement.getName(), enumElement);\n+      }\n+    }\n+    for (TypeElement typeElement : update) {\n+      if (typeElement instanceof MessageElement) {\n+        MessageElement messageElement = (MessageElement) typeElement;\n+        updateMessages.put(messageElement.getName(), messageElement);\n+      } else if (typeElement instanceof EnumElement) {\n+        EnumElement enumElement = (EnumElement) typeElement;\n+        updateEnums.put(enumElement.getName(), enumElement);\n+      }\n+    }", "originalCommit": "9a35dc580d31b77f5f502aeaf20471dd3c133027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0NDkyNA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370344924", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-23T20:45:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI3NDA0NA=="}], "type": "inlineReview", "revised_code": {"commit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "chunk": "diff --git a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiff.java b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiff.java\nindex e24b91ce5..68b04d3b0 100644\n--- a/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiff.java\n+++ b/protobuf-provider/src/main/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiff.java\n\n@@ -30,6 +30,7 @@ import java.util.Set;\n \n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_ADDED;\n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_ADDED;\n+import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_CHANGED;\n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_CONST_REMOVED;\n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.ENUM_REMOVED;\n import static io.confluent.kafka.schemaregistry.protobuf.diff.Difference.Type.FIELD_ADDED;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI4MzAyOA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r369283028", "bodyText": "Nit: can we not reuse the ResourceLoader instead of this ?", "author": "dragosvictor", "createdAt": "2020-01-21T22:41:16Z", "path": "protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiffTest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf.diff;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.fasterxml.jackson.databind.node.ArrayNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import com.squareup.wire.schema.Location;\n+import com.squareup.wire.schema.internal.parser.ProtoFileElement;\n+import com.squareup.wire.schema.internal.parser.ProtoParser;\n+import org.junit.Test;\n+\n+import java.io.BufferedReader;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n+\n+public class SchemaDiffTest {\n+\n+  @Test\n+  public void checkProtobufSchemaCompatibility() throws Exception {\n+    ObjectMapper objectMapper = new ObjectMapper();\n+    ArrayNode nodes = (ArrayNode) objectMapper.readTree(readFile(\"diff-schema-examples.json\"));\n+\n+    for (int i = 0; i < nodes.size(); i++) {\n+      final ObjectNode testCase = (ObjectNode) nodes.get(i);\n+      String originalSchema = testCase.get(\"original_schema\").asText();\n+      String updateSchema = testCase.get(\"update_schema\").asText();\n+      ProtoFileElement original = ProtoParser.parse(Location.get(\"unknown\"), originalSchema);\n+      ProtoFileElement update = ProtoParser.parse(Location.get(\"unknown\"), updateSchema);\n+      final ArrayNode changes = (ArrayNode) testCase.get(\"changes\");\n+      boolean isCompatible = testCase.get(\"compatible\").asBoolean();\n+      final List<String> errorMessages = new ArrayList<>();\n+      for (int j = 0; j < changes.size(); j++) {\n+        errorMessages.add(changes.get(j).asText());\n+      }\n+      final String description = testCase.get(\"description\").asText();\n+\n+      List<Difference> differences = SchemaDiff.compare(original, update);\n+      final List<Difference> incompatibleDiffs = differences.stream()\n+          .filter(diff -> !SchemaDiff.COMPATIBLE_CHANGES.contains(diff.getType()))\n+          .collect(Collectors.toList());\n+      assertThat(\n+          description,\n+          differences.stream()\n+              .map(change -> change.getType().toString() + \" \" + change.getFullPath())\n+              .collect(toList()),\n+          is(errorMessages)\n+      );\n+      assertEquals(description, isCompatible, incompatibleDiffs.isEmpty());\n+    }\n+  }\n+\n+  @Test\n+  public void checkCompatibilityUsingProtoFiles() throws Exception {\n+\n+    ResourceLoader resourceLoader = new ResourceLoader(\n+        \"/io/confluent/kafka/schemaregistry/protobuf/diff/\");\n+    ProtoFileElement original = resourceLoader.readObj(\"TestProto.proto\");\n+    ProtoFileElement update = resourceLoader.readObj(\"TestProto2.proto\");\n+\n+    List<Difference> changes = SchemaDiff.compare(original, update);\n+\n+    assertTrue(changes.contains(new Difference(\n+        Difference.Type.FIELD_NAME_CHANGED,\n+        \"#/TestMessage/2\"\n+    )));\n+    assertTrue(changes.contains(new Difference(\n+        Difference.Type.FIELD_SCALAR_KIND_CHANGED,\n+        \"#/TestMessage/2\"\n+    )));\n+  }\n+\n+  public static String readFile(String fileName) {\n+    ClassLoader classLoader = ClassLoader.getSystemClassLoader();\n+    InputStream is = classLoader.getResourceAsStream(fileName);\n+    if (is != null) {\n+      BufferedReader reader = new BufferedReader(new InputStreamReader(is));\n+      return reader.lines().collect(Collectors.joining(System.lineSeparator()));\n+    }\n+    return null;\n+  }", "originalCommit": "9a35dc580d31b77f5f502aeaf20471dd3c133027", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0NDk0Mw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370344943", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-23T20:45:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTI4MzAyOA=="}], "type": "inlineReview", "revised_code": {"commit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "chunk": "diff --git a/protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiffTest.java b/protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiffTest.java\nindex 73cd7f217..fd93f3498 100644\n--- a/protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiffTest.java\n+++ b/protobuf-provider/src/test/java/io/confluent/kafka/schemaregistry/protobuf/diff/SchemaDiffTest.java\n\n@@ -93,13 +93,8 @@ public class SchemaDiffTest {\n     )));\n   }\n \n-  public static String readFile(String fileName) {\n-    ClassLoader classLoader = ClassLoader.getSystemClassLoader();\n-    InputStream is = classLoader.getResourceAsStream(fileName);\n-    if (is != null) {\n-      BufferedReader reader = new BufferedReader(new InputStreamReader(is));\n-      return reader.lines().collect(Collectors.joining(System.lineSeparator()));\n-    }\n-    return null;\n+  private static String readFile(String fileName) {\n+    ResourceLoader resourceLoader = new ResourceLoader(\"/\");\n+    return resourceLoader.toString(fileName);\n   }\n }\n\\ No newline at end of file\n"}}, {"oid": "f8e9a5856ae678854627c5d5f0d5e23e94251595", "url": "https://github.com/confluentinc/schema-registry/commit/f8e9a5856ae678854627c5d5f0d5e23e94251595", "message": "Add derive.type config for RecordNameStrategy", "committedDate": "2020-01-22T00:31:34Z", "type": "commit"}, {"oid": "608dc6fe66d38a27af0ed343e53cde071a91dfcd", "url": "https://github.com/confluentinc/schema-registry/commit/608dc6fe66d38a27af0ed343e53cde071a91dfcd", "message": "Add kafak-connect maven plugin for Protobuf", "committedDate": "2020-01-22T18:42:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDMwMjk5Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370302992", "bodyText": "what do you think about using the word dependencies instead of references here and in some other places? this seems more unambiguous to me.\nI think the name SchemaReference shouldn't change.", "author": "mhowlett", "createdAt": "2020-01-23T19:10:58Z", "path": "core/src/test/java/io/confluent/kafka/schemaregistry/rest/protobuf/RestApiTest.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.kafka.schemaregistry.rest.protobuf;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.client.rest.RestService;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaReference;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.SchemaString;\n+import io.confluent.kafka.schemaregistry.client.rest.entities.requests.RegisterSchemaRequest;\n+import io.confluent.kafka.schemaregistry.client.rest.exceptions.RestClientException;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();\n+\n+  public RestApiTest() {\n+    super(1, true);\n+  }\n+\n+  @Override\n+  protected Properties getSchemaRegistryProperties() {\n+    Properties props = new Properties();\n+    props.setProperty(\"schema.providers\", ProtobufSchemaProvider.class.getName());\n+    return props;\n+  }\n+\n+  @Test\n+  public void testBasic() throws Exception {\n+    String subject1 = \"testTopic1\";\n+    String subject2 = \"testTopic2\";\n+    int schemasInSubject1 = 10;\n+    List<Integer> allVersionsInSubject1 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject1 = getRandomProtobufSchemas(schemasInSubject1);\n+    int schemasInSubject2 = 5;\n+    List<Integer> allVersionsInSubject2 = new ArrayList<Integer>();\n+    List<String> allSchemasInSubject2 = getRandomProtobufSchemas(schemasInSubject2);\n+    List<String> allSubjects = new ArrayList<String>();\n+\n+    // test getAllSubjects with no existing data\n+    assertEquals(\"Getting all subjects should return empty\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+\n+    // test registering and verifying new schemas in subject1\n+    int schemaIdCounter = 1;\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      String schema = allSchemasInSubject1.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject1);\n+      schemaIdCounter++;\n+      allVersionsInSubject1.add(expectedVersion);\n+    }\n+    allSubjects.add(subject1);\n+\n+    // test re-registering existing schemas\n+    for (int i = 0; i < schemasInSubject1; i++) {\n+      int expectedId = i + 1;\n+      String schemaString = allSchemasInSubject1.get(i);\n+      int foundId = restApp.restClient.registerSchema(schemaString,\n+          ProtobufSchema.TYPE,\n+          Collections.emptyList(),\n+          subject1\n+      );\n+      assertEquals(\"Re-registering an existing schema should return the existing version\",\n+          expectedId,\n+          foundId\n+      );\n+    }\n+\n+    // test registering schemas in subject2\n+    for (int i = 0; i < schemasInSubject2; i++) {\n+      String schema = allSchemasInSubject2.get(i);\n+      int expectedVersion = i + 1;\n+      registerAndVerifySchema(restApp.restClient, schema, schemaIdCounter, subject2);\n+      schemaIdCounter++;\n+      allVersionsInSubject2.add(expectedVersion);\n+    }\n+    allSubjects.add(subject2);\n+\n+    // test getAllVersions with existing data\n+    assertEquals(\n+        \"Getting all versions from subject1 should match all registered versions\",\n+        allVersionsInSubject1,\n+        restApp.restClient.getAllVersions(subject1)\n+    );\n+    assertEquals(\n+        \"Getting all versions from subject2 should match all registered versions\",\n+        allVersionsInSubject2,\n+        restApp.restClient.getAllVersions(subject2)\n+    );\n+\n+    // test getAllSubjects with existing data\n+    assertEquals(\"Getting all subjects should match all registered subjects\",\n+        allSubjects,\n+        restApp.restClient.getAllSubjects()\n+    );\n+  }\n+\n+  @Test\n+  public void testSchemaReferences() throws Exception {\n+    Map<String, String> schemas = getProtobufSchemaWithReferences();", "originalCommit": "608dc6fe66d38a27af0ed343e53cde071a91dfcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM0NjE3OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370346179", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-23T20:47:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDMwMjk5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "chunk": "diff --git a/core/src/test/java/io/confluent/kafka/schemaregistry/rest/protobuf/RestApiTest.java b/core/src/test/java/io/confluent/kafka/schemaregistry/rest/protobuf/RestApiTest.java\nindex 8a4c66730..5f3e66003 100644\n--- a/core/src/test/java/io/confluent/kafka/schemaregistry/rest/protobuf/RestApiTest.java\n+++ b/core/src/test/java/io/confluent/kafka/schemaregistry/rest/protobuf/RestApiTest.java\n\n@@ -128,8 +128,8 @@ public class RestApiTest extends ClusterTestHarness {\n   }\n \n   @Test\n-  public void testSchemaReferences() throws Exception {\n-    Map<String, String> schemas = getProtobufSchemaWithReferences();\n+  public void testSchemaDependencies() throws Exception {\n+    Map<String, String> schemas = getProtobufSchemaWithDependencies();\n     String subject = \"reference\";\n     registerAndVerifySchema(restApp.restClient, schemas.get(\"ref.proto\"), 1, subject);\n \n"}}, {"oid": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "url": "https://github.com/confluentinc/schema-registry/commit/8e65cc65b67dbea699887326c15fb260b1cdff1a", "message": "Incorporate review feedback\n\nAlso some minor refactoring in compat check", "committedDate": "2020-01-23T20:50:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5MTI4MQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370391281", "bodyText": "Nit: this is not used anywhere.", "author": "dragosvictor", "createdAt": "2020-01-23T22:34:57Z", "path": "protobuf-serializer/src/test/java/io/confluent/kafka/schemaregistry/protobuf/rest/RestApiSerializerTest.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.protobuf.rest;\n+\n+import com.criteo.glup.ExampleProtoCriteo;\n+import com.criteo.glup.MetadataProto;\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Timestamp;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Properties;\n+import java.util.Random;\n+\n+import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n+import io.confluent.kafka.schemaregistry.ParsedSchema;\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaUtils;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializerConfig;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerConfig;\n+import io.confluent.kafka.serializers.protobuf.test.DependencyTestProto.DependencyMessage;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto.ComplexType;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto.NestedMessage;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto.Status;\n+import io.confluent.kafka.serializers.protobuf.test.NestedTestProto.UserId;\n+import io.confluent.kafka.serializers.protobuf.test.TestMessageProtos.TestMessage;\n+\n+import static io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerTest.getField;\n+import static org.junit.Assert.assertEquals;\n+\n+public class RestApiSerializerTest extends ClusterTestHarness {\n+\n+  private static final Random random = new Random();", "originalCommit": "608dc6fe66d38a27af0ed343e53cde071a91dfcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0ODAxMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370748013", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:18:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5MTI4MQ=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-serializer/src/test/java/io/confluent/kafka/schemaregistry/protobuf/rest/RestApiSerializerTest.java b/protobuf-serializer/src/test/java/io/confluent/kafka/schemaregistry/protobuf/rest/RestApiSerializerTest.java\nindex aa2587637..9a22e3086 100644\n--- a/protobuf-serializer/src/test/java/io/confluent/kafka/schemaregistry/protobuf/rest/RestApiSerializerTest.java\n+++ b/protobuf-serializer/src/test/java/io/confluent/kafka/schemaregistry/protobuf/rest/RestApiSerializerTest.java\n\n@@ -25,7 +25,6 @@ import org.junit.Test;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Properties;\n-import java.util.Random;\n \n import io.confluent.kafka.schemaregistry.ClusterTestHarness;\n import io.confluent.kafka.schemaregistry.ParsedSchema;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5NDg5Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370394892", "bodyText": "Nit: do we need the throws declaration here ? ConfigException is a RuntimeException.", "author": "dragosvictor", "createdAt": "2020-01-23T22:45:14Z", "path": "protobuf-serializer/src/main/java/io/confluent/kafka/formatter/protobuf/ProtobufMessageReader.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.formatter.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.InvalidProtocolBufferException;\n+import com.google.protobuf.util.JsonFormat;\n+import kafka.common.KafkaException;\n+import kafka.common.MessageReader;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.common.serialization.Serializer;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider;\n+import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+\n+/**\n+ * Example\n+ * To use ProtobufMessageReader, first make sure that Zookeeper, Kafka and schema registry server\n+ * are\n+ * all started. Second, make sure the jar for ProtobufMessageReader and its dependencies are\n+ * included\n+ * in the classpath of kafka-console-producer.sh. Then run the following\n+ * command.\n+ *\n+ * <p>Send Protobuf record as value.\n+ * bin/kafka-console-producer.sh --broker-list localhost:9092 --topic t1 \\\n+ * --line-reader io.confluent.kafka.formatter.ProtobufMessageReader \\\n+ * --property schema.registry.url=http://localhost:8081 \\\n+ * --property value.schema='syntax = \"proto3\"; message MyRecord { string f1 = 1; }'\n+ *\n+ * <p>In the shell, type in the following.\n+ * {\"f1\": \"value1\"}\n+ */\n+public class ProtobufMessageReader extends AbstractKafkaProtobufSerializer\n+    implements MessageReader {\n+\n+  private String topic = null;\n+  private BufferedReader reader = null;\n+  private Boolean parseKey = false;\n+  private String keySeparator = \"\\t\";\n+  private boolean ignoreError = false;\n+  private ProtobufSchema keySchema = null;\n+  private ProtobufSchema valueSchema = null;\n+  private String keySubject = null;\n+  private String valueSubject = null;\n+  private Serializer keySerializer;\n+\n+  /**\n+   * Constructor needed by kafka console producer.\n+   */\n+  public ProtobufMessageReader() {\n+  }\n+\n+  /**\n+   * For testing only.\n+   */\n+  ProtobufMessageReader(\n+      SchemaRegistryClient schemaRegistryClient,\n+      ProtobufSchema keySchema,\n+      ProtobufSchema valueSchema,\n+      String topic,\n+      boolean parseKey,\n+      BufferedReader reader,\n+      boolean autoRegister\n+  ) {\n+    this.schemaRegistry = schemaRegistryClient;\n+    this.keySchema = keySchema;\n+    this.valueSchema = valueSchema;\n+    this.topic = topic;\n+    this.keySubject = topic + \"-key\";\n+    this.valueSubject = topic + \"-value\";\n+    this.parseKey = parseKey;\n+    this.reader = reader;\n+    this.autoRegisterSchema = autoRegister;\n+  }\n+\n+  @Override\n+  public void init(java.io.InputStream inputStream, Properties props) {\n+    topic = props.getProperty(\"topic\");\n+    if (props.containsKey(\"parse.key\")) {\n+      parseKey = props.getProperty(\"parse.key\").trim().toLowerCase().equals(\"true\");\n+    }\n+    if (props.containsKey(\"key.separator\")) {\n+      keySeparator = props.getProperty(\"key.separator\");\n+    }\n+    if (props.containsKey(\"ignore.error\")) {\n+      ignoreError = props.getProperty(\"ignore.error\").trim().toLowerCase().equals(\"true\");\n+    }\n+    reader = new BufferedReader(new InputStreamReader(inputStream, StandardCharsets.UTF_8));\n+    String url = props.getProperty(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG);\n+    if (url == null) {\n+      throw new ConfigException(\"Missing schema registry url!\");\n+    }\n+\n+    Map<String, Object> originals = getPropertiesMap(props);\n+\n+    schemaRegistry = new CachedSchemaRegistryClient(Collections.singletonList(url),\n+        AbstractKafkaSchemaSerDeConfig.MAX_SCHEMAS_PER_SUBJECT_DEFAULT,\n+        Collections.singletonList(new ProtobufSchemaProvider()),\n+        originals\n+    );\n+    if (!props.containsKey(\"value.schema\")) {\n+      throw new ConfigException(\"Must provide the Protobuf schema string in value.schema\");\n+    }\n+    String valueSchemaString = props.getProperty(\"value.schema\");\n+    valueSchema = new ProtobufSchema(valueSchemaString);\n+\n+    keySerializer = getKeySerializer(props);\n+\n+    if (needsKeySchema()) {\n+      if (!props.containsKey(\"key.schema\")) {\n+        throw new ConfigException(\"Must provide the Protobuf schema string in key.schema\");\n+      }\n+      String keySchemaString = props.getProperty(\"key.schema\");\n+      keySchema = new ProtobufSchema(keySchemaString);\n+    }\n+    keySubject = topic + \"-key\";\n+    valueSubject = topic + \"-value\";\n+    if (props.containsKey(\"auto.register\")) {\n+      this.autoRegisterSchema = Boolean.parseBoolean(props.getProperty(\"auto.register\").trim());\n+    } else {\n+      this.autoRegisterSchema = true;\n+    }\n+  }\n+\n+  private Serializer getKeySerializer(Properties props) throws ConfigException {", "originalCommit": "608dc6fe66d38a27af0ed343e53cde071a91dfcd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0ODU1NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370748555", "bodyText": "It's not strictly necessary to declare the exception; however, this is also in the AvroMessageReader so I think I'll leave it for now", "author": "rayokota", "createdAt": "2020-01-24T17:20:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5NDg5Mg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNDgxNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370424816", "bodyText": "v can be null here if value == null.", "author": "dragosvictor", "createdAt": "2020-01-24T00:29:20Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.errors.DataException;\n+import org.apache.kafka.connect.storage.Converter;\n+\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufDeserializer;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializerConfig;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerConfig;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+\n+/**\n+ * Implementation of Converter that uses Protobuf schemas and objects.\n+ */\n+public class ProtobufConverter implements Converter {\n+\n+  private SchemaRegistryClient schemaRegistry;\n+  private Serializer serializer;\n+  private Deserializer deserializer;\n+\n+  private boolean isKey;\n+  private ProtobufData protobufData;\n+\n+  public ProtobufConverter() {\n+  }\n+\n+  // Public only for testing\n+  public ProtobufConverter(SchemaRegistryClient client) {\n+    schemaRegistry = client;\n+  }\n+\n+  @Override\n+  public void configure(Map<String, ?> configs, boolean isKey) {\n+    this.isKey = isKey;\n+    ProtobufConverterConfig protobufConverterConfig = new ProtobufConverterConfig(configs);\n+\n+    if (schemaRegistry == null) {\n+      schemaRegistry =\n+          new CachedSchemaRegistryClient(protobufConverterConfig.getSchemaRegistryUrls(),\n+          protobufConverterConfig.getMaxSchemasPerSubject(),\n+          configs\n+      );\n+    }\n+\n+    serializer = new Serializer(configs, schemaRegistry);\n+    deserializer = new Deserializer(configs, schemaRegistry);\n+    protobufData = new ProtobufData(new ProtobufDataConfig(configs));\n+  }\n+\n+  @Override\n+  public byte[] fromConnectData(String topic, Schema schema, Object value) {\n+    try {\n+      ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n+      Object v = schemaAndValue.getValue();\n+      if (v instanceof Message) {\n+        return serializer.serialize(topic,\n+            isKey,\n+            (Message) v,\n+            schemaAndValue.getSchema()\n+        );\n+      } else {\n+        throw new DataException(\"Unsupported object of class \" + v.getClass().getName());", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0OTUzMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370749530", "bodyText": "Good catch :)", "author": "rayokota", "createdAt": "2020-01-24T17:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNDgxNg=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\nindex d74ed940c..ea3787b03 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n\n@@ -79,7 +79,9 @@ public class ProtobufConverter implements Converter {\n     try {\n       ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n       Object v = schemaAndValue.getValue();\n-      if (v instanceof Message) {\n+      if (v == null) {\n+        return null;\n+      } else if (v instanceof Message) {\n         return serializer.serialize(topic,\n             isKey,\n             (Message) v,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNTM2Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370425362", "bodyText": "Question: how do we decide in which case we use String.format and in which case we just 'add' them together via the '+' operator ?", "author": "dragosvictor", "createdAt": "2020-01-24T00:31:40Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.errors.DataException;\n+import org.apache.kafka.connect.storage.Converter;\n+\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufDeserializer;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializerConfig;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerConfig;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+\n+/**\n+ * Implementation of Converter that uses Protobuf schemas and objects.\n+ */\n+public class ProtobufConverter implements Converter {\n+\n+  private SchemaRegistryClient schemaRegistry;\n+  private Serializer serializer;\n+  private Deserializer deserializer;\n+\n+  private boolean isKey;\n+  private ProtobufData protobufData;\n+\n+  public ProtobufConverter() {\n+  }\n+\n+  // Public only for testing\n+  public ProtobufConverter(SchemaRegistryClient client) {\n+    schemaRegistry = client;\n+  }\n+\n+  @Override\n+  public void configure(Map<String, ?> configs, boolean isKey) {\n+    this.isKey = isKey;\n+    ProtobufConverterConfig protobufConverterConfig = new ProtobufConverterConfig(configs);\n+\n+    if (schemaRegistry == null) {\n+      schemaRegistry =\n+          new CachedSchemaRegistryClient(protobufConverterConfig.getSchemaRegistryUrls(),\n+          protobufConverterConfig.getMaxSchemasPerSubject(),\n+          configs\n+      );\n+    }\n+\n+    serializer = new Serializer(configs, schemaRegistry);\n+    deserializer = new Deserializer(configs, schemaRegistry);\n+    protobufData = new ProtobufData(new ProtobufDataConfig(configs));\n+  }\n+\n+  @Override\n+  public byte[] fromConnectData(String topic, Schema schema, Object value) {\n+    try {\n+      ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n+      Object v = schemaAndValue.getValue();\n+      if (v instanceof Message) {\n+        return serializer.serialize(topic,\n+            isKey,\n+            (Message) v,\n+            schemaAndValue.getSchema()\n+        );\n+      } else {\n+        throw new DataException(\"Unsupported object of class \" + v.getClass().getName());\n+      }\n+    } catch (SerializationException e) {\n+      throw new DataException(String.format(\n+          \"Failed to serialize Protobuf data from topic %s :\",\n+          topic\n+      ), e);\n+    }\n+  }\n+\n+  @Override\n+  public SchemaAndValue toConnectData(String topic, byte[] value) {\n+    try {\n+      ProtobufSchemaAndValue deserialized = deserializer.deserialize(topic, isKey, value);\n+\n+      if (deserialized == null || deserialized.getValue() == null) {\n+        return SchemaAndValue.NULL;\n+      } else {\n+        Object object = deserialized.getValue();\n+        if (object instanceof DynamicMessage) {\n+          DynamicMessage message = (DynamicMessage) object;\n+          return protobufData.toConnectData(deserialized.getSchema(), message);\n+        }\n+      }\n+      throw new DataException(String.format(\"Unsupported type returned during deserialization of \"\n+              + \"topic %s \",\n+          topic", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0OTgzMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370749833", "bodyText": "I'll remove the +, that was added by the IDE because the line was too long", "author": "rayokota", "createdAt": "2020-01-24T17:23:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNTM2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1MTIzNQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370751235", "bodyText": "My bad, what I meant was: how do I decide between these two ?\nthrow new Exception(\"Unsupported type for topic \" + topic);\nand\nthrow new Exception(String.format(\"Unsupported type for topic %s\", topic));", "author": "dragosvictor", "createdAt": "2020-01-24T17:26:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNTM2Mg=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\nindex d74ed940c..ea3787b03 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n\n@@ -79,7 +79,9 @@ public class ProtobufConverter implements Converter {\n     try {\n       ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n       Object v = schemaAndValue.getValue();\n-      if (v instanceof Message) {\n+      if (v == null) {\n+        return null;\n+      } else if (v instanceof Message) {\n         return serializer.serialize(topic,\n             isKey,\n             (Message) v,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNTUwOQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370425509", "bodyText": "Nit: extra line.", "author": "dragosvictor", "createdAt": "2020-01-24T00:32:11Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import org.apache.kafka.common.errors.SerializationException;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.errors.DataException;\n+import org.apache.kafka.connect.storage.Converter;\n+\n+import java.util.Map;\n+\n+import io.confluent.kafka.schemaregistry.client.CachedSchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.client.SchemaRegistryClient;\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufDeserializer;\n+import io.confluent.kafka.serializers.protobuf.AbstractKafkaProtobufSerializer;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializerConfig;\n+import io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializerConfig;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+\n+/**\n+ * Implementation of Converter that uses Protobuf schemas and objects.\n+ */\n+public class ProtobufConverter implements Converter {\n+\n+  private SchemaRegistryClient schemaRegistry;\n+  private Serializer serializer;\n+  private Deserializer deserializer;\n+\n+  private boolean isKey;\n+  private ProtobufData protobufData;\n+\n+  public ProtobufConverter() {\n+  }\n+\n+  // Public only for testing\n+  public ProtobufConverter(SchemaRegistryClient client) {\n+    schemaRegistry = client;\n+  }\n+\n+  @Override\n+  public void configure(Map<String, ?> configs, boolean isKey) {\n+    this.isKey = isKey;\n+    ProtobufConverterConfig protobufConverterConfig = new ProtobufConverterConfig(configs);\n+\n+    if (schemaRegistry == null) {\n+      schemaRegistry =\n+          new CachedSchemaRegistryClient(protobufConverterConfig.getSchemaRegistryUrls(),\n+          protobufConverterConfig.getMaxSchemasPerSubject(),\n+          configs\n+      );\n+    }\n+\n+    serializer = new Serializer(configs, schemaRegistry);\n+    deserializer = new Deserializer(configs, schemaRegistry);\n+    protobufData = new ProtobufData(new ProtobufDataConfig(configs));\n+  }\n+\n+  @Override\n+  public byte[] fromConnectData(String topic, Schema schema, Object value) {\n+    try {\n+      ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n+      Object v = schemaAndValue.getValue();\n+      if (v instanceof Message) {\n+        return serializer.serialize(topic,\n+            isKey,\n+            (Message) v,\n+            schemaAndValue.getSchema()\n+        );\n+      } else {\n+        throw new DataException(\"Unsupported object of class \" + v.getClass().getName());\n+      }\n+    } catch (SerializationException e) {\n+      throw new DataException(String.format(\n+          \"Failed to serialize Protobuf data from topic %s :\",\n+          topic\n+      ), e);\n+    }\n+  }\n+\n+  @Override\n+  public SchemaAndValue toConnectData(String topic, byte[] value) {\n+    try {\n+      ProtobufSchemaAndValue deserialized = deserializer.deserialize(topic, isKey, value);\n+\n+      if (deserialized == null || deserialized.getValue() == null) {\n+        return SchemaAndValue.NULL;\n+      } else {\n+        Object object = deserialized.getValue();\n+        if (object instanceof DynamicMessage) {\n+          DynamicMessage message = (DynamicMessage) object;\n+          return protobufData.toConnectData(deserialized.getSchema(), message);\n+        }\n+      }\n+      throw new DataException(String.format(\"Unsupported type returned during deserialization of \"\n+              + \"topic %s \",\n+          topic\n+      ));\n+    } catch (SerializationException e) {\n+      throw new DataException(String.format(\n+          \"Failed to deserialize data for topic %s to Protobuf: \",\n+          topic\n+      ), e);\n+    }\n+  }\n+\n+  private static class Serializer extends AbstractKafkaProtobufSerializer {\n+\n+    public Serializer(SchemaRegistryClient client, boolean autoRegisterSchema) {\n+      schemaRegistry = client;\n+      this.autoRegisterSchema = autoRegisterSchema;\n+    }\n+\n+    public Serializer(Map<String, ?> configs, SchemaRegistryClient client) {\n+", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc0OTk4MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370749980", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:23:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQyNTUwOQ=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\nindex d74ed940c..ea3787b03 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufConverter.java\n\n@@ -79,7 +79,9 @@ public class ProtobufConverter implements Converter {\n     try {\n       ProtobufSchemaAndValue schemaAndValue = protobufData.fromConnectData(schema, value);\n       Object v = schemaAndValue.getValue();\n-      if (v instanceof Message) {\n+      if (v == null) {\n+        return null;\n+      } else if (v instanceof Message) {\n         return serializer.serialize(topic,\n             isKey,\n             (Message) v,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2Nzc2OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370467769", "bodyText": "Why not a DataException like above ?", "author": "dragosvictor", "createdAt": "2020-01-24T04:15:24Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1MDIxOA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370750218", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:24:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2Nzc2OQ=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2ODAwMw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370468003", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-24T04:17:02Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+          } else {\n+            String scopedStructName = scope + getNameOrDefault(structName);\n+            DynamicMessage.Builder messageBuilder =\n+                protobufSchema.newMessageBuilder(scopedStructName);\n+            if (messageBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1MDI0Nw==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370750247", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:24:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2ODAwMw=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2ODA1OQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370468059", "bodyText": "Ditto.", "author": "dragosvictor", "createdAt": "2020-01-24T04:17:27Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+          } else {\n+            String scopedStructName = scope + getNameOrDefault(structName);\n+            DynamicMessage.Builder messageBuilder =\n+                protobufSchema.newMessageBuilder(scopedStructName);\n+            if (messageBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+            }\n+            for (Field field : schema.fields()) {\n+              Object fieldValue = fromConnectData(\n+                  field.schema(),\n+                  scopedStructName + \".\",\n+                  struct.get(field),\n+                  protobufSchema\n+              );\n+              if (fieldValue != null) {\n+                FieldDescriptor fieldDescriptor;\n+                if (fieldValue instanceof Pair) {\n+                  Pair<String, Object> union = (Pair<String, Object>) fieldValue;\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(union.getKey());\n+                  fieldValue = union.getValue();\n+                } else {\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(field.name());\n+                }\n+                if (fieldDescriptor == null) {\n+                  throw new IllegalArgumentException(\"Cannot find field with name \" + field.name());", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1MDI3NQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370750275", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:24:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQ2ODA1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczNDMwNg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370734306", "bodyText": "Can value be null here ?", "author": "dragosvictor", "createdAt": "2020-01-24T16:49:30Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+          } else {\n+            String scopedStructName = scope + getNameOrDefault(structName);\n+            DynamicMessage.Builder messageBuilder =\n+                protobufSchema.newMessageBuilder(scopedStructName);\n+            if (messageBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+            }\n+            for (Field field : schema.fields()) {\n+              Object fieldValue = fromConnectData(\n+                  field.schema(),\n+                  scopedStructName + \".\",\n+                  struct.get(field),\n+                  protobufSchema\n+              );\n+              if (fieldValue != null) {\n+                FieldDescriptor fieldDescriptor;\n+                if (fieldValue instanceof Pair) {\n+                  Pair<String, Object> union = (Pair<String, Object>) fieldValue;\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(union.getKey());\n+                  fieldValue = union.getValue();\n+                } else {\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(field.name());\n+                }\n+                if (fieldDescriptor == null) {\n+                  throw new IllegalArgumentException(\"Cannot find field with name \" + field.name());\n+                }\n+                messageBuilder.setField(fieldDescriptor, fieldValue);\n+              }\n+            }\n+            return messageBuilder.build();\n+          }\n+\n+        default:\n+          throw new DataException(\"Unknown schema type: \" + schema.type());\n+      }\n+    } catch (ClassCastException e) {\n+      throw new DataException(\"Invalid type for \" + schema.type() + \": \" + value.getClass());\n+    }\n+  }\n+\n+  private ProtobufSchema fromConnectSchema(Schema schema) {\n+    if (schema == null) {\n+      return null;\n+    }\n+    ProtobufSchema cachedSchema = fromConnectSchemaCache.get(schema);\n+    if (cachedSchema != null) {\n+      return cachedSchema;\n+    }\n+    String name = schema.name();\n+    if (name == null) {\n+      name = DEFAULT_SCHEMA_NAME + \"1\";\n+    }\n+    ProtobufSchema resultSchema =\n+        new ProtobufSchema(dynamicSchemaFromConnectSchema(schema).getMessageDescriptor(\n+        name));\n+    fromConnectSchemaCache.put(schema, resultSchema);\n+    return resultSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private DynamicSchema dynamicSchemaFromConnectSchema(Schema rootElem) {\n+    if (rootElem.type() != Schema.Type.STRUCT) {\n+      throw new IllegalArgumentException(\"Unsupported root schema of type \" + rootElem.type());\n+    }\n+    try {\n+      DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+      String name = getNameOrDefault(rootElem.name());\n+      schema.addMessageDefinition(messageDefinitionFromConnectSchema(schema, name, rootElem));\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private MessageDefinition messageDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema messageElem\n+  ) {\n+    MessageDefinition.Builder message = MessageDefinition.newBuilder(name);\n+    int index = 1;\n+    for (Field field : messageElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : index++;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          fieldSchema,\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        message.addField(fieldDef.getLabel(),\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+    return message.build();\n+  }\n+\n+  private void oneofDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema unionElem,\n+      String unionName\n+  ) {\n+    MessageDefinition.OneofBuilder oneof = message.addOneof(unionName);\n+    for (Field field : unionElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : 0;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          field.schema(),\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        oneof.addField(\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+  }\n+\n+  private FieldDefinition fieldDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema fieldSchema,\n+      String name,\n+      int tag\n+  ) {\n+    try {\n+      String label = fieldSchema.isOptional() ? \"optional\" : \"required\";\n+      if (fieldSchema.type() == Schema.Type.ARRAY) {\n+        label = \"repeated\";\n+        fieldSchema = fieldSchema.valueSchema();\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        label = \"repeated\";\n+      }\n+      String type = dataTypeFromConnectSchema(fieldSchema);\n+      if (fieldSchema.type() == Schema.Type.STRUCT) {\n+        String fieldSchemaName = fieldSchema.name();\n+        if (fieldSchemaName != null && fieldSchemaName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+          String unionName = fieldSchemaName.substring(PROTOBUF_TYPE_UNION_PREFIX.length());\n+          oneofDefinitionFromConnectSchema(schema, message, fieldSchema, unionName);\n+          return null;\n+        } else {\n+          message.addMessageDefinition(messageDefinitionFromConnectSchema(\n+              schema,\n+              type,\n+              fieldSchema\n+          ));\n+        }\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        message.addMessageDefinition(mapDefinitionFromConnectSchema(schema, type, fieldSchema));\n+      } else if (fieldSchema.parameters() != null && fieldSchema.parameters()\n+          .containsKey(PROTOBUF_TYPE_ENUM)) {\n+        message.addEnumDefinition(enumDefinitionFromConnectSchema(schema, fieldSchema));\n+      } else if (type.equals(GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME)) {\n+        DynamicSchema.Builder timestampSchema = DynamicSchema.newBuilder();\n+        timestampSchema.setName(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+        timestampSchema.setPackage(GOOGLE_PROTOBUF_PACKAGE);\n+        timestampSchema.addMessageDefinition(timestampDefinition());\n+        schema.addSchema(timestampSchema.build());\n+        schema.addDependency(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+      }\n+      Object defaultVal = fieldSchema.defaultValue();\n+      return new FieldDefinition(\n+          label,\n+          type,\n+          name,\n+          tag,\n+          defaultVal != null ? defaultVal.toString() : null\n+      );\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  static class FieldDefinition {\n+    private final String label;\n+    private final String type;\n+    private final String name;\n+    private final int num;\n+    private final String defaultVal;\n+\n+    public FieldDefinition(String label, String type, String name, int num, String defaultVal) {\n+      this.label = label;\n+      this.type = type;\n+      this.name = name;\n+      this.num = num;\n+      this.defaultVal = defaultVal;\n+    }\n+\n+    public String getType() {\n+      return type;\n+    }\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public int getNum() {\n+      return num;\n+    }\n+\n+    public String getDefaultVal() {\n+      return defaultVal;\n+    }\n+\n+    public String getLabel() {\n+      return label;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      FieldDefinition field = (FieldDefinition) o;\n+      return num == field.num && Objects.equals(label, field.label) && Objects.equals(\n+          type,\n+          field.type\n+      ) && Objects.equals(name, field.name) && Objects.equals(defaultVal, field.defaultVal);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(label, type, name, num, defaultVal);\n+    }\n+  }\n+\n+  private MessageDefinition mapDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema mapElem\n+  ) {\n+    MessageDefinition.Builder map = MessageDefinition.newBuilder(name);\n+    FieldDefinition key = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.keySchema(),\n+        KEY_FIELD,\n+        1\n+    );\n+    map.addField(key.getLabel(), key.getType(), key.getName(), key.getNum(), key.getDefaultVal());\n+    FieldDefinition val = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.valueSchema(),\n+        VALUE_FIELD,\n+        2\n+    );\n+    map.addField(val.getLabel(), val.getType(), val.getName(), val.getNum(), val.getDefaultVal());\n+    return map.build();\n+  }\n+\n+  private EnumDefinition enumDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      Schema enumElem\n+  ) {\n+    EnumDefinition.Builder enumer = EnumDefinition.newBuilder(enumElem.name());\n+    for (Map.Entry<String, String> entry : enumElem.parameters().entrySet()) {\n+      if (entry.getKey().startsWith(PROTOBUF_TYPE_ENUM_PREFIX)) {\n+        String name = entry.getKey().substring(PROTOBUF_TYPE_ENUM_PREFIX.length());\n+        int tag = Integer.parseInt(entry.getValue());\n+        enumer.addValue(name, tag);\n+      }\n+    }\n+    return enumer.build();\n+  }\n+\n+  private String dataTypeFromConnectSchema(Schema schema) {\n+    switch (schema.type()) {\n+      case INT8:\n+      case INT16:\n+      case INT32:\n+        return FieldDescriptor.Type.INT32.toString().toLowerCase();\n+      case INT64:\n+        if (isProtobufTimestamp(schema)) {\n+          return GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME;\n+        }\n+        return FieldDescriptor.Type.INT64.toString().toLowerCase();\n+      case FLOAT32:\n+        return FieldDescriptor.Type.FLOAT.toString().toLowerCase();\n+      case FLOAT64:\n+        return FieldDescriptor.Type.DOUBLE.toString().toLowerCase();\n+      case BOOLEAN:\n+        return FieldDescriptor.Type.BOOL.toString().toLowerCase();\n+      case STRING:\n+        return FieldDescriptor.Type.STRING.toString().toLowerCase();\n+      case BYTES:\n+        return FieldDescriptor.Type.BYTES.toString().toLowerCase();\n+      case ARRAY:\n+        // Array should not occur here\n+        throw new IllegalArgumentException(\"Array cannot be nested\");\n+      case MAP:\n+        return ProtobufSchema.toMapEntry(getNameOrDefault(schema.name()));\n+      case STRUCT:\n+        return getNameOrDefault(schema.name());\n+      default:\n+        throw new DataException(\"Unknown schema type: \" + schema.type());\n+    }\n+  }\n+\n+  private boolean isProtobufTimestamp(Schema schema) {\n+    return Timestamp.SCHEMA.name().equals(schema.name());\n+  }\n+\n+  public SchemaAndValue toConnectData(ProtobufSchema protobufSchema, DynamicMessage message) {\n+    if (message == null) {\n+      return SchemaAndValue.NULL;\n+    }\n+\n+    Schema schema = toConnectSchema(protobufSchema);\n+\n+    return new SchemaAndValue(schema, toConnectData(schema, message));\n+  }\n+\n+  // Visible for testing\n+  @SuppressWarnings(\"unchecked\")\n+  protected Object toConnectData(Schema schema, Object value) {\n+    try {\n+      if (isProtobufTimestamp(schema)) {\n+        DynamicMessage message = (DynamicMessage) value;\n+\n+        long seconds = 0L;\n+        int nanos = 0;\n+        for (Map.Entry<FieldDescriptor, Object> entry : message.getAllFields().entrySet()) {\n+          if (entry.getKey().getName().equals(\"seconds\")) {\n+            seconds = ((Number) entry.getValue()).longValue();\n+          } else if (entry.getKey().getName().equals(\"nanos\")) {\n+            nanos = ((Number) entry.getValue()).intValue();\n+          }\n+        }\n+        com.google.protobuf.Timestamp timestamp = com.google.protobuf.Timestamp.newBuilder()\n+            .setSeconds(seconds)\n+            .setNanos(nanos)\n+            .build();\n+        return Timestamp.toLogical(schema, Timestamps.toMillis(timestamp));\n+      }\n+\n+      Object converted = null;\n+      switch (schema.type()) {\n+        case INT8:\n+        case INT16:\n+        case INT32:\n+          if (value instanceof Number) {\n+            converted = ((Number) value).intValue(); // Validate type\n+          } else if (value instanceof Enum) {\n+            converted = ((Enum) value).ordinal();\n+          } else if (value instanceof EnumValueDescriptor) {\n+            converted = ((EnumValueDescriptor) value).getNumber();\n+          }\n+          break;\n+        case INT64:\n+          long longValue;\n+          if (value instanceof Long) {\n+            longValue = (Long) value;\n+          } else {\n+            longValue = Integer.toUnsignedLong(((Number) value).intValue());\n+          }\n+          converted = longValue;\n+          break;\n+        case FLOAT32:\n+          float floatValue = ((Number) value).floatValue(); // Validate type\n+          converted = value;\n+          break;\n+        case FLOAT64:\n+          double doubleValue = ((Number) value).doubleValue(); // Validate type\n+          converted = value;\n+          break;\n+        case BOOLEAN:\n+          Boolean boolValue = (Boolean) value; // Validate type\n+          converted = value;\n+          break;\n+        case STRING:\n+          if (value instanceof String) {\n+            converted = value;\n+          } else if (value instanceof CharSequence) {\n+            converted = value.toString();\n+          } else {\n+            throw new DataException(\"Invalid class for string type, expecting String or \"\n+                + \"CharSequence but found \"\n+                + value.getClass());", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1NTM1MQ==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370755351", "bodyText": "Good catch :)", "author": "rayokota", "createdAt": "2020-01-24T17:36:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczNDMwNg=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczNDc2Ng==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370734766", "bodyText": "Can value be null here ?\nNit: ByteString is also accepted.", "author": "dragosvictor", "createdAt": "2020-01-24T16:50:22Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+          } else {\n+            String scopedStructName = scope + getNameOrDefault(structName);\n+            DynamicMessage.Builder messageBuilder =\n+                protobufSchema.newMessageBuilder(scopedStructName);\n+            if (messageBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+            }\n+            for (Field field : schema.fields()) {\n+              Object fieldValue = fromConnectData(\n+                  field.schema(),\n+                  scopedStructName + \".\",\n+                  struct.get(field),\n+                  protobufSchema\n+              );\n+              if (fieldValue != null) {\n+                FieldDescriptor fieldDescriptor;\n+                if (fieldValue instanceof Pair) {\n+                  Pair<String, Object> union = (Pair<String, Object>) fieldValue;\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(union.getKey());\n+                  fieldValue = union.getValue();\n+                } else {\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(field.name());\n+                }\n+                if (fieldDescriptor == null) {\n+                  throw new IllegalArgumentException(\"Cannot find field with name \" + field.name());\n+                }\n+                messageBuilder.setField(fieldDescriptor, fieldValue);\n+              }\n+            }\n+            return messageBuilder.build();\n+          }\n+\n+        default:\n+          throw new DataException(\"Unknown schema type: \" + schema.type());\n+      }\n+    } catch (ClassCastException e) {\n+      throw new DataException(\"Invalid type for \" + schema.type() + \": \" + value.getClass());\n+    }\n+  }\n+\n+  private ProtobufSchema fromConnectSchema(Schema schema) {\n+    if (schema == null) {\n+      return null;\n+    }\n+    ProtobufSchema cachedSchema = fromConnectSchemaCache.get(schema);\n+    if (cachedSchema != null) {\n+      return cachedSchema;\n+    }\n+    String name = schema.name();\n+    if (name == null) {\n+      name = DEFAULT_SCHEMA_NAME + \"1\";\n+    }\n+    ProtobufSchema resultSchema =\n+        new ProtobufSchema(dynamicSchemaFromConnectSchema(schema).getMessageDescriptor(\n+        name));\n+    fromConnectSchemaCache.put(schema, resultSchema);\n+    return resultSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private DynamicSchema dynamicSchemaFromConnectSchema(Schema rootElem) {\n+    if (rootElem.type() != Schema.Type.STRUCT) {\n+      throw new IllegalArgumentException(\"Unsupported root schema of type \" + rootElem.type());\n+    }\n+    try {\n+      DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+      String name = getNameOrDefault(rootElem.name());\n+      schema.addMessageDefinition(messageDefinitionFromConnectSchema(schema, name, rootElem));\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private MessageDefinition messageDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema messageElem\n+  ) {\n+    MessageDefinition.Builder message = MessageDefinition.newBuilder(name);\n+    int index = 1;\n+    for (Field field : messageElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : index++;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          fieldSchema,\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        message.addField(fieldDef.getLabel(),\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+    return message.build();\n+  }\n+\n+  private void oneofDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema unionElem,\n+      String unionName\n+  ) {\n+    MessageDefinition.OneofBuilder oneof = message.addOneof(unionName);\n+    for (Field field : unionElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : 0;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          field.schema(),\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        oneof.addField(\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+  }\n+\n+  private FieldDefinition fieldDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema fieldSchema,\n+      String name,\n+      int tag\n+  ) {\n+    try {\n+      String label = fieldSchema.isOptional() ? \"optional\" : \"required\";\n+      if (fieldSchema.type() == Schema.Type.ARRAY) {\n+        label = \"repeated\";\n+        fieldSchema = fieldSchema.valueSchema();\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        label = \"repeated\";\n+      }\n+      String type = dataTypeFromConnectSchema(fieldSchema);\n+      if (fieldSchema.type() == Schema.Type.STRUCT) {\n+        String fieldSchemaName = fieldSchema.name();\n+        if (fieldSchemaName != null && fieldSchemaName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+          String unionName = fieldSchemaName.substring(PROTOBUF_TYPE_UNION_PREFIX.length());\n+          oneofDefinitionFromConnectSchema(schema, message, fieldSchema, unionName);\n+          return null;\n+        } else {\n+          message.addMessageDefinition(messageDefinitionFromConnectSchema(\n+              schema,\n+              type,\n+              fieldSchema\n+          ));\n+        }\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        message.addMessageDefinition(mapDefinitionFromConnectSchema(schema, type, fieldSchema));\n+      } else if (fieldSchema.parameters() != null && fieldSchema.parameters()\n+          .containsKey(PROTOBUF_TYPE_ENUM)) {\n+        message.addEnumDefinition(enumDefinitionFromConnectSchema(schema, fieldSchema));\n+      } else if (type.equals(GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME)) {\n+        DynamicSchema.Builder timestampSchema = DynamicSchema.newBuilder();\n+        timestampSchema.setName(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+        timestampSchema.setPackage(GOOGLE_PROTOBUF_PACKAGE);\n+        timestampSchema.addMessageDefinition(timestampDefinition());\n+        schema.addSchema(timestampSchema.build());\n+        schema.addDependency(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+      }\n+      Object defaultVal = fieldSchema.defaultValue();\n+      return new FieldDefinition(\n+          label,\n+          type,\n+          name,\n+          tag,\n+          defaultVal != null ? defaultVal.toString() : null\n+      );\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  static class FieldDefinition {\n+    private final String label;\n+    private final String type;\n+    private final String name;\n+    private final int num;\n+    private final String defaultVal;\n+\n+    public FieldDefinition(String label, String type, String name, int num, String defaultVal) {\n+      this.label = label;\n+      this.type = type;\n+      this.name = name;\n+      this.num = num;\n+      this.defaultVal = defaultVal;\n+    }\n+\n+    public String getType() {\n+      return type;\n+    }\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public int getNum() {\n+      return num;\n+    }\n+\n+    public String getDefaultVal() {\n+      return defaultVal;\n+    }\n+\n+    public String getLabel() {\n+      return label;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      FieldDefinition field = (FieldDefinition) o;\n+      return num == field.num && Objects.equals(label, field.label) && Objects.equals(\n+          type,\n+          field.type\n+      ) && Objects.equals(name, field.name) && Objects.equals(defaultVal, field.defaultVal);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(label, type, name, num, defaultVal);\n+    }\n+  }\n+\n+  private MessageDefinition mapDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema mapElem\n+  ) {\n+    MessageDefinition.Builder map = MessageDefinition.newBuilder(name);\n+    FieldDefinition key = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.keySchema(),\n+        KEY_FIELD,\n+        1\n+    );\n+    map.addField(key.getLabel(), key.getType(), key.getName(), key.getNum(), key.getDefaultVal());\n+    FieldDefinition val = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.valueSchema(),\n+        VALUE_FIELD,\n+        2\n+    );\n+    map.addField(val.getLabel(), val.getType(), val.getName(), val.getNum(), val.getDefaultVal());\n+    return map.build();\n+  }\n+\n+  private EnumDefinition enumDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      Schema enumElem\n+  ) {\n+    EnumDefinition.Builder enumer = EnumDefinition.newBuilder(enumElem.name());\n+    for (Map.Entry<String, String> entry : enumElem.parameters().entrySet()) {\n+      if (entry.getKey().startsWith(PROTOBUF_TYPE_ENUM_PREFIX)) {\n+        String name = entry.getKey().substring(PROTOBUF_TYPE_ENUM_PREFIX.length());\n+        int tag = Integer.parseInt(entry.getValue());\n+        enumer.addValue(name, tag);\n+      }\n+    }\n+    return enumer.build();\n+  }\n+\n+  private String dataTypeFromConnectSchema(Schema schema) {\n+    switch (schema.type()) {\n+      case INT8:\n+      case INT16:\n+      case INT32:\n+        return FieldDescriptor.Type.INT32.toString().toLowerCase();\n+      case INT64:\n+        if (isProtobufTimestamp(schema)) {\n+          return GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME;\n+        }\n+        return FieldDescriptor.Type.INT64.toString().toLowerCase();\n+      case FLOAT32:\n+        return FieldDescriptor.Type.FLOAT.toString().toLowerCase();\n+      case FLOAT64:\n+        return FieldDescriptor.Type.DOUBLE.toString().toLowerCase();\n+      case BOOLEAN:\n+        return FieldDescriptor.Type.BOOL.toString().toLowerCase();\n+      case STRING:\n+        return FieldDescriptor.Type.STRING.toString().toLowerCase();\n+      case BYTES:\n+        return FieldDescriptor.Type.BYTES.toString().toLowerCase();\n+      case ARRAY:\n+        // Array should not occur here\n+        throw new IllegalArgumentException(\"Array cannot be nested\");\n+      case MAP:\n+        return ProtobufSchema.toMapEntry(getNameOrDefault(schema.name()));\n+      case STRUCT:\n+        return getNameOrDefault(schema.name());\n+      default:\n+        throw new DataException(\"Unknown schema type: \" + schema.type());\n+    }\n+  }\n+\n+  private boolean isProtobufTimestamp(Schema schema) {\n+    return Timestamp.SCHEMA.name().equals(schema.name());\n+  }\n+\n+  public SchemaAndValue toConnectData(ProtobufSchema protobufSchema, DynamicMessage message) {\n+    if (message == null) {\n+      return SchemaAndValue.NULL;\n+    }\n+\n+    Schema schema = toConnectSchema(protobufSchema);\n+\n+    return new SchemaAndValue(schema, toConnectData(schema, message));\n+  }\n+\n+  // Visible for testing\n+  @SuppressWarnings(\"unchecked\")\n+  protected Object toConnectData(Schema schema, Object value) {\n+    try {\n+      if (isProtobufTimestamp(schema)) {\n+        DynamicMessage message = (DynamicMessage) value;\n+\n+        long seconds = 0L;\n+        int nanos = 0;\n+        for (Map.Entry<FieldDescriptor, Object> entry : message.getAllFields().entrySet()) {\n+          if (entry.getKey().getName().equals(\"seconds\")) {\n+            seconds = ((Number) entry.getValue()).longValue();\n+          } else if (entry.getKey().getName().equals(\"nanos\")) {\n+            nanos = ((Number) entry.getValue()).intValue();\n+          }\n+        }\n+        com.google.protobuf.Timestamp timestamp = com.google.protobuf.Timestamp.newBuilder()\n+            .setSeconds(seconds)\n+            .setNanos(nanos)\n+            .build();\n+        return Timestamp.toLogical(schema, Timestamps.toMillis(timestamp));\n+      }\n+\n+      Object converted = null;\n+      switch (schema.type()) {\n+        case INT8:\n+        case INT16:\n+        case INT32:\n+          if (value instanceof Number) {\n+            converted = ((Number) value).intValue(); // Validate type\n+          } else if (value instanceof Enum) {\n+            converted = ((Enum) value).ordinal();\n+          } else if (value instanceof EnumValueDescriptor) {\n+            converted = ((EnumValueDescriptor) value).getNumber();\n+          }\n+          break;\n+        case INT64:\n+          long longValue;\n+          if (value instanceof Long) {\n+            longValue = (Long) value;\n+          } else {\n+            longValue = Integer.toUnsignedLong(((Number) value).intValue());\n+          }\n+          converted = longValue;\n+          break;\n+        case FLOAT32:\n+          float floatValue = ((Number) value).floatValue(); // Validate type\n+          converted = value;\n+          break;\n+        case FLOAT64:\n+          double doubleValue = ((Number) value).doubleValue(); // Validate type\n+          converted = value;\n+          break;\n+        case BOOLEAN:\n+          Boolean boolValue = (Boolean) value; // Validate type\n+          converted = value;\n+          break;\n+        case STRING:\n+          if (value instanceof String) {\n+            converted = value;\n+          } else if (value instanceof CharSequence) {\n+            converted = value.toString();\n+          } else {\n+            throw new DataException(\"Invalid class for string type, expecting String or \"\n+                + \"CharSequence but found \"\n+                + value.getClass());\n+          }\n+          break;\n+        case BYTES:\n+          if (value instanceof byte[]) {\n+            converted = ByteBuffer.wrap((byte[]) value);\n+          } else if (value instanceof ByteBuffer) {\n+            converted = value;\n+          } else if (value instanceof ByteString) {\n+            converted = ((ByteString) value).asReadOnlyByteBuffer();\n+          } else {\n+            throw new DataException(\"Invalid class for bytes type, expecting byte[] or ByteBuffer \"\n+                + \"but found \"\n+                + value.getClass());", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1NTcxMA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370755710", "bodyText": "Ack", "author": "rayokota", "createdAt": "2020-01-24T17:37:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczNDc2Ng=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczODA5Mg==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370738092", "bodyText": "Question: can this be accessed concurrently ? If so, we should switch to an atomic type for defaultSchemeNameIndex.", "author": "dragosvictor", "createdAt": "2020-01-24T16:57:03Z", "path": "protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java", "diffHunk": "@@ -0,0 +1,932 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ */\n+\n+package io.confluent.connect.protobuf;\n+\n+import com.google.protobuf.ByteString;\n+import com.google.protobuf.Descriptors;\n+import com.google.protobuf.Descriptors.Descriptor;\n+import com.google.protobuf.Descriptors.EnumDescriptor;\n+import com.google.protobuf.Descriptors.EnumValueDescriptor;\n+import com.google.protobuf.Descriptors.FieldDescriptor;\n+import com.google.protobuf.Descriptors.OneofDescriptor;\n+import com.google.protobuf.DynamicMessage;\n+import com.google.protobuf.Message;\n+import com.google.protobuf.util.Timestamps;\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.kafka.common.cache.Cache;\n+import org.apache.kafka.common.cache.LRUCache;\n+import org.apache.kafka.common.cache.SynchronizedCache;\n+import org.apache.kafka.connect.data.Field;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.data.Timestamp;\n+import org.apache.kafka.connect.errors.DataException;\n+\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+import io.confluent.kafka.schemaregistry.protobuf.ProtobufSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.DynamicSchema;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.EnumDefinition;\n+import io.confluent.kafka.schemaregistry.protobuf.dynamic.MessageDefinition;\n+import io.confluent.kafka.serializers.protobuf.ProtobufSchemaAndValue;\n+\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_CONFIG;\n+import static io.confluent.connect.protobuf.ProtobufDataConfig.SCHEMAS_CACHE_SIZE_DEFAULT;\n+\n+\n+public class ProtobufData {\n+\n+  public static final String NAMESPACE = \"io.confluent.connect.protobuf\";\n+\n+  public static final String DEFAULT_SCHEMA_NAME = \"ConnectDefault\";\n+  public static final String MAP_ENTRY_SUFFIX = ProtobufSchema.MAP_ENTRY_SUFFIX;  // Suffix used\n+  // by protoc\n+  public static final String KEY_FIELD = ProtobufSchema.KEY_FIELD;\n+  public static final String VALUE_FIELD = ProtobufSchema.VALUE_FIELD;\n+\n+  public static final String PROTOBUF_TYPE_ENUM = NAMESPACE + \".Enum\";\n+  public static final String PROTOBUF_TYPE_ENUM_PREFIX = PROTOBUF_TYPE_ENUM + \".\";\n+  public static final String PROTOBUF_TYPE_UNION = NAMESPACE + \".Union\";\n+  public static final String PROTOBUF_TYPE_UNION_PREFIX = PROTOBUF_TYPE_UNION + \".\";\n+  public static final String PROTOBUF_TYPE_TAG = NAMESPACE + \".Tag\";\n+\n+  public static final String GOOGLE_PROTOBUF_PACKAGE = \"google.protobuf\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_NAME = \"Timestamp\";\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME = GOOGLE_PROTOBUF_PACKAGE\n+      + \".\"\n+      + GOOGLE_PROTOBUF_TIMESTAMP_NAME;\n+  public static final String GOOGLE_PROTOBUF_TIMESTAMP_LOCATION = \"google/protobuf/timestamp.proto\";\n+\n+  private int defaultSchemaNameIndex = 0;\n+\n+  private final Cache<Schema, ProtobufSchema> fromConnectSchemaCache;\n+  private final Cache<ProtobufSchema, Schema> toConnectSchemaCache;\n+\n+  public ProtobufData() {\n+    this(new ProtobufDataConfig.Builder().with(\n+        SCHEMAS_CACHE_SIZE_CONFIG,\n+        SCHEMAS_CACHE_SIZE_DEFAULT\n+    ).build());\n+  }\n+\n+  public ProtobufData(int cacheSize) {\n+    this(new ProtobufDataConfig.Builder().with(SCHEMAS_CACHE_SIZE_CONFIG, cacheSize).build());\n+  }\n+\n+  public ProtobufData(ProtobufDataConfig protobufDataConfig) {\n+    fromConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+    toConnectSchemaCache =\n+        new SynchronizedCache<>(new LRUCache<>(protobufDataConfig.getSchemasCacheSize()));\n+  }\n+\n+  /**\n+   * Convert this object, in Connect data format, into an Protobuf object.\n+   */\n+  public ProtobufSchemaAndValue fromConnectData(Schema schema, Object value) {\n+    ProtobufSchema protobufSchema = fromConnectSchema(schema);\n+    // Reset the default schema name index.\n+    // Note: the assumption is that default names will be generated in an order in both the\n+    // schema and data\n+    // such that the names correspond to each other in the schema and data.\n+    // TODO try to break this assumption\n+    defaultSchemaNameIndex = 0;\n+    return new ProtobufSchemaAndValue(\n+        protobufSchema,\n+        fromConnectData(schema, \"\", value, protobufSchema)\n+    );\n+  }\n+\n+  // Visible for testing\n+  protected ProtobufSchemaAndValue fromConnectData(SchemaAndValue schemaAndValue) {\n+    return fromConnectData(schemaAndValue.schema(), schemaAndValue.value());\n+  }\n+\n+  private Object fromConnectData(\n+      Schema schema,\n+      String scope,\n+      Object value,\n+      ProtobufSchema protobufSchema\n+  ) {\n+    if (value == null) {\n+      // Ignore missing values\n+      return null;\n+    }\n+\n+    final Schema.Type schemaType = schema.type();\n+    try {\n+      switch (schemaType) {\n+        case INT8:\n+        case INT16:\n+        case INT32: {\n+          final int intValue = ((Number) value).intValue(); // Check for correct type\n+          if (intValue == 0) {\n+            return null;\n+          }\n+          return intValue;\n+        }\n+\n+        case INT64: {\n+          if (isProtobufTimestamp(schema)) {\n+            final java.util.Date timestamp = (java.util.Date) value;\n+            return Timestamps.fromMillis(Timestamp.fromLogical(schema, timestamp));\n+          }\n+\n+          final long longValue = ((Number) value).longValue(); // Check for correct type\n+          if (longValue == 0L) {\n+            return null;\n+          }\n+          return longValue;\n+        }\n+\n+        case FLOAT32: {\n+          final float floatValue = ((Number) value).floatValue(); // Check for correct type\n+          if (floatValue == 0.0f) {\n+            return null;\n+          }\n+          return floatValue;\n+        }\n+\n+        case FLOAT64: {\n+          final double doubleValue = ((Number) value).doubleValue(); // Check for correct type\n+          if (doubleValue == 0.0d) {\n+            return null;\n+          }\n+          return doubleValue;\n+        }\n+\n+        case BOOLEAN: {\n+          final Boolean boolValue = (Boolean) value; // Check for correct type\n+          if (boolValue == false) {\n+            return null;\n+          }\n+          return boolValue;\n+        }\n+\n+        case STRING: {\n+          final String stringValue = (String) value; // Check for correct type\n+          if (stringValue.isEmpty()) {\n+            return null;\n+          }\n+          return stringValue;\n+        }\n+\n+        case BYTES: {\n+          final ByteBuffer bytesValue = value instanceof byte[]\n+                                        ? ByteBuffer.wrap((byte[]) value)\n+                                        : (ByteBuffer) value;\n+          if (bytesValue.array().length == 0) {\n+            return null;\n+          }\n+          return ByteString.copyFrom(bytesValue);\n+        }\n+        case ARRAY:\n+          final Collection<?> listValue = (Collection<?>) value;\n+          List<Object> newListValue = new ArrayList<>();\n+          for (Object o : listValue) {\n+            newListValue.add(fromConnectData(schema.valueSchema(), scope, o, protobufSchema));\n+          }\n+          return newListValue;\n+        case MAP:\n+          final Map<?, ?> mapValue = (Map<?, ?>) value;\n+          String mapName = getNameOrDefault(schema.name());\n+          String scopedMapName = scope + ProtobufSchema.toMapEntry(mapName);\n+          List<Message> newMapValue = new ArrayList<>();\n+          for (Map.Entry<?, ?> mapEntry : mapValue.entrySet()) {\n+            DynamicMessage.Builder mapBuilder = protobufSchema.newMessageBuilder(scopedMapName);\n+            if (mapBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedMapName);\n+            }\n+            Descriptor mapDescriptor = mapBuilder.getDescriptorForType();\n+            final FieldDescriptor keyDescriptor = mapDescriptor.findFieldByName(KEY_FIELD);\n+            final FieldDescriptor valueDescriptor = mapDescriptor.findFieldByName(VALUE_FIELD);\n+            Object entryKey = fromConnectData(\n+                schema.keySchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getKey(),\n+                protobufSchema\n+            );\n+            Object entryValue = fromConnectData(\n+                schema.valueSchema(),\n+                scopedMapName + \".\",\n+                mapEntry.getValue(),\n+                protobufSchema\n+            );\n+            mapBuilder.setField(keyDescriptor, entryKey);\n+            mapBuilder.setField(valueDescriptor, entryValue);\n+            newMapValue.add(mapBuilder.build());\n+          }\n+          return newMapValue;\n+        case STRUCT:\n+          final Struct struct = (Struct) value;\n+          if (!struct.schema().equals(schema)) {\n+            throw new DataException(\"Mismatching struct schema\");\n+          }\n+          String structName = schema.name();\n+          //This handles the inverting of a union which is held as a struct, where each field is\n+          // one of the union types.\n+          if (structName != null && structName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+            for (Field field : schema.fields()) {\n+              Object object = struct.get(field);\n+              if (object != null) {\n+                return Pair.of(field.name(),\n+                    fromConnectData(field.schema(), scope, object, protobufSchema)\n+                );\n+              }\n+            }\n+            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+          } else {\n+            String scopedStructName = scope + getNameOrDefault(structName);\n+            DynamicMessage.Builder messageBuilder =\n+                protobufSchema.newMessageBuilder(scopedStructName);\n+            if (messageBuilder == null) {\n+              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+            }\n+            for (Field field : schema.fields()) {\n+              Object fieldValue = fromConnectData(\n+                  field.schema(),\n+                  scopedStructName + \".\",\n+                  struct.get(field),\n+                  protobufSchema\n+              );\n+              if (fieldValue != null) {\n+                FieldDescriptor fieldDescriptor;\n+                if (fieldValue instanceof Pair) {\n+                  Pair<String, Object> union = (Pair<String, Object>) fieldValue;\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(union.getKey());\n+                  fieldValue = union.getValue();\n+                } else {\n+                  fieldDescriptor = messageBuilder.getDescriptorForType()\n+                      .findFieldByName(field.name());\n+                }\n+                if (fieldDescriptor == null) {\n+                  throw new IllegalArgumentException(\"Cannot find field with name \" + field.name());\n+                }\n+                messageBuilder.setField(fieldDescriptor, fieldValue);\n+              }\n+            }\n+            return messageBuilder.build();\n+          }\n+\n+        default:\n+          throw new DataException(\"Unknown schema type: \" + schema.type());\n+      }\n+    } catch (ClassCastException e) {\n+      throw new DataException(\"Invalid type for \" + schema.type() + \": \" + value.getClass());\n+    }\n+  }\n+\n+  private ProtobufSchema fromConnectSchema(Schema schema) {\n+    if (schema == null) {\n+      return null;\n+    }\n+    ProtobufSchema cachedSchema = fromConnectSchemaCache.get(schema);\n+    if (cachedSchema != null) {\n+      return cachedSchema;\n+    }\n+    String name = schema.name();\n+    if (name == null) {\n+      name = DEFAULT_SCHEMA_NAME + \"1\";\n+    }\n+    ProtobufSchema resultSchema =\n+        new ProtobufSchema(dynamicSchemaFromConnectSchema(schema).getMessageDescriptor(\n+        name));\n+    fromConnectSchemaCache.put(schema, resultSchema);\n+    return resultSchema;\n+  }\n+\n+  /*\n+   * DynamicSchema is used as a temporary helper class and should not be exposed in the API.\n+   */\n+  private DynamicSchema dynamicSchemaFromConnectSchema(Schema rootElem) {\n+    if (rootElem.type() != Schema.Type.STRUCT) {\n+      throw new IllegalArgumentException(\"Unsupported root schema of type \" + rootElem.type());\n+    }\n+    try {\n+      DynamicSchema.Builder schema = DynamicSchema.newBuilder();\n+      String name = getNameOrDefault(rootElem.name());\n+      schema.addMessageDefinition(messageDefinitionFromConnectSchema(schema, name, rootElem));\n+      return schema.build();\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  private MessageDefinition messageDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema messageElem\n+  ) {\n+    MessageDefinition.Builder message = MessageDefinition.newBuilder(name);\n+    int index = 1;\n+    for (Field field : messageElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : index++;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          fieldSchema,\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        message.addField(fieldDef.getLabel(),\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+    return message.build();\n+  }\n+\n+  private void oneofDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema unionElem,\n+      String unionName\n+  ) {\n+    MessageDefinition.OneofBuilder oneof = message.addOneof(unionName);\n+    for (Field field : unionElem.fields()) {\n+      Schema fieldSchema = field.schema();\n+      String fieldTag = fieldSchema.parameters() != null ? fieldSchema.parameters()\n+          .get(PROTOBUF_TYPE_TAG) : null;\n+      int tag = fieldTag != null ? Integer.parseInt(fieldTag) : 0;\n+      FieldDefinition fieldDef = fieldDefinitionFromConnectSchema(\n+          schema,\n+          message,\n+          field.schema(),\n+          field.name(),\n+          tag\n+      );\n+      if (fieldDef != null) {\n+        oneof.addField(\n+            fieldDef.getType(),\n+            fieldDef.getName(),\n+            fieldDef.getNum(),\n+            fieldDef.getDefaultVal()\n+        );\n+      }\n+    }\n+  }\n+\n+  private FieldDefinition fieldDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      MessageDefinition.Builder message,\n+      Schema fieldSchema,\n+      String name,\n+      int tag\n+  ) {\n+    try {\n+      String label = fieldSchema.isOptional() ? \"optional\" : \"required\";\n+      if (fieldSchema.type() == Schema.Type.ARRAY) {\n+        label = \"repeated\";\n+        fieldSchema = fieldSchema.valueSchema();\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        label = \"repeated\";\n+      }\n+      String type = dataTypeFromConnectSchema(fieldSchema);\n+      if (fieldSchema.type() == Schema.Type.STRUCT) {\n+        String fieldSchemaName = fieldSchema.name();\n+        if (fieldSchemaName != null && fieldSchemaName.startsWith(PROTOBUF_TYPE_UNION_PREFIX)) {\n+          String unionName = fieldSchemaName.substring(PROTOBUF_TYPE_UNION_PREFIX.length());\n+          oneofDefinitionFromConnectSchema(schema, message, fieldSchema, unionName);\n+          return null;\n+        } else {\n+          message.addMessageDefinition(messageDefinitionFromConnectSchema(\n+              schema,\n+              type,\n+              fieldSchema\n+          ));\n+        }\n+      } else if (fieldSchema.type() == Schema.Type.MAP) {\n+        message.addMessageDefinition(mapDefinitionFromConnectSchema(schema, type, fieldSchema));\n+      } else if (fieldSchema.parameters() != null && fieldSchema.parameters()\n+          .containsKey(PROTOBUF_TYPE_ENUM)) {\n+        message.addEnumDefinition(enumDefinitionFromConnectSchema(schema, fieldSchema));\n+      } else if (type.equals(GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME)) {\n+        DynamicSchema.Builder timestampSchema = DynamicSchema.newBuilder();\n+        timestampSchema.setName(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+        timestampSchema.setPackage(GOOGLE_PROTOBUF_PACKAGE);\n+        timestampSchema.addMessageDefinition(timestampDefinition());\n+        schema.addSchema(timestampSchema.build());\n+        schema.addDependency(GOOGLE_PROTOBUF_TIMESTAMP_LOCATION);\n+      }\n+      Object defaultVal = fieldSchema.defaultValue();\n+      return new FieldDefinition(\n+          label,\n+          type,\n+          name,\n+          tag,\n+          defaultVal != null ? defaultVal.toString() : null\n+      );\n+    } catch (Descriptors.DescriptorValidationException e) {\n+      throw new IllegalStateException(e);\n+    }\n+  }\n+\n+  static class FieldDefinition {\n+    private final String label;\n+    private final String type;\n+    private final String name;\n+    private final int num;\n+    private final String defaultVal;\n+\n+    public FieldDefinition(String label, String type, String name, int num, String defaultVal) {\n+      this.label = label;\n+      this.type = type;\n+      this.name = name;\n+      this.num = num;\n+      this.defaultVal = defaultVal;\n+    }\n+\n+    public String getType() {\n+      return type;\n+    }\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public int getNum() {\n+      return num;\n+    }\n+\n+    public String getDefaultVal() {\n+      return defaultVal;\n+    }\n+\n+    public String getLabel() {\n+      return label;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      FieldDefinition field = (FieldDefinition) o;\n+      return num == field.num && Objects.equals(label, field.label) && Objects.equals(\n+          type,\n+          field.type\n+      ) && Objects.equals(name, field.name) && Objects.equals(defaultVal, field.defaultVal);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return Objects.hash(label, type, name, num, defaultVal);\n+    }\n+  }\n+\n+  private MessageDefinition mapDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema, String name, Schema mapElem\n+  ) {\n+    MessageDefinition.Builder map = MessageDefinition.newBuilder(name);\n+    FieldDefinition key = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.keySchema(),\n+        KEY_FIELD,\n+        1\n+    );\n+    map.addField(key.getLabel(), key.getType(), key.getName(), key.getNum(), key.getDefaultVal());\n+    FieldDefinition val = fieldDefinitionFromConnectSchema(\n+        schema,\n+        map,\n+        mapElem.valueSchema(),\n+        VALUE_FIELD,\n+        2\n+    );\n+    map.addField(val.getLabel(), val.getType(), val.getName(), val.getNum(), val.getDefaultVal());\n+    return map.build();\n+  }\n+\n+  private EnumDefinition enumDefinitionFromConnectSchema(\n+      DynamicSchema.Builder schema,\n+      Schema enumElem\n+  ) {\n+    EnumDefinition.Builder enumer = EnumDefinition.newBuilder(enumElem.name());\n+    for (Map.Entry<String, String> entry : enumElem.parameters().entrySet()) {\n+      if (entry.getKey().startsWith(PROTOBUF_TYPE_ENUM_PREFIX)) {\n+        String name = entry.getKey().substring(PROTOBUF_TYPE_ENUM_PREFIX.length());\n+        int tag = Integer.parseInt(entry.getValue());\n+        enumer.addValue(name, tag);\n+      }\n+    }\n+    return enumer.build();\n+  }\n+\n+  private String dataTypeFromConnectSchema(Schema schema) {\n+    switch (schema.type()) {\n+      case INT8:\n+      case INT16:\n+      case INT32:\n+        return FieldDescriptor.Type.INT32.toString().toLowerCase();\n+      case INT64:\n+        if (isProtobufTimestamp(schema)) {\n+          return GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME;\n+        }\n+        return FieldDescriptor.Type.INT64.toString().toLowerCase();\n+      case FLOAT32:\n+        return FieldDescriptor.Type.FLOAT.toString().toLowerCase();\n+      case FLOAT64:\n+        return FieldDescriptor.Type.DOUBLE.toString().toLowerCase();\n+      case BOOLEAN:\n+        return FieldDescriptor.Type.BOOL.toString().toLowerCase();\n+      case STRING:\n+        return FieldDescriptor.Type.STRING.toString().toLowerCase();\n+      case BYTES:\n+        return FieldDescriptor.Type.BYTES.toString().toLowerCase();\n+      case ARRAY:\n+        // Array should not occur here\n+        throw new IllegalArgumentException(\"Array cannot be nested\");\n+      case MAP:\n+        return ProtobufSchema.toMapEntry(getNameOrDefault(schema.name()));\n+      case STRUCT:\n+        return getNameOrDefault(schema.name());\n+      default:\n+        throw new DataException(\"Unknown schema type: \" + schema.type());\n+    }\n+  }\n+\n+  private boolean isProtobufTimestamp(Schema schema) {\n+    return Timestamp.SCHEMA.name().equals(schema.name());\n+  }\n+\n+  public SchemaAndValue toConnectData(ProtobufSchema protobufSchema, DynamicMessage message) {\n+    if (message == null) {\n+      return SchemaAndValue.NULL;\n+    }\n+\n+    Schema schema = toConnectSchema(protobufSchema);\n+\n+    return new SchemaAndValue(schema, toConnectData(schema, message));\n+  }\n+\n+  // Visible for testing\n+  @SuppressWarnings(\"unchecked\")\n+  protected Object toConnectData(Schema schema, Object value) {\n+    try {\n+      if (isProtobufTimestamp(schema)) {\n+        DynamicMessage message = (DynamicMessage) value;\n+\n+        long seconds = 0L;\n+        int nanos = 0;\n+        for (Map.Entry<FieldDescriptor, Object> entry : message.getAllFields().entrySet()) {\n+          if (entry.getKey().getName().equals(\"seconds\")) {\n+            seconds = ((Number) entry.getValue()).longValue();\n+          } else if (entry.getKey().getName().equals(\"nanos\")) {\n+            nanos = ((Number) entry.getValue()).intValue();\n+          }\n+        }\n+        com.google.protobuf.Timestamp timestamp = com.google.protobuf.Timestamp.newBuilder()\n+            .setSeconds(seconds)\n+            .setNanos(nanos)\n+            .build();\n+        return Timestamp.toLogical(schema, Timestamps.toMillis(timestamp));\n+      }\n+\n+      Object converted = null;\n+      switch (schema.type()) {\n+        case INT8:\n+        case INT16:\n+        case INT32:\n+          if (value instanceof Number) {\n+            converted = ((Number) value).intValue(); // Validate type\n+          } else if (value instanceof Enum) {\n+            converted = ((Enum) value).ordinal();\n+          } else if (value instanceof EnumValueDescriptor) {\n+            converted = ((EnumValueDescriptor) value).getNumber();\n+          }\n+          break;\n+        case INT64:\n+          long longValue;\n+          if (value instanceof Long) {\n+            longValue = (Long) value;\n+          } else {\n+            longValue = Integer.toUnsignedLong(((Number) value).intValue());\n+          }\n+          converted = longValue;\n+          break;\n+        case FLOAT32:\n+          float floatValue = ((Number) value).floatValue(); // Validate type\n+          converted = value;\n+          break;\n+        case FLOAT64:\n+          double doubleValue = ((Number) value).doubleValue(); // Validate type\n+          converted = value;\n+          break;\n+        case BOOLEAN:\n+          Boolean boolValue = (Boolean) value; // Validate type\n+          converted = value;\n+          break;\n+        case STRING:\n+          if (value instanceof String) {\n+            converted = value;\n+          } else if (value instanceof CharSequence) {\n+            converted = value.toString();\n+          } else {\n+            throw new DataException(\"Invalid class for string type, expecting String or \"\n+                + \"CharSequence but found \"\n+                + value.getClass());\n+          }\n+          break;\n+        case BYTES:\n+          if (value instanceof byte[]) {\n+            converted = ByteBuffer.wrap((byte[]) value);\n+          } else if (value instanceof ByteBuffer) {\n+            converted = value;\n+          } else if (value instanceof ByteString) {\n+            converted = ((ByteString) value).asReadOnlyByteBuffer();\n+          } else {\n+            throw new DataException(\"Invalid class for bytes type, expecting byte[] or ByteBuffer \"\n+                + \"but found \"\n+                + value.getClass());\n+          }\n+          break;\n+        case ARRAY:\n+          final Schema elemSchema = schema.valueSchema();\n+          final Collection<Object> array = (Collection<Object>) value;\n+          final List<Object> newArray = new ArrayList<>(array.size());\n+          for (Object elem : array) {\n+            newArray.add(toConnectData(elemSchema, elem));\n+          }\n+          converted = newArray;\n+          break;\n+        case MAP:\n+          final Schema keySchema = schema.keySchema();\n+          final Schema valueSchema = schema.valueSchema();\n+          final Collection<DynamicMessage> map = (Collection<DynamicMessage>) value;\n+          final Map<Object, Object> newMap = new HashMap<>();\n+          for (DynamicMessage message : map) {\n+            Descriptor descriptor = message.getDescriptorForType();\n+            Object elemKey = message.getField(descriptor.findFieldByName(KEY_FIELD));\n+            Object elemValue = message.getField(descriptor.findFieldByName(VALUE_FIELD));\n+            newMap.put(toConnectData(keySchema, elemKey), toConnectData(valueSchema, elemValue));\n+          }\n+          converted = newMap;\n+          break;\n+        case STRUCT:\n+          final DynamicMessage message = (DynamicMessage) value; // Validate type\n+          if (message.equals(message.getDefaultInstanceForType())) {\n+            // Note: this is so that fields that are omitted are not set with the default values.\n+            return null;\n+          }\n+\n+          final Struct struct = new Struct(schema.schema());\n+          final Descriptor descriptor = message.getDescriptorForType();\n+\n+          for (OneofDescriptor oneOfDescriptor : descriptor.getOneofs()) {\n+            FieldDescriptor fieldDescriptor = message.getOneofFieldDescriptor(oneOfDescriptor);\n+            Object obj = message.getField(fieldDescriptor);\n+            if (obj != null) {\n+              setUnionField(schema, message, struct, oneOfDescriptor, fieldDescriptor);\n+              break;\n+            }\n+          }\n+\n+          for (FieldDescriptor fieldDescriptor : descriptor.getFields()) {\n+            OneofDescriptor oneOfDescriptor = fieldDescriptor.getContainingOneof();\n+            if (oneOfDescriptor != null) {\n+              // Already added field as oneof\n+              continue;\n+            }\n+            setStructField(schema, message, struct, fieldDescriptor);\n+          }\n+\n+          converted = struct;\n+          break;\n+        default:\n+          throw new DataException(\"Unknown Connect schema type: \" + schema.type());\n+      }\n+\n+      return converted;\n+    } catch (ClassCastException e) {\n+      throw new DataException(\"Invalid type for \" + schema.type() + \": \" + value.getClass());\n+    }\n+  }\n+\n+  private void setUnionField(\n+      Schema schema,\n+      Message message,\n+      Struct result,\n+      OneofDescriptor oneOfDescriptor,\n+      FieldDescriptor fieldDescriptor\n+  ) {\n+    String unionName = oneOfDescriptor.getName() + \"_\" + oneOfDescriptor.getIndex();\n+    Field unionField = schema.field(unionName);\n+    Schema unionSchema = unionField.schema();\n+    Struct union = new Struct(unionSchema);\n+\n+    final String fieldName = fieldDescriptor.getName();\n+    final Field field = unionSchema.field(fieldName);\n+    Object obj = message.getField(fieldDescriptor);\n+    union.put(fieldName, toConnectData(field.schema(), obj));\n+\n+    result.put(unionField, union);\n+  }\n+\n+  private void setStructField(\n+      Schema schema,\n+      Message message,\n+      Struct result,\n+      FieldDescriptor fieldDescriptor\n+  ) {\n+    final String fieldName = fieldDescriptor.getName();\n+    final Field field = schema.field(fieldName);\n+    Object obj = message.getField(fieldDescriptor);\n+    result.put(fieldName, toConnectData(field.schema(), obj));\n+  }\n+\n+  private Schema toConnectSchema(ProtobufSchema schema) {\n+    if (schema == null) {\n+      return null;\n+    }\n+    Schema cachedSchema = toConnectSchemaCache.get(schema);\n+    if (cachedSchema != null) {\n+      return cachedSchema;\n+    }\n+    Schema resultSchema = toConnectSchema(schema.toDescriptor(), schema.version()).build();\n+    toConnectSchemaCache.put(schema, resultSchema);\n+    return resultSchema;\n+  }\n+\n+  private SchemaBuilder toConnectSchema(Descriptor descriptor, Integer version) {\n+    List<FieldDescriptor> fieldDescriptors = descriptor.getFields();\n+    if (isMapDescriptor(descriptor, fieldDescriptors)) {\n+      String name = ProtobufSchema.toMapField(descriptor.getName());\n+      return SchemaBuilder.map(toConnectSchema(fieldDescriptors.get(0)),\n+          toConnectSchema(fieldDescriptors.get(1))\n+      )\n+          .name(name);\n+    }\n+    SchemaBuilder builder = SchemaBuilder.struct();\n+    builder.name(descriptor.getName());\n+    List<OneofDescriptor> oneOfDescriptors = descriptor.getOneofs();\n+    for (OneofDescriptor oneOfDescriptor : oneOfDescriptors) {\n+      String unionName = oneOfDescriptor.getName() + \"_\" + oneOfDescriptor.getIndex();\n+      builder.field(unionName, toConnectSchema(oneOfDescriptor));\n+    }\n+    for (FieldDescriptor fieldDescriptor : fieldDescriptors) {\n+      OneofDescriptor oneOfDescriptor = fieldDescriptor.getContainingOneof();\n+      if (oneOfDescriptor != null) {\n+        // Already added field as oneof\n+        continue;\n+      }\n+      builder.field(fieldDescriptor.getName(), toConnectSchema(fieldDescriptor));\n+    }\n+\n+    if (version != null) {\n+      builder.version(version);\n+    }\n+\n+    return builder;\n+  }\n+\n+  private Schema toConnectSchema(OneofDescriptor descriptor) {\n+    SchemaBuilder builder = SchemaBuilder.struct();\n+    builder.name(PROTOBUF_TYPE_UNION_PREFIX + descriptor.getName());\n+    List<FieldDescriptor> fieldDescriptors = descriptor.getFields();\n+    for (FieldDescriptor fieldDescriptor : fieldDescriptors) {\n+      builder.field(fieldDescriptor.getName(), toConnectSchema(fieldDescriptor));\n+    }\n+    return builder.build();\n+  }\n+\n+  private Schema toConnectSchema(FieldDescriptor descriptor) {\n+    SchemaBuilder builder;\n+\n+    switch (descriptor.getType()) {\n+      case INT32:\n+      case SINT32:\n+      case SFIXED32: {\n+        builder = SchemaBuilder.int32();\n+        break;\n+      }\n+\n+      case UINT32:\n+      case FIXED32:\n+      case INT64:\n+      case UINT64:\n+      case SINT64:\n+      case FIXED64:\n+      case SFIXED64: {\n+        builder = SchemaBuilder.int64();\n+        break;\n+      }\n+\n+      case FLOAT: {\n+        builder = SchemaBuilder.float32();\n+        break;\n+      }\n+\n+      case DOUBLE: {\n+        builder = SchemaBuilder.float64();\n+        break;\n+      }\n+\n+      case BOOL: {\n+        builder = SchemaBuilder.bool();\n+        break;\n+      }\n+\n+      case STRING:\n+        builder = SchemaBuilder.string();\n+        break;\n+\n+      case BYTES:\n+        builder = SchemaBuilder.bytes();\n+        break;\n+\n+      case ENUM:\n+        builder = SchemaBuilder.int32();\n+        EnumDescriptor enumDescriptor = descriptor.getEnumType();\n+        builder.name(enumDescriptor.getName());\n+        builder.parameter(PROTOBUF_TYPE_ENUM, enumDescriptor.getName());\n+        for (EnumValueDescriptor enumValueDesc : enumDescriptor.getValues()) {\n+          String enumSymbol = enumValueDesc.getName();\n+          String enumTag = String.valueOf(enumValueDesc.getNumber());\n+          builder.parameter(PROTOBUF_TYPE_ENUM_PREFIX + enumSymbol, enumTag);\n+        }\n+        break;\n+\n+      case MESSAGE: {\n+        if (isTimestampDescriptor(descriptor)) {\n+          builder = Timestamp.builder();\n+          break;\n+        }\n+\n+        builder = toConnectSchema(descriptor.getMessageType(), null);\n+        break;\n+      }\n+\n+      default:\n+        throw new DataException(\"Unknown Connect schema type: \" + descriptor.getType());\n+    }\n+\n+    if (descriptor.isRepeated() && builder.type() != Schema.Type.MAP) {\n+      Schema schema = builder.optional().build();\n+      builder = SchemaBuilder.array(schema);\n+    }\n+\n+    builder.optional();\n+    builder.parameter(PROTOBUF_TYPE_TAG, String.valueOf(descriptor.getNumber()));\n+    return builder.build();\n+  }\n+\n+  private static MessageDefinition timestampDefinition() {\n+    MessageDefinition.Builder timestampType = MessageDefinition.newBuilder(\n+        GOOGLE_PROTOBUF_TIMESTAMP_NAME);\n+    timestampType.addField(\"optional\", \"int64\", \"seconds\", 1, null);\n+    timestampType.addField(\"optional\", \"int32\", \"nanos\", 2, null);\n+    return timestampType.build();\n+  }\n+\n+  private static boolean isTimestampDescriptor(FieldDescriptor descriptor) {\n+    String name = descriptor.getMessageType().getFullName();\n+    return GOOGLE_PROTOBUF_TIMESTAMP_FULL_NAME.equals(name);\n+  }\n+\n+  private static boolean isMapDescriptor(\n+      Descriptor descriptor,\n+      List<FieldDescriptor> fieldDescriptors\n+  ) {\n+    return descriptor.getName().endsWith(MAP_ENTRY_SUFFIX)\n+        && fieldDescriptors.size() == 2\n+        && fieldDescriptors.get(0).getName().equals(KEY_FIELD)\n+        && fieldDescriptors.get(1).getName().equals(VALUE_FIELD);\n+  }\n+\n+  private String getNameOrDefault(String name) {\n+    return name != null && !name.isEmpty()\n+           ? name\n+           : DEFAULT_SCHEMA_NAME + (++defaultSchemaNameIndex);", "originalCommit": "8e65cc65b67dbea699887326c15fb260b1cdff1a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDc1NTk4MA==", "url": "https://github.com/confluentinc/schema-registry/pull/1285#discussion_r370755980", "bodyText": "No it's not accessed concurrently.", "author": "rayokota", "createdAt": "2020-01-24T17:37:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDczODA5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "8b9bfb95c00096890ef58813c8c395b2a411795f", "chunk": "diff --git a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\nindex 2dbdc1a07..0d2a5ea2a 100644\n--- a/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n+++ b/protobuf-converter/src/main/java/io/confluent/connect/protobuf/ProtobufData.java\n\n@@ -256,13 +256,13 @@ public class ProtobufData {\n                 );\n               }\n             }\n-            throw new IllegalArgumentException(\"Cannot find non-null field\");\n+            throw new DataException(\"Cannot find non-null field\");\n           } else {\n             String scopedStructName = scope + getNameOrDefault(structName);\n             DynamicMessage.Builder messageBuilder =\n                 protobufSchema.newMessageBuilder(scopedStructName);\n             if (messageBuilder == null) {\n-              throw new IllegalStateException(\"Invalid message name: \" + scopedStructName);\n+              throw new DataException(\"Invalid message name: \" + scopedStructName);\n             }\n             for (Field field : schema.fields()) {\n               Object fieldValue = fromConnectData(\n"}}, {"oid": "8b9bfb95c00096890ef58813c8c395b2a411795f", "url": "https://github.com/confluentinc/schema-registry/commit/8b9bfb95c00096890ef58813c8c395b2a411795f", "message": "Incorporate review feedback", "committedDate": "2020-01-24T18:39:41Z", "type": "commit"}]}