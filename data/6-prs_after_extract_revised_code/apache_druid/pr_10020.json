{"pr_number": 10020, "pr_title": "global table datasource for broadcast segments", "pr_createdAt": "2020-06-11T13:26:03Z", "pr_url": "https://github.com/apache/druid/pull/10020", "timeline": [{"oid": "489fdc67f40a99fb12087bb4492cf3d9adde0660", "url": "https://github.com/apache/druid/commit/489fdc67f40a99fb12087bb4492cf3d9adde0660", "message": "global table datasource for broadcast segments", "committedDate": "2020-06-11T13:18:57Z", "type": "commit"}, {"oid": "eeb174a22d6a2d6823c45c997ef88adb625d6e3e", "url": "https://github.com/apache/druid/commit/eeb174a22d6a2d6823c45c997ef88adb625d6e3e", "message": "tests", "committedDate": "2020-06-12T11:51:25Z", "type": "commit"}, {"oid": "bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "url": "https://github.com/apache/druid/commit/bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "message": "fix", "committedDate": "2020-06-12T13:34:04Z", "type": "commit"}, {"oid": "aff3b0ab96f11bb94cbe3a441adb8b1c1ec78b28", "url": "https://github.com/apache/druid/commit/aff3b0ab96f11bb94cbe3a441adb8b1c1ec78b28", "message": "fix test", "committedDate": "2020-06-12T19:29:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NjcwMg==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439596702", "bodyText": "Should the global datasource type be added to the docs? https://druid.apache.org/docs/0.18.1/querying/query-execution.html#datasource-type", "author": "ccaominh", "createdAt": "2020-06-12T19:07:52Z", "path": "processing/src/main/java/org/apache/druid/query/DataSource.java", "diffHunk": "@@ -35,7 +35,8 @@\n     @JsonSubTypes.Type(value = UnionDataSource.class, name = \"union\"),\n     @JsonSubTypes.Type(value = JoinDataSource.class, name = \"join\"),\n     @JsonSubTypes.Type(value = LookupDataSource.class, name = \"lookup\"),\n-    @JsonSubTypes.Type(value = InlineDataSource.class, name = \"inline\")\n+    @JsonSubTypes.Type(value = InlineDataSource.class, name = \"inline\"),\n+    @JsonSubTypes.Type(value = GlobalTableDataSource.class, name = \"global\")", "originalCommit": "bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4OTUzNQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439689535", "bodyText": "I'm not sure if it makes sense to add yet until more of #9953 is implemented since making effective use of this requires GlobalTableDataSource to be bound to a JoinableFactory, otherwise it behaves the same as TableDataSource", "author": "clintropolis", "createdAt": "2020-06-13T00:34:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NjcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU5OTY0NA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440599644", "bodyText": "I think globalTable would be a better name. We only have one chance to get it right!\nIMO for some consistency between SQL and native, we'll need to either transparently globalify the regular table type (perhaps a rewrite step like ClientQuerySegmentWalker's inlining?) or we'll need to document the globalTable type. I think the former is nicer, because the latter comes with too many caveats (you have to make sure to use it in the proper conditions).", "author": "gianm", "createdAt": "2020-06-16T05:47:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NjcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwOTg1Ng==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440609856", "bodyText": "By the way, to be clear, I don't think we need to do (2) in this PR. That was more a suggestion for future work.", "author": "gianm", "createdAt": "2020-06-16T06:17:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NjcwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxOTcxMA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440819710", "bodyText": "Renamed to globalTable. For 2 it sounds nice to be automatic since it negates the need to document this, so will add that behavior in a follow-up.", "author": "clintropolis", "createdAt": "2020-06-16T12:44:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NjcwMg=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/DataSource.java b/processing/src/main/java/org/apache/druid/query/DataSource.java\nindex 47fb9ecdef..ff768eda25 100644\n--- a/processing/src/main/java/org/apache/druid/query/DataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/DataSource.java\n\n@@ -36,7 +36,7 @@ import java.util.Set;\n     @JsonSubTypes.Type(value = JoinDataSource.class, name = \"join\"),\n     @JsonSubTypes.Type(value = LookupDataSource.class, name = \"lookup\"),\n     @JsonSubTypes.Type(value = InlineDataSource.class, name = \"inline\"),\n-    @JsonSubTypes.Type(value = GlobalTableDataSource.class, name = \"global\")\n+    @JsonSubTypes.Type(value = GlobalTableDataSource.class, name = \"globalTable\")\n })\n public interface DataSource\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NzQ1NA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439597454", "bodyText": "What do you think about adding a javadoc explaining why this datasource type exists?", "author": "ccaominh", "createdAt": "2020-06-12T19:09:38Z", "path": "processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.annotation.JsonTypeName;\n+\n+@JsonTypeName(\"global\")\n+public class GlobalTableDataSource extends TableDataSource", "originalCommit": "bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4OTY3OQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439689679", "bodyText": "Added javadoc \ud83d\udc4d", "author": "clintropolis", "createdAt": "2020-06-13T00:35:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU5NzQ1NA=="}], "type": "inlineReview", "revised_code": {"commit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\nindex 44a93699aa..7e9048df41 100644\n--- a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n\n@@ -23,6 +23,16 @@ import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n \n+/**\n+ * {@link TableDataSource} variant for globally available 'broadcast' segments. If bound to a\n+ * {@link org.apache.druid.segment.join.JoinableFactory} that can create an\n+ * {@link org.apache.druid.segment.join.table.IndexedTable} using DruidBinders.joinableFactoryBinder, this allows\n+ * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be pushed\n+ * down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n+ * {@link InlineDataSource} to construct an {@link org.apache.druid.segment.join.table.IndexedTable} on the fly on the\n+ * broker. Because it is also a {@link TableDataSource}, when queried directly, or on the left hand side of a join,\n+ * they will be treated as any normal segment.\n+ */\n @JsonTypeName(\"global\")\n public class GlobalTableDataSource extends TableDataSource\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwMTAzNw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439601037", "bodyText": "Why are all of the datasources broadcast?", "author": "ccaominh", "createdAt": "2020-06-12T19:18:28Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -196,119 +207,130 @@ public DruidSchema(\n   public void start() throws InterruptedException\n   {\n     cacheExec.submit(\n-        new Runnable()\n-        {\n-          @Override\n-          public void run()\n-          {\n-            try {\n-              while (!Thread.currentThread().isInterrupted()) {\n-                final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n-                final Set<String> dataSourcesToRebuild = new TreeSet<>();\n-\n-                try {\n-                  synchronized (lock) {\n-                    final long nextRefreshNoFuzz = DateTimes\n-                        .utc(lastRefresh)\n-                        .plus(config.getMetadataRefreshPeriod())\n-                        .getMillis();\n-\n-                    // Fuzz a bit to spread load out when we have multiple brokers.\n-                    final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n-\n-                    while (true) {\n-                      // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n-                      final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n-                                                                .plus(config.getMetadataRefreshPeriod())\n-                                                                .isAfterNow();\n-\n-                      if (isServerViewInitialized &&\n-                          !wasRecentFailure &&\n-                          (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n-                          (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n-                        // We need to do a refresh. Break out of the waiting loop.\n-                        break;\n-                      }\n-\n-                      if (isServerViewInitialized) {\n-                        // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n-                        // no segments in the system yet. Just mark us as initialized, then.\n-                        initialized.countDown();\n-                      }\n-\n-                      // Wait some more, we'll wake up when it might be time to do another refresh.\n-                      lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n+        () -> {\n+          try {\n+            while (!Thread.currentThread().isInterrupted()) {\n+              final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n+              final Set<String> dataSourcesToRebuild = new TreeSet<>();\n+\n+              try {\n+                synchronized (lock) {\n+                  final long nextRefreshNoFuzz = DateTimes\n+                      .utc(lastRefresh)\n+                      .plus(config.getMetadataRefreshPeriod())\n+                      .getMillis();\n+\n+                  // Fuzz a bit to spread load out when we have multiple brokers.\n+                  final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n+\n+                  while (true) {\n+                    // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n+                    final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n+                                                              .plus(config.getMetadataRefreshPeriod())\n+                                                              .isAfterNow();\n+\n+                    if (isServerViewInitialized &&\n+                        !wasRecentFailure &&\n+                        (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n+                        (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n+                      // We need to do a refresh. Break out of the waiting loop.\n+                      break;\n                     }\n \n-                    segmentsToRefresh.addAll(segmentsNeedingRefresh);\n-                    segmentsNeedingRefresh.clear();\n-\n-                    // Mutable segments need a refresh every period, since new columns could be added dynamically.\n-                    segmentsNeedingRefresh.addAll(mutableSegments);\n+                    if (isServerViewInitialized) {\n+                      // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n+                      // no segments in the system yet. Just mark us as initialized, then.\n+                      initialized.countDown();\n+                    }\n \n-                    lastFailure = 0L;\n-                    lastRefresh = System.currentTimeMillis();\n-                    refreshImmediately = false;\n+                    // Wait some more, we'll wake up when it might be time to do another refresh.\n+                    lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n                   }\n \n-                  // Refresh the segments.\n-                  final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n+                  segmentsToRefresh.addAll(segmentsNeedingRefresh);\n+                  segmentsNeedingRefresh.clear();\n \n-                  synchronized (lock) {\n-                    // Add missing segments back to the refresh list.\n-                    segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n+                  // Mutable segments need a refresh every period, since new columns could be added dynamically.\n+                  segmentsNeedingRefresh.addAll(mutableSegments);\n \n-                    // Compute the list of dataSources to rebuild tables for.\n-                    dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n-                    refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n-                    dataSourcesNeedingRebuild.clear();\n+                  lastFailure = 0L;\n+                  lastRefresh = System.currentTimeMillis();\n+                  refreshImmediately = false;\n+                }\n \n-                    lock.notifyAll();\n-                  }\n+                // Refresh the segments.\n+                final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n \n-                  // Rebuild the dataSources.\n-                  for (String dataSource : dataSourcesToRebuild) {\n-                    final DruidTable druidTable = buildDruidTable(dataSource);\n-                    final DruidTable oldTable = tables.put(dataSource, druidTable);\n-                    if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n-                      log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n-                    } else {\n-                      log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n-                    }\n-                  }\n+                synchronized (lock) {\n+                  // Add missing segments back to the refresh list.\n+                  segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n \n-                  initialized.countDown();\n-                }\n-                catch (InterruptedException e) {\n-                  // Fall through.\n-                  throw e;\n+                  // Compute the list of dataSources to rebuild tables for.\n+                  dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n+                  refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n+                  dataSourcesNeedingRebuild.clear();\n+\n+                  lock.notifyAll();\n                 }\n-                catch (Exception e) {\n-                  log.warn(e, \"Metadata refresh failed, trying again soon.\");\n-\n-                  synchronized (lock) {\n-                    // Add our segments and dataSources back to their refresh and rebuild lists.\n-                    segmentsNeedingRefresh.addAll(segmentsToRefresh);\n-                    dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n-                    lastFailure = System.currentTimeMillis();\n-                    lock.notifyAll();\n+\n+                // Rebuild the dataSources.\n+                for (String dataSource : dataSourcesToRebuild) {\n+                  final DruidTable druidTable = buildDruidTable(dataSource);\n+                  final DruidTable oldTable = tables.put(dataSource, druidTable);\n+                  if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n+                    log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n+                  } else {\n+                    log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n                   }\n                 }\n+\n+                initialized.countDown();\n+              }\n+              catch (InterruptedException e) {\n+                // Fall through.\n+                throw e;\n+              }\n+              catch (Exception e) {\n+                log.warn(e, \"Metadata refresh failed, trying again soon.\");\n+\n+                synchronized (lock) {\n+                  // Add our segments and dataSources back to their refresh and rebuild lists.\n+                  segmentsNeedingRefresh.addAll(segmentsToRefresh);\n+                  dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n+                  lastFailure = System.currentTimeMillis();\n+                  lock.notifyAll();\n+                }\n               }\n-            }\n-            catch (InterruptedException e) {\n-              // Just exit.\n-            }\n-            catch (Throwable e) {\n-              // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n-              // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n-              log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n-              throw e;\n-            }\n-            finally {\n-              log.info(\"Metadata refresh stopped.\");\n             }\n           }\n+          catch (InterruptedException e) {\n+            // Just exit.\n+          }\n+          catch (Throwable e) {\n+            // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n+            // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n+            log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n+            throw e;\n+          }\n+          finally {\n+            log.info(\"Metadata refresh stopped.\");\n+          }\n+        }\n+    );\n+\n+    ScheduledExecutors.scheduleWithFixedDelay(\n+        localSegmentExec,\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        () -> {\n+          synchronized (lock) {\n+            // refresh known broadcast segments\n+            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n+            dataSourcesNeedingRebuild.addAll(localSegmentDatasources);\n+            broadcastDatasources.clear();\n+            broadcastDatasources.addAll(localSegmentDatasources);", "originalCommit": "bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5MDI5Mg==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439690292", "bodyText": "added a comment to clarify that since this code only runs on the broker, and the broker can only have broadcast segments, that it's a safe assumption. If we ever load brokers load normal segments we might need to reconsider how this works, and it might be more appropriate longer term to use the load rules instead of inferring from context, but it should be ok for now I think", "author": "clintropolis", "createdAt": "2020-06-13T00:36:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwMTAzNw=="}], "type": "inlineReview", "revised_code": {"commit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 6d4db1f7fc..9fae29a704 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -324,7 +324,9 @@ public class DruidSchema extends AbstractSchema\n         config.getMetadataRefreshPeriod().toStandardDuration(),\n         () -> {\n           synchronized (lock) {\n-            // refresh known broadcast segments\n+            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n+            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n+            // future by fetching load rules from the coordinator\n             Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n             dataSourcesNeedingRebuild.addAll(localSegmentDatasources);\n             broadcastDatasources.clear();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwNTA3OA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r439605078", "bodyText": "This change is causing SegmentMetadataQueryTest.testSerde() to fail: https://travis-ci.org/github/apache/druid/jobs/697612764#L5001\nThe query variable has a LegacyDataSource whereas the deserialized serialized version has a TableDataSource.", "author": "ccaominh", "createdAt": "2020-06-12T19:28:27Z", "path": "processing/src/main/java/org/apache/druid/query/TableDataSource.java", "diffHunk": "@@ -98,7 +98,7 @@ public final boolean equals(Object o)\n     if (this == o) {\n       return true;\n     }\n-    if (!(o instanceof TableDataSource)) {\n+    if (!(o instanceof TableDataSource) || !getClass().equals(o.getClass())) {", "originalCommit": "bda1c7f86a1f74b0868472f0dd9b1b10ec9c9ed1", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "aff3b0ab96f11bb94cbe3a441adb8b1c1ec78b28", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/TableDataSource.java b/processing/src/main/java/org/apache/druid/query/TableDataSource.java\nindex e75a7e8bbb..917545d894 100644\n--- a/processing/src/main/java/org/apache/druid/query/TableDataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/TableDataSource.java\n\n@@ -98,7 +98,12 @@ public class TableDataSource implements DataSource\n     if (this == o) {\n       return true;\n     }\n-    if (!(o instanceof TableDataSource) || !getClass().equals(o.getClass())) {\n+    if (!(o instanceof TableDataSource)) {\n+      return false;\n+    }\n+\n+    if ((o instanceof GlobalTableDataSource || this instanceof GlobalTableDataSource) &&\n+        !getClass().equals(o.getClass())) {\n       return false;\n     }\n \n"}}, {"oid": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "url": "https://github.com/apache/druid/commit/9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "message": "comments and javadocs", "committedDate": "2020-06-12T22:59:27Z", "type": "commit"}, {"oid": "d0631b53e13436e59c4fe52671763956fec95d9b", "url": "https://github.com/apache/druid/commit/d0631b53e13436e59c4fe52671763956fec95d9b", "message": "Merge remote-tracking branch 'upstream/master' into global-table-for-broadcast-segments", "committedDate": "2020-06-16T04:04:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU5ODU2Nw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440598567", "bodyText": "localSegmentDataSources would be more consistent spelling, I think.", "author": "gianm", "createdAt": "2020-06-16T05:43:16Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -196,119 +207,132 @@ public DruidSchema(\n   public void start() throws InterruptedException\n   {\n     cacheExec.submit(\n-        new Runnable()\n-        {\n-          @Override\n-          public void run()\n-          {\n-            try {\n-              while (!Thread.currentThread().isInterrupted()) {\n-                final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n-                final Set<String> dataSourcesToRebuild = new TreeSet<>();\n-\n-                try {\n-                  synchronized (lock) {\n-                    final long nextRefreshNoFuzz = DateTimes\n-                        .utc(lastRefresh)\n-                        .plus(config.getMetadataRefreshPeriod())\n-                        .getMillis();\n-\n-                    // Fuzz a bit to spread load out when we have multiple brokers.\n-                    final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n-\n-                    while (true) {\n-                      // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n-                      final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n-                                                                .plus(config.getMetadataRefreshPeriod())\n-                                                                .isAfterNow();\n-\n-                      if (isServerViewInitialized &&\n-                          !wasRecentFailure &&\n-                          (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n-                          (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n-                        // We need to do a refresh. Break out of the waiting loop.\n-                        break;\n-                      }\n-\n-                      if (isServerViewInitialized) {\n-                        // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n-                        // no segments in the system yet. Just mark us as initialized, then.\n-                        initialized.countDown();\n-                      }\n-\n-                      // Wait some more, we'll wake up when it might be time to do another refresh.\n-                      lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n+        () -> {\n+          try {\n+            while (!Thread.currentThread().isInterrupted()) {\n+              final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n+              final Set<String> dataSourcesToRebuild = new TreeSet<>();\n+\n+              try {\n+                synchronized (lock) {\n+                  final long nextRefreshNoFuzz = DateTimes\n+                      .utc(lastRefresh)\n+                      .plus(config.getMetadataRefreshPeriod())\n+                      .getMillis();\n+\n+                  // Fuzz a bit to spread load out when we have multiple brokers.\n+                  final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n+\n+                  while (true) {\n+                    // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n+                    final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n+                                                              .plus(config.getMetadataRefreshPeriod())\n+                                                              .isAfterNow();\n+\n+                    if (isServerViewInitialized &&\n+                        !wasRecentFailure &&\n+                        (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n+                        (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n+                      // We need to do a refresh. Break out of the waiting loop.\n+                      break;\n                     }\n \n-                    segmentsToRefresh.addAll(segmentsNeedingRefresh);\n-                    segmentsNeedingRefresh.clear();\n-\n-                    // Mutable segments need a refresh every period, since new columns could be added dynamically.\n-                    segmentsNeedingRefresh.addAll(mutableSegments);\n+                    if (isServerViewInitialized) {\n+                      // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n+                      // no segments in the system yet. Just mark us as initialized, then.\n+                      initialized.countDown();\n+                    }\n \n-                    lastFailure = 0L;\n-                    lastRefresh = System.currentTimeMillis();\n-                    refreshImmediately = false;\n+                    // Wait some more, we'll wake up when it might be time to do another refresh.\n+                    lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n                   }\n \n-                  // Refresh the segments.\n-                  final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n+                  segmentsToRefresh.addAll(segmentsNeedingRefresh);\n+                  segmentsNeedingRefresh.clear();\n \n-                  synchronized (lock) {\n-                    // Add missing segments back to the refresh list.\n-                    segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n+                  // Mutable segments need a refresh every period, since new columns could be added dynamically.\n+                  segmentsNeedingRefresh.addAll(mutableSegments);\n \n-                    // Compute the list of dataSources to rebuild tables for.\n-                    dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n-                    refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n-                    dataSourcesNeedingRebuild.clear();\n+                  lastFailure = 0L;\n+                  lastRefresh = System.currentTimeMillis();\n+                  refreshImmediately = false;\n+                }\n \n-                    lock.notifyAll();\n-                  }\n+                // Refresh the segments.\n+                final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n \n-                  // Rebuild the dataSources.\n-                  for (String dataSource : dataSourcesToRebuild) {\n-                    final DruidTable druidTable = buildDruidTable(dataSource);\n-                    final DruidTable oldTable = tables.put(dataSource, druidTable);\n-                    if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n-                      log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n-                    } else {\n-                      log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n-                    }\n-                  }\n+                synchronized (lock) {\n+                  // Add missing segments back to the refresh list.\n+                  segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n \n-                  initialized.countDown();\n-                }\n-                catch (InterruptedException e) {\n-                  // Fall through.\n-                  throw e;\n+                  // Compute the list of dataSources to rebuild tables for.\n+                  dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n+                  refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n+                  dataSourcesNeedingRebuild.clear();\n+\n+                  lock.notifyAll();\n                 }\n-                catch (Exception e) {\n-                  log.warn(e, \"Metadata refresh failed, trying again soon.\");\n-\n-                  synchronized (lock) {\n-                    // Add our segments and dataSources back to their refresh and rebuild lists.\n-                    segmentsNeedingRefresh.addAll(segmentsToRefresh);\n-                    dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n-                    lastFailure = System.currentTimeMillis();\n-                    lock.notifyAll();\n+\n+                // Rebuild the dataSources.\n+                for (String dataSource : dataSourcesToRebuild) {\n+                  final DruidTable druidTable = buildDruidTable(dataSource);\n+                  final DruidTable oldTable = tables.put(dataSource, druidTable);\n+                  if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n+                    log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n+                  } else {\n+                    log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n                   }\n                 }\n+\n+                initialized.countDown();\n+              }\n+              catch (InterruptedException e) {\n+                // Fall through.\n+                throw e;\n+              }\n+              catch (Exception e) {\n+                log.warn(e, \"Metadata refresh failed, trying again soon.\");\n+\n+                synchronized (lock) {\n+                  // Add our segments and dataSources back to their refresh and rebuild lists.\n+                  segmentsNeedingRefresh.addAll(segmentsToRefresh);\n+                  dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n+                  lastFailure = System.currentTimeMillis();\n+                  lock.notifyAll();\n+                }\n               }\n-            }\n-            catch (InterruptedException e) {\n-              // Just exit.\n-            }\n-            catch (Throwable e) {\n-              // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n-              // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n-              log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n-              throw e;\n-            }\n-            finally {\n-              log.info(\"Metadata refresh stopped.\");\n             }\n           }\n+          catch (InterruptedException e) {\n+            // Just exit.\n+          }\n+          catch (Throwable e) {\n+            // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n+            // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n+            log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n+            throw e;\n+          }\n+          finally {\n+            log.info(\"Metadata refresh stopped.\");\n+          }\n+        }\n+    );\n+\n+    ScheduledExecutors.scheduleWithFixedDelay(\n+        localSegmentExec,\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        () -> {\n+          synchronized (lock) {\n+            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n+            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n+            // future by fetching load rules from the coordinator\n+            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODcxMA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818710", "bodyText": "this logic has been removed", "author": "clintropolis", "createdAt": "2020-06-16T12:42:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU5ODU2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 9fae29a704..6762aaf13c 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -318,24 +319,6 @@ public class DruidSchema extends AbstractSchema\n         }\n     );\n \n-    ScheduledExecutors.scheduleWithFixedDelay(\n-        localSegmentExec,\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        () -> {\n-          synchronized (lock) {\n-            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n-            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n-            // future by fetching load rules from the coordinator\n-            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n-            dataSourcesNeedingRebuild.addAll(localSegmentDatasources);\n-            broadcastDatasources.clear();\n-            broadcastDatasources.addAll(localSegmentDatasources);\n-            lock.notifyAll();\n-          }\n-        }\n-    );\n-\n     if (config.isAwaitInitializationOnStart()) {\n       final long startNanos = System.nanoTime();\n       log.debug(\"%s waiting for initialization.\", getClass().getSimpleName());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMDc1Mw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440600753", "bodyText": "\"datasource\" makes more sense here than \"segment\".", "author": "gianm", "createdAt": "2020-06-16T05:51:04Z", "path": "processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.annotation.JsonTypeName;\n+\n+/**\n+ * {@link TableDataSource} variant for globally available 'broadcast' segments. If bound to a\n+ * {@link org.apache.druid.segment.join.JoinableFactory} that can create an\n+ * {@link org.apache.druid.segment.join.table.IndexedTable} using DruidBinders.joinableFactoryBinder, this allows\n+ * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be pushed\n+ * down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n+ * {@link InlineDataSource} to construct an {@link org.apache.druid.segment.join.table.IndexedTable} on the fly on the\n+ * broker. Because it is also a {@link TableDataSource}, when queried directly, or on the left hand side of a join,\n+ * they will be treated as any normal segment.", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODYyNA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818624", "bodyText": "fixed", "author": "clintropolis", "createdAt": "2020-06-16T12:42:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMDc1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\nindex 7e9048df41..da5f1390ca 100644\n--- a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n\n@@ -27,13 +27,13 @@ import com.fasterxml.jackson.annotation.JsonTypeName;\n  * {@link TableDataSource} variant for globally available 'broadcast' segments. If bound to a\n  * {@link org.apache.druid.segment.join.JoinableFactory} that can create an\n  * {@link org.apache.druid.segment.join.table.IndexedTable} using DruidBinders.joinableFactoryBinder, this allows\n- * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be pushed\n- * down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n+ * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be\n+ * pushed down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n  * {@link InlineDataSource} to construct an {@link org.apache.druid.segment.join.table.IndexedTable} on the fly on the\n  * broker. Because it is also a {@link TableDataSource}, when queried directly, or on the left hand side of a join,\n- * they will be treated as any normal segment.\n+ * they will be treated as any normal table datasource.\n  */\n-@JsonTypeName(\"global\")\n+@JsonTypeName(\"globalTable\")\n public class GlobalTableDataSource extends TableDataSource\n {\n   @JsonCreator\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMDkzOQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440600939", "bodyText": "Why shouldn't it be cacheable?", "author": "gianm", "createdAt": "2020-06-16T05:51:48Z", "path": "processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import com.fasterxml.jackson.annotation.JsonTypeName;\n+\n+/**\n+ * {@link TableDataSource} variant for globally available 'broadcast' segments. If bound to a\n+ * {@link org.apache.druid.segment.join.JoinableFactory} that can create an\n+ * {@link org.apache.druid.segment.join.table.IndexedTable} using DruidBinders.joinableFactoryBinder, this allows\n+ * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be pushed\n+ * down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n+ * {@link InlineDataSource} to construct an {@link org.apache.druid.segment.join.table.IndexedTable} on the fly on the\n+ * broker. Because it is also a {@link TableDataSource}, when queried directly, or on the left hand side of a join,\n+ * they will be treated as any normal segment.\n+ */\n+@JsonTypeName(\"global\")\n+public class GlobalTableDataSource extends TableDataSource\n+{\n+  @JsonCreator\n+  public GlobalTableDataSource(@JsonProperty(\"name\") String name)\n+  {\n+    super(name);\n+  }\n+\n+  @Override\n+  public boolean isCacheable()\n+  {\n+    return false;", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODU5Nw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818597", "bodyText": "I think it should be, changed.", "author": "clintropolis", "createdAt": "2020-06-16T12:42:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMDkzOQ=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\nindex 7e9048df41..da5f1390ca 100644\n--- a/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/GlobalTableDataSource.java\n\n@@ -27,13 +27,13 @@ import com.fasterxml.jackson.annotation.JsonTypeName;\n  * {@link TableDataSource} variant for globally available 'broadcast' segments. If bound to a\n  * {@link org.apache.druid.segment.join.JoinableFactory} that can create an\n  * {@link org.apache.druid.segment.join.table.IndexedTable} using DruidBinders.joinableFactoryBinder, this allows\n- * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be pushed\n- * down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n+ * optimal usage of segments using this DataSource type in join operations (because they are global), and so can be\n+ * pushed down to historicals as a {@link JoinDataSource}, instead of requiring a subquery join using\n  * {@link InlineDataSource} to construct an {@link org.apache.druid.segment.join.table.IndexedTable} on the fly on the\n  * broker. Because it is also a {@link TableDataSource}, when queried directly, or on the left hand side of a join,\n- * they will be treated as any normal segment.\n+ * they will be treated as any normal table datasource.\n  */\n-@JsonTypeName(\"global\")\n+@JsonTypeName(\"globalTable\")\n public class GlobalTableDataSource extends TableDataSource\n {\n   @JsonCreator\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTU1OQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440601559", "bodyText": "I think you can make this a little less gross by replacing this equals impl (and the one in GlobalTableDataSource) with a new auto-generated one that checks getClass.", "author": "gianm", "createdAt": "2020-06-16T05:53:49Z", "path": "processing/src/main/java/org/apache/druid/query/TableDataSource.java", "diffHunk": "@@ -102,6 +102,11 @@ public final boolean equals(Object o)\n       return false;\n     }\n \n+    if ((o instanceof GlobalTableDataSource || this instanceof GlobalTableDataSource) &&\n+        !getClass().equals(o.getClass())) {", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODUzNA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818534", "bodyText": "I haven't made this change yet, still considering the best way and need to think about it. LegacyDataSource is also a TableDataSource and they need to be equal to each other, so I think something somewhere is going to be gross...", "author": "clintropolis", "createdAt": "2020-06-16T12:42:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwMjY5Mg==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441102692", "bodyText": "#10037 removes LegacyDataSource (inspired by this thread).", "author": "gianm", "createdAt": "2020-06-16T19:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTU1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTE2MjA4Nw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441162087", "bodyText": "generated equals and hashcode for TableDataSource after ^ was merged", "author": "clintropolis", "createdAt": "2020-06-16T21:49:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTU1OQ=="}], "type": "inlineReview", "revised_code": {"commit": "5f0e4c2096540f70f3235567ae9e3c8399f61d57", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/query/TableDataSource.java b/processing/src/main/java/org/apache/druid/query/TableDataSource.java\nindex 917545d894..469d5be217 100644\n--- a/processing/src/main/java/org/apache/druid/query/TableDataSource.java\n+++ b/processing/src/main/java/org/apache/druid/query/TableDataSource.java\n\n@@ -93,32 +100,21 @@ public class TableDataSource implements DataSource\n   }\n \n   @Override\n-  public final boolean equals(Object o)\n+  public boolean equals(Object o)\n   {\n     if (this == o) {\n       return true;\n     }\n-    if (!(o instanceof TableDataSource)) {\n-      return false;\n-    }\n-\n-    if ((o instanceof GlobalTableDataSource || this instanceof GlobalTableDataSource) &&\n-        !getClass().equals(o.getClass())) {\n+    if (o == null || getClass() != o.getClass()) {\n       return false;\n     }\n-\n     TableDataSource that = (TableDataSource) o;\n-\n-    if (!name.equals(that.name)) {\n-      return false;\n-    }\n-\n-    return true;\n+    return name.equals(that.name);\n   }\n \n   @Override\n-  public final int hashCode()\n+  public int hashCode()\n   {\n-    return name.hashCode();\n+    return Objects.hash(name);\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTcwMA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440601700", "bodyText": "Please add a test for nonequality with a TableDataSource of the same name.", "author": "gianm", "createdAt": "2020-06-16T05:54:18Z", "path": "processing/src/test/java/org/apache/druid/query/GlobalTableDataSourceTest.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.query;\n+\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import nl.jqno.equalsverifier.EqualsVerifier;\n+import org.apache.druid.segment.TestHelper;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class GlobalTableDataSourceTest\n+{\n+  private static final GlobalTableDataSource GLOBAL_TABLE_DATA_SOURCE = new GlobalTableDataSource(\"foo\");\n+\n+  @Test\n+  public void testEquals()", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODQ4NA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818484", "bodyText": "added", "author": "clintropolis", "createdAt": "2020-06-16T12:42:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMTcwMA=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/query/GlobalTableDataSourceTest.java b/processing/src/test/java/org/apache/druid/query/GlobalTableDataSourceTest.java\nindex bd9c478212..fea379015a 100644\n--- a/processing/src/test/java/org/apache/druid/query/GlobalTableDataSourceTest.java\n+++ b/processing/src/test/java/org/apache/druid/query/GlobalTableDataSourceTest.java\n\n@@ -40,15 +40,17 @@ public class GlobalTableDataSourceTest\n   }\n \n   @Test\n-  public void testIsGlobal()\n+  public void testGlobalTableIsNotEqualsTable()\n   {\n-    Assert.assertTrue(GLOBAL_TABLE_DATA_SOURCE.isGlobal());\n+    TableDataSource tbl = new TableDataSource(GLOBAL_TABLE_DATA_SOURCE.getName());\n+    Assert.assertNotEquals(GLOBAL_TABLE_DATA_SOURCE, tbl);\n+    Assert.assertNotEquals(tbl, GLOBAL_TABLE_DATA_SOURCE);\n   }\n \n   @Test\n-  public void testIsCacheable()\n+  public void testIsGlobal()\n   {\n-    Assert.assertFalse(GLOBAL_TABLE_DATA_SOURCE.isCacheable());\n+    Assert.assertTrue(GLOBAL_TABLE_DATA_SOURCE.isGlobal());\n   }\n \n   @Test\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjEwNA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440602104", "bodyText": "broadcastDataSources is more consistent spelling. Please add a comment too.", "author": "gianm", "createdAt": "2020-06-16T05:55:22Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -122,6 +128,8 @@\n   // All dataSources that need tables regenerated.\n   private final Set<String> dataSourcesNeedingRebuild = new HashSet<>();\n \n+  private final Set<String> broadcastDatasources = new HashSet<>();", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMzQ5Ng==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440603496", "bodyText": "This should be @GuardedBy(\"lock\"), and so should dataSourcesNeedingRebuild, mutableSegments, segmentsNeedingRefresh, refreshImmediately, lastRefresh, lastFailure, and isServerViewInitialized.\nCould you please add those, and also remove the comment on lock, which is woefully out of date. (Thanks in advance for the housekeeping work.)", "author": "gianm", "createdAt": "2020-06-16T05:59:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODQyNQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818425", "bodyText": "added annotations, though I actually ended up removing this field in a refactor", "author": "clintropolis", "createdAt": "2020-06-16T12:42:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwMzYzMQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441103631", "bodyText": "Thanks.", "author": "gianm", "createdAt": "2020-06-16T19:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjEwNA=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 9fae29a704..6762aaf13c 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -123,22 +120,27 @@ public class DruidSchema extends AbstractSchema\n   private int totalSegments = 0;\n \n   // All mutable segments.\n+  @GuardedBy(\"lock\")\n   private final Set<SegmentId> mutableSegments = new TreeSet<>(SEGMENT_ORDER);\n \n   // All dataSources that need tables regenerated.\n+  @GuardedBy(\"lock\")\n   private final Set<String> dataSourcesNeedingRebuild = new HashSet<>();\n \n-  private final Set<String> broadcastDatasources = new HashSet<>();\n-\n   // All segments that need to be refreshed.\n+  @GuardedBy(\"lock\")\n   private final TreeSet<SegmentId> segmentsNeedingRefresh = new TreeSet<>(SEGMENT_ORDER);\n \n   // Escalator, so we can attach an authentication result to queries we generate.\n   private final Escalator escalator;\n \n+  @GuardedBy(\"lock\")\n   private boolean refreshImmediately = false;\n+  @GuardedBy(\"lock\")\n   private long lastRefresh = 0L;\n+  @GuardedBy(\"lock\")\n   private long lastFailure = 0L;\n+  @GuardedBy(\"lock\")\n   private boolean isServerViewInitialized = false;\n \n   @Inject\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjYyNw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440602627", "bodyText": "Why not do this as part of the loop in the main thread?", "author": "gianm", "createdAt": "2020-06-16T05:57:07Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -196,119 +207,132 @@ public DruidSchema(\n   public void start() throws InterruptedException\n   {\n     cacheExec.submit(\n-        new Runnable()\n-        {\n-          @Override\n-          public void run()\n-          {\n-            try {\n-              while (!Thread.currentThread().isInterrupted()) {\n-                final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n-                final Set<String> dataSourcesToRebuild = new TreeSet<>();\n-\n-                try {\n-                  synchronized (lock) {\n-                    final long nextRefreshNoFuzz = DateTimes\n-                        .utc(lastRefresh)\n-                        .plus(config.getMetadataRefreshPeriod())\n-                        .getMillis();\n-\n-                    // Fuzz a bit to spread load out when we have multiple brokers.\n-                    final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n-\n-                    while (true) {\n-                      // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n-                      final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n-                                                                .plus(config.getMetadataRefreshPeriod())\n-                                                                .isAfterNow();\n-\n-                      if (isServerViewInitialized &&\n-                          !wasRecentFailure &&\n-                          (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n-                          (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n-                        // We need to do a refresh. Break out of the waiting loop.\n-                        break;\n-                      }\n-\n-                      if (isServerViewInitialized) {\n-                        // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n-                        // no segments in the system yet. Just mark us as initialized, then.\n-                        initialized.countDown();\n-                      }\n-\n-                      // Wait some more, we'll wake up when it might be time to do another refresh.\n-                      lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n+        () -> {\n+          try {\n+            while (!Thread.currentThread().isInterrupted()) {\n+              final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n+              final Set<String> dataSourcesToRebuild = new TreeSet<>();\n+\n+              try {\n+                synchronized (lock) {\n+                  final long nextRefreshNoFuzz = DateTimes\n+                      .utc(lastRefresh)\n+                      .plus(config.getMetadataRefreshPeriod())\n+                      .getMillis();\n+\n+                  // Fuzz a bit to spread load out when we have multiple brokers.\n+                  final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n+\n+                  while (true) {\n+                    // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n+                    final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n+                                                              .plus(config.getMetadataRefreshPeriod())\n+                                                              .isAfterNow();\n+\n+                    if (isServerViewInitialized &&\n+                        !wasRecentFailure &&\n+                        (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n+                        (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n+                      // We need to do a refresh. Break out of the waiting loop.\n+                      break;\n                     }\n \n-                    segmentsToRefresh.addAll(segmentsNeedingRefresh);\n-                    segmentsNeedingRefresh.clear();\n-\n-                    // Mutable segments need a refresh every period, since new columns could be added dynamically.\n-                    segmentsNeedingRefresh.addAll(mutableSegments);\n+                    if (isServerViewInitialized) {\n+                      // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n+                      // no segments in the system yet. Just mark us as initialized, then.\n+                      initialized.countDown();\n+                    }\n \n-                    lastFailure = 0L;\n-                    lastRefresh = System.currentTimeMillis();\n-                    refreshImmediately = false;\n+                    // Wait some more, we'll wake up when it might be time to do another refresh.\n+                    lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n                   }\n \n-                  // Refresh the segments.\n-                  final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n+                  segmentsToRefresh.addAll(segmentsNeedingRefresh);\n+                  segmentsNeedingRefresh.clear();\n \n-                  synchronized (lock) {\n-                    // Add missing segments back to the refresh list.\n-                    segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n+                  // Mutable segments need a refresh every period, since new columns could be added dynamically.\n+                  segmentsNeedingRefresh.addAll(mutableSegments);\n \n-                    // Compute the list of dataSources to rebuild tables for.\n-                    dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n-                    refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n-                    dataSourcesNeedingRebuild.clear();\n+                  lastFailure = 0L;\n+                  lastRefresh = System.currentTimeMillis();\n+                  refreshImmediately = false;\n+                }\n \n-                    lock.notifyAll();\n-                  }\n+                // Refresh the segments.\n+                final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n \n-                  // Rebuild the dataSources.\n-                  for (String dataSource : dataSourcesToRebuild) {\n-                    final DruidTable druidTable = buildDruidTable(dataSource);\n-                    final DruidTable oldTable = tables.put(dataSource, druidTable);\n-                    if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n-                      log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n-                    } else {\n-                      log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n-                    }\n-                  }\n+                synchronized (lock) {\n+                  // Add missing segments back to the refresh list.\n+                  segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n \n-                  initialized.countDown();\n-                }\n-                catch (InterruptedException e) {\n-                  // Fall through.\n-                  throw e;\n+                  // Compute the list of dataSources to rebuild tables for.\n+                  dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n+                  refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n+                  dataSourcesNeedingRebuild.clear();\n+\n+                  lock.notifyAll();\n                 }\n-                catch (Exception e) {\n-                  log.warn(e, \"Metadata refresh failed, trying again soon.\");\n-\n-                  synchronized (lock) {\n-                    // Add our segments and dataSources back to their refresh and rebuild lists.\n-                    segmentsNeedingRefresh.addAll(segmentsToRefresh);\n-                    dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n-                    lastFailure = System.currentTimeMillis();\n-                    lock.notifyAll();\n+\n+                // Rebuild the dataSources.\n+                for (String dataSource : dataSourcesToRebuild) {\n+                  final DruidTable druidTable = buildDruidTable(dataSource);\n+                  final DruidTable oldTable = tables.put(dataSource, druidTable);\n+                  if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n+                    log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n+                  } else {\n+                    log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n                   }\n                 }\n+\n+                initialized.countDown();\n+              }\n+              catch (InterruptedException e) {\n+                // Fall through.\n+                throw e;\n+              }\n+              catch (Exception e) {\n+                log.warn(e, \"Metadata refresh failed, trying again soon.\");\n+\n+                synchronized (lock) {\n+                  // Add our segments and dataSources back to their refresh and rebuild lists.\n+                  segmentsNeedingRefresh.addAll(segmentsToRefresh);\n+                  dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n+                  lastFailure = System.currentTimeMillis();\n+                  lock.notifyAll();\n+                }\n               }\n-            }\n-            catch (InterruptedException e) {\n-              // Just exit.\n-            }\n-            catch (Throwable e) {\n-              // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n-              // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n-              log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n-              throw e;\n-            }\n-            finally {\n-              log.info(\"Metadata refresh stopped.\");\n             }\n           }\n+          catch (InterruptedException e) {\n+            // Just exit.\n+          }\n+          catch (Throwable e) {\n+            // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n+            // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n+            log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n+            throw e;\n+          }\n+          finally {\n+            log.info(\"Metadata refresh stopped.\");\n+          }\n+        }\n+    );\n+\n+    ScheduledExecutors.scheduleWithFixedDelay(", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODM5NA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818394", "bodyText": "Removed this executor entirely in a refactor", "author": "clintropolis", "createdAt": "2020-06-16T12:41:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwMzc0MA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441103740", "bodyText": "Great.", "author": "gianm", "createdAt": "2020-06-16T19:52:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjYyNw=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 9fae29a704..6762aaf13c 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -318,24 +319,6 @@ public class DruidSchema extends AbstractSchema\n         }\n     );\n \n-    ScheduledExecutors.scheduleWithFixedDelay(\n-        localSegmentExec,\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        () -> {\n-          synchronized (lock) {\n-            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n-            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n-            // future by fetching load rules from the coordinator\n-            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n-            dataSourcesNeedingRebuild.addAll(localSegmentDatasources);\n-            broadcastDatasources.clear();\n-            broadcastDatasources.addAll(localSegmentDatasources);\n-            lock.notifyAll();\n-          }\n-        }\n-    );\n-\n     if (config.isAwaitInitializationOnStart()) {\n       final long startNanos = System.nanoTime();\n       log.debug(\"%s waiting for initialization.\", getClass().getSimpleName());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjc1NA==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440602754", "bodyText": "Why do we need to rebuild them all continuously?", "author": "gianm", "createdAt": "2020-06-16T05:57:31Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -196,119 +207,132 @@ public DruidSchema(\n   public void start() throws InterruptedException\n   {\n     cacheExec.submit(\n-        new Runnable()\n-        {\n-          @Override\n-          public void run()\n-          {\n-            try {\n-              while (!Thread.currentThread().isInterrupted()) {\n-                final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n-                final Set<String> dataSourcesToRebuild = new TreeSet<>();\n-\n-                try {\n-                  synchronized (lock) {\n-                    final long nextRefreshNoFuzz = DateTimes\n-                        .utc(lastRefresh)\n-                        .plus(config.getMetadataRefreshPeriod())\n-                        .getMillis();\n-\n-                    // Fuzz a bit to spread load out when we have multiple brokers.\n-                    final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n-\n-                    while (true) {\n-                      // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n-                      final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n-                                                                .plus(config.getMetadataRefreshPeriod())\n-                                                                .isAfterNow();\n-\n-                      if (isServerViewInitialized &&\n-                          !wasRecentFailure &&\n-                          (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n-                          (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n-                        // We need to do a refresh. Break out of the waiting loop.\n-                        break;\n-                      }\n-\n-                      if (isServerViewInitialized) {\n-                        // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n-                        // no segments in the system yet. Just mark us as initialized, then.\n-                        initialized.countDown();\n-                      }\n-\n-                      // Wait some more, we'll wake up when it might be time to do another refresh.\n-                      lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n+        () -> {\n+          try {\n+            while (!Thread.currentThread().isInterrupted()) {\n+              final Set<SegmentId> segmentsToRefresh = new TreeSet<>();\n+              final Set<String> dataSourcesToRebuild = new TreeSet<>();\n+\n+              try {\n+                synchronized (lock) {\n+                  final long nextRefreshNoFuzz = DateTimes\n+                      .utc(lastRefresh)\n+                      .plus(config.getMetadataRefreshPeriod())\n+                      .getMillis();\n+\n+                  // Fuzz a bit to spread load out when we have multiple brokers.\n+                  final long nextRefresh = nextRefreshNoFuzz + (long) ((nextRefreshNoFuzz - lastRefresh) * 0.10);\n+\n+                  while (true) {\n+                    // Do not refresh if it's too soon after a failure (to avoid rapid cycles of failure).\n+                    final boolean wasRecentFailure = DateTimes.utc(lastFailure)\n+                                                              .plus(config.getMetadataRefreshPeriod())\n+                                                              .isAfterNow();\n+\n+                    if (isServerViewInitialized &&\n+                        !wasRecentFailure &&\n+                        (!segmentsNeedingRefresh.isEmpty() || !dataSourcesNeedingRebuild.isEmpty()) &&\n+                        (refreshImmediately || nextRefresh < System.currentTimeMillis())) {\n+                      // We need to do a refresh. Break out of the waiting loop.\n+                      break;\n                     }\n \n-                    segmentsToRefresh.addAll(segmentsNeedingRefresh);\n-                    segmentsNeedingRefresh.clear();\n-\n-                    // Mutable segments need a refresh every period, since new columns could be added dynamically.\n-                    segmentsNeedingRefresh.addAll(mutableSegments);\n+                    if (isServerViewInitialized) {\n+                      // Server view is initialized, but we don't need to do a refresh. Could happen if there are\n+                      // no segments in the system yet. Just mark us as initialized, then.\n+                      initialized.countDown();\n+                    }\n \n-                    lastFailure = 0L;\n-                    lastRefresh = System.currentTimeMillis();\n-                    refreshImmediately = false;\n+                    // Wait some more, we'll wake up when it might be time to do another refresh.\n+                    lock.wait(Math.max(1, nextRefresh - System.currentTimeMillis()));\n                   }\n \n-                  // Refresh the segments.\n-                  final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n+                  segmentsToRefresh.addAll(segmentsNeedingRefresh);\n+                  segmentsNeedingRefresh.clear();\n \n-                  synchronized (lock) {\n-                    // Add missing segments back to the refresh list.\n-                    segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n+                  // Mutable segments need a refresh every period, since new columns could be added dynamically.\n+                  segmentsNeedingRefresh.addAll(mutableSegments);\n \n-                    // Compute the list of dataSources to rebuild tables for.\n-                    dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n-                    refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n-                    dataSourcesNeedingRebuild.clear();\n+                  lastFailure = 0L;\n+                  lastRefresh = System.currentTimeMillis();\n+                  refreshImmediately = false;\n+                }\n \n-                    lock.notifyAll();\n-                  }\n+                // Refresh the segments.\n+                final Set<SegmentId> refreshed = refreshSegments(segmentsToRefresh);\n \n-                  // Rebuild the dataSources.\n-                  for (String dataSource : dataSourcesToRebuild) {\n-                    final DruidTable druidTable = buildDruidTable(dataSource);\n-                    final DruidTable oldTable = tables.put(dataSource, druidTable);\n-                    if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n-                      log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n-                    } else {\n-                      log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n-                    }\n-                  }\n+                synchronized (lock) {\n+                  // Add missing segments back to the refresh list.\n+                  segmentsNeedingRefresh.addAll(Sets.difference(segmentsToRefresh, refreshed));\n \n-                  initialized.countDown();\n-                }\n-                catch (InterruptedException e) {\n-                  // Fall through.\n-                  throw e;\n+                  // Compute the list of dataSources to rebuild tables for.\n+                  dataSourcesToRebuild.addAll(dataSourcesNeedingRebuild);\n+                  refreshed.forEach(segment -> dataSourcesToRebuild.add(segment.getDataSource()));\n+                  dataSourcesNeedingRebuild.clear();\n+\n+                  lock.notifyAll();\n                 }\n-                catch (Exception e) {\n-                  log.warn(e, \"Metadata refresh failed, trying again soon.\");\n-\n-                  synchronized (lock) {\n-                    // Add our segments and dataSources back to their refresh and rebuild lists.\n-                    segmentsNeedingRefresh.addAll(segmentsToRefresh);\n-                    dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n-                    lastFailure = System.currentTimeMillis();\n-                    lock.notifyAll();\n+\n+                // Rebuild the dataSources.\n+                for (String dataSource : dataSourcesToRebuild) {\n+                  final DruidTable druidTable = buildDruidTable(dataSource);\n+                  final DruidTable oldTable = tables.put(dataSource, druidTable);\n+                  if (oldTable == null || !oldTable.getRowSignature().equals(druidTable.getRowSignature())) {\n+                    log.info(\"dataSource [%s] has new signature: %s.\", dataSource, druidTable.getRowSignature());\n+                  } else {\n+                    log.debug(\"dataSource [%s] signature is unchanged.\", dataSource);\n                   }\n                 }\n+\n+                initialized.countDown();\n+              }\n+              catch (InterruptedException e) {\n+                // Fall through.\n+                throw e;\n+              }\n+              catch (Exception e) {\n+                log.warn(e, \"Metadata refresh failed, trying again soon.\");\n+\n+                synchronized (lock) {\n+                  // Add our segments and dataSources back to their refresh and rebuild lists.\n+                  segmentsNeedingRefresh.addAll(segmentsToRefresh);\n+                  dataSourcesNeedingRebuild.addAll(dataSourcesToRebuild);\n+                  lastFailure = System.currentTimeMillis();\n+                  lock.notifyAll();\n+                }\n               }\n-            }\n-            catch (InterruptedException e) {\n-              // Just exit.\n-            }\n-            catch (Throwable e) {\n-              // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n-              // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n-              log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n-              throw e;\n-            }\n-            finally {\n-              log.info(\"Metadata refresh stopped.\");\n             }\n           }\n+          catch (InterruptedException e) {\n+            // Just exit.\n+          }\n+          catch (Throwable e) {\n+            // Throwables that fall out to here (not caught by an inner try/catch) are potentially gnarly, like\n+            // OOMEs. Anyway, let's just emit an alert and stop refreshing metadata.\n+            log.makeAlert(e, \"Metadata refresh failed permanently\").emit();\n+            throw e;\n+          }\n+          finally {\n+            log.info(\"Metadata refresh stopped.\");\n+          }\n+        }\n+    );\n+\n+    ScheduledExecutors.scheduleWithFixedDelay(\n+        localSegmentExec,\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        config.getMetadataRefreshPeriod().toStandardDuration(),\n+        () -> {\n+          synchronized (lock) {\n+            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n+            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n+            // future by fetching load rules from the coordinator\n+            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n+            dataSourcesNeedingRebuild.addAll(localSegmentDatasources);", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODI2MQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818261", "bodyText": "We don't really, I've refactored this pretty heavily.\nI reworked the fix I did for #10017, to instead now preserve the segment/timeline event callbacks from the BrokerServerView (but still not add the server/segment to the timeline to avoid the weird loops), which in turn allows DruidSchema get these events for broker segments to mark datasources to be rebuilt in the normal path (just skipping metadata fetch on the assumption that the segment will appear, so that the existing loop handles these changes correctly.", "author": "clintropolis", "createdAt": "2020-06-16T12:41:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjc1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwNDIwNg==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441104206", "bodyText": "OK, I will take another look.", "author": "gianm", "createdAt": "2020-06-16T19:52:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMjc1NA=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 9fae29a704..6762aaf13c 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -318,24 +319,6 @@ public class DruidSchema extends AbstractSchema\n         }\n     );\n \n-    ScheduledExecutors.scheduleWithFixedDelay(\n-        localSegmentExec,\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        config.getMetadataRefreshPeriod().toStandardDuration(),\n-        () -> {\n-          synchronized (lock) {\n-            // refresh known broadcast segments. Since DruidSchema is only present on the broker, any segment we have\n-            // locally in the SegmentManager must be broadcast datasources. This could potentially be replaced in the\n-            // future by fetching load rules from the coordinator\n-            Set<String> localSegmentDatasources = segmentManager.getDataSourceNames();\n-            dataSourcesNeedingRebuild.addAll(localSegmentDatasources);\n-            broadcastDatasources.clear();\n-            broadcastDatasources.addAll(localSegmentDatasources);\n-            lock.notifyAll();\n-          }\n-        }\n-    );\n-\n     if (config.isAwaitInitializationOnStart()) {\n       final long startNanos = System.nanoTime();\n       log.debug(\"%s waiting for initialization.\", getClass().getSimpleName());\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMzI4NQ==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440603285", "bodyText": "tiny nit: the logic is laid out a bit weirdly here; it'd make more sense to emphasize what's different by having the dataSource be created in the if block, but the DruidTable created outside of it.", "author": "gianm", "createdAt": "2020-06-16T05:59:17Z", "path": "sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java", "diffHunk": "@@ -616,6 +631,12 @@ private DruidTable buildDruidTable(final String dataSource)\n \n       final RowSignature.Builder builder = RowSignature.builder();\n       columnTypes.forEach(builder::add);\n+      if (broadcastDatasources.contains(dataSource)) {", "originalCommit": "9d7a9b6fce0b4d8ba4f9e7f4321aa6eb24458f01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDgxODMzNg==", "url": "https://github.com/apache/druid/pull/10020#discussion_r440818336", "bodyText": "modified", "author": "clintropolis", "createdAt": "2020-06-16T12:41:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMzI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwNDI1Nw==", "url": "https://github.com/apache/druid/pull/10020#discussion_r441104257", "bodyText": "\ud83c\udd92", "author": "gianm", "createdAt": "2020-06-16T19:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDYwMzI4NQ=="}], "type": "inlineReview", "revised_code": {"commit": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "chunk": "diff --git a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\nindex 9fae29a704..6762aaf13c 100644\n--- a/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n+++ b/sql/src/main/java/org/apache/druid/sql/calcite/schema/DruidSchema.java\n\n@@ -631,13 +625,14 @@ public class DruidSchema extends AbstractSchema\n \n       final RowSignature.Builder builder = RowSignature.builder();\n       columnTypes.forEach(builder::add);\n-      if (broadcastDatasources.contains(dataSource)) {\n-        return new DruidTable(\n-            new GlobalTableDataSource(dataSource),\n-            builder.build()\n-        );\n+\n+      final TableDataSource tableDataSource;\n+      if (segmentManager.getDataSourceNames().contains(dataSource)) {\n+        tableDataSource = new GlobalTableDataSource(dataSource);\n+      } else {\n+        tableDataSource = new TableDataSource(dataSource);\n       }\n-      return new DruidTable(new TableDataSource(dataSource), builder.build());\n+      return new DruidTable(tableDataSource, builder.build());\n     }\n   }\n \n"}}, {"oid": "7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "url": "https://github.com/apache/druid/commit/7c5067460dd691ba3e090af67fa6da8fe05e3ddc", "message": "review stuffs", "committedDate": "2020-06-16T12:18:59Z", "type": "commit"}, {"oid": "a7d3443c55b9c76d914b9e3f49f85f3911462937", "url": "https://github.com/apache/druid/commit/a7d3443c55b9c76d914b9e3f49f85f3911462937", "message": "Merge remote-tracking branch 'upstream/master' into global-table-for-broadcast-segments", "committedDate": "2020-06-16T21:41:12Z", "type": "commit"}, {"oid": "5f0e4c2096540f70f3235567ae9e3c8399f61d57", "url": "https://github.com/apache/druid/commit/5f0e4c2096540f70f3235567ae9e3c8399f61d57", "message": "use generated equals and hashcode", "committedDate": "2020-06-16T21:46:19Z", "type": "commit"}]}