{"pr_number": 10288, "pr_title": "Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided", "pr_createdAt": "2020-08-15T04:06:13Z", "pr_url": "https://github.com/apache/druid/pull/10288", "timeline": [{"oid": "e488894a94139c60ee1a8416ff954d24c44f7786", "url": "https://github.com/apache/druid/commit/e488894a94139c60ee1a8416ff954d24c44f7786", "message": "Store hash partition function in dataSegment and allow segment pruning only when hash partition function is provided", "committedDate": "2020-08-15T03:45:53Z", "type": "commit"}, {"oid": "ac812f853b3ac75cd8ee22212e657d8559ac8231", "url": "https://github.com/apache/druid/commit/ac812f853b3ac75cd8ee22212e657d8559ac8231", "message": "query context", "committedDate": "2020-08-15T03:58:46Z", "type": "commit"}, {"oid": "6975c18b47e6e87baed5b9228042790b9fbd1002", "url": "https://github.com/apache/druid/commit/6975c18b47e6e87baed5b9228042790b9fbd1002", "message": "fix tests; add more test", "committedDate": "2020-08-15T16:25:15Z", "type": "commit"}, {"oid": "22e4ede88a1531c261d9e9f3df3fac9b880b5d4e", "url": "https://github.com/apache/druid/commit/22e4ede88a1531c261d9e9f3df3fac9b880b5d4e", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function", "committedDate": "2020-09-15T00:43:51Z", "type": "commit"}, {"oid": "1ad79133125547c3ddca8e4b787ca2384c18876a", "url": "https://github.com/apache/druid/commit/1ad79133125547c3ddca8e4b787ca2384c18876a", "message": "javadoc", "committedDate": "2020-09-16T01:51:48Z", "type": "commit"}, {"oid": "31bf9a2ce9d772d6512b1567d8544ef54bd4451f", "url": "https://github.com/apache/druid/commit/31bf9a2ce9d772d6512b1567d8544ef54bd4451f", "message": "docs and more tests", "committedDate": "2020-09-17T01:31:47Z", "type": "commit"}, {"oid": "e138d501ac6ee85a3fd5d6cd069a832e90c71c72", "url": "https://github.com/apache/druid/commit/e138d501ac6ee85a3fd5d6cd069a832e90c71c72", "message": "remove default and hadoop tests", "committedDate": "2020-09-17T02:01:34Z", "type": "commit"}, {"oid": "003bf3bbc459caee95f373f5084cfdb22fa872da", "url": "https://github.com/apache/druid/commit/003bf3bbc459caee95f373f5084cfdb22fa872da", "message": "consistent name and fix javadoc", "committedDate": "2020-09-17T02:09:35Z", "type": "commit"}, {"oid": "a5b78ee302b8f0cb57bca03d3a0c4b763355a9e3", "url": "https://github.com/apache/druid/commit/a5b78ee302b8f0cb57bca03d3a0c4b763355a9e3", "message": "spelling and field name", "committedDate": "2020-09-17T05:55:37Z", "type": "commit"}, {"oid": "2d437b67b05a038813a120c245dcd610bb32cf85", "url": "https://github.com/apache/druid/commit/2d437b67b05a038813a120c245dcd610bb32cf85", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function", "committedDate": "2020-09-17T06:03:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MTgwMw==", "url": "https://github.com/apache/druid/pull/10288#discussion_r491751803", "bodyText": "it might be worth mentioning why this class should only be used for ingestion so it isn't left as an exercise for the reader", "author": "clintropolis", "createdAt": "2020-09-21T00:08:08Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.databind.ObjectMapper;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.druid.data.input.InputRow;\n+import org.apache.druid.data.input.Rows;\n+\n+import javax.annotation.Nullable;\n+import java.util.List;\n+\n+/**\n+ * This class is used for hash partitioning during ingestion. The {@link ShardSpecLookup} returned from\n+ * {@link #createHashLookup} is used to determine what hash bucket the given input row will belong to.\n+ *\n+ * Note: this class must be used only for ingestion. For segment pruning at query time,\n+ * {@link HashBasedNumberedShardSpec#partitionFunction} should be used instead.", "originalCommit": "2d437b67b05a038813a120c245dcd610bb32cf85", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODczMg==", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438732", "bodyText": "Hmm, the reason I added this note was that the partition function could be null before and HashPartitioner uses a default partition function when it's null unlike in query. So, if it was used in a query, then segment pruning could be done based on a wrong default. However, now, the partition function can never be null in ingestion tasks, HashPartitioner doesn't have to be used only in ingestion even though it is for now. I just removed this note.", "author": "jihoonson", "createdAt": "2020-09-22T02:02:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MTgwMw=="}], "type": "inlineReview", "revised_code": {"commit": "9c39f1cebc04e0ad334968d171e5d220d4a8087b", "chunk": "diff --git a/core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java b/core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java\nindex 0e1ae75a23..c5e342aaee 100644\n--- a/core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java\n+++ b/core/src/main/java/org/apache/druid/timeline/partition/HashPartitioner.java\n\n@@ -26,15 +26,11 @@ import com.google.common.collect.Lists;\n import org.apache.druid.data.input.InputRow;\n import org.apache.druid.data.input.Rows;\n \n-import javax.annotation.Nullable;\n import java.util.List;\n \n /**\n  * This class is used for hash partitioning during ingestion. The {@link ShardSpecLookup} returned from\n  * {@link #createHashLookup} is used to determine what hash bucket the given input row will belong to.\n- *\n- * Note: this class must be used only for ingestion. For segment pruning at query time,\n- * {@link HashBasedNumberedShardSpec#partitionFunction} should be used instead.\n  */\n class HashPartitioner\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjM0MQ==", "url": "https://github.com/apache/druid/pull/10288#discussion_r491752341", "bodyText": "should we also stress here that anything added here must be backwards compatible, forever?", "author": "clintropolis", "createdAt": "2020-09-21T00:13:00Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.timeline.partition;\n+\n+import com.fasterxml.jackson.annotation.JsonCreator;\n+import com.fasterxml.jackson.annotation.JsonValue;\n+import com.google.common.hash.Hashing;\n+import org.apache.druid.java.util.common.StringUtils;\n+\n+/**\n+ * An enum of supported hash partition functions. This enum should be updated when we want to use a new function\n+ * for hash partitioning. This function is a part of {@link HashBasedNumberedShardSpec} which is stored\n+ * in the metadata store.", "originalCommit": "2d437b67b05a038813a120c245dcd610bb32cf85", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODc2NA==", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438764", "bodyText": "Sounds good. Added.", "author": "jihoonson", "createdAt": "2020-09-22T02:02:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTc1MjM0MQ=="}], "type": "inlineReview", "revised_code": {"commit": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "chunk": "diff --git a/core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java b/core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java\nindex 3543a566bc..f6cccc69f4 100644\n--- a/core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java\n+++ b/core/src/main/java/org/apache/druid/timeline/partition/HashPartitionFunction.java\n\n@@ -26,8 +26,9 @@ import org.apache.druid.java.util.common.StringUtils;\n \n /**\n  * An enum of supported hash partition functions. This enum should be updated when we want to use a new function\n- * for hash partitioning. This function is a part of {@link HashBasedNumberedShardSpec} which is stored\n- * in the metadata store.\n+ * for hash partitioning. All partition functions listed in this enum must be backwards-compatible as the hash\n+ * function should apply to all segments in the same way no matter what Druid version was used to create those segments.\n+ * This function is a part of {@link HashBasedNumberedShardSpec} which is stored in the metadata store.\n  */\n public enum HashPartitionFunction\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4NDIxMg==", "url": "https://github.com/apache/druid/pull/10288#discussion_r491984212", "bodyText": "is there any reason not to set the default in the shard specs used for making new segments?", "author": "clintropolis", "createdAt": "2020-09-21T11:53:46Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java", "diffHunk": "@@ -58,6 +61,7 @@ public BuildingHashBasedNumberedShardSpec(\n     this.partitionDimensions = partitionDimensions == null\n                                ? HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS\n                                : partitionDimensions;\n+    this.partitionFunction = partitionFunction;", "originalCommit": "2d437b67b05a038813a120c245dcd610bb32cf85", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjQzODc3OQ==", "url": "https://github.com/apache/druid/pull/10288#discussion_r492438779", "bodyText": "Good point. I thought it will break some things at first.. But, thinking about it more, I think having a default in partitionsSpec will be fine even though there are at least 2 issues.\n\nIn auto compaction, the last compaction state of each segment includes the partitionsSpec of the compaction task which created the segment. The auto compaction searches for all segments which don't have the last compaction state matched to the current configuration. So, for the segments compacted before, the HashedPartitionsSpec can have an empty partition function. This can be translated to different default hash functions if we ever change the default hash function. Then the auto compaction can silently ignore the changed hash function. However, this doesn't seem that bad.\nIn rolling upgrades which replace machines of an old version with ones of a new version, there can be a mixed version of middleManagers or indexers. In this case, all parallel tasks will be broken as each subtask can use different partition functions if we have changed the default partition function between those two versions. I think this will be acceptable as long as we announce it as a known issue in the release notes. (And we are not probably going to change the default partition function in the near future.)", "author": "jihoonson", "createdAt": "2020-09-22T02:02:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk4NDIxMg=="}], "type": "inlineReview", "revised_code": {"commit": "9c39f1cebc04e0ad334968d171e5d220d4a8087b", "chunk": "diff --git a/core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java b/core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java\nindex 1736d468f7..f5c43b075b 100644\n--- a/core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java\n+++ b/core/src/main/java/org/apache/druid/timeline/partition/BuildingHashBasedNumberedShardSpec.java\n\n@@ -61,7 +61,7 @@ public class BuildingHashBasedNumberedShardSpec implements BuildingShardSpec<Has\n     this.partitionDimensions = partitionDimensions == null\n                                ? HashBasedNumberedShardSpec.DEFAULT_PARTITION_DIMENSIONS\n                                : partitionDimensions;\n-    this.partitionFunction = partitionFunction;\n+    this.partitionFunction = Preconditions.checkNotNull(partitionFunction, \"partitionFunction\");\n     this.jsonMapper = jsonMapper;\n   }\n \n"}}, {"oid": "9c39f1cebc04e0ad334968d171e5d220d4a8087b", "url": "https://github.com/apache/druid/commit/9c39f1cebc04e0ad334968d171e5d220d4a8087b", "message": "default function for partitionsSpec", "committedDate": "2020-09-22T01:18:17Z", "type": "commit"}, {"oid": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "url": "https://github.com/apache/druid/commit/b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "message": "other comments", "committedDate": "2020-09-22T02:01:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAyNTk5NQ==", "url": "https://github.com/apache/druid/pull/10288#discussion_r493025995", "bodyText": "suggest rewording the second line to something like \"The current version of Druid will always specify a partitionFunction on newly created segments\"", "author": "jon-wei", "createdAt": "2020-09-22T20:51:31Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -98,141 +107,44 @@ public int getNumBuckets()\n     return partitionDimensions;\n   }\n \n-  @Override\n-  public List<String> getDomainDimensions()\n+  @JsonProperty\n+  public @Nullable HashPartitionFunction getPartitionFunction()\n   {\n-    return partitionDimensions;\n+    return partitionFunction;\n   }\n \n   @Override\n-  public boolean isInChunk(long timestamp, InputRow inputRow)\n-  {\n-    return getBucketIndex(hash(timestamp, inputRow), numBuckets) == bucketId % numBuckets;\n-  }\n-\n-  /**\n-   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n-   * are of {@code partitionDimensionsValues}\n-   *\n-   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n-   *\n-   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n-   */\n-  private boolean isInChunk(Map<String, String> partitionDimensionsValues)\n-  {\n-    assert !partitionDimensions.isEmpty();\n-    List<Object> groupKey = Lists.transform(\n-        partitionDimensions,\n-        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n-    );\n-    try {\n-      return getBucketIndex(hash(jsonMapper, groupKey), numBuckets) == bucketId % numBuckets;\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  /**\n-   * This method calculates the hash based on whether {@param partitionDimensions} is null or not.\n-   * If yes, then both {@param timestamp} and dimension columns in {@param inputRow} are used {@link Rows#toGroupKey}\n-   * Or else, columns in {@param partitionDimensions} are used\n-   *\n-   * @param timestamp should be bucketed with query granularity\n-   * @param inputRow row from input data\n-   *\n-   * @return hash value\n-   */\n-  protected int hash(long timestamp, InputRow inputRow)\n-  {\n-    return hash(jsonMapper, partitionDimensions, timestamp, inputRow);\n-  }\n-\n-  public static int hash(ObjectMapper jsonMapper, List<String> partitionDimensions, long timestamp, InputRow inputRow)\n-  {\n-    final List<Object> groupKey = getGroupKey(partitionDimensions, timestamp, inputRow);\n-    try {\n-      return hash(jsonMapper, groupKey);\n-    }\n-    catch (JsonProcessingException e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  static List<Object> getGroupKey(final List<String> partitionDimensions, final long timestamp, final InputRow inputRow)\n-  {\n-    if (partitionDimensions.isEmpty()) {\n-      return Rows.toGroupKey(timestamp, inputRow);\n-    } else {\n-      return Lists.transform(partitionDimensions, inputRow::getDimension);\n-    }\n-  }\n-\n-  @VisibleForTesting\n-  public static int hash(ObjectMapper jsonMapper, List<Object> objects) throws JsonProcessingException\n+  public List<String> getDomainDimensions()\n   {\n-    return HASH_FUNCTION.hashBytes(jsonMapper.writeValueAsBytes(objects)).asInt();\n+    return partitionDimensions;\n   }\n \n   @Override\n   public ShardSpecLookup getLookup(final List<? extends ShardSpec> shardSpecs)\n   {\n-    return createHashLookup(jsonMapper, partitionDimensions, shardSpecs, numBuckets);\n-  }\n-\n-  static ShardSpecLookup createHashLookup(\n-      ObjectMapper jsonMapper,\n-      List<String> partitionDimensions,\n-      List<? extends ShardSpec> shardSpecs,\n-      int numBuckets\n-  )\n-  {\n-    return (long timestamp, InputRow row) -> {\n-      int index = getBucketIndex(hash(jsonMapper, partitionDimensions, timestamp, row), numBuckets);\n-      return shardSpecs.get(index);\n-    };\n+    // partitionFunction can be null when you read a shardSpec of a segment created in an old version of Druid.\n+    // It can never be null for segments to create during ingestion.", "originalCommit": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "d54304a9acd2d9bcc80bb25bab9bd571d2129e2b", "chunk": "diff --git a/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java b/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java\nindex 86e7c78b8e..06f86eb260 100644\n--- a/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java\n+++ b/core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java\n\n@@ -123,7 +123,7 @@ public class HashBasedNumberedShardSpec extends NumberedShardSpec\n   public ShardSpecLookup getLookup(final List<? extends ShardSpec> shardSpecs)\n   {\n     // partitionFunction can be null when you read a shardSpec of a segment created in an old version of Druid.\n-    // It can never be null for segments to create during ingestion.\n+    // The current version of Druid will always specify a partitionFunction on newly created segments.\n     if (partitionFunction == null) {\n       throw new ISE(\"Cannot create a hashPartitioner since partitionFunction is null\");\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMzA0OA==", "url": "https://github.com/apache/druid/pull/10288#discussion_r493033048", "bodyText": "I wonder if we'll eventually need a versioning scheme that's considered by the segment pruning (like \"serialization version\"). It doesn't seem needed for now since we only have primitive + String dimension types and the serialization doesn't seem likely to change there.\nIf we had \"complex\" dimensions at some point, I could see that being difficult to support backwards compatibility for if the serialization format of a complex dim changed.", "author": "jon-wei", "createdAt": "2020-09-22T21:05:03Z", "path": "core/src/main/java/org/apache/druid/timeline/partition/HashBasedNumberedShardSpec.java", "diffHunk": "@@ -298,8 +211,73 @@ private boolean chunkPossibleInDomain(\n     return false;\n   }\n \n-  private static int getBucketIndex(int hash, int numBuckets)\n+  /**\n+   * Check if the current segment possibly holds records if the values of dimensions in {@link #partitionDimensions}\n+   * are of {@code partitionDimensionsValues}\n+   *\n+   * @param hashPartitionFunction     hash function used to create segments at ingestion time\n+   * @param partitionDimensionsValues An instance of values of dimensions in {@link #partitionDimensions}\n+   *\n+   * @return Whether the current segment possibly holds records for the given values of partition dimensions\n+   */\n+  private boolean isInChunk(HashPartitionFunction hashPartitionFunction, Map<String, String> partitionDimensionsValues)\n   {\n-    return Math.abs(hash % numBuckets);\n+    assert !partitionDimensions.isEmpty();\n+    List<Object> groupKey = Lists.transform(\n+        partitionDimensions,\n+        o -> Collections.singletonList(partitionDimensionsValues.get(o))\n+    );\n+    return hashPartitionFunction.hash(serializeGroupKey(jsonMapper, groupKey), numBuckets) == bucketId;\n+  }\n+\n+  /**\n+   * Serializes a group key into a byte array. The serialization algorithm can affect hash values of partition keys\n+   * since {@link HashPartitionFunction#hash} takes the result of this method as its input. This means, the returned\n+   * byte array should be backwards-compatible in cases where we need to modify this method.", "originalCommit": "b3c1ae6c0f1fab203e121c17e395f8e59b74cf96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA4MDc1Mg==", "url": "https://github.com/apache/druid/pull/10288#discussion_r493080752", "bodyText": "Hmm, good point. Do you see some good use cases for using complex dimensions for partitioning? If you want to benefit from secondary partition pruning, you will need to have a filter on partition dimensions. I'm not sure what the filter on complex dimensions would look like.", "author": "jihoonson", "createdAt": "2020-09-22T23:02:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzAzMzA0OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "d54304a9acd2d9bcc80bb25bab9bd571d2129e2b", "url": "https://github.com/apache/druid/commit/d54304a9acd2d9bcc80bb25bab9bd571d2129e2b", "message": "address comments", "committedDate": "2020-09-22T22:54:13Z", "type": "commit"}, {"oid": "2903072d316d69ad53ae0f6b98674378147037db", "url": "https://github.com/apache/druid/commit/2903072d316d69ad53ae0f6b98674378147037db", "message": "fix tests and spelling", "committedDate": "2020-09-22T23:02:18Z", "type": "commit"}, {"oid": "ec266e6a7a5dc614b416cc096dd33e23e1194dbf", "url": "https://github.com/apache/druid/commit/ec266e6a7a5dc614b416cc096dd33e23e1194dbf", "message": "test", "committedDate": "2020-09-23T20:15:47Z", "type": "commit"}, {"oid": "6fa121fac1cb686e33191611875cb2f7071989b9", "url": "https://github.com/apache/druid/commit/6fa121fac1cb686e33191611875cb2f7071989b9", "message": "doc", "committedDate": "2020-09-24T18:10:34Z", "type": "commit"}, {"oid": "27a15210173fe88dfc34409e05e9e4b63ddf5a02", "url": "https://github.com/apache/druid/commit/27a15210173fe88dfc34409e05e9e4b63ddf5a02", "message": "Merge branch 'master' of github.com:apache/druid into hash-partition-function", "committedDate": "2020-09-24T21:10:40Z", "type": "commit"}]}