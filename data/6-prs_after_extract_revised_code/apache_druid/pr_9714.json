{"pr_number": 9714, "pr_title": "More Hadoop integration tests", "pr_createdAt": "2020-04-16T11:13:27Z", "pr_url": "https://github.com/apache/druid/pull/9714", "timeline": [{"oid": "86493d16d3cc404b2b52374e855497547af363a4", "url": "https://github.com/apache/druid/commit/86493d16d3cc404b2b52374e855497547af363a4", "message": "More Hadoop integration tests", "committedDate": "2020-04-16T11:02:27Z", "type": "commit"}, {"oid": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "url": "https://github.com/apache/druid/commit/1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "message": "Add missing s3 instructions", "committedDate": "2020-04-16T11:21:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTUxNQ==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919515", "bodyText": "Maybe worth mentioning about seting the -Dresource.file.dir.path and GOOGLE_APPLICATION_CREDENTIALS since you need to make druid-google-extensions  happy on the druid nodes.", "author": "maytasm", "createdAt": "2020-04-17T00:12:12Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjY3MQ==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316671", "bodyText": "Added a note about -Dresource.file.dir.path", "author": "jon-wei", "createdAt": "2020-04-28T04:25:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTUxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java\nindex 1ef3249b44..5b0e4ec955 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java\n\n@@ -29,17 +29,18 @@ import org.testng.annotations.Test;\n  * To run this test, you must:\n  * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n  *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n- * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ * 2) Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_GCS)\n+@Test(groups = TestNGGroup.HADOOP_GCS_TO_GCS)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITGcsInputToGcsHadoopIndexTest extends AbstractGcsInputHadoopIndexTest\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTczNQ==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409919735", "bodyText": "same as above", "author": "maytasm", "createdAt": "2020-04-17T00:12:55Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n+ *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n+ * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n+ *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n+ *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n+ * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n+ * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjcyNg==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316726", "bodyText": "Added a note about -Dresource.file.dir.path", "author": "jon-wei", "createdAt": "2020-04-28T04:25:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkxOTczNQ=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java\nindex 504ecf84ed..314d5d7164 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToHdfsHadoopIndexTest.java\n\n@@ -33,13 +33,14 @@ import org.testng.annotations.Test;\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_hdfs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_GCS)\n+@Test(groups = TestNGGroup.HADOOP_GCS_TO_HDFS)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITGcsInputToHdfsHadoopIndexTest extends AbstractGcsInputHadoopIndexTest\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMDY1Nw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409920657", "bodyText": "step 1 is this step. The location should be /resources/data/batch_index on the hadoop container fs and also at /batch_index on hdfs", "author": "maytasm", "createdAt": "2020-04-17T00:15:48Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjg0Mw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316843", "bodyText": "I updated the instructions to address both the json and tsv directories", "author": "jon-wei", "createdAt": "2020-04-28T04:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMDY1Nw=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\nindex e6db97d555..496729ad45 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n\n@@ -40,13 +40,16 @@ import java.util.function.Function;\n  * IMPORTANT:\n  * To run this test, you must:\n  * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n- *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+ *    at /batch_index/tsv/\n  *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n  * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n  * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_INDEX)\n+@Test(groups = TestNGGroup.HDFS_DEEP_STORAGE)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMTIwOA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409921208", "bodyText": "Maybe we should combine TestNGGroup.HADOOP_INDEX with TestNGGroup.HDFS_DEEP_STORAGE", "author": "maytasm", "createdAt": "2020-04-17T00:17:30Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjkyNQ==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416316925", "bodyText": "I combined the groups into HDFS_DEEP_STORAGE and deleted HADOOP_INDEX", "author": "jon-wei", "createdAt": "2020-04-28T04:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMTIwOA=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\nindex e6db97d555..496729ad45 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n\n@@ -40,13 +40,16 @@ import java.util.function.Function;\n  * IMPORTANT:\n  * To run this test, you must:\n  * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n- *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+ *    at /batch_index/tsv/\n  *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n  * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n  * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_INDEX)\n+@Test(groups = TestNGGroup.HDFS_DEEP_STORAGE)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409922084", "bodyText": "currently /batch_index/tsv requires manual setup\nFrom the integration-tests/README.md ...\nCurrently, ITHadoopIndexTest can only be run with your own Druid + Hadoop cluster by following the below steps:\nCreate a directory called batchHadoop1 in the hadoop file system\n(anywhere you want) and put batch_hadoop.data (integration-tests/src/test/resources/hadoop/batch_hadoop.data) \ninto that directory (as its only file).\n\nWe should automatically setup this dir for the hadoop docker container (similar to how we setup the wikipedia json files). You can create a new dir in integration-tests/src/test/resources/data/batch_index called tsv and copy integration-tests/src/test/resources/hadoop/batch_hadoop.data to integration-tests/src/test/resources/data/batch_index/tsv (the run-cluster script should handle the rest and create /batch_index/tsv with batch_hadoop.data inside)", "author": "maytasm", "createdAt": "2020-04-17T00:20:34Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java", "diffHunk": "@@ -19,87 +19,147 @@\n \n package org.apache.druid.tests.hadoop;\n \n-import com.google.inject.Inject;\n+import com.google.common.collect.ImmutableList;\n+import org.apache.druid.indexer.partitions.DimensionBasedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.HashedPartitionsSpec;\n+import org.apache.druid.indexer.partitions.SingleDimensionPartitionsSpec;\n import org.apache.druid.java.util.common.StringUtils;\n import org.apache.druid.java.util.common.logger.Logger;\n-import org.apache.druid.testing.IntegrationTestingConfig;\n import org.apache.druid.testing.guice.DruidTestModuleFactory;\n-import org.apache.druid.testing.utils.ITRetryUtil;\n import org.apache.druid.tests.TestNGGroup;\n-import org.apache.druid.tests.indexer.AbstractIndexerTest;\n-import org.testng.annotations.AfterClass;\n-import org.testng.annotations.BeforeClass;\n+import org.apache.druid.tests.indexer.AbstractITBatchIndexTest;\n+import org.testng.annotations.DataProvider;\n import org.testng.annotations.Guice;\n import org.testng.annotations.Test;\n \n+import java.io.Closeable;\n+import java.util.UUID;\n+import java.util.function.Function;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n+ * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n @Test(groups = TestNGGroup.HADOOP_INDEX)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n-public class ITHadoopIndexTest extends AbstractIndexerTest\n+public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n   private static final Logger LOG = new Logger(ITHadoopIndexTest.class);\n+\n   private static final String BATCH_TASK = \"/hadoop/batch_hadoop_indexer.json\";\n   private static final String BATCH_QUERIES_RESOURCE = \"/hadoop/batch_hadoop_queries.json\";\n   private static final String BATCH_DATASOURCE = \"batchHadoop\";\n-  private boolean dataLoaded = false;\n \n-  @Inject\n-  private IntegrationTestingConfig config;\n+  private static final String INDEX_TASK = \"/hadoop/wikipedia_hadoop_index_task.json\";\n+  private static final String INDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_index_queries.json\";\n+  private static final String INDEX_DATASOURCE = \"wikipedia_hadoop_index_test\";\n \n-  @BeforeClass\n-  public void beforeClass()\n-  {\n-    loadData(config.getProperty(\"hadoopTestDir\") + \"/batchHadoop1\");\n-    dataLoaded = true;\n-  }\n+  private static final String REINDEX_TASK = \"/hadoop/wikipedia_hadoop_reindex_task.json\";\n+  private static final String REINDEX_QUERIES_RESOURCE = \"/indexer/wikipedia_reindex_queries.json\";\n+  private static final String REINDEX_DATASOURCE = \"wikipedia_hadoop_reindex_test\";\n \n-  @Test\n-  public void testHadoopIndex() throws Exception\n+  @DataProvider\n+  public static Object[][] resources()\n   {\n-    queryHelper.testQueriesFromFile(BATCH_QUERIES_RESOURCE, 2);\n+    return new Object[][]{\n+        {new HashedPartitionsSpec(3, null, null)},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\"))},\n+        {new HashedPartitionsSpec(null, 3, ImmutableList.of(\"page\", \"user\"))},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, \"page\", false)},\n+        {new SingleDimensionPartitionsSpec(1000, null, null, true)},\n+\n+        //{new HashedPartitionsSpec(null, 3, null)} // this results in a bug where the segments have 0 rows\n+    };\n   }\n \n-  private void loadData(String hadoopDir)\n+  @Test\n+  public void testLegacyITHadoopIndexTest() throws Exception\n   {\n-    String indexerSpec;\n+    try (\n+        final Closeable ignored0 = unloader(BATCH_DATASOURCE + config.getExtraDatasourceNameSuffix());\n+    ) {\n+      final Function<String, String> specPathsTransform = spec -> {\n+        try {\n+          String path = \"/batch_index/tsv\";", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMTk3MA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409931970", "bodyText": "Maybe also just get rid of the hadoopTestDir in the DockerConfigProvider. I think it's no longer needed. If using hadoop container then everything is automatically setup. If running your own hadoop then they should copy to /batch_index/tsv since the path is hardcoded in the specPathsTransform anyway", "author": "maytasm", "createdAt": "2020-04-17T00:56:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkzMjM4NA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409932384", "bodyText": "Can you also update the integration-tests/README.md ITHadoopIndexTest can be run in hadoop docker container hence the existing README.md is out of date.", "author": "maytasm", "createdAt": "2020-04-17T00:58:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNzcwNg==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416317706", "bodyText": "For the first comment, I had already moved the batch_hadoop.data file to integration-tests/src/test/resources/data/batch_index/tsv as you suggest so we're good there.\nI got rid of hadoopTestDir and updated the README.md file with new instructions", "author": "jon-wei", "createdAt": "2020-04-28T04:28:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyMjA4NA=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\nindex e6db97d555..496729ad45 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITHadoopIndexTest.java\n\n@@ -40,13 +40,16 @@ import java.util.function.Function;\n  * IMPORTANT:\n  * To run this test, you must:\n  * 1) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n- *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at the location set in step 1.\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your HDFS at /batch_index/json/\n+ *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n+ * 2) Copy batch_hadoop.data located in integration-tests/src/test/resources/data/batch_index/tsv to your HDFS\n+ *    at /batch_index/tsv/\n  *    If using the Docker-based Hadoop container, this is automatically done by the integration tests.\n  * 2) Provide -Doverride.config.path=<PATH_TO_FILE> with HDFS configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hdfs for env vars to provide.\n  * 3) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_INDEX)\n+@Test(groups = TestNGGroup.HDFS_DEEP_STORAGE)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITHadoopIndexTest extends AbstractITBatchIndexTest\n {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNDMyNw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r409924327", "bodyText": "I think ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest should be in different groups. The groups can be the different deep storage. The reason for my suggestion is that if you run the whole group then you cannot switch  -Doverride.config.path between different test class. What will happens is that you will run the whole group (ITS3InputToHdfsHadoopIndexTest and ITS3InputToS3HadoopIndexTest) with the same druid config file which basically will be the exact same test (same deep storage). Same for the other cloud storages.", "author": "maytasm", "createdAt": "2020-04-17T00:28:25Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.tests.hadoop;\n+\n+import org.apache.druid.testing.guice.DruidTestModuleFactory;\n+import org.apache.druid.tests.TestNGGroup;\n+import org.testng.annotations.Guice;\n+import org.testng.annotations.Test;\n+\n+/**\n+ * IMPORTANT:\n+ * To run this test, you must:\n+ * 1) Set the bucket, path, and region for your data.\n+ *    This can be done by setting -Ddruid.test.config.cloudBucket, -Ddruid.test.config.cloudPath\n+ *    and -Ddruid.test.config.cloudRegion or setting \"cloud_bucket\",\"cloud_path\", and \"cloud_region\" in the config file.\n+ * 2) Set -Ddruid.s3.accessKey and -Ddruid.s3.secretKey when running the tests to your access/secret keys.\n+ * 3) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ *    located in integration-tests/src/test/resources/data/batch_index/json to your S3 at the location set in step 1.\n+ * 4) Provide -Doverride.config.path=<PATH_TO_FILE> with s3 credentials and hdfs deep storage configs set. See\n+ *    integration-tests/docker/environment-configs/override-examples/hadoop/s3_to_hdfs for env vars to provide.\n+ * 5) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ */\n+@Test(groups = TestNGGroup.HADOOP_S3)", "originalCommit": "1dea53a188402e2c73ecb7f4e363cfa650c8b17e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNzc3Nw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416317777", "bodyText": "Good point, I moved each of these into their own group", "author": "jon-wei", "createdAt": "2020-04-28T04:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTkyNDMyNw=="}], "type": "inlineReview", "revised_code": {"commit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "chunk": "diff --git a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java\nindex 01f0fecdfd..01aa8e006d 100644\n--- a/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java\n+++ b/integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITS3InputToHdfsHadoopIndexTest.java\n\n@@ -37,7 +37,7 @@ import org.testng.annotations.Test;\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/s3_to_hdfs for env vars to provide.\n  * 5) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n  */\n-@Test(groups = TestNGGroup.HADOOP_S3)\n+@Test(groups = TestNGGroup.HADOOP_S3_TO_HDFS)\n @Guice(moduleFactory = DruidTestModuleFactory.class)\n public class ITS3InputToHdfsHadoopIndexTest extends AbstractS3InputHadoopIndexTest\n {\n"}}, {"oid": "fc146929a371a0e7e27e5e7349b58af92405986e", "url": "https://github.com/apache/druid/commit/fc146929a371a0e7e27e5e7349b58af92405986e", "message": "Merge remote-tracking branch 'upstream/master' into more_hadoop", "committedDate": "2020-04-28T03:27:35Z", "type": "commit"}, {"oid": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "url": "https://github.com/apache/druid/commit/3aafc2c2109f96ab3ec854b0e087968f386a3a58", "message": "Address PR comments", "committedDate": "2020-04-28T04:21:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MzM5Nw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416953397", "bodyText": "nit: maybe mention that -Dextra.datasource.name.suffix=''  is due to github issue xxx (not sure if we have an issue for this)", "author": "maytasm", "createdAt": "2020-04-28T22:07:53Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/hadoop/ITGcsInputToGcsHadoopIndexTest.java", "diffHunk": "@@ -29,17 +29,18 @@\n  * To run this test, you must:\n  * 1) Set the bucket and path for your data. This can be done by setting -Ddruid.test.config.cloudBucket and\n  *    -Ddruid.test.config.cloudPath or setting \"cloud_bucket\" and \"cloud_path\" in the config file.\n- * 2. Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n+ * 2) Set -Ddruid.test.config.hadoopGcsCredentialsPath to the location of your Google credentials file as it\n  *    exists within the Hadoop cluster that will ingest the data. The credentials file can be placed in the\n  *    shared folder used by the integration test containers if running the Docker-based Hadoop container,\n  *    in which case this property can be set to /shared/<path_of_your_credentials_file>\n- * 2) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n+ * 3) Provide -Dresource.file.dir.path=<PATH_TO_FOLDER> with folder that contains GOOGLE_APPLICATION_CREDENTIALS file\n+ * 4) Copy wikipedia_index_data1.json, wikipedia_index_data2.json, and wikipedia_index_data3.json\n  *    located in integration-tests/src/test/resources/data/batch_index/json to your GCS at the location set in step 1.\n- * 3) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n+ * 5) Provide -Doverride.config.path=<PATH_TO_FILE> with gcs configs set. See\n  *    integration-tests/docker/environment-configs/override-examples/hadoop/gcs_to_gcs for env vars to provide.\n- * 4) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command\n+ * 6) Run the test with -Dstart.hadoop.docker=true -Dextra.datasource.name.suffix='' in the mvn command", "originalCommit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNDk0Mw==", "url": "https://github.com/apache/druid/pull/9714#discussion_r417034943", "bodyText": "Added a note about that to the README", "author": "jon-wei", "createdAt": "2020-04-29T02:33:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk1MzM5Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416965658", "bodyText": "I think you also have to exclude this from Travis's other integration test group", "author": "maytasm", "createdAt": "2020-04-28T22:37:48Z", "path": "integration-tests/src/test/java/org/apache/druid/tests/TestNGGroup.java", "diffHunk": "@@ -84,6 +82,15 @@\n    */\n   public static final String HDFS_DEEP_STORAGE = \"hdfs-deep-storage\";\n \n+  public static final String HADOOP_S3_TO_S3 = \"hadoop-s3-to-s3-deep-storage\";", "originalCommit": "3aafc2c2109f96ab3ec854b0e087968f386a3a58", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2OTQ4Mg==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416969482", "bodyText": "Ahhh actually you don't since we exclude the hadoop test package in testng.xml.", "author": "maytasm", "createdAt": "2020-04-28T22:47:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk3MjQxMg==", "url": "https://github.com/apache/druid/pull/9714#discussion_r416972412", "bodyText": "Wait then your -Dgroups will not work. I believe if you run mvn cmd with -Dgroups it will use the suite defined in testng.xml. This suite excludes running hadoop package (org.apache.druid.tests.hadoop). I think one solution is you will have to remove the exclusion of the hadoop package from testng.xml and add all the new groups to the exclusion list in the Travis's other integration test group", "author": "maytasm", "createdAt": "2020-04-28T22:55:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzAzNTA5MA==", "url": "https://github.com/apache/druid/pull/9714#discussion_r417035090", "bodyText": "Updated as suggested", "author": "jon-wei", "createdAt": "2020-04-29T02:34:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjk2NTY1OA=="}], "type": "inlineReview", "revised_code": null}, {"oid": "4984ec384b1ee9e07e6fb277870e07bbfaac169e", "url": "https://github.com/apache/druid/commit/4984ec384b1ee9e07e6fb277870e07bbfaac169e", "message": "Address PR comments", "committedDate": "2020-04-29T02:14:08Z", "type": "commit"}, {"oid": "23f4be169431a7836e84b2ff44fc257f5d40083d", "url": "https://github.com/apache/druid/commit/23f4be169431a7836e84b2ff44fc257f5d40083d", "message": "PR comments", "committedDate": "2020-04-29T02:33:38Z", "type": "commit"}, {"oid": "d12acba6aecb9731c29ed7f695d77748b370e917", "url": "https://github.com/apache/druid/commit/d12acba6aecb9731c29ed7f695d77748b370e917", "message": "Fix typo", "committedDate": "2020-04-29T18:01:29Z", "type": "commit"}, {"oid": "2e0178b493bcbae3f07b5d7ed0d52218cb71f647", "url": "https://github.com/apache/druid/commit/2e0178b493bcbae3f07b5d7ed0d52218cb71f647", "message": "Merge remote-tracking branch 'upstream/master' into more_hadoop", "committedDate": "2020-04-30T01:51:49Z", "type": "commit"}]}