{"pr_number": 10366, "pr_title": "Add caching support to join queries", "pr_createdAt": "2020-09-08T18:14:36Z", "pr_url": "https://github.com/apache/druid/pull/10366", "timeline": [{"oid": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "url": "https://github.com/apache/druid/commit/43e948fa549cad7bf86b14f7529c076c2a794e2a", "message": "Proposed changes for making joins cacheable", "committedDate": "2020-09-08T16:17:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzI5MQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467291", "bodyText": "Why does this matter? Shouldn't it just be on the joinable factory to decide if it can cache or not and we should supply the PreJoinableClause or the JoinConditionAnalysis from PreJoinableClause.getCondition to it?", "author": "clintropolis", "createdAt": "2020-09-13T01:26:50Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +126,47 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n+   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n+   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n+   * participating in the join do not support caching yet\n+   *\n+   * @param dataSourceAnalysis\n+   * @param joinableFactory\n+   * @return\n+   */\n+  public static Optional<byte[]> computeDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis,\n+      final JoinableFactory joinableFactory\n+  )\n+  {\n+    final CacheKeyBuilder keyBuilder;\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);\n+    } else {\n+      keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+      for (PreJoinableClause clause : clauses) {\n+        if (!clause.getCondition().canHashJoin()) {", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY5NTMyNg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487695326", "bodyText": "I was thinking to keep it here so that I could just pass the data-source inside the computeJoinCacheKey.  The engine for hash join works outside the boundaries of joinable factories. so it felt simpler to just keep it here and joinable factories just worry about the data source. The other cache info from the condition is derived here in this class as well (original expression etc)", "author": "abhishekagarwal87", "createdAt": "2020-09-14T07:10:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzI5MQ=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex 1a2a3c0602..e054b3ab6c 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -127,46 +114,73 @@ public class Joinables\n   }\n \n   /**\n-   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n-   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n-   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n-   * participating in the join do not support caching yet\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Empty byte array - If there is no join datasource involved\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n    *\n    * @param dataSourceAnalysis\n-   * @param joinableFactory\n    * @return\n    */\n-  public static Optional<byte[]> computeDataSourceCacheKey(\n-      final DataSourceAnalysis dataSourceAnalysis,\n-      final JoinableFactory joinableFactory\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n   )\n   {\n-    final CacheKeyBuilder keyBuilder;\n+\n     final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n     if (clauses.isEmpty()) {\n-      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);\n-    } else {\n-      keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n-      for (PreJoinableClause clause : clauses) {\n-        if (!clause.getCondition().canHashJoin()) {\n-          log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n-          return Optional.empty();\n-        }\n-        Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n-        if (!bytes.isPresent()) {\n-          // Encountered a data source which didn't support cache yet\n-          log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n-          return Optional.empty();\n-        }\n-        keyBuilder.appendByteArray(bytes.get());\n-        keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO\n-        keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n-        keyBuilder.appendString(clause.getJoinType().name());\n+      return Optional.of(StringUtils.EMPTY_BYTES);\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      if (!clause.getCondition().canHashJoin()) {\n+        log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n+        return Optional.empty();\n+      }\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n       }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO\n+      keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n+      keyBuilder.appendString(clause.getJoinType().name());\n     }\n     return Optional.ofNullable(keyBuilder.build());\n   }\n \n+  /**\n+   * Checks that \"prefix\" is a valid prefix for a join clause (see {@link JoinableClause#getPrefix()}) and, if so,\n+   * returns it. Otherwise, throws an exception.\n+   */\n+  public static String validatePrefix(@Nullable final String prefix)\n+  {\n+    if (prefix == null || prefix.isEmpty()) {\n+      throw new IAE(\"Join clause cannot have null or empty prefix\");\n+    } else if (isPrefixedBy(ColumnHolder.TIME_COLUMN_NAME, prefix) || ColumnHolder.TIME_COLUMN_NAME.equals(prefix)) {\n+      throw new IAE(\n+          \"Join clause cannot have prefix[%s], since it would shadow %s\",\n+          prefix,\n+          ColumnHolder.TIME_COLUMN_NAME\n+      );\n+    } else {\n+      return prefix;\n+    }\n+  }\n+\n+  public static boolean isPrefixedBy(final String columnName, final String prefix)\n+  {\n+    return columnName.length() > prefix.length() && columnName.startsWith(prefix);\n+  }\n+\n   /**\n    * Check if any prefixes in the provided list duplicate or shadow each other.\n    *\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzU4NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467584", "bodyText": "Since multiple joinable factories can support the same class of datasource, I guess this means that this method needs to mimic the same selection logic as build to make sure that a joinable factory only computes a cache key for a table that it would build a joinable for in the query? I think we should probably supply the PreJoinableClause or the JoinConditionAnalysis  to this method so that it can have the JoinConditionAnalysis that build has access to. Then these factories can make better decisions about if they can compute a key for the query or not.", "author": "clintropolis", "createdAt": "2020-09-13T01:30:57Z", "path": "processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java", "diffHunk": "@@ -80,4 +80,21 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     }\n     return maybeJoinable;\n   }\n+\n+  @Override\n+  public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY5ODUwOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487698508", "bodyText": "Right now as I see that additional info comes from the data source itself. The joinable factory is deciding based on the dataSource whether it can build the joinable factory.  can you share any hypothetical examples wherein one factory decides to build Joinable but the other one does not on the basis of JoinConditionAnalysis?", "author": "abhishekagarwal87", "createdAt": "2020-09-14T07:16:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzU4NA=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java b/processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java\nindex e274db171e..e26e7b4c99 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/MapJoinableFactory.java\n\n@@ -67,34 +68,37 @@ public class MapJoinableFactory implements JoinableFactory\n   @Override\n   public Optional<Joinable> build(DataSource dataSource, JoinConditionAnalysis condition)\n   {\n-    Set<JoinableFactory> factories = joinableFactories.get(dataSource.getClass());\n-    Optional<Joinable> maybeJoinable = Optional.empty();\n-    for (JoinableFactory factory : factories) {\n-      Optional<Joinable> candidate = factory.build(dataSource, condition);\n-      if (candidate.isPresent()) {\n-        if (maybeJoinable.isPresent()) {\n-          throw new ISE(\"Multiple joinable factories are valid for table[%s]\", dataSource);\n-        }\n-        maybeJoinable = candidate;\n-      }\n-    }\n-    return maybeJoinable;\n+    return getSingleResult(dataSource, factory -> factory.build(dataSource, condition));\n   }\n \n   @Override\n   public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)\n+  {\n+    return getSingleResult(dataSource, factory -> factory.computeJoinCacheKey(dataSource));\n+  }\n+\n+  /**\n+   * Computes the given function assuming that only one joinable factory will return a non-empty result. If we get\n+   * results from two {@link JoinableFactory}, then throw an exception.\n+   *\n+   * @param dataSource\n+   * @param function\n+   * @param <T>\n+   * @return\n+   */\n+  private <T> Optional<T> getSingleResult(DataSource dataSource, Function<JoinableFactory, Optional<T>> function)\n   {\n     Set<JoinableFactory> factories = joinableFactories.get(dataSource.getClass());\n-    Optional<byte[]> maybeCacheKey = Optional.empty();\n+    Optional<T> mayBeFinalResult = Optional.empty();\n     for (JoinableFactory joinableFactory : factories) {\n-      Optional<byte[]> candidate = joinableFactory.computeJoinCacheKey(dataSource);\n-      if (candidate.isPresent() && maybeCacheKey.isPresent()) {\n+      Optional<T> candidate = function.apply(joinableFactory);\n+      if (candidate.isPresent() && mayBeFinalResult.isPresent()) {\n         throw new ISE(\"Multiple joinable factories are valid for table[%s]\", dataSource);\n       }\n       if (candidate.isPresent()) {\n-        maybeCacheKey = candidate;\n+        mayBeFinalResult = candidate;\n       }\n     }\n-    return maybeCacheKey;\n+    return mayBeFinalResult;\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487467627", "bodyText": "Could this just be a method on SegmentId?", "author": "clintropolis", "createdAt": "2020-09-13T01:31:44Z", "path": "processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java", "diffHunk": "@@ -241,6 +244,22 @@ public ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, bo\n     );\n   }\n \n+  @Override\n+  public Optional<byte[]> computeCacheKey()\n+  {\n+    SegmentId segmentId = segment.getId();", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY5OTExOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487699118", "bodyText": "are you suggesting to move this logic to SegmentId class?", "author": "abhishekagarwal87", "createdAt": "2020-09-14T07:17:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgzNTkwMg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487835902", "bodyText": "I wanted to use CacheUtil here but that is in server module and not accessible here. May be I can move that class to core?", "author": "abhishekagarwal87", "createdAt": "2020-09-14T11:18:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTI3MzQzMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491273433", "bodyText": "I think this logic should be here instead of in SegmentId since a segmentId represents a whole segment while we can partially read a broadcast segment based on an interval filter in the future.\n@abhishekagarwal87 regarding using CacheUtil, if you just want to add a new method similar to computeSegmentCacheKey(), I think you don't have to since the logic is different anyway. I would rather suggest using CacheKeyBuilder.", "author": "jihoonson", "createdAt": "2020-09-19T05:36:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk5NTkyMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491995923", "bodyText": "CacheKeyBuilder requires a prefix byte key in its constructor and hence I was not using it everywhere. I could add some fixed byte key though not sure how much does that help.", "author": "abhishekagarwal87", "createdAt": "2020-09-21T12:14:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3NDM0MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493174340", "bodyText": "CacheKeyBuilder requires a prefix byte key in its constructor and hence I was not using it everywhere. I could add some fixed byte key though not sure how much does that help.\n\nThis makes me wonder if broadcast segment needs a prefix key. I'm not sure if it would be useful yet. I still find using CacheKeyBuilder better as it's less error-prone than computing cache key manually, but I also agree with your point. I will leave this up to you.", "author": "jihoonson", "createdAt": "2020-09-23T03:24:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MTg4Mg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494691882", "bodyText": "I think there actually should be a prefix byte here, because the prefix bytes' purpose is to prevent cache key collisions for two implementations that are different but compute cache keys the same way. So each implementation should have its own prefix byte.\nI don't have a strong opinion on where the code should live, but it would be good to share it somehow, if that's not too much trouble.", "author": "gianm", "createdAt": "2020-09-25T01:09:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgzODA0NQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494838045", "bodyText": "Thinking out loud here. For the cache key of queries, I understand that a prefix can help when e.g. a groupBy and TimeSeries have the same filters, aggregators, etc. For IndexedTable, a similar scenario can occur if we have two different implementations of IndexedTable that can be invoked on the same segment. when can that occur? May be if the cache is external and the historical/broker is upgraded to a new implementation of IndexedTable applicable to that segment?\nI don't mind adding the key prefix. Just trying to understand it better.", "author": "abhishekagarwal87", "createdAt": "2020-09-25T08:41:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzNzgxOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495137818", "bodyText": "I'm not sure how there could be cache key collisions between IndexedTables. Even though there could be multiple IndexedTable implementations, the way it is used in the callers (IndexedTableJoinMatcher, IndexedTableColumnValueSelector, etc) should be same across those implementations which will result in the same query result. There will also not be cache key collisions between a broadcast segment (IndexedTable) and a regular segment as their cache key computation is different (the datasource name is included in only the former). But, the current way doesn't look very well-structured.\nAnother approach I've been thinking is making ReferenceCountedObject (or SegmentReference) cacheable as they represent a segment (either a physical or a virtual). In this way, the cache key for segments will be computed in each ReferenceCountedObject implementation and so even CachingQueryRunner doesn't have to know how to compute the cache key for segments. The interface to compute a cache key should accept SegmentDescriptor. In this case, the prefix will indicate how the segment will be processed (in a hash join or an aggregation without join for now). This approach seems most reasonable to me, but I didn't mention it before since you already went a different way.", "author": "jihoonson", "createdAt": "2020-09-25T17:39:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTczNzgwOQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495737809", "bodyText": "ReferenceCountedObject has wide variety of implementations. I am not sure how can that be made cacheable while many of these implementations have nothing to do with query caching.\nhow is making SegmentReference cacheable different from the very first alternative where we could add computeCacheKey method to Segment  class?\n\nIn this case, the prefix will indicate how the segment will be processed (in a hash join or an aggregation without join for now)\n\nBy the way, prefix for a join operation is already being used. where do you see this prefix being passed from?", "author": "abhishekagarwal87", "createdAt": "2020-09-28T07:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ2ODc1Mg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r496468752", "bodyText": "For IndexedTable, a similar scenario can occur if we have two different implementations of IndexedTable that can be invoked on the same segment. when can that occur? May be if the cache is external and the historical/broker is upgraded to a new implementation of IndexedTable applicable to that segment?\n\nI didn\u2019t have a specific scenario in mind, I was just thinking that it\u2019s generally a good practice to have a namespace prefix like this when defining a cache key for an interface implementation. Even if there aren\u2019t others today, it\u2019s possible there will be in the future, and it\u2019s easy for new implementers to forget to update the cache keys of the existing implementations. Better to do it ahead of time.", "author": "gianm", "createdAt": "2020-09-29T07:15:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg0Mjc0Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r496842746", "bodyText": "I have added the prefix here", "author": "abhishekagarwal87", "createdAt": "2020-09-29T15:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2NzYyNw=="}], "type": "inlineReview", "revised_code": {"commit": "1e065d252cf7c18d26d103fe5058e5a9537d08a0", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java b/processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java\nindex e1f82bda6c..a54ed2aedb 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/table/BroadcastSegmentIndexedTable.java\n\n@@ -245,19 +245,25 @@ public class BroadcastSegmentIndexedTable implements IndexedTable\n   }\n \n   @Override\n-  public Optional<byte[]> computeCacheKey()\n+  public byte[] computeCacheKey()\n   {\n     SegmentId segmentId = segment.getId();\n     byte[] versionBytes = StringUtils.toUtf8(segmentId.getVersion());\n     byte[] dataSourceBytes = StringUtils.toUtf8(segmentId.getDataSource());\n-    return Optional.of(ByteBuffer\n+    return ByteBuffer\n                            .allocate(16 + versionBytes.length + dataSourceBytes.length + 4)\n                            .putLong(segmentId.getInterval().getStartMillis())\n                            .putLong(segmentId.getInterval().getEndMillis())\n                            .put(versionBytes)\n                            .put(dataSourceBytes)\n                            .putInt(segmentId.getPartitionNum())\n-                           .array());\n+                           .array();\n+  }\n+\n+  @Override\n+  public boolean isCacheable()\n+  {\n+    return true;\n   }\n \n   @Override\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2OTcyOQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487469729", "bodyText": "I guess this would bust cache on upgrade since all non-join cache keys would have this new byte prefix? Could we just not add a prefix if the query isn't a join query?", "author": "clintropolis", "createdAt": "2020-09-13T02:00:40Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +126,47 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n+   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n+   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n+   * participating in the join do not support caching yet\n+   *\n+   * @param dataSourceAnalysis\n+   * @param joinableFactory\n+   * @return\n+   */\n+  public static Optional<byte[]> computeDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis,\n+      final JoinableFactory joinableFactory\n+  )\n+  {\n+    final CacheKeyBuilder keyBuilder;\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgxOTc3MQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487819771", "bodyText": "You are right. I was thinking that it will only be one time bust. And, that having a distinct prefix will eliminate the possibility of a cache key collision.", "author": "abhishekagarwal87", "createdAt": "2020-09-14T10:47:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2OTcyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzI0MDY5NQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493240695", "bodyText": "Modified it such that the existing cache will continue to be usable.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T06:58:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ2OTcyOQ=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex 1a2a3c0602..e054b3ab6c 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -127,46 +114,73 @@ public class Joinables\n   }\n \n   /**\n-   * Compute a cache key prefix for data sources that is to be used in segment level and result level caches. The\n-   * data source can either be base (clauses is empty) or RHS of a join (clauses is non-empty). In both of the cases,\n-   * a non-null cache is returned. However, the cache key is null if there is a join and some of the right data sources\n-   * participating in the join do not support caching yet\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Empty byte array - If there is no join datasource involved\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n    *\n    * @param dataSourceAnalysis\n-   * @param joinableFactory\n    * @return\n    */\n-  public static Optional<byte[]> computeDataSourceCacheKey(\n-      final DataSourceAnalysis dataSourceAnalysis,\n-      final JoinableFactory joinableFactory\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n   )\n   {\n-    final CacheKeyBuilder keyBuilder;\n+\n     final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n     if (clauses.isEmpty()) {\n-      keyBuilder = new CacheKeyBuilder(REGULAR_OPERATION);\n-    } else {\n-      keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n-      for (PreJoinableClause clause : clauses) {\n-        if (!clause.getCondition().canHashJoin()) {\n-          log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n-          return Optional.empty();\n-        }\n-        Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n-        if (!bytes.isPresent()) {\n-          // Encountered a data source which didn't support cache yet\n-          log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n-          return Optional.empty();\n-        }\n-        keyBuilder.appendByteArray(bytes.get());\n-        keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO\n-        keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n-        keyBuilder.appendString(clause.getJoinType().name());\n+      return Optional.of(StringUtils.EMPTY_BYTES);\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      if (!clause.getCondition().canHashJoin()) {\n+        log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n+        return Optional.empty();\n+      }\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n       }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO\n+      keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n+      keyBuilder.appendString(clause.getJoinType().name());\n     }\n     return Optional.ofNullable(keyBuilder.build());\n   }\n \n+  /**\n+   * Checks that \"prefix\" is a valid prefix for a join clause (see {@link JoinableClause#getPrefix()}) and, if so,\n+   * returns it. Otherwise, throws an exception.\n+   */\n+  public static String validatePrefix(@Nullable final String prefix)\n+  {\n+    if (prefix == null || prefix.isEmpty()) {\n+      throw new IAE(\"Join clause cannot have null or empty prefix\");\n+    } else if (isPrefixedBy(ColumnHolder.TIME_COLUMN_NAME, prefix) || ColumnHolder.TIME_COLUMN_NAME.equals(prefix)) {\n+      throw new IAE(\n+          \"Join clause cannot have prefix[%s], since it would shadow %s\",\n+          prefix,\n+          ColumnHolder.TIME_COLUMN_NAME\n+      );\n+    } else {\n+      return prefix;\n+    }\n+  }\n+\n+  public static boolean isPrefixedBy(final String columnName, final String prefix)\n+  {\n+    return columnName.length() > prefix.length() && columnName.startsWith(prefix);\n+  }\n+\n   /**\n    * Check if any prefixes in the provided list duplicate or shadow each other.\n    *\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDU2Nw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487470567", "bodyText": "nit: this seems a little strange that the cache key is turned into the namespace and another key is used as the bytes, rather than making some sort of composite, but maybe it is ok?", "author": "clintropolis", "createdAt": "2020-09-13T02:13:21Z", "path": "server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java", "diffHunk": "@@ -86,7 +92,15 @@ public ResultLevelCachingQueryRunner(\n     if (useResultCache || populateResultCache) {\n \n       final String cacheKeyStr = StringUtils.fromUtf8(strategy.computeResultLevelCacheKey(query));\n-      final byte[] cachedResultSet = fetchResultsFromResultLevelCache(cacheKeyStr);\n+      DataSourceAnalysis analysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n+      byte[] dataSourceCacheKey = Joinables.computeDataSourceCacheKey(analysis, joinableFactory).orElse(null);\n+      if (null == dataSourceCacheKey) {\n+        return baseRunner.run(\n+            queryPlus,\n+            responseContext\n+        );\n+      }\n+      final byte[] cachedResultSet = fetchResultsFromResultLevelCache(cacheKeyStr, dataSourceCacheKey);", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY5OTY2Mg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487699662", "bodyText": "yeah. this is not well thought out right now. I will revisit this.", "author": "abhishekagarwal87", "createdAt": "2020-09-14T07:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDU2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgzNzMzNg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487837336", "bodyText": "earlier cacheKeyStr was being used as namespace as well as the key which was just like unnecessary duplication. Internally, the namespace and key are composed together into another bigger key. By the way, this too would cause cache bust once on an upgrade.", "author": "abhishekagarwal87", "createdAt": "2020-09-14T11:21:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDU2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzI0MTA3Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493241076", "bodyText": "This change has been removed now.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T06:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDU2Nw=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\nindex 6a3a5a723f..0c7620dca6 100644\n--- a/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\n+++ b/server/src/main/java/org/apache/druid/query/ResultLevelCachingQueryRunner.java\n\n@@ -93,7 +92,7 @@ public class ResultLevelCachingQueryRunner<T> implements QueryRunner<T>\n \n       final String cacheKeyStr = StringUtils.fromUtf8(strategy.computeResultLevelCacheKey(query));\n       DataSourceAnalysis analysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n-      byte[] dataSourceCacheKey = Joinables.computeDataSourceCacheKey(analysis, joinableFactory).orElse(null);\n+      byte[] dataSourceCacheKey = joinables.computeJoinDataSourceCacheKey(analysis).orElse(null);\n       if (null == dataSourceCacheKey) {\n         return baseRunner.run(\n             queryPlus,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDc2MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487470760", "bodyText": "seems like this and build could share a method that gets the ReferenceCountingIndexedTable if possible, and build can make the joinable and this method compute the key on the results", "author": "clintropolis", "createdAt": "2020-09-13T02:16:19Z", "path": "server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java", "diffHunk": "@@ -76,4 +76,27 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     }\n     return Optional.empty();\n   }\n+\n+  @Override\n+  public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzY5OTk5MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487699990", "bodyText": "Yes. good point.", "author": "abhishekagarwal87", "createdAt": "2020-09-14T07:19:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MDc2MA=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java b/server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java\nindex 86c299471c..767814a9c3 100644\n--- a/server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java\n+++ b/server/src/main/java/org/apache/druid/segment/join/BroadcastTableJoinableFactory.java\n\n@@ -55,30 +56,19 @@ public class BroadcastTableJoinableFactory implements JoinableFactory\n       JoinConditionAnalysis condition\n   )\n   {\n-    GlobalTableDataSource broadcastDatasource = (GlobalTableDataSource) dataSource;\n-    if (condition.canHashJoin()) {\n-      DataSourceAnalysis analysis = DataSourceAnalysis.forDataSource(broadcastDatasource);\n-      return segmentManager.getIndexedTables(analysis).map(tables -> {\n-        Iterator<ReferenceCountingIndexedTable> tableIterator = tables.iterator();\n-        if (!tableIterator.hasNext()) {\n-          return null;\n-        }\n-        try {\n-          return new IndexedTableJoinable(Iterators.getOnlyElement(tableIterator));\n-        }\n-        catch (IllegalArgumentException iae) {\n-          throw new ISE(\n-              \"Currently only single segment datasources are supported for broadcast joins, dataSource[%s] has multiple segments. Reingest the data so that it is entirely contained within a single segment to use in JOIN queries.\",\n-              broadcastDatasource.getName()\n-          );\n-        }\n-      });\n+    if (!condition.canHashJoin()) {\n+      return Optional.empty();\n     }\n-    return Optional.empty();\n+    return getIndexedTable(dataSource).map(IndexedTableJoinable::new);\n   }\n \n   @Override\n   public Optional<byte[]> computeJoinCacheKey(DataSource dataSource)\n+  {\n+    return getIndexedTable(dataSource).flatMap(IndexedTable::computeCacheKey);\n+  }\n+\n+  private Optional<ReferenceCountingIndexedTable> getIndexedTable(DataSource dataSource)\n   {\n     GlobalTableDataSource broadcastDataSource = (GlobalTableDataSource) dataSource;\n     DataSourceAnalysis analysis = DataSourceAnalysis.forDataSource(dataSource);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MTExNg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487471116", "bodyText": "CachingQueryRunner already has the machinery to decide whether or not to use the caches, should we just pass the optional in and use it as another input to the existing useCache/populateCache equation so we don't need these if statements in ServerManager and SinkQuerySegmentWalker?", "author": "clintropolis", "createdAt": "2020-09-13T02:21:14Z", "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -293,21 +302,28 @@ public ServerManager(\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    CachingQueryRunner<T> cachingQueryRunner = new CachingQueryRunner<>(\n-        segmentIdString,\n-        segmentDescriptor,\n-        objectMapper,\n-        cache,\n-        toolChest,\n-        metricsEmittingQueryRunnerInner,\n-        cachePopulator,\n-        cacheConfig\n-    );\n+    QueryRunner<T> queryRunner;", "originalCommit": "43e948fa549cad7bf86b14f7529c076c2a794e2a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzgxODQyNw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r487818427", "bodyText": "I was thinking that less places worry about the different meanings of cacheKeyPrefix content, the better it is. So for CachingQueryRunner, cacheKeyPrefix is just a plain dumb byte array", "author": "abhishekagarwal87", "createdAt": "2020-09-14T10:44:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzQ3MTExNg=="}], "type": "inlineReview", "revised_code": {"commit": "0fbe8d836a3885672779de32c7a3beffc5486100", "chunk": "diff --git a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\nindex 21d3ac9a93..9c6d87901b 100644\n--- a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n+++ b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n\n@@ -302,23 +300,17 @@ public class ServerManager implements QuerySegmentWalker\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    QueryRunner<T> queryRunner;\n-    if (cacheKeyPrefix.isPresent()) {\n-      queryRunner = new CachingQueryRunner<>(\n-          segmentIdString,\n-          cacheKeyPrefix.get(),\n-          segmentDescriptor,\n-          objectMapper,\n-          cache,\n-          toolChest,\n-          metricsEmittingQueryRunnerInner,\n-          cachePopulator,\n-          cacheConfig\n-      );\n-    } else {\n-      // Empty key prefix implies that its a join segment but unsupported data sources\n-      queryRunner = metricsEmittingQueryRunnerInner;\n-    }\n+    QueryRunner<T> queryRunner = new CachingQueryRunner<>(\n+        segmentIdString,\n+        cacheKeyPrefix,\n+        segmentDescriptor,\n+        objectMapper,\n+        cache,\n+        toolChest,\n+        metricsEmittingQueryRunnerInner,\n+        cachePopulator,\n+        cacheConfig\n+    );\n \n     BySegmentQueryRunner<T> bySegmentQueryRunner = new BySegmentQueryRunner<>(\n         segmentId,\n"}}, {"oid": "ff9f92d66175271a14fa827b8f5fc81e0be1b137", "url": "https://github.com/apache/druid/commit/ff9f92d66175271a14fa827b8f5fc81e0be1b137", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable", "committedDate": "2020-09-14T07:21:04Z", "type": "commit"}, {"oid": "0fbe8d836a3885672779de32c7a3beffc5486100", "url": "https://github.com/apache/druid/commit/0fbe8d836a3885672779de32c7a3beffc5486100", "message": "Add unit tests", "committedDate": "2020-09-15T15:16:51Z", "type": "commit"}, {"oid": "9f97e22421be9cc9d52b510cca0bb0d1a6bcc085", "url": "https://github.com/apache/druid/commit/9f97e22421be9cc9d52b510cca0bb0d1a6bcc085", "message": "Fix tests", "committedDate": "2020-09-16T09:21:19Z", "type": "commit"}, {"oid": "1e065d252cf7c18d26d103fe5058e5a9537d08a0", "url": "https://github.com/apache/druid/commit/1e065d252cf7c18d26d103fe5058e5a9537d08a0", "message": "simplify logic", "committedDate": "2020-09-16T10:02:29Z", "type": "commit"}, {"oid": "78290510b36620f4898164805309d2ae5f70a1b0", "url": "https://github.com/apache/druid/commit/78290510b36620f4898164805309d2ae5f70a1b0", "message": "Pull empty byte array logic out of CachingQueryRunner", "committedDate": "2020-09-16T11:09:54Z", "type": "commit"}, {"oid": "73dcc8e26e98b048c1a9e61fb65a9dd106a24a22", "url": "https://github.com/apache/druid/commit/73dcc8e26e98b048c1a9e61fb65a9dd106a24a22", "message": "remove useless null check", "committedDate": "2020-09-16T11:12:04Z", "type": "commit"}, {"oid": "50d585f762daebc2fdbcbbe7bc89eb103e4552dd", "url": "https://github.com/apache/druid/commit/50d585f762daebc2fdbcbbe7bc89eb103e4552dd", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable", "committedDate": "2020-09-16T13:29:33Z", "type": "commit"}, {"oid": "4ff3b58e9cf5080ae1bf7bca127335f0e66abf30", "url": "https://github.com/apache/druid/commit/4ff3b58e9cf5080ae1bf7bca127335f0e66abf30", "message": "Minor refactor", "committedDate": "2020-09-16T13:29:57Z", "type": "commit"}, {"oid": "03a00f218eb85e33bc343a2744742d7394baadf3", "url": "https://github.com/apache/druid/commit/03a00f218eb85e33bc343a2744742d7394baadf3", "message": "Fix tests", "committedDate": "2020-09-16T18:48:36Z", "type": "commit"}, {"oid": "12b6104a909c074f272df70797da08247e8b7db6", "url": "https://github.com/apache/druid/commit/12b6104a909c074f272df70797da08247e8b7db6", "message": "Merge branch 'master' of github.com:apache/druid into join_cacheable", "committedDate": "2020-09-17T06:33:51Z", "type": "commit"}, {"oid": "5a05961c00ac017741887fa3b77e9921dcb02a07", "url": "https://github.com/apache/druid/commit/5a05961c00ac017741887fa3b77e9921dcb02a07", "message": "Fix segment caching on Broker", "committedDate": "2020-09-18T10:50:56Z", "type": "commit"}, {"oid": "50ebf74f1080e3a877c4ec19ca95be070825f922", "url": "https://github.com/apache/druid/commit/50ebf74f1080e3a877c4ec19ca95be070825f922", "message": "Move join cache key computation in Broker\n\nMove join cache key computation in Broker from ResultLevelCachingQueryRunner to CachingClusteredClient", "committedDate": "2020-09-18T12:37:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491217786", "bodyText": "Using a plural name is a convention for utility classes like Collections. Maybe we should rename it if necessary since this class will not be a simple utility class anymore. But, does this class need to be a wrapper class? It doesn't seem like so. Also, some static util methods and non-static util methods are mixed here which seems confusing. Can we keep this class as a non-instantiable utility class?", "author": "jihoonson", "createdAt": "2020-09-18T22:17:27Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -35,57 +39,47 @@\n import javax.annotation.Nullable;\n import java.util.Comparator;\n import java.util.List;\n+import java.util.Optional;\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.function.Function;\n \n /**\n- * Utility methods for working with {@link Joinable} related classes.\n+ * A wrapper class over {@link JoinableFactory} for working with {@link Joinable} related classes.", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg5NzQ1Mw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491897453", "bodyText": "I want to at least have some methods non-static which are somewhat non-trivial.  The unit testing becomes easier with that as I can pass the mocks etc. I can also create another class and move non-static methods there and will leave rest of the methods as it is. does that work?", "author": "abhishekagarwal87", "createdAt": "2020-09-21T09:17:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3NDM2OQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493174369", "bodyText": "It works for me.", "author": "jihoonson", "createdAt": "2020-09-23T03:24:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzMwMzk4Nw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493303987", "bodyText": "can I do that as a follow-up PR? It's a harmless refactoring but will just clutter this PR. Doing it separately will ensure that no unintended change is going in.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T08:24:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5OTY5Mw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493799693", "bodyText": "I'm OK with doing it as a follow-up.", "author": "jihoonson", "createdAt": "2020-09-23T18:25:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTQ0NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494681444", "bodyText": "I think it'd be better to make the change now, since \"Joinables\" is meant to be a non-constructible holder of utility functions, and so making it constructible is unnecessarily changing its character. As to cluttering the PR, IMO adding a new class is likely to yield a less cluttered PR, because it won't involve changes to the pre-existing utility class.", "author": "gianm", "createdAt": "2020-09-25T00:28:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIxNzc4Ng=="}], "type": "inlineReview", "revised_code": {"commit": "c9d758467af197e4beee954fba86676c6828941b", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex b8c260bbf1..d7600c572c 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -61,11 +61,6 @@ public class Joinables\n     this.joinableFactory = Preconditions.checkNotNull(joinableFactory, \"joinableFactory\");\n   }\n \n-  public JoinableFactory getJoinableFactory()\n-  {\n-    return this.joinableFactory;\n-  }\n-\n   /**\n    * Creates a Function that maps base segments to {@link HashJoinSegment} if needed (i.e. if the number of join\n    * clauses is > 0). If mapping is not needed, this method will return {@link Function#identity()}.\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyMTcyOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491221728", "bodyText": "Please remove param and return tags if they don't need descriptions.", "author": "jihoonson", "createdAt": "2020-09-18T22:32:45Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDQ5NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491480494", "bodyText": "I found a couple of more places with empty param or return tags. Please either add a description or remove them.", "author": "jihoonson", "createdAt": "2020-09-19T18:25:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyMTcyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTg5NzYwOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491897608", "bodyText": "Ack", "author": "abhishekagarwal87", "createdAt": "2020-09-21T09:18:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyMTcyOA=="}], "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex b8c260bbf1..b0c0781e28 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -122,17 +122,16 @@ public class Joinables\n    *  in the JOIN is not cacheable.\n    *\n    * @throws {@link IAE} if this operation is called on a non-join data source\n-   * @param dataSourceAnalysis\n-   * @return\n+   * @param dataSourceAnalysis for the join datasource\n+   * @return the optional cache key to be used as part of query cache key\n    */\n   public Optional<byte[]> computeJoinDataSourceCacheKey(\n       final DataSourceAnalysis dataSourceAnalysis\n   )\n   {\n-\n     final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n     if (clauses.isEmpty()) {\n-      throw new IAE(\"No join clauses to build the cache key\");\n+      throw new IAE(\"No join clauses to build the cache key for data source [%s]\", dataSourceAnalysis.getDataSource());\n     }\n \n     final CacheKeyBuilder keyBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTIyNzIyNw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491227227", "bodyText": "Yeah, it doesn't seem necessary for now.", "author": "jihoonson", "createdAt": "2020-09-18T22:55:34Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key\");\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      if (!clause.getCondition().canHashJoin()) {\n+        log.debug(\"skipping caching for join since [%s] does not support hash-join\", clause.getCondition());\n+        return Optional.empty();\n+      }\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n+      }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getPrefix());    //TODO - prefix shouldn't be required IMO", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex b8c260bbf1..b0c0781e28 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -122,17 +122,16 @@ public class Joinables\n    *  in the JOIN is not cacheable.\n    *\n    * @throws {@link IAE} if this operation is called on a non-join data source\n-   * @param dataSourceAnalysis\n-   * @return\n+   * @param dataSourceAnalysis for the join datasource\n+   * @return the optional cache key to be used as part of query cache key\n    */\n   public Optional<byte[]> computeJoinDataSourceCacheKey(\n       final DataSourceAnalysis dataSourceAnalysis\n   )\n   {\n-\n     final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n     if (clauses.isEmpty()) {\n-      throw new IAE(\"No join clauses to build the cache key\");\n+      throw new IAE(\"No join clauses to build the cache key for data source [%s]\", dataSourceAnalysis.getDataSource());\n     }\n \n     final CacheKeyBuilder keyBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491480204", "bodyText": "How about supplying a Supplier<Optional<byte[]>> to the CachingQueryRunner? Then, computing cacheKeyPrefix can be done only when useCache or populateCache is true which seems more legit.", "author": "jihoonson", "createdAt": "2020-09-19T18:22:31Z", "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -197,13 +197,16 @@ public ServerManager(\n     }\n \n     // segmentMapFn maps each base Segment into a joined Segment if necessary.\n-    final Function<SegmentReference, SegmentReference> segmentMapFn = Joinables.createSegmentMapFn(\n+    final Function<SegmentReference, SegmentReference> segmentMapFn = joinables.createSegmentMapFn(\n         analysis.getPreJoinableClauses(),\n-        joinableFactory,\n         cpuTimeAccumulator,\n         analysis.getBaseQuery().orElse(query)\n     );\n \n+    final Optional<byte[]> cacheKeyPrefix = analysis.isJoin()", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxNzExMg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491917112", "bodyText": "I am not expecting cache key computation to be a costly operation. do you think it should still be passed via a supplier?", "author": "abhishekagarwal87", "createdAt": "2020-09-21T09:48:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3NDM5NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493174394", "bodyText": "Hmm, what I want is having cache key computation in one place since it's easier to follow how the key is computed. I think my previous comment was not enough for that. How about passing DataSourceAnalysis and Joinables to CachingQueryRunner so that cache key computation can be solely done there?", "author": "jihoonson", "createdAt": "2020-09-23T03:24:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzI0OTU2MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493249560", "bodyText": "I remember now that what you are describing is how I started. But changed it later on so that the caching key is not computed for every segment participating in the query. Thats why I compute the cacheKeyPrefix in getQueryRunnerForSegments and the pass it on to each SpecificSegmentQueryRunner.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T07:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc5OTcxNA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493799714", "bodyText": "I see. It sounds reasonable to me although the cache key computation should be cheap. I think we can introduce another class responsible for computing cache keys. ServerManager can create it and pass it to CachingQueryRunner. The new class will take DatasourceAnalysis and Joinables in its constructor, and have an interface providing a computed cache key for a given segment. I don't think this refactoring should be necessarily done in this PR, but it seems good to have. Are you interested in this refactoring as well?", "author": "jihoonson", "createdAt": "2020-09-23T18:25:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzgzNDk1OQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493834959", "bodyText": "I think this piece of code is too small to be refactored into its own class as such. May be in future if the cache key computation becomes more complex.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T19:13:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MDIwNA=="}], "type": "inlineReview", "revised_code": {"commit": "e9f1ae8a946ba4c5fc22e3fd780ca8d170d96c72", "chunk": "diff --git a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\nindex eeee0f45de..07927cefb5 100644\n--- a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n+++ b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n\n@@ -202,7 +202,7 @@ public class ServerManager implements QuerySegmentWalker\n         cpuTimeAccumulator,\n         analysis.getBaseQuery().orElse(query)\n     );\n-\n+    // We compute the join cache key here itself so it doesn't need to be re-computed for every segment\n     final Optional<byte[]> cacheKeyPrefix = analysis.isJoin()\n                                             ? joinables.computeJoinDataSourceCacheKey(analysis)\n                                             : Optional.of(StringUtils.EMPTY_BYTES);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491482315", "bodyText": "Per contract of JoinableFactory, build() can return an Optional.empty() even when it is directlyJoinable. I think computeJoinCacheKey() should match with build(); it should return a computed cache key only when build() returns a Joinable.\nAs you mentioned at #10366 (comment), perhaps canHashJoin() could be checked on the caller side. I don't see why not for now except that you should also change the build() method and its callers to do the same. However, I'm not sure if that's better. That means, I'm not sure if this interface or the interface you may change makes sense for other join algorithms we will add in the future. I would suggest to keep this interface until we come up with a good one.", "author": "jihoonson", "createdAt": "2020-09-19T18:48:20Z", "path": "processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java", "diffHunk": "@@ -43,8 +43,18 @@\n    *\n    * @param dataSource the datasource to join on\n    * @param condition  the condition to join on\n-   *\n    * @return a Joinable if this datasource + condition combo is joinable; empty if not\n    */\n   Optional<Joinable> build(DataSource dataSource, JoinConditionAnalysis condition);\n+\n+  /**\n+   * Compute the cache key for a data source participating in join operation. This is done separately from {{@link #build(DataSource, JoinConditionAnalysis)}}\n+   * which can be an expensive operation and can potentially be avoided if cached results can be used.\n+   *\n+   * @param dataSource the datasource to join on\n+   */\n+  default Optional<byte[]> computeJoinCacheKey(DataSource dataSource)", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjkwNzE2OA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r492907168", "bodyText": "hmm. I do believe this condition should be outside. In some cases, when one of the conditions cannot be hash-joined, we can skip building the other expensive Joinable objects themselves when query involve multiple joins. That being said, I understand that we do not want to change the interfaces right now. Though when new join algorithms come into being, I believe these interfaces will change as you have to somehow pass what join algorithm you intend to use.", "author": "abhishekagarwal87", "createdAt": "2020-09-22T17:21:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3NDQwNg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493174406", "bodyText": "Good point. I do agree that it could be better to check the condition outside so that we can stop creating potentially expensive joinables early. However, it doesn't mean that this interface should not match to build(). So, I would suggest either adding a description in the Javadoc why those interfaces don't match or modifying this interface to match since it's confusing when you see this code without knowing why they are different even though they seem to be supposed to match.", "author": "jihoonson", "createdAt": "2020-09-23T03:25:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE4ODYzNg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493188636", "bodyText": "+1 for having a consistent interface with build since they need to make the same decisions. I'm not sure this PR is the correct venue to refactor build so I would be in favor of adding JoinConditionAnalysis argument", "author": "clintropolis", "createdAt": "2020-09-23T04:23:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzMwNjI3Mw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493306273", "bodyText": "Thank you for your suggestions. I agree that functions should have the parity. I changed the code accordingly.", "author": "abhishekagarwal87", "createdAt": "2020-09-23T08:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTQ4MjMxNQ=="}], "type": "inlineReview", "revised_code": {"commit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java b/processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java\nindex 6d773c60d9..c482a75407 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/JoinableFactory.java\n\n@@ -52,8 +52,9 @@ public interface JoinableFactory\n    * which can be an expensive operation and can potentially be avoided if cached results can be used.\n    *\n    * @param dataSource the datasource to join on\n+   * @param condition  the condition to join on\n    */\n-  default Optional<byte[]> computeJoinCacheKey(DataSource dataSource)\n+  default Optional<byte[]> computeJoinCacheKey(DataSource dataSource, JoinConditionAnalysis condition)\n   {\n     return Optional.empty();\n   }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4NTQ2Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491585466", "bodyText": "Would you please add some comment on this variable? Since the cacheKey is not set in anywhere, people would not understand when it can be null or even how it could be set until they read this PR.", "author": "jihoonson", "createdAt": "2020-09-19T22:52:00Z", "path": "processing/src/main/java/org/apache/druid/segment/join/table/RowBasedIndexedTable.java", "diffHunk": "@@ -52,6 +54,8 @@\n   private final List<Function<RowType, Object>> columnFunctions;\n   private final Set<String> keyColumns;\n   private final String version;\n+  @Nullable\n+  private final byte[] cacheKey;", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4ODQxOQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491588419", "bodyText": "Please include useful information in the error message such as datasource.", "author": "jihoonson", "createdAt": "2020-09-19T22:58:10Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +112,74 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis\n+   * @return\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key\");", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyMzQzMg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r492023432", "bodyText": "While I am going to add dataSourceAnalysis.getDataSource() to the exception string, I just hope it doesn't blow up the message. On a quick look, it looks alright though.", "author": "abhishekagarwal87", "createdAt": "2020-09-21T12:57:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4ODQxOQ=="}], "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nindex b8c260bbf1..b0c0781e28 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n\n@@ -122,17 +122,16 @@ public class Joinables\n    *  in the JOIN is not cacheable.\n    *\n    * @throws {@link IAE} if this operation is called on a non-join data source\n-   * @param dataSourceAnalysis\n-   * @return\n+   * @param dataSourceAnalysis for the join datasource\n+   * @return the optional cache key to be used as part of query cache key\n    */\n   public Optional<byte[]> computeJoinDataSourceCacheKey(\n       final DataSourceAnalysis dataSourceAnalysis\n   )\n   {\n-\n     final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n     if (clauses.isEmpty()) {\n-      throw new IAE(\"No join clauses to build the cache key\");\n+      throw new IAE(\"No join clauses to build the cache key for data source [%s]\", dataSourceAnalysis.getDataSource());\n     }\n \n     final CacheKeyBuilder keyBuilder;\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU4ODY4NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491588684", "bodyText": "Please use expectedException and verifies the error message as well.", "author": "jihoonson", "createdAt": "2020-09-19T22:58:44Z", "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\nindex e8f5565cda..6fc0b7d68d 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n\n@@ -196,13 +196,21 @@ public class JoinablesTest\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n-  @Test(expected = IAE.class)\n+  @Test\n   public void test_computeJoinDataSourceCacheKey_noClauses()\n   {\n     DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n-    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    DataSource dataSource = new NoopDataSource();\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList());\n+    EasyMock.expect(analysis.getDataSource()).andReturn(dataSource);\n     EasyMock.replay(analysis);\n     Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    expectedException.expect(IAE.class);\n+    expectedException.expectMessage(String.format(\n+        \"No join clauses to build the cache key for data source [%s]\",\n+        dataSource\n+    ));\n     joinables.computeJoinDataSourceCacheKey(analysis);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5MzU0Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491593546", "bodyText": "Should this be Assert.assertArrayEquals(cacheKey_1.get(), cacheKey_2.get());?", "author": "jihoonson", "createdAt": "2020-09-19T23:08:46Z", "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)\n+  public void test_computeJoinDataSourceCacheKey_noClauses()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    joinables.computeJoinDataSourceCacheKey(analysis);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_noHashJoin()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x != \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_cachingUnsupported()\n+  {\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    DataSource dataSource = new LookupDataSource(\"lookup\");\n+    PreJoinableClause clause_2 = makePreJoinableClause(dataSource, \"x == \\\"h.x\\\"\", \"h.\", JoinType.LEFT);\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertFalse(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_usableClauses()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"h.x\\\"\", \"h.\");\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Arrays.asList(clause_1, clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    Optional<byte[]> cacheKey = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertTrue(cacheKey.isPresent());\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithExpression()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"y == \\\"j.y\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinType()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.LEFT);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\", JoinType.INNER);\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithPrefix()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"h.x\\\"\", \"h.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_keyChangesWithJoinable()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_2\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_sameKeyForSameJoin()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_1)).anyTimes();\n+    EasyMock.replay(analysis);\n+\n+    Optional<byte[]> cacheKey_1 = joinables.computeJoinDataSourceCacheKey(analysis);\n+    Assert.assertTrue(cacheKey_1.isPresent());\n+    Assert.assertNotEquals(0, cacheKey_1.get().length);\n+\n+    PreJoinableClause clause_2 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");\n+    EasyMock.reset(analysis);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.singletonList(clause_2)).anyTimes();\n+    EasyMock.replay(analysis);\n+    Optional<byte[]> cacheKey_2 = joinables.computeJoinDataSourceCacheKey(analysis);\n+\n+    Assert.assertNotEquals(cacheKey_1, cacheKey_2);", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NTA4NQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491595085", "bodyText": "Similar comment for other tests you added. Should they use Assert.assertFalse(Arrays.equals(cacheKey_1.get(), cacheKey_2.get()));?", "author": "jihoonson", "createdAt": "2020-09-19T23:11:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5MzU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTk5NjI1Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491996256", "bodyText": "oops. Thanks for catching this.", "author": "abhishekagarwal87", "createdAt": "2020-09-21T12:15:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5MzU0Ng=="}], "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\nindex e8f5565cda..6fc0b7d68d 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n\n@@ -196,13 +196,21 @@ public class JoinablesTest\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n-  @Test(expected = IAE.class)\n+  @Test\n   public void test_computeJoinDataSourceCacheKey_noClauses()\n   {\n     DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n-    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    DataSource dataSource = new NoopDataSource();\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList());\n+    EasyMock.expect(analysis.getDataSource()).andReturn(dataSource);\n     EasyMock.replay(analysis);\n     Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    expectedException.expect(IAE.class);\n+    expectedException.expectMessage(String.format(\n+        \"No join clauses to build the cache key for data source [%s]\",\n+        dataSource\n+    ));\n     joinables.computeJoinDataSourceCacheKey(analysis);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5Mzk5OA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491593998", "bodyText": "We don't use underscore in variable names. Please rename all variables containing underscores.", "author": "jihoonson", "createdAt": "2020-09-19T23:09:41Z", "path": "processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java", "diffHunk": "@@ -190,6 +196,176 @@ public boolean isDirectlyJoinable(DataSource dataSource)\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n+  @Test(expected = IAE.class)\n+  public void test_computeJoinDataSourceCacheKey_noClauses()\n+  {\n+    DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    EasyMock.replay(analysis);\n+    Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+    joinables.computeJoinDataSourceCacheKey(analysis);\n+  }\n+\n+  @Test\n+  public void test_computeJoinDataSourceCacheKey_noHashJoin()\n+  {\n+\n+    PreJoinableClause clause_1 = makeGlobalPreJoinableClause(\"dataSource_1\", \"x == \\\"j.x\\\"\", \"j.\");", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\nindex e8f5565cda..6fc0b7d68d 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/JoinablesTest.java\n\n@@ -196,13 +196,21 @@ public class JoinablesTest\n     Assert.assertNotSame(Function.identity(), segmentMapFn);\n   }\n \n-  @Test(expected = IAE.class)\n+  @Test\n   public void test_computeJoinDataSourceCacheKey_noClauses()\n   {\n     DataSourceAnalysis analysis = EasyMock.mock(DataSourceAnalysis.class);\n-    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList()).anyTimes();\n+    DataSource dataSource = new NoopDataSource();\n+    EasyMock.expect(analysis.getPreJoinableClauses()).andReturn(Collections.emptyList());\n+    EasyMock.expect(analysis.getDataSource()).andReturn(dataSource);\n     EasyMock.replay(analysis);\n     Joinables joinables = new Joinables(new JoinableFactoryWithCacheKey());\n+\n+    expectedException.expect(IAE.class);\n+    expectedException.expectMessage(String.format(\n+        \"No join clauses to build the cache key for data source [%s]\",\n+        dataSource\n+    ));\n     joinables.computeJoinDataSourceCacheKey(analysis);\n   }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NjIyMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491596223", "bodyText": "Please use expectedException and verify the error message as well.", "author": "jihoonson", "createdAt": "2020-09-19T23:14:20Z", "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -83,6 +84,36 @@ public void testBuildDataSourceIsRegisteredShouldReturnJoinableFromFactory()\n     EasyMock.replay(noopJoinableFactory);\n     Optional<Joinable> joinable = target.build(noopDataSource, condition);\n     Assert.assertEquals(mockJoinable, joinable.get());\n+\n+  }\n+\n+  @Test\n+  public void testComputeJoinCacheKey()\n+  {\n+    Optional<byte[]> expected = Optional.of(new byte[]{1, 2, 3});\n+    EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n+    EasyMock.replay(noopJoinableFactory);\n+    Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n+    Assert.assertEquals(expected, actual);\n+  }\n+\n+  @Test(expected = ISE.class)", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\nindex bcf5b9a593..0c0e969dec 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n\n@@ -94,10 +98,10 @@ public class MapJoinableFactoryTest\n     EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n     EasyMock.replay(noopJoinableFactory);\n     Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n-    Assert.assertEquals(expected, actual);\n+    Assert.assertSame(expected, actual);\n   }\n \n-  @Test(expected = ISE.class)\n+  @Test\n   public void testBuildExceptionWhenTwoJoinableFactoryForSameDataSource()\n   {\n     JoinableFactory anotherNoopJoinableFactory = EasyMock.mock(MapJoinableFactory.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NzY0NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491597644", "bodyText": "Should use Assert.assertArrayEquals().", "author": "jihoonson", "createdAt": "2020-09-19T23:17:19Z", "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -83,6 +84,36 @@ public void testBuildDataSourceIsRegisteredShouldReturnJoinableFromFactory()\n     EasyMock.replay(noopJoinableFactory);\n     Optional<Joinable> joinable = target.build(noopDataSource, condition);\n     Assert.assertEquals(mockJoinable, joinable.get());\n+\n+  }\n+\n+  @Test\n+  public void testComputeJoinCacheKey()\n+  {\n+    Optional<byte[]> expected = Optional.of(new byte[]{1, 2, 3});\n+    EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n+    EasyMock.replay(noopJoinableFactory);\n+    Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n+    Assert.assertEquals(expected, actual);", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjAyODE3Nw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r492028177", "bodyText": "Replaced with Assert.assertSame", "author": "abhishekagarwal87", "createdAt": "2020-09-21T13:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTU5NzY0NA=="}], "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\nindex bcf5b9a593..0c0e969dec 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n\n@@ -94,10 +98,10 @@ public class MapJoinableFactoryTest\n     EasyMock.expect(noopJoinableFactory.computeJoinCacheKey(noopDataSource)).andReturn(expected);\n     EasyMock.replay(noopJoinableFactory);\n     Optional<byte[]> actual = target.computeJoinCacheKey(noopDataSource);\n-    Assert.assertEquals(expected, actual);\n+    Assert.assertSame(expected, actual);\n   }\n \n-  @Test(expected = ISE.class)\n+  @Test\n   public void testBuildExceptionWhenTwoJoinableFactoryForSameDataSource()\n   {\n     JoinableFactory anotherNoopJoinableFactory = EasyMock.mock(MapJoinableFactory.class);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMDEyOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491600128", "bodyText": "Please use Assert.assertArrayEquals(). Also the expected result should be the first argument.", "author": "jihoonson", "createdAt": "2020-09-19T23:22:26Z", "path": "processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java", "diffHunk": "@@ -179,4 +179,13 @@ public void testVersion()\n     Assert.assertEquals(JoinTestHelper.INDEXED_TABLE_VERSION, countriesTable.version());\n     Assert.assertEquals(JoinTestHelper.INDEXED_TABLE_VERSION, regionsTable.version());\n   }\n+\n+  @Test\n+  public void testIsCacheable() throws IOException\n+  {\n+    Assert.assertFalse(countriesTable.isCacheable());\n+    RowBasedIndexedTable<Map<String, Object>> countriesTableWithCacheKey = JoinTestHelper.createCountriesIndexedTableWithCacheKey();\n+    Assert.assertTrue(countriesTableWithCacheKey.isCacheable());\n+    Assert.assertEquals(countriesTableWithCacheKey.computeCacheKey(), JoinTestHelper.INDEXED_TABLE_CACHE_KEY);", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java b/processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java\nindex 229bfa316a..371457f424 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/table/RowBasedIndexedTableTest.java\n\n@@ -186,6 +186,6 @@ public class RowBasedIndexedTableTest\n     Assert.assertFalse(countriesTable.isCacheable());\n     RowBasedIndexedTable<Map<String, Object>> countriesTableWithCacheKey = JoinTestHelper.createCountriesIndexedTableWithCacheKey();\n     Assert.assertTrue(countriesTableWithCacheKey.isCacheable());\n-    Assert.assertEquals(countriesTableWithCacheKey.computeCacheKey(), JoinTestHelper.INDEXED_TABLE_CACHE_KEY);\n+    Assert.assertArrayEquals(JoinTestHelper.INDEXED_TABLE_CACHE_KEY, countriesTableWithCacheKey.computeCacheKey());\n   }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMTkxNA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491601914", "bodyText": "Type argument is not required.", "author": "jihoonson", "createdAt": "2020-09-19T23:26:01Z", "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java", "diffHunk": "@@ -224,8 +228,9 @@ public SinkQuerySegmentWalker(\n                       // 1) Only use caching if data is immutable\n                       // 2) Hydrants are not the same between replicas, make sure cache is local\n                       if (hydrantDefinitelySwapped && cache.isLocal()) {\n-                        runner = new CachingQueryRunner<>(\n+                        runner = new CachingQueryRunner<T>(", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java\nindex fe6dc348da..be4952aff0 100644\n--- a/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java\n+++ b/server/src/main/java/org/apache/druid/segment/realtime/appenderator/SinkQuerySegmentWalker.java\n\n@@ -228,7 +228,7 @@ public class SinkQuerySegmentWalker implements QuerySegmentWalker\n                       // 1) Only use caching if data is immutable\n                       // 2) Hydrants are not the same between replicas, make sure cache is local\n                       if (hydrantDefinitelySwapped && cache.isLocal()) {\n-                        runner = new CachingQueryRunner<T>(\n+                        runner = new CachingQueryRunner<>(\n                             makeHydrantCacheIdentifier(hydrant),\n                             cacheKeyPrefix,\n                             descriptor,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMzA0NQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491603045", "bodyText": "Why is this change needed?", "author": "jihoonson", "createdAt": "2020-09-19T23:28:24Z", "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -293,8 +300,9 @@ public ServerManager(\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    CachingQueryRunner<T> cachingQueryRunner = new CachingQueryRunner<>(\n+    QueryRunner<T> queryRunner = new CachingQueryRunner<>(", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTkxMzUyOQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491913529", "bodyText": "will revert. must have been some temporary refactoring.", "author": "abhishekagarwal87", "createdAt": "2020-09-21T09:45:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMzA0NQ=="}], "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\nindex eeee0f45de..48d2ca86e8 100644\n--- a/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n+++ b/server/src/main/java/org/apache/druid/server/coordination/ServerManager.java\n\n@@ -300,7 +300,7 @@ public class ServerManager implements QuerySegmentWalker\n         queryMetrics -> queryMetrics.segment(segmentIdString)\n     );\n \n-    QueryRunner<T> queryRunner = new CachingQueryRunner<>(\n+    CachingQueryRunner<T> cachingQueryRunner = new CachingQueryRunner<>(\n         segmentIdString,\n         cacheKeyPrefix,\n         segmentDescriptor,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTYwMzY3MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r491603670", "bodyText": "Please use Intervals.utc() instead.", "author": "jihoonson", "createdAt": "2020-09-19T23:29:40Z", "path": "server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java", "diffHunk": "@@ -340,4 +356,15 @@ private DataSegment createSegment(IncrementalIndex data, String interval, String\n         ImmutableMap.of(\"type\", \"local\", \"path\", segmentDir.getAbsolutePath())\n     );\n   }\n+\n+  private void assertSegmentIdEquals(SegmentId id, byte[] bytes)\n+  {\n+    ByteBuffer byteBuffer = ByteBuffer.wrap(bytes);\n+    long start = byteBuffer.getLong();\n+    long end = byteBuffer.getLong();\n+    String version = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getVersion()));\n+    String dataSource = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getDataSource()));\n+    int partition = byteBuffer.getInt();\n+    Assert.assertEquals(id, SegmentId.of(dataSource, new Interval(start, end, DateTimeZone.UTC), version, partition));", "originalCommit": "50ebf74f1080e3a877c4ec19ca95be070825f922", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "chunk": "diff --git a/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java b/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java\nindex 8ef907ac98..5b7655af91 100644\n--- a/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java\n+++ b/server/src/test/java/org/apache/druid/server/SegmentManagerBroadcastJoinIndexedTableTest.java\n\n@@ -365,6 +363,6 @@ public class SegmentManagerBroadcastJoinIndexedTableTest extends InitializedNull\n     String version = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getVersion()));\n     String dataSource = StringUtils.fromUtf8(byteBuffer, StringUtils.estimatedBinaryLengthAsUTF8(id.getDataSource()));\n     int partition = byteBuffer.getInt();\n-    Assert.assertEquals(id, SegmentId.of(dataSource, new Interval(start, end, DateTimeZone.UTC), version, partition));\n+    Assert.assertEquals(id, SegmentId.of(dataSource, Intervals.utc(start, end), version, partition));\n   }\n }\n"}}, {"oid": "12fd5e8d823b9377589fcf1fbaf252d50727b14f", "url": "https://github.com/apache/druid/commit/12fd5e8d823b9377589fcf1fbaf252d50727b14f", "message": "Fix compilation", "committedDate": "2020-09-21T09:10:37Z", "type": "commit"}, {"oid": "cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "url": "https://github.com/apache/druid/commit/cf435b999fde89d6c6a658bf5c3d5755c8bb743f", "message": "Review comments", "committedDate": "2020-09-21T13:06:59Z", "type": "commit"}, {"oid": "e9f1ae8a946ba4c5fc22e3fd780ca8d170d96c72", "url": "https://github.com/apache/druid/commit/e9f1ae8a946ba4c5fc22e3fd780ca8d170d96c72", "message": "Add more tests", "committedDate": "2020-09-22T14:40:53Z", "type": "commit"}, {"oid": "c9d758467af197e4beee954fba86676c6828941b", "url": "https://github.com/apache/druid/commit/c9d758467af197e4beee954fba86676c6828941b", "message": "Fix inspection errors", "committedDate": "2020-09-22T16:55:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MTg1Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493091856", "bodyText": "Please use StringUtils.format() instead here and in other places.", "author": "jihoonson", "createdAt": "2020-09-22T23:38:11Z", "path": "processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java", "diffHunk": "@@ -113,6 +118,12 @@ public void testBuildExceptionWhenTwoJoinableFactoryForSameDataSource()\n     EasyMock.expect(noopJoinableFactory.build(noopDataSource, condition)).andReturn(Optional.of(mockJoinable));\n     EasyMock.expect(anotherNoopJoinableFactory.build(noopDataSource, condition)).andReturn(Optional.of(mockJoinable));\n     EasyMock.replay(noopJoinableFactory, anotherNoopJoinableFactory);\n+    expectedException.expect(ISE.class);\n+    expectedException.expectMessage(String.format(", "originalCommit": "c9d758467af197e4beee954fba86676c6828941b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\nindex bcde19214c..74f21b3bf9 100644\n--- a/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n+++ b/processing/src/test/java/org/apache/druid/segment/join/MapJoinableFactoryTest.java\n\n@@ -119,8 +119,7 @@ public class MapJoinableFactoryTest\n     EasyMock.expect(anotherNoopJoinableFactory.build(noopDataSource, condition)).andReturn(Optional.of(mockJoinable));\n     EasyMock.replay(noopJoinableFactory, anotherNoopJoinableFactory);\n     expectedException.expect(ISE.class);\n-    expectedException.expectMessage(String.format(\n-        Locale.getDefault(),\n+    expectedException.expectMessage(StringUtils.format(\n         \"Multiple joinable factories are valid for table[%s]\",\n         noopDataSource\n     ));\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzA5MzA2Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r493093066", "bodyText": "This is nice \ud83d\udc4d", "author": "jihoonson", "createdAt": "2020-09-22T23:41:48Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -796,4 +749,102 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>", "originalCommit": "c9d758467af197e4beee954fba86676c6828941b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "61ed8df518cecb725bbf9155618ffcac693f476f", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 31f1c2cb80..fd6ea57c30 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -760,7 +761,7 @@ public class CachingClusteredClient implements QuerySegmentWalker\n     private final Query<T> query;\n     private final CacheStrategy<T, Object, Query<T>> strategy;\n     private final DataSourceAnalysis dataSourceAnalysis;\n-    private final Joinables joinables;\n+    private final JoinableFactoryWrapper joinableFactoryWrapper;\n     private final boolean isSegmentLevelCachingEnable;\n \n     CacheKeyManager(\n"}}, {"oid": "f06269ad504cf0efbfa9939c9c746928ef75b542", "url": "https://github.com/apache/druid/commit/f06269ad504cf0efbfa9939c9c746928ef75b542", "message": "Pushed condition analysis to JoinableFactory", "committedDate": "2020-09-23T08:28:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTI2MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494681260", "bodyText": "Consider naming this getCacheKey() and extending Cacheable. That interface is mainly used by CacheKeyBuilder, which it doesn't look like we actually need here, but it's still nice to use that interface for things that can generate their own cache keys.", "author": "gianm", "createdAt": "2020-09-25T00:27:36Z", "path": "processing/src/main/java/org/apache/druid/segment/join/table/IndexedTable.java", "diffHunk": "@@ -87,6 +88,26 @@ default ColumnSelectorFactory makeColumnSelectorFactory(ReadableOffset offset, b\n     return null;\n   }\n \n+  /**\n+   * Computes a {@code byte[]} key for the table that can be used for computing cache keys for join operations.\n+   * see {@link org.apache.druid.segment.join.JoinableFactory#computeJoinCacheKey}\n+   *\n+   * @return the byte array for cache key\n+   * @throws {@link IAE} if caching is not supported\n+   */\n+  default byte[] computeCacheKey()", "originalCommit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgwNzQ2Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494807466", "bodyText": "I think part of the reason for not doing so was that not all IndexedTables are Cacheable. That is why I have two methods\nisCacheable\ncomputeCacheKey\nI could get rid of isCacheable and return null from computeCacheKey when caching is unsupported. That didn't look very robust to me.", "author": "abhishekagarwal87", "createdAt": "2020-09-25T07:44:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTI2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzMDgzOQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495030839", "bodyText": "Sounds good. It's probably not a good idea to make getCacheKey() unreliable in the Cacheable interface, so it makes sense not to use that interface here.", "author": "gianm", "createdAt": "2020-09-25T14:34:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4MTI2MA=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4Mjk5Mg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494682992", "bodyText": "I think\u00a0you just copied this from somewhere else, but IMO it would be better as a precondition check (i.e. throw ISE or NPE if it fails) instead of an assertion.\nGenerally we'll use assertions to note things that are meant to be impossible, and precondition checks for things that are possible but are incorrect usage. Since it is possible to create this CacheKeyManager class with a null strategy and then call computeQueryCacheKeyWithJoin, it'd be better for this to be a precondition check.", "author": "gianm", "createdAt": "2020-09-25T00:34:24Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -770,4 +749,102 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>\n+  {\n+    private final Query<T> query;\n+    private final CacheStrategy<T, Object, Query<T>> strategy;\n+    private final DataSourceAnalysis dataSourceAnalysis;\n+    private final Joinables joinables;\n+    private final boolean isSegmentLevelCachingEnable;\n+\n+    CacheKeyManager(\n+        final Query<T> query,\n+        final CacheStrategy<T, Object, Query<T>> strategy,\n+        final boolean useCache,\n+        final boolean populateCache,\n+        final DataSourceAnalysis dataSourceAnalysis,\n+        final Joinables joinables\n+    )\n+    {\n+\n+      this.query = query;\n+      this.strategy = strategy;\n+      this.dataSourceAnalysis = dataSourceAnalysis;\n+      this.joinables = joinables;\n+      this.isSegmentLevelCachingEnable = ((populateCache || useCache)\n+                                          && !QueryContexts.isBySegment(query));   // explicit bySegment queries are never cached\n+\n+    }\n+\n+    @Nullable\n+    byte[] computeSegmentLevelQueryCacheKey()\n+    {\n+      if (isSegmentLevelCachingEnable) {\n+        return computeQueryCacheKeyWithJoin();\n+      }\n+      return null;\n+    }\n+\n+    /**\n+     * It computes the ETAG which is used by {@link org.apache.druid.query.ResultLevelCachingQueryRunner} for\n+     * result level caches. queryCacheKey can be null if segment level cache is not being used. However, ETAG\n+     * is still computed since result level cache may still be on.\n+     */\n+    @Nullable\n+    String computeResultLevelCachingEtag(\n+        final Set<SegmentServerSelector> segments,\n+        @Nullable byte[] queryCacheKey\n+    )\n+    {\n+      Hasher hasher = Hashing.sha1().newHasher();\n+      boolean hasOnlyHistoricalSegments = true;\n+      for (SegmentServerSelector p : segments) {\n+        if (!p.getServer().pick().getServer().isSegmentReplicationTarget()) {\n+          hasOnlyHistoricalSegments = false;\n+          break;\n+        }\n+        hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);\n+        // it is important to add the \"query interval\" as part ETag calculation\n+        // to have result level cache work correctly for queries with different\n+        // intervals covering the same set of segments\n+        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);\n+      }\n+\n+      if (!hasOnlyHistoricalSegments) {\n+        return null;\n+      }\n+\n+      // query cache key can be null if segment level caching is disabled\n+      final byte[] queryCacheKeyFinal = (queryCacheKey == null) ? computeQueryCacheKeyWithJoin() : queryCacheKey;\n+      if (queryCacheKeyFinal == null) {\n+        return null;\n+      }\n+      hasher.putBytes(queryCacheKeyFinal);\n+      String currEtag = StringUtils.encodeBase64String(hasher.hash().asBytes());\n+      return currEtag;\n+    }\n+\n+    /**\n+     * Adds the cache key prefix for join data sources. Return null if its a join but caching is not supported\n+     */\n+    @Nullable\n+    private byte[] computeQueryCacheKeyWithJoin()\n+    {\n+      assert strategy != null;  // implies strategy != null", "originalCommit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg0Mzc2MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494843760", "bodyText": "I think it was someone suppressing the intellij warning about strategy being null. I replaced it with Precondition.checkNotNull", "author": "abhishekagarwal87", "createdAt": "2020-09-25T08:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4Mjk5Mg=="}], "type": "inlineReview", "revised_code": {"commit": "61ed8df518cecb725bbf9155618ffcac693f476f", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 31f1c2cb80..fd6ea57c30 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -760,7 +761,7 @@ public class CachingClusteredClient implements QuerySegmentWalker\n     private final Query<T> query;\n     private final CacheStrategy<T, Object, Query<T>> strategy;\n     private final DataSourceAnalysis dataSourceAnalysis;\n-    private final Joinables joinables;\n+    private final JoinableFactoryWrapper joinableFactoryWrapper;\n     private final boolean isSegmentLevelCachingEnable;\n \n     CacheKeyManager(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494684538", "bodyText": "I don't think the concept of computing cache keys in CachingClusteredClient works properly here, because of the following scenario:\n\nThere is a broadcast table that we'll be joining against.\nThe Broker (which runs CCC) updates its broadcast table to a newer version.\nThe Broker gets a query and uses the newer version in the cache key.\nIt fans out the query to a data server that hasn't got the newest broadcast table yet.\nThe data server returns results for the older table, and the Broker caches them.\nNow, the Broker's cache is wrong (it refers to older data with a newer key).\n\nThe solution that comes to mind is that the data servers should keep both the old and new version around for a bit when they swap them out, and the Broker should send the specific version that it wants them to use, so it can be sure it's getting the right one. But this is out of the scope of this PR. For now I suggest not implementing caching for join datasources on the Broker. (Broker caching is off by default, anyway, so it's not the end of the world.)", "author": "gianm", "createdAt": "2020-09-25T00:39:07Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -293,6 +301,14 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.intervals = dataSourceAnalysis.getBaseQuerySegmentSpec()\n                                          .map(QuerySegmentSpec::getIntervals)\n                                          .orElseGet(() -> query.getIntervals());\n+      this.cacheKeyManager = new CacheKeyManager<>(", "originalCommit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgzOTE4Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494839186", "bodyText": "That is a good corner case. I didn't think of this. I don't see an easy flag to selectively disable result level caching for join. So I am just going to hard-code this in the new CacheKeyManager.  sounds good?", "author": "abhishekagarwal87", "createdAt": "2020-09-25T08:43:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzMDE0NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495030144", "bodyText": "I think this is an issue for both kinds of caching (segment and result-level) on the Broker, because they both use keys that might not match the inputs that are used by the data server.", "author": "gianm", "createdAt": "2020-09-25T14:33:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEyMjk1Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495122956", "bodyText": "Hmm, good catch. I think the real fix would probably be including the version of broadcast segments in the query which is sent to data servers.", "author": "jihoonson", "createdAt": "2020-09-25T17:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTEzNzI4Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495137286", "bodyText": "wouldn't it be better for data servers to send the segment version to the broker which it can either use in cache key or not cache at all. though doing this might be trickier than passing extra information in the query.", "author": "abhishekagarwal87", "createdAt": "2020-09-25T17:38:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0NTM1MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495145350", "bodyText": "I think @jihoonson's suggestion is best, because that would not only make caching correct, it would also ensure that the same broadcast table version is used across all data servers involved in a query. That'd make the query results more internally consistent (today, potentially, a different version of the table could be used on different data servers).", "author": "gianm", "createdAt": "2020-09-25T17:55:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcyOTk4MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495729980", "bodyText": "I suppose, in that case, we also keep both the versions on a data server for a while. Or otherwise, the table will not be in a queryable state till all the data servers see the new version.", "author": "abhishekagarwal87", "createdAt": "2020-09-28T07:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjA5OTQ4MQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r496099481", "bodyText": "Yeah, we'd need to do that. The Coordinator would need to participate in the logic somehow (it would need to not issue any drops for the old segments until the new ones are fully loaded).", "author": "gianm", "createdAt": "2020-09-28T16:59:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4NDUzOA=="}], "type": "inlineReview", "revised_code": {"commit": "61ed8df518cecb725bbf9155618ffcac693f476f", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 31f1c2cb80..fd6ea57c30 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -307,7 +308,7 @@ public class CachingClusteredClient implements QuerySegmentWalker\n           useCache,\n           populateCache,\n           dataSourceAnalysis,\n-          joinables\n+          joinableFactoryWrapper\n       );\n     }\n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDMxMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494690313", "bodyText": "Is it not possible for strategy to be null?", "author": "gianm", "createdAt": "2020-09-25T01:02:18Z", "path": "server/src/main/java/org/apache/druid/client/CachingQueryRunner.java", "diffHunk": "@@ -77,20 +83,15 @@ public CachingQueryRunner(\n   {\n     Query<T> query = queryPlus.getQuery();\n     final CacheStrategy strategy = toolChest.getCacheStrategy(query);\n-    final boolean populateCache = CacheUtil.isPopulateSegmentCache(\n-        query,\n-        strategy,\n-        cacheConfig,\n-        CacheUtil.ServerType.DATA\n-    );\n-    final boolean useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.DATA);\n+    final boolean populateCache = canPopulateCache(query, strategy);\n+    final boolean useCache = canUseCache(query, strategy);\n \n     final Cache.NamedKey key;\n-    if (strategy != null && (useCache || populateCache)) {", "originalCommit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDgyNjY1MQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494826651", "bodyText": "if its null, then both useCache and populateCache will be set to false. if that doesn't hold, then an NPE can occur later in the code. I can still add another check in canUseCache, canPopulateCache.", "author": "abhishekagarwal87", "createdAt": "2020-09-25T08:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDMxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTAzMTg1NA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495031854", "bodyText": "Got it. I don't think the extra check is necessary, given what you just said. Maybe just a comment.", "author": "gianm", "createdAt": "2020-09-25T14:35:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDMxMw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MjgwMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r494692803", "bodyText": "The clause's prefix is important too, because it controls the names of the columns.", "author": "gianm", "createdAt": "2020-09-25T01:13:08Z", "path": "processing/src/main/java/org/apache/druid/segment/join/Joinables.java", "diffHunk": "@@ -118,6 +107,68 @@ public static boolean isPrefixedBy(final String columnName, final String prefix)\n     );\n   }\n \n+  /**\n+   * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n+   * can be used in segment level cache or result level cache. The function can return following wrapped in an\n+   * Optional\n+   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   *  in the JOIN is not cacheable.\n+   *\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n+   * @param dataSourceAnalysis for the join datasource\n+   * @return the optional cache key to be used as part of query cache key\n+   */\n+  public Optional<byte[]> computeJoinDataSourceCacheKey(\n+      final DataSourceAnalysis dataSourceAnalysis\n+  )\n+  {\n+    final List<PreJoinableClause> clauses = dataSourceAnalysis.getPreJoinableClauses();\n+    if (clauses.isEmpty()) {\n+      throw new IAE(\"No join clauses to build the cache key for data source [%s]\", dataSourceAnalysis.getDataSource());\n+    }\n+\n+    final CacheKeyBuilder keyBuilder;\n+    keyBuilder = new CacheKeyBuilder(JOIN_OPERATION);\n+    for (PreJoinableClause clause : clauses) {\n+      Optional<byte[]> bytes = joinableFactory.computeJoinCacheKey(clause.getDataSource(), clause.getCondition());\n+      if (!bytes.isPresent()) {\n+        // Encountered a data source which didn't support cache yet\n+        log.debug(\"skipping caching for join since [%s] does not support caching\", clause.getDataSource());\n+        return Optional.empty();\n+      }\n+      keyBuilder.appendByteArray(bytes.get());\n+      keyBuilder.appendString(clause.getCondition().getOriginalExpression());\n+      keyBuilder.appendString(clause.getJoinType().name());", "originalCommit": "f06269ad504cf0efbfa9939c9c746928ef75b542", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTA4NTg3MQ==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495085871", "bodyText": "functionally it makes no difference, right? do the cached results store column names with the prefix?", "author": "abhishekagarwal87", "createdAt": "2020-09-25T16:01:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE0NDU1Ng==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495144556", "bodyText": "It makes a difference because the prefixed fields might be inputs to aggregators, the group by clause, etc. If the prefix changes then those things would behave differently.", "author": "gianm", "createdAt": "2020-09-25T17:53:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MjgwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcyODk3MA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r495728970", "bodyText": "added the prefix.", "author": "abhishekagarwal87", "createdAt": "2020-09-28T07:06:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MjgwMw=="}], "type": "inlineReview", "revised_code": {"commit": "61ed8df518cecb725bbf9155618ffcac693f476f", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java b/processing/src/main/java/org/apache/druid/segment/join/JoinableFactoryWrapper.java\nsimilarity index 68%\nrename from processing/src/main/java/org/apache/druid/segment/join/Joinables.java\nrename to processing/src/main/java/org/apache/druid/segment/join/JoinableFactoryWrapper.java\nindex 668828a0c1..df7e7f3ba6 100644\n--- a/processing/src/main/java/org/apache/druid/segment/join/Joinables.java\n+++ b/processing/src/main/java/org/apache/druid/segment/join/JoinableFactoryWrapper.java\n\n@@ -111,14 +106,14 @@ public class Joinables\n    * Compute a cache key prefix for data sources that participate in the RHS of a join. This key prefix\n    * can be used in segment level cache or result level cache. The function can return following wrapped in an\n    * Optional\n-   *  - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n-   *  join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n-   *  - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n-   *  in the JOIN is not cacheable.\n+   * - Non-empty byte array - If there is join datasource involved and caching is possible. The result includes\n+   * join condition expression, join type and cache key returned by joinable factory for each {@link PreJoinableClause}\n+   * - NULL - There is a join but caching is not possible. It may happen if one of the participating datasource\n+   * in the JOIN is not cacheable.\n    *\n-   * @throws {@link IAE} if this operation is called on a non-join data source\n    * @param dataSourceAnalysis for the join datasource\n    * @return the optional cache key to be used as part of query cache key\n+   * @throws {@link IAE} if this operation is called on a non-join data source\n    */\n   public Optional<byte[]> computeJoinDataSourceCacheKey(\n       final DataSourceAnalysis dataSourceAnalysis\n"}}, {"oid": "61ed8df518cecb725bbf9155618ffcac693f476f", "url": "https://github.com/apache/druid/commit/61ed8df518cecb725bbf9155618ffcac693f476f", "message": "review comments", "committedDate": "2020-09-28T07:05:20Z", "type": "commit"}, {"oid": "e153fa70cb0610870f76eb9a524cbf20794e0ff5", "url": "https://github.com/apache/druid/commit/e153fa70cb0610870f76eb9a524cbf20794e0ff5", "message": "Disable join caching for broker and add prefix key to BroadcastSegmentIndexedTable", "committedDate": "2020-09-29T14:21:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDEyOTg3Nw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r500129877", "bodyText": "JoinableFactoryWrapper still doesn't seem quite right. These walkers all take a JoinableFactory just to make a JoinableFactoryWrapper in the constructor. JoinableFactoryWrapper doesn't seem to have any state of its own, why not just make it in the joinable module and inject it directly? Or maybe it didn't really need to change from static methods?\nRegardless, it doesn't need to change in this PR, we can refactor this in the future, since it feels like maybe there is also another refactor lurking that I haven't quite figured out in wrapping CacheConfig, CachePopulator, Cache (and maybe CacheKeyManager for brokers) into some tidy package to handle caching stuffs for walkers with implementations that do the right thing for where they are running.", "author": "clintropolis", "createdAt": "2020-10-06T09:20:37Z", "path": "server/src/main/java/org/apache/druid/server/coordination/ServerManager.java", "diffHunk": "@@ -117,7 +117,7 @@ public ServerManager(\n \n     this.cacheConfig = cacheConfig;\n     this.segmentManager = segmentManager;\n-    this.joinableFactory = joinableFactory;\n+    this.joinableFactoryWrapper = new JoinableFactoryWrapper(joinableFactory);", "originalCommit": "e153fa70cb0610870f76eb9a524cbf20794e0ff5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE0MTkxMw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r500141913", "bodyText": "My primary reason for not using static methods, was easier mocking and unit-testing. I think injecting JoinableFactoryWrapper directly makes sense. Another option is to merge MapJoinableFactory and JoinableFactoryWrapper and cut down on a class. Both these classes are kind of providing a utility function on top of JoinableFactory.", "author": "abhishekagarwal87", "createdAt": "2020-10-06T09:40:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDEyOTg3Nw=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDUxNjE3Mg==", "url": "https://github.com/apache/druid/pull/10366#discussion_r500516172", "bodyText": "Please remove these lines. We can add them back later when we enable it.", "author": "jihoonson", "createdAt": "2020-10-06T18:38:20Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -770,4 +749,103 @@ private void addSequencesFromServer(\n           .flatMerge(seq -> seq, query.getResultOrdering());\n     }\n   }\n+\n+  /**\n+   * An inner class that is used solely for computing cache keys. Its a separate class to allow extensive unit testing\n+   * of cache key generation.\n+   */\n+  @VisibleForTesting\n+  static class CacheKeyManager<T>\n+  {\n+    private final Query<T> query;\n+    private final CacheStrategy<T, Object, Query<T>> strategy;\n+    private final DataSourceAnalysis dataSourceAnalysis;\n+    private final JoinableFactoryWrapper joinableFactoryWrapper;\n+    private final boolean isSegmentLevelCachingEnable;\n+\n+    CacheKeyManager(\n+        final Query<T> query,\n+        final CacheStrategy<T, Object, Query<T>> strategy,\n+        final boolean useCache,\n+        final boolean populateCache,\n+        final DataSourceAnalysis dataSourceAnalysis,\n+        final JoinableFactoryWrapper joinableFactoryWrapper\n+    )\n+    {\n+\n+      this.query = query;\n+      this.strategy = strategy;\n+      this.dataSourceAnalysis = dataSourceAnalysis;\n+      this.joinableFactoryWrapper = joinableFactoryWrapper;\n+      this.isSegmentLevelCachingEnable = ((populateCache || useCache)\n+                                          && !QueryContexts.isBySegment(query));   // explicit bySegment queries are never cached\n+\n+    }\n+\n+    @Nullable\n+    byte[] computeSegmentLevelQueryCacheKey()\n+    {\n+      if (isSegmentLevelCachingEnable) {\n+        return computeQueryCacheKeyWithJoin();\n+      }\n+      return null;\n+    }\n+\n+    /**\n+     * It computes the ETAG which is used by {@link org.apache.druid.query.ResultLevelCachingQueryRunner} for\n+     * result level caches. queryCacheKey can be null if segment level cache is not being used. However, ETAG\n+     * is still computed since result level cache may still be on.\n+     */\n+    @Nullable\n+    String computeResultLevelCachingEtag(\n+        final Set<SegmentServerSelector> segments,\n+        @Nullable byte[] queryCacheKey\n+    )\n+    {\n+      Hasher hasher = Hashing.sha1().newHasher();\n+      boolean hasOnlyHistoricalSegments = true;\n+      for (SegmentServerSelector p : segments) {\n+        if (!p.getServer().pick().getServer().isSegmentReplicationTarget()) {\n+          hasOnlyHistoricalSegments = false;\n+          break;\n+        }\n+        hasher.putString(p.getServer().getSegment().getId().toString(), StandardCharsets.UTF_8);\n+        // it is important to add the \"query interval\" as part ETag calculation\n+        // to have result level cache work correctly for queries with different\n+        // intervals covering the same set of segments\n+        hasher.putString(p.rhs.getInterval().toString(), StandardCharsets.UTF_8);\n+      }\n+\n+      if (!hasOnlyHistoricalSegments) {\n+        return null;\n+      }\n+\n+      // query cache key can be null if segment level caching is disabled\n+      final byte[] queryCacheKeyFinal = (queryCacheKey == null) ? computeQueryCacheKeyWithJoin() : queryCacheKey;\n+      if (queryCacheKeyFinal == null) {\n+        return null;\n+      }\n+      hasher.putBytes(queryCacheKeyFinal);\n+      String currEtag = StringUtils.encodeBase64String(hasher.hash().asBytes());\n+      return currEtag;\n+    }\n+\n+    /**\n+     * Adds the cache key prefix for join data sources. Return null if its a join but caching is not supported\n+     */\n+    @Nullable\n+    private byte[] computeQueryCacheKeyWithJoin()\n+    {\n+      Preconditions.checkNotNull(strategy, \"strategy cannot be null\");\n+      if (dataSourceAnalysis.isJoin()) {\n+        return null; // Broker join caching disabled - https://github.com/apache/druid/issues/10444\n+       /* byte[] joinDataSourceCacheKey = joinableFactoryWrapper.computeJoinDataSourceCacheKey(dataSourceAnalysis).orElse(null);\n+        if (null == joinDataSourceCacheKey) {\n+          return null;    // A join operation that does not support caching\n+        }\n+        return Bytes.concat(joinDataSourceCacheKey, strategy.computeCacheKey(query));*/", "originalCommit": "e153fa70cb0610870f76eb9a524cbf20794e0ff5", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "c10be24e9571deee57fbde958b80862634fb7e9e", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 9dda2eed8f..5dd55f9f13 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -839,11 +839,6 @@ public class CachingClusteredClient implements QuerySegmentWalker\n       Preconditions.checkNotNull(strategy, \"strategy cannot be null\");\n       if (dataSourceAnalysis.isJoin()) {\n         return null; // Broker join caching disabled - https://github.com/apache/druid/issues/10444\n-       /* byte[] joinDataSourceCacheKey = joinableFactoryWrapper.computeJoinDataSourceCacheKey(dataSourceAnalysis).orElse(null);\n-        if (null == joinDataSourceCacheKey) {\n-          return null;    // A join operation that does not support caching\n-        }\n-        return Bytes.concat(joinDataSourceCacheKey, strategy.computeCacheKey(query));*/\n       }\n       return strategy.computeCacheKey(query);\n     }\n"}}, {"oid": "c10be24e9571deee57fbde958b80862634fb7e9e", "url": "https://github.com/apache/druid/commit/c10be24e9571deee57fbde958b80862634fb7e9e", "message": "Remove commented lines", "committedDate": "2020-10-07T13:28:49Z", "type": "commit"}, {"oid": "91c3532abef45b4d7a4759848fc7074847d36f00", "url": "https://github.com/apache/druid/commit/91c3532abef45b4d7a4759848fc7074847d36f00", "message": "Fix populateCache", "committedDate": "2020-10-08T10:49:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA4MjE2Nw==", "url": "https://github.com/apache/druid/pull/10366#discussion_r502082167", "bodyText": "I think this check should be not here, but inside CacheUtil.isUseSegmentCache(). The reason we pass the ServerType in isUseSegmentCache() is that we want to make different decisions based on the ServerType. We are disabling the cache for join on brokers, so it should be in there.", "author": "jihoonson", "createdAt": "2020-10-08T23:45:52Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -289,14 +289,17 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.query = queryPlus.getQuery();\n       this.toolChest = warehouse.getToolChest(query);\n       this.strategy = toolChest.getCacheStrategy(query);\n+      this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n+                      && !dataSourceAnalysis.isJoin();", "originalCommit": "91c3532abef45b4d7a4759848fc7074847d36f00", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f1a62f8b333da76aa814e3c7b89da95135697645", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 68e9a1d829..13c87a1525 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -291,11 +292,8 @@ public class CachingClusteredClient implements QuerySegmentWalker\n       this.strategy = toolChest.getCacheStrategy(query);\n       this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n-                      && !dataSourceAnalysis.isJoin();\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n-                           && !dataSourceAnalysis.isJoin();\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n       this.isBySegment = QueryContexts.isBySegment(query);\n       // Note that enabling this leads to putting uncovered intervals information in the response headers\n       // and might blow up in some cases https://github.com/apache/druid/issues/2108\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA4MjE5OA==", "url": "https://github.com/apache/druid/pull/10366#discussion_r502082198", "bodyText": "Same comment.", "author": "jihoonson", "createdAt": "2020-10-08T23:45:59Z", "path": "server/src/main/java/org/apache/druid/client/CachingClusteredClient.java", "diffHunk": "@@ -289,14 +289,17 @@ private ClusterQueryResult(Sequence<T> sequence, int numQueryServers)\n       this.query = queryPlus.getQuery();\n       this.toolChest = warehouse.getToolChest(query);\n       this.strategy = toolChest.getCacheStrategy(query);\n+      this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n+                      && !dataSourceAnalysis.isJoin();\n+      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)", "originalCommit": "91c3532abef45b4d7a4759848fc7074847d36f00", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "f1a62f8b333da76aa814e3c7b89da95135697645", "chunk": "diff --git a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\nindex 68e9a1d829..13c87a1525 100644\n--- a/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n+++ b/server/src/main/java/org/apache/druid/client/CachingClusteredClient.java\n\n@@ -291,11 +292,8 @@ public class CachingClusteredClient implements QuerySegmentWalker\n       this.strategy = toolChest.getCacheStrategy(query);\n       this.dataSourceAnalysis = DataSourceAnalysis.forDataSource(query.getDataSource());\n \n-      // Broker join caching is disabled - https://github.com/apache/druid/issues/10444\n-      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n-                      && !dataSourceAnalysis.isJoin();\n-      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER)\n-                           && !dataSourceAnalysis.isJoin();\n+      this.useCache = CacheUtil.isUseSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n+      this.populateCache = CacheUtil.isPopulateSegmentCache(query, strategy, cacheConfig, CacheUtil.ServerType.BROKER);\n       this.isBySegment = QueryContexts.isBySegment(query);\n       // Note that enabling this leads to putting uncovered intervals information in the response headers\n       // and might blow up in some cases https://github.com/apache/druid/issues/2108\n"}}, {"oid": "f1a62f8b333da76aa814e3c7b89da95135697645", "url": "https://github.com/apache/druid/commit/f1a62f8b333da76aa814e3c7b89da95135697645", "message": "Disable caching for selective datasources\n\nRefactored the code so that we can decide at the data source level, whether to enable cache for broker or data nodes", "committedDate": "2020-10-09T15:32:31Z", "type": "commit"}]}