{"pr_number": 10336, "pr_title": "More structured way to handle parse exceptions", "pr_createdAt": "2020-09-01T01:24:21Z", "pr_url": "https://github.com/apache/druid/pull/10336", "timeline": [{"oid": "91381e3ca5268c0334cec4a7ccb6ac4542020e5a", "url": "https://github.com/apache/druid/commit/91381e3ca5268c0334cec4a7ccb6ac4542020e5a", "message": "More structured way to handle parse exceptions", "committedDate": "2020-09-01T01:12:26Z", "type": "commit"}, {"oid": "067d742daf0377521359beb2d6cdc37cd115df0a", "url": "https://github.com/apache/druid/commit/067d742daf0377521359beb2d6cdc37cd115df0a", "message": "checkstyle; add more tests", "committedDate": "2020-09-01T04:46:53Z", "type": "commit"}, {"oid": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4", "url": "https://github.com/apache/druid/commit/6fde0beb21c3c9955a81e0956ac5b9920e173cd4", "message": "forbidden api; test", "committedDate": "2020-09-01T05:43:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA0MTM3NA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r481041374", "bodyText": "just curious, can this be private final CircularBuffer<ParseException> itself?", "author": "abhishekagarwal87", "createdAt": "2020-09-01T10:35:42Z", "path": "processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import com.google.common.base.Preconditions;\n+import org.apache.druid.java.util.common.RE;\n+import org.apache.druid.java.util.common.logger.Logger;\n+import org.apache.druid.java.util.common.parsers.ParseException;\n+import org.apache.druid.utils.CircularBuffer;\n+\n+import javax.annotation.Nullable;\n+\n+/**\n+ * A handler for {@link ParseException}s thrown during ingestion. Based on the given configuration, this handler can\n+ *\n+ * - log ParseExceptions.\n+ * - keep most recent N ParseExceptions in memory.\n+ * - throw a RuntimeException when it sees more ParseExceptions than {@link #maxAllowedParseExceptions}.\n+ *\n+ * No matter what the handler does, the relevant metric should be updated first.\n+ */\n+public class ParseExceptionHandler\n+{\n+  private static final Logger LOG = new Logger(ParseExceptionHandler.class);\n+\n+  private final RowIngestionMeters rowIngestionMeters;\n+  private final boolean logParseExceptions;\n+  private final int maxAllowedParseExceptions;\n+  @Nullable\n+  private final CircularBuffer<Throwable> savedParseExceptions;", "originalCommit": "6fde0beb21c3c9955a81e0956ac5b9920e173cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQwODY1Ng==", "url": "https://github.com/apache/druid/pull/10336#discussion_r481408656", "bodyText": "Oh yeah, it can. I just copied existing code without noticing it. Changed it now.", "author": "jihoonson", "createdAt": "2020-09-01T20:22:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTA0MTM3NA=="}], "type": "inlineReview", "revised_code": {"commit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java b/processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java\nindex d47d0a8177..20faa2c0aa 100644\n--- a/processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java\n+++ b/processing/src/main/java/org/apache/druid/segment/incremental/ParseExceptionHandler.java\n\n@@ -44,7 +44,7 @@ public class ParseExceptionHandler\n   private final boolean logParseExceptions;\n   private final int maxAllowedParseExceptions;\n   @Nullable\n-  private final CircularBuffer<Throwable> savedParseExceptions;\n+  private final CircularBuffer<ParseException> savedParseExceptions;\n \n   public ParseExceptionHandler(\n       RowIngestionMeters rowIngestionMeters,\n"}}, {"oid": "d7e51be606e39aa696fccaf559460f36327d0a9d", "url": "https://github.com/apache/druid/commit/d7e51be606e39aa696fccaf559460f36327d0a9d", "message": "address comment; new test", "committedDate": "2020-09-01T20:21:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1MDE1Ng==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482150156", "bodyText": "wow, I never thought of this case", "author": "suneet-s", "createdAt": "2020-09-02T15:11:53Z", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\nindex de77328583..8436ad843c 100644\n--- a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n+++ b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n\n@@ -96,14 +98,24 @@ public class MapInputRowParser implements InputRowParser<Map<String, Object>>\n       timestamp = timestampSpec.extractTimestamp(theMap);\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          e,\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (timestamp == null) {\n-      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (!Intervals.ETERNITY.contains(timestamp)) {\n       throw new ParseException(\n-          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          \"Encountered row with timestamp[%s] that cannot be represented as a long: [%s]\",\n+          timestamp,\n           rawMapToPrint(theMap)\n       );\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1NTc2MQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482155761", "bodyText": "Instead of printing the whole map, I think it would be better to just print the timestamp string that was unparsable.\nSimilar comment on line 99, this would get rid of the need of rawMapToPrint and guarantees that the invalid timestamp is always logged regardless of where in the event it is.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n          \n          \n            \n                  throw new ParseException(\"Unparseable timestamp found! Timestamp column (%s): %s\", timestampSpec.getTimestampColumn(), theMap.get(timestampSpec.getTimestampColumn()));", "author": "suneet-s", "createdAt": "2020-09-02T15:19:12Z", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzU5OA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313598", "bodyText": "It is a good idea to print the timestamp string, but I would like to keep the current behavior as well (this logging is not what I added in this PR). I modified the error message to include timestamp.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1NTc2MQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\nindex de77328583..8436ad843c 100644\n--- a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n+++ b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n\n@@ -96,14 +98,24 @@ public class MapInputRowParser implements InputRowParser<Map<String, Object>>\n       timestamp = timestampSpec.extractTimestamp(theMap);\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          e,\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (timestamp == null) {\n-      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (!Intervals.ETERNITY.contains(timestamp)) {\n       throw new ParseException(\n-          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          \"Encountered row with timestamp[%s] that cannot be represented as a long: [%s]\",\n+          timestamp,\n           rawMapToPrint(theMap)\n       );\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1OTQ3Mg==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482159472", "bodyText": "After your refactoring in this change, the parse functions on line 65 and 70 can be made private and package private (VisibleForTesting) respectively", "author": "suneet-s", "createdAt": "2020-09-02T15:24:09Z", "path": "core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java", "diffHunk": "@@ -93,23 +94,32 @@ public static InputRow parse(\n     final DateTime timestamp;\n     try {\n       timestamp = timestampSpec.extractTimestamp(theMap);\n-      if (timestamp == null) {\n-        final String input = theMap.toString();\n-        throw new NullPointerException(\n-            StringUtils.format(\n-                \"Null timestamp in input: %s\",\n-                input.length() < 100 ? input : input.substring(0, 100) + \"...\"\n-            )\n-        );\n-      }\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", theMap);\n+      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (timestamp == null) {\n+      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+    }\n+    if (!Intervals.ETERNITY.contains(timestamp)) {\n+      throw new ParseException(\n+          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          rawMapToPrint(theMap)\n+      );", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzY0Nw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313647", "bodyText": "Done.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE1OTQ3Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\nindex de77328583..8436ad843c 100644\n--- a/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n+++ b/core/src/main/java/org/apache/druid/data/input/impl/MapInputRowParser.java\n\n@@ -96,14 +98,24 @@ public class MapInputRowParser implements InputRowParser<Map<String, Object>>\n       timestamp = timestampSpec.extractTimestamp(theMap);\n     }\n     catch (Exception e) {\n-      throw new ParseException(e, \"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          e,\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (timestamp == null) {\n-      throw new ParseException(\"Unparseable timestamp found! Event: %s\", rawMapToPrint(theMap));\n+      throw new ParseException(\n+          \"Timestamp[%s] is unparseable! Event: %s\",\n+          timestampSpec.getRawTimestamp(theMap),\n+          rawMapToPrint(theMap)\n+      );\n     }\n     if (!Intervals.ETERNITY.contains(timestamp)) {\n       throw new ParseException(\n-          \"Encountered row with timestamp that cannot be represented as a long: [%s]\",\n+          \"Encountered row with timestamp[%s] that cannot be represented as a long: [%s]\",\n+          timestamp,\n           rawMapToPrint(theMap)\n       );\n     }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NDQ3Ng==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482164476", "bodyText": "nit: Maybe these 3 classes should be moved into their own sub package org.apache.druid.segment.incremental.stats", "author": "suneet-s", "createdAt": "2020-09-02T15:30:50Z", "path": "extensions-core/kafka-indexing-service/src/test/java/org/apache/druid/indexing/kafka/KafkaIndexTaskTest.java", "diffHunk": "@@ -106,6 +103,9 @@\n import org.apache.druid.query.timeseries.TimeseriesQueryEngine;\n import org.apache.druid.query.timeseries.TimeseriesQueryQueryToolChest;\n import org.apache.druid.query.timeseries.TimeseriesQueryRunnerFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMeters;\n+import org.apache.druid.segment.incremental.RowIngestionMetersFactory;\n+import org.apache.druid.segment.incremental.RowIngestionMetersTotals;", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzc0Nw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313747", "bodyText": "That might be nice. I think we probably need new classes for metrics of native batch ingestion. I will reorganize the package in a follow-up PR if is good.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NDQ3Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NjE1Mg==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482166152", "bodyText": "javadocs please", "author": "suneet-s", "createdAt": "2020-09-02T15:33:12Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzgwMQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313801", "bodyText": "Added.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE2NjE1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\nindex b7fa23dd68..f8596ec17a 100644\n--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\n+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\n\n@@ -148,6 +149,12 @@ public abstract class AbstractBatchIndexTask extends AbstractTask\n     }\n   }\n \n+  /**\n+   * Returns an {@link InputRow} iterator which iterates over an input source.\n+   * The returned iterator filters out rows which don't satisfy the given filter or cannot be parsed properly.\n+   * The returned iterator can throw {@link org.apache.druid.java.util.common.parsers.ParseException}s in\n+   * {@link Iterator#hasNext()} when it hits {@link ParseExceptionHandler#maxAllowedParseExceptions}.\n+   */\n   public static FilteringCloseableInputRowIterator inputSourceReader(\n       File tmpDir,\n       DataSchema dataSchema,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3MTMxOA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482171318", "bodyText": "Should this also be annotated with @MonotonicNonNull", "author": "suneet-s", "createdAt": "2020-09-02T15:40:19Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -151,7 +151,7 @@ private static String makeTaskId(RealtimeAppenderatorIngestionSpec spec)\n   private volatile Thread runThread = null;\n \n   @JsonIgnore\n-  private CircularBuffer<Throwable> savedParseExceptions;\n+  private ParseExceptionHandler parseExceptionHandler;", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxMzg2Mg==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482313862", "bodyText": "Added.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3MTMxOA=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java\nindex e782cefd1c..064d2caa33 100644\n--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java\n+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java\n\n@@ -151,21 +152,26 @@ public class AppenderatorDriverRealtimeIndexTask extends AbstractTask implements\n   private volatile Thread runThread = null;\n \n   @JsonIgnore\n-  private ParseExceptionHandler parseExceptionHandler;\n+  private final LockGranularity lockGranularity;\n \n   @JsonIgnore\n-  private final LockGranularity lockGranularity;\n+  @MonotonicNonNull\n+  private ParseExceptionHandler parseExceptionHandler;\n \n   @JsonIgnore\n+  @MonotonicNonNull\n   private IngestionState ingestionState;\n \n   @JsonIgnore\n+  @MonotonicNonNull\n   private AuthorizerMapper authorizerMapper;\n \n   @JsonIgnore\n+  @MonotonicNonNull\n   private RowIngestionMeters rowIngestionMeters;\n \n   @JsonIgnore\n+  @MonotonicNonNull\n   private String errorMsg;\n \n \n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3Nzg3NQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482177875", "bodyText": "This is calculated in 3 places in the code - AbstractBatchIndexTask, InputSourceSampler and SeekableStreamIndexTaskRunner. I think it would be a good idea to consolidate these methods, so that metricNames is always calculated the same way given a dataSchema", "author": "suneet-s", "createdAt": "2020-09-02T15:49:38Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java", "diffHunk": "@@ -135,6 +148,49 @@ public void stopGracefully(TaskConfig taskConfig)\n     }\n   }\n \n+  public static FilteringCloseableInputRowIterator inputSourceReader(\n+      File tmpDir,\n+      DataSchema dataSchema,\n+      InputSource inputSource,\n+      @Nullable InputFormat inputFormat,\n+      Predicate<InputRow> rowFilter,\n+      RowIngestionMeters ingestionMeters,\n+      ParseExceptionHandler parseExceptionHandler\n+  ) throws IOException\n+  {\n+    final List<String> metricsNames = Arrays.stream(dataSchema.getAggregators())\n+                                            .map(AggregatorFactory::getName)\n+                                            .collect(Collectors.toList());\n+    final InputSourceReader inputSourceReader = dataSchema.getTransformSpec().decorate(\n+        inputSource.reader(\n+            new InputRowSchema(\n+                dataSchema.getTimestampSpec(),\n+                dataSchema.getDimensionsSpec(),\n+                metricsNames\n+            ),", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDIzMw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314233", "bodyText": "Yeah, it would be useful. Or we can change InputRowSchema to have all type information of metrics as well, so that we can check exceptions in parsing and type casting in only InputEntityReader. It means, after this change, you will not have to worry about parseExceptions in IncrementalIndex anymore. Probably we don't need ParseExceptionHandler at all in that case. I haven't done this refactoring here because it will be quite complicated and this PR is already big enough.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE3Nzg3NQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\nindex b7fa23dd68..f8596ec17a 100644\n--- a/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\n+++ b/indexing-service/src/main/java/org/apache/druid/indexing/common/task/AbstractBatchIndexTask.java\n\n@@ -148,6 +149,12 @@ public abstract class AbstractBatchIndexTask extends AbstractTask\n     }\n   }\n \n+  /**\n+   * Returns an {@link InputRow} iterator which iterates over an input source.\n+   * The returned iterator filters out rows which don't satisfy the given filter or cannot be parsed properly.\n+   * The returned iterator can throw {@link org.apache.druid.java.util.common.parsers.ParseException}s in\n+   * {@link Iterator#hasNext()} when it hits {@link ParseExceptionHandler#maxAllowedParseExceptions}.\n+   */\n   public static FilteringCloseableInputRowIterator inputSourceReader(\n       File tmpDir,\n       DataSchema dataSchema,\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4MzQ4Ng==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482183486", "bodyText": "parseExceptionHandler can be null if the task.run() hasn't been called. This is probably unlikely?", "author": "suneet-s", "createdAt": "2020-09-02T15:57:18Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/AppenderatorDriverRealtimeIndexTask.java", "diffHunk": "@@ -550,7 +546,9 @@ public Response getUnparseableEvents(\n   )\n   {\n     IndexTaskUtils.datasourceAuthorizationCheck(req, Action.READ, getDataSource(), authorizerMapper);\n-    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(savedParseExceptions);\n+    List<String> events = IndexTaskUtils.getMessagesFromSavedParseExceptions(\n+        parseExceptionHandler.getSavedParseExceptions()", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDM1NQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314355", "bodyText": "This is fine. Any APIs cannot be called until chatHandler is registered. The chatHandler is registered in run() after parseExceptionHandler is created.", "author": "jihoonson", "createdAt": "2020-09-02T19:02:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjE4MzQ4Ng=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482202239", "bodyText": "Is there ever a case where the rowIngestionMeters is different than the one used in parseExceptionHandler? If not, I think we should just pass in the parseExceptionHandler.\nSimilar comment on line 45", "author": "suneet-s", "createdAt": "2020-09-02T16:26:24Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/common/task/BatchAppenderators.java", "diffHunk": "@@ -59,7 +65,9 @@ public static Appenderator newAppenderator(\n       TaskToolbox toolbox,\n       DataSchema dataSchema,\n       AppenderatorConfig appenderatorConfig,\n-      DataSegmentPusher segmentPusher\n+      DataSegmentPusher segmentPusher,\n+      RowIngestionMeters rowIngestionMeters,\n+      ParseExceptionHandler parseExceptionHandler", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDY1NQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314655", "bodyText": "I see your point, but I'm not sure if it's good to have parseExceptionHandler be a rowIngestionMeters provider. I guess we have a couple of options for alternatives:\n\nMerge RowIngestioMeters and ParseExceptionHandler. RowIngestionMeters will have handle(ParseException) method and throw an exception when it hits the limit. This seems a bit strange to me since metrics registry can throw an exception.\nMake RowIngestionMeters to be only a registry and use individual Meters. For example, ParseExceptionHandler will have only a Meter for counting unparseable events. AppenderatorImpl will have only a Meter for counting processed events. This seems sane, but can make code error-prone since we will have multiple Meters which should be properly chosen whenever we use them.\n\nI'm not sure any of these are better than now. Do you have any better idea?", "author": "jihoonson", "createdAt": "2020-09-02T19:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMyNTMwMQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482325301", "bodyText": "fwiw, I think current implementation is better one.", "author": "abhishekagarwal87", "createdAt": "2020-09-02T19:15:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwMjIzOQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwNDYzOQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482204639", "bodyText": "General comment here for all the new functions that accept parseExceptionHandler. parseExceptionHandler tends to be lazily initialized throughout the codebase, but I don't see safeguards against using an un-initialized parseExceptionHandler. Perhaps more use of @Nullable annotation will help surface potential places where it may be used before it's initialized", "author": "suneet-s", "createdAt": "2020-09-02T16:30:04Z", "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/PeonAppenderatorsManager.java", "diffHunk": "@@ -129,7 +137,9 @@ public Appenderator createOfflineAppenderatorForTask(\n           dataSegmentPusher,\n           objectMapper,\n           indexIO,\n-          indexMerger\n+          indexMerger,\n+          rowIngestionMeters,\n+          parseExceptionHandler", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDc0MA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314740", "bodyText": "They must not be null. I added null checks in AppenderatorImpl.", "author": "jihoonson", "createdAt": "2020-09-02T19:03:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIwNDYzOQ=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482212651", "bodyText": "This is a change in behavior. parseExceptionMessages will add to the parseExceptionMessages that were found on line 165. Previously it was re-set. Was this intentional?", "author": "suneet-s", "createdAt": "2020-09-02T16:43:13Z", "path": "processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java", "diffHunk": "@@ -186,7 +185,7 @@ protected AddToFactsResult addToFacts(\n       } else {\n         // We lost a race\n         aggs = concurrentGet(prev);\n-        parseExceptionMessages = doAggregate(metrics, aggs, rowContainer, row);\n+        doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDg2NA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314864", "bodyText": "Nice finding. Clearing parseExceptionMessages now.", "author": "jihoonson", "createdAt": "2020-09-02T19:03:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQwODI0OQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482408249", "bodyText": "why not just return a List like before? That seems less risky than assuming the list is mutated correctly. In general, I'm not a big fan of returning values from a function using parameters because of cases like this.\nEither way, this comment shouldn't be considered a blocker, just my 2 cents", "author": "suneet-s", "createdAt": "2020-09-02T20:20:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUwMTY3Ng==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482501676", "bodyText": "I'm not a big fan of that either, but it seems strange to me to return exception messages as a result of aggregation. I'm actually not sure if we can return early without the second aggregation when the first aggregation throws parseExceptions. I don't want to touch that part yet at least in this PR.", "author": "jihoonson", "createdAt": "2020-09-02T21:41:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQ1NjAzOQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r484456039", "bodyText": "Currently (before and after this PR), a row that contains partially corrupted/incorrect data is aggregated anyway (partially).\nThis may result in wrong statistics inference by the user due to inconsistencies between the number of rows each aggregator accumulated.\nIMO, the decision of allowing this scenario should be of the user.\nBut since currently, there is no way to roll-back aggregation, I don't see how this can be easily implemented.\nMaybe adding parse as a preliminary step to aggregation, but this is an entirely different subject.\n\"return early without the second aggregation\" would only aggravate this scenario, so I don't think it is the way to go either.", "author": "liran-funaro", "createdAt": "2020-09-07T14:11:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA2OTE1MQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r485069151", "bodyText": "Good point. I'm not sure if that behaviour is intended or not, but seems strange to me. I think all parse operations should be done in InputSourceReader.read() as a preliminary step so that we don't have to worry about ParseExceptions in IncrementalIndex. But yeah, this is a different subject.", "author": "jihoonson", "createdAt": "2020-09-08T17:02:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIxMjY1MQ=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java b/processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java\nindex 72c2c9184f..9bbdf60dbf 100644\n--- a/processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java\n+++ b/processing/src/main/java/org/apache/druid/segment/incremental/OnheapIncrementalIndex.java\n\n@@ -184,6 +184,7 @@ public class OnheapIncrementalIndex extends IncrementalIndex<Aggregator>\n         sizeInBytes.addAndGet(estimatedRowSize);\n       } else {\n         // We lost a race\n+        parseExceptionMessages.clear();\n         aggs = concurrentGet(prev);\n         doAggregate(metrics, aggs, rowContainer, row, parseExceptionMessages);\n         // Free up the misfire\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzgwNg==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482223806", "bodyText": "This is a change in behavior. Previously the aggs would fail fast, but now it tries to parse all of them before bubbling up all the parse exceptions. Was this change intentional? I see a similar change in at least the Onheap implementation.", "author": "suneet-s", "createdAt": "2020-09-02T17:00:37Z", "path": "processing/src/main/java/org/apache/druid/segment/incremental/OffheapIncrementalIndex.java", "diffHunk": "@@ -233,16 +232,13 @@ protected AddToFactsResult addToFacts(\n         }\n         catch (ParseException e) {\n           // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-          if (getReportParseExceptions()) {\n-            throw new ParseException(e, \"Encountered parse error for aggregator[%s]\", getMetricAggs()[i].getName());\n-          } else {\n-            log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n-          }\n+          log.debug(e, \"Encountered parse error, skipping aggregator[%s].\", getMetricAggs()[i].getName());\n+          parseExceptionMessages.add(e.getMessage());", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDkwOA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314908", "bodyText": "This is intentional to make it same with OnheapIncrementalIndex.", "author": "jihoonson", "createdAt": "2020-09-02T19:03:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyMzgwNg=="}], "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDE5OA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482224198", "bodyText": "Javadocs please.\nShould this just be a singleton?", "author": "suneet-s", "createdAt": "2020-09-02T17:01:15Z", "path": "processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.druid.segment.incremental;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public class NoopRowIngestionMeters implements RowIngestionMeters", "originalCommit": "d7e51be606e39aa696fccaf559460f36327d0a9d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMxNDk2Mw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482314963", "bodyText": "This is used only in RealtimeIndexTask which is used by Tranquility. It can be a singleton, but doesn't seem matter.", "author": "jihoonson", "createdAt": "2020-09-02T19:03:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDE5OA=="}], "type": "inlineReview", "revised_code": {"commit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "chunk": "diff --git a/processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java b/processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java\nindex 3e01286035..3ce11386c9 100644\n--- a/processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java\n+++ b/processing/src/main/java/org/apache/druid/segment/incremental/NoopRowIngestionMeters.java\n\n@@ -22,6 +22,11 @@ package org.apache.druid.segment.incremental;\n import java.util.Collections;\n import java.util.Map;\n \n+/**\n+ * This class is used only in {@code RealtimeIndexTask} which is deprecated now.\n+ *\n+ * Consider using {@link RowIngestionMetersFactory} instead.\n+ */\n public class NoopRowIngestionMeters implements RowIngestionMeters\n {\n   private static final RowIngestionMetersTotals EMPTY_TOTALS = new RowIngestionMetersTotals(0, 0, 0, 0);\n"}}, {"oid": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "url": "https://github.com/apache/druid/commit/7cdc6434420eb564a8c510abfd99b60d66b708b9", "message": "address review comments", "committedDate": "2020-09-02T19:01:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQyMTc5NQ==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482421795", "bodyText": "I don't think we should use a Predicate<InputRow> here and in other similar places. Throwing a ParseException is expected behavior, so I think callers should know that they need to handle that exception or throw it themselves.\nIt's not obvious to me how the caller in FilteringCloseableInputRowIterator should know they should handle a parse exception https://github.com/apache/druid/pull/10336/files#diff-b7f61f3d28afdd25bedf2efa607fdf88R67\nHow can I check that the ParseException is handled at the correct level everywhere? I realize this is challenging because ParseException is a RuntimeException.", "author": "suneet-s", "createdAt": "2020-09-02T20:30:34Z", "path": "indexing-service/src/main/java/org/apache/druid/indexing/seekablestream/StreamChunkParser.java", "diffHunk": "@@ -54,12 +64,16 @@\n       @Nullable InputFormat inputFormat,\n       InputRowSchema inputRowSchema,\n       TransformSpec transformSpec,\n-      File indexingTmpDir\n+      File indexingTmpDir,\n+      Predicate<InputRow> rowFilter,", "originalCommit": "7cdc6434420eb564a8c510abfd99b60d66b708b9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUwMjI2MA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r482502260", "bodyText": "I don't think we should use a Predicate here and in other similar places. Throwing a ParseException is expected behavior, so I think callers should know that they need to handle that exception or throw it themselves.\nIt's not obvious to me how the caller in FilteringCloseableInputRowIterator should know they should handle a parse exception https://github.com/apache/druid/pull/10336/files#diff-b7f61f3d28afdd25bedf2efa607fdf88R67\n\nHmm, I'm not sure I understand your concern. Can you elaborate more?. rowFilter is a simple predicate to filter out unnecessary rows and does nothing with parseExceptions. Instead, it just passes the ParseException to the caller if it is thrown in test(). The intention here is that the callers of FilteringCloseableInputRowIterator can iterate rows without worrying about validation of rows or parseExceptions.", "author": "jihoonson", "createdAt": "2020-09-02T21:42:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjQyMTc5NQ=="}], "type": "inlineReview", "revised_code": null}, {"oid": "6eacb0e49639ca11d655d2f5c4fcf54f2ac48b5f", "url": "https://github.com/apache/druid/commit/6eacb0e49639ca11d655d2f5c4fcf54f2ac48b5f", "message": "javadoc for parseException; remove redundant parseException in streaming ingestion", "committedDate": "2020-09-02T21:41:36Z", "type": "commit"}, {"oid": "cfc693c1edbddad63947556951f114f4af7c3c85", "url": "https://github.com/apache/druid/commit/cfc693c1edbddad63947556951f114f4af7c3c85", "message": "fix tests", "committedDate": "2020-09-02T22:49:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM3NTUyNw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r483375527", "bodyText": "Test coverage bot is complaining about a lack of coverage for this condition. I think we should add tests for these branches. Right now AppenderatorTest only tests the case where addResult.isRowAdded() is true.\nCovering all 4 branches should make the code coverage bot happy too.", "author": "suneet-s", "createdAt": "2020-09-04T04:14:37Z", "path": "server/src/main/java/org/apache/druid/segment/realtime/appenderator/AppenderatorImpl.java", "diffHunk": "@@ -267,6 +275,12 @@ public AppenderatorAddResult add(\n       throw new SegmentNotWritableException(\"Attempt to add row to swapped-out sink for segment[%s].\", identifier);\n     }\n \n+    if (addResult.isRowAdded()) {\n+      rowIngestionMeters.incrementProcessed();\n+    } else if (addResult.hasParseException()) {\n+      parseExceptionHandler.handle(addResult.getParseException());\n+    }", "originalCommit": "cfc693c1edbddad63947556951f114f4af7c3c85", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": null}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwMjI1Mw==", "url": "https://github.com/apache/druid/pull/10336#discussion_r484402253", "bodyText": "If we never intend to log/report this exception, we can remove the try/catch clause entirely.", "author": "liran-funaro", "createdAt": "2020-09-07T12:29:07Z", "path": "processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java", "diffHunk": "@@ -227,9 +224,7 @@ protected AddToFactsResult addToFacts(\n           }\n           catch (ParseException e) {\n             // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-            if (getReportParseExceptions()) {\n-              throw e;\n-            }\n+            throw e;", "originalCommit": "cfc693c1edbddad63947556951f114f4af7c3c85", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTA3MDEwMA==", "url": "https://github.com/apache/druid/pull/10336#discussion_r485070100", "bodyText": "Good catch. Fixed.", "author": "jihoonson", "createdAt": "2020-09-08T17:04:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQwMjI1Mw=="}], "type": "inlineReview", "revised_code": {"commit": "8c7b7aaac0a1fdd5fb6043eebfbddb51ac2fa2d8", "chunk": "diff --git a/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java b/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java\nindex 770f51885d..c366911930 100644\n--- a/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java\n+++ b/processing/src/test/java/org/apache/druid/segment/incremental/OnheapIncrementalIndexBenchmark.java\n\n@@ -219,13 +219,7 @@ public class OnheapIncrementalIndexBenchmark extends AbstractBenchmark\n \n       for (Aggregator agg : aggs) {\n         synchronized (agg) {\n-          try {\n-            agg.aggregate();\n-          }\n-          catch (ParseException e) {\n-            // \"aggregate\" can throw ParseExceptions if a selector expects something but gets something else.\n-            throw e;\n-          }\n+          agg.aggregate();\n         }\n       }\n \n"}}, {"oid": "8c7b7aaac0a1fdd5fb6043eebfbddb51ac2fa2d8", "url": "https://github.com/apache/druid/commit/8c7b7aaac0a1fdd5fb6043eebfbddb51ac2fa2d8", "message": "unnecessary catch", "committedDate": "2020-09-08T17:03:53Z", "type": "commit"}, {"oid": "2da5c4008c048c614c93b97f9ec53e18358ad1f5", "url": "https://github.com/apache/druid/commit/2da5c4008c048c614c93b97f9ec53e18358ad1f5", "message": "unused imports", "committedDate": "2020-09-09T16:47:14Z", "type": "commit"}, {"oid": "238381ebbe0413b3634865e02ce55df352d74b4e", "url": "https://github.com/apache/druid/commit/238381ebbe0413b3634865e02ce55df352d74b4e", "message": "appenderator test", "committedDate": "2020-09-10T01:22:11Z", "type": "commit"}, {"oid": "8e5036232571e3379bfdaf9f6ab6705c463ce81c", "url": "https://github.com/apache/druid/commit/8e5036232571e3379bfdaf9f6ab6705c463ce81c", "message": "unused import", "committedDate": "2020-09-10T02:35:28Z", "type": "commit"}]}