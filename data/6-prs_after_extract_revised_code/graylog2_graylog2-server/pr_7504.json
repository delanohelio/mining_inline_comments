{"pr_number": 7504, "pr_title": "Add support for high level Kafka consumer mode", "pr_createdAt": "2020-02-19T17:11:34Z", "pr_url": "https://github.com/Graylog2/graylog2-server/pull/7504", "timeline": [{"oid": "3a3bac202fef76eaf51f37a067c68ea12f8a36c0", "url": "https://github.com/Graylog2/graylog2-server/commit/3a3bac202fef76eaf51f37a067c68ea12f8a36c0", "message": "Add support for newer Kafka versions\n\nNewer Kafka versions are supported by using the new high level consumer API.\nLucky for us, support for the new API is already included in our currently used Kafka\nclient library. This means we can avoid updating it, and still be\ncompatible with our on-disk journal.\n\nInstead of creating a new Transport class, use a legacy switch to keep\nsupporting older Kafka versions.\n\nMost of this was spearheaded by Muralidhar Basani in\n https://github.com/Graylog2/graylog2-server/pull/4770\nThank you very much! :-)\n\nCo-authored-by: Muralidhar Basani <murali.basani@gmail.com>", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "15e931acc94ed413661f5abadd8e764db89357a3", "url": "https://github.com/Graylog2/graylog2-server/commit/15e931acc94ed413661f5abadd8e764db89357a3", "message": "Drop stale comment", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "153ecabd2623e9313f4460fe78fa11158418e23a", "url": "https://github.com/Graylog2/graylog2-server/commit/153ecabd2623e9313f4460fe78fa11158418e23a", "message": "Allow ordering of configuration fields\n\nFields in generated configuration forms are not placed in any specific\norder. (Apart from the fact that optional fields are ordered at the end)\n\nWith this change it is possible to specify an explicit position\nattribute on a config field.\nThe default positioning value is 100.\nIf you want your field higher up in the form, choose a value below 100.\nIf you want it at the end, set it higher than 100.", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "ea9249d0d3bb26706b7895f877da2a8e73b7ee86", "url": "https://github.com/Graylog2/graylog2-server/commit/ea9249d0d3bb26706b7895f877da2a8e73b7ee86", "message": "Fix validation, adjust comments", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "3fead00be15b3f683d2f313ec7683da458b8f1b5", "url": "https://github.com/Graylog2/graylog2-server/commit/3fead00be15b3f683d2f313ec7683da458b8f1b5", "message": "Add support for custom properties\n\nThis allows users to configure things like TLS.", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "url": "https://github.com/Graylog2/graylog2-server/commit/dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "message": "Set system wide default for TLS protocols", "committedDate": "2020-02-21T10:26:56Z", "type": "commit"}, {"oid": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "url": "https://github.com/Graylog2/graylog2-server/commit/dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "message": "Set system wide default for TLS protocols", "committedDate": "2020-02-21T10:26:56Z", "type": "forcePushed"}, {"oid": "85beb41a344e52093bd411f9cc8b236bdee6299d", "url": "https://github.com/Graylog2/graylog2-server/commit/85beb41a344e52093bd411f9cc8b236bdee6299d", "message": "Revert \"Set system wide default for TLS protocols\"\n\nBetter leave this entirely up to the user\n\nThis reverts commit dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507.", "committedDate": "2020-02-25T17:43:20Z", "type": "commit"}, {"oid": "125bde86aa64d068f90b8b1f8a7b08dc92489b39", "url": "https://github.com/Graylog2/graylog2-server/commit/125bde86aa64d068f90b8b1f8a7b08dc92489b39", "message": "Avoid logging ConsumerConfig\n\nIt might contain sensitive credentials", "committedDate": "2020-02-25T17:44:08Z", "type": "commit"}, {"oid": "76283dde3c04124e9a39ad5eace7829f7538a7f2", "url": "https://github.com/Graylog2/graylog2-server/commit/76283dde3c04124e9a39ad5eace7829f7538a7f2", "message": "Adjust comment", "committedDate": "2020-02-25T17:58:23Z", "type": "commit"}, {"oid": "a4681b9505cab2f5794cccb60f0c6575edf36641", "url": "https://github.com/Graylog2/graylog2-server/commit/a4681b9505cab2f5794cccb60f0c6575edf36641", "message": "Merge branch 'master' into kafka-consumer", "committedDate": "2020-02-25T18:08:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk1MjE0MA==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383952140", "bodyText": "Since we are already using Kafka 0.9, we cannot connect to 0.8 brokers anymore. This comment needs to be adjusted.\nWe only need the legacy flag to support existing Kafka inputs.", "author": "bernd", "createdAt": "2020-02-25T15:29:30Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -340,13 +473,27 @@ public MetricSet getMetricSet() {\n         public ConfigurationRequest getRequestedConfiguration() {\n             final ConfigurationRequest cr = super.getRequestedConfiguration();\n \n+            cr.addField(new BooleanField(CK_LEGACY,\n+                    \"Legacy mode\",\n+                    true,\n+                    \"Use old ZooKeeper-based consumer API. Needed for Kafka clusters < 0.9\",", "originalCommit": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "76283dde3c04124e9a39ad5eace7829f7538a7f2", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex 7eedd11c9b..b9b6177648 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -476,7 +469,7 @@ public class KafkaTransport extends ThrottleableTransport {\n             cr.addField(new BooleanField(CK_LEGACY,\n                     \"Legacy mode\",\n                     true,\n-                    \"Use old ZooKeeper-based consumer API. Needed for Kafka clusters < 0.9\",\n+                    \"Use old ZooKeeper-based consumer API. (Used before Graylog 3.3)\",\n                     10\n             ));\n             cr.addField(new TextField(\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5MTEzMA==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383991130", "bodyText": "Is this comment still valid? And can the commented code be removed?", "author": "bernd", "createdAt": "2020-02-25T16:44:27Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();", "originalCommit": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM0MzQ1Mg==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384343452", "bodyText": "the first one, i think yes. but the code is a leftover, deleting it", "author": "mpfz0r", "createdAt": "2020-02-26T08:45:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5MTEzMA=="}], "type": "inlineReview", "revised_code": {"commit": "85beb41a344e52093bd411f9cc8b236bdee6299d", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex 7eedd11c9b..5156e8d143 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -234,9 +230,6 @@ public class KafkaTransport extends ThrottleableTransport {\n         props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n \n-        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n-            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n-        }\n         insertCustomProperties(props);\n \n         final int numThreads = configuration.getInt(CK_THREADS);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mzk5NTUyNA==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r383995524", "bodyText": "The javadoc for KafkaConsumer#close() says:\n\nClose the consumer, waiting indefinitely for any needed cleanup.\n\nNewer versions of the API support a close(long timeout, TimeUnit timeUnit).\nCan you please add a TODO to consumer.close() as a reminder to update this call once we use a newer client library?", "author": "bernd", "createdAt": "2020-02-25T16:50:58Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +191,127 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n+            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n+        }\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+                        // noinspection WhileLoopReplaceableByForEach\n+                        while (consumerIterator.hasNext()) {\n+                            if (paused) {\n+                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                            }\n+                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                            if (stopped) {\n+                                break;\n+                            }\n+                            if (isThrottled()) {\n+                                blockUntilUnthrottled();\n+                            }\n+\n+                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                            // if we get an exception about processing it down below.\n+                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n+\n+                            final byte[] bytes = consumerIterator.next().value();\n+\n+                            // it is possible that the message is null\n+                            if (bytes == null) {\n+                                continue;\n+                            }\n+\n+                            totalBytesRead.addAndGet(bytes.length);\n+                            lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                            final RawMessage rawMessage = new RawMessage(bytes);\n+\n+                            input.processRawMessage(rawMessage);\n+                        }\n+                    } catch (Exception e) {\n+                        LOG.error(\"Kafka error in consumer thread.\", e);\n+                    }\n+                }\n+                // explicitly commit our offsets when stopping.\n+                // this might trigger a couple of times, but it won't hurt\n+                consumer.commitAsync();\n+                stopLatch.countDown();\n+                consumer.close();", "originalCommit": "dabaa00d4f3891f31c8a56c8a3194b8ecd4f7507", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "85beb41a344e52093bd411f9cc8b236bdee6299d", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex 7eedd11c9b..5156e8d143 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -234,9 +230,6 @@ public class KafkaTransport extends ThrottleableTransport {\n         props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n         props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n \n-        if (!graylogConfiguration.getEnabledTlsProtocols().isEmpty()) {\n-            props.put(\"ssl.enabled.protocols\", StringUtils.join(graylogConfiguration.getEnabledTlsProtocols(), \",\"));\n-        }\n         insertCustomProperties(props);\n \n         final int numThreads = configuration.getInt(CK_THREADS);\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384058508", "bodyText": "Is that comment still valid? I checked the code briefly and I cannot see where next() would mark the message as processed.", "author": "bernd", "createdAt": "2020-02-25T18:53:42Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n+                    try {\n+                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately", "originalCommit": "a4681b9505cab2f5794cccb60f0c6575edf36641", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDM0ODE2Mg==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384348162", "bodyText": "I guess that depends. The default is auto.commit.enable = true. So I'd say it does.", "author": "mpfz0r", "createdAt": "2020-02-26T08:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY0MjAyNQ==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389642025", "bodyText": "Nevermind. You're right :)", "author": "mpfz0r", "createdAt": "2020-03-09T12:56:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1ODUwOA=="}], "type": "inlineReview", "revised_code": {"commit": "51665cec51fffbe0e92d7470452852658a202e87", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex b9b6177648..28fda89c60 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -239,69 +245,103 @@ public class KafkaTransport extends ThrottleableTransport {\n         // this is to avoid yanking away the connection from the consumer runnables\n         stopLatch = new CountDownLatch(numThreads);\n \n-        IntStream.range(0, numThreads).forEach(i -> {\n-            executor.submit(() -> {\n-                final Properties nprops = (Properties) props.clone();\n-                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n-                final KafkaConsumer<byte[], byte[]> consumer;\n-                try {\n-                    consumer = new KafkaConsumer<>(nprops);\n-                    //noinspection ConstantConditions\n-                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n-                } catch (Exception e) {\n-                    LOG.warn(\"Could not create KafkaConsumer\", e);\n-                    throw e;\n-                }\n+        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n+    }\n \n-                while (!stopped) {\n-                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n-                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n-                    try {\n-                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n-                        // noinspection WhileLoopReplaceableByForEach\n-                        while (consumerIterator.hasNext()) {\n-                            if (paused) {\n-                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n-                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n-                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n-                            }\n-                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n-                            if (stopped) {\n-                                break;\n-                            }\n-                            if (isThrottled()) {\n-                                blockUntilUnthrottled();\n-                            }\n+    private class ConsumerRunnable implements Runnable {\n+        private final Properties props;\n+        private final MessageInput input;\n+        private final KafkaConsumer<byte[], byte[]> consumer;\n+\n+        public ConsumerRunnable(Properties props, MessageInput input, int threadId) {\n+            this.input = input;\n+            final Properties nprops = (Properties) props.clone();\n+            nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + threadId);\n+            this.props = nprops;\n+            consumer = new KafkaConsumer<>(props);\n+            //noinspection ConstantConditions\n+            consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+        }\n \n-                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n-                            // if we get an exception about processing it down below.\n-                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n+        private void consumeRecords(Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator) {\n+            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+            // noinspection WhileLoopReplaceableByForEach\n+            while (consumerIterator.hasNext()) {\n+                if (paused) {\n+                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                    LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                }\n+                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                if (stopped) {\n+                    break;\n+                }\n+                if (isThrottled()) {\n+                    blockUntilUnthrottled();\n+                }\n \n-                            final byte[] bytes = consumerIterator.next().value();\n+                // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                // if we get an exception about processing it down below.\n+                final byte[] bytes = consumerIterator.next().value();\n \n-                            // it is possible that the message is null\n-                            if (bytes == null) {\n-                                continue;\n-                            }\n+                // it is possible that the message is null\n+                if (bytes == null) {\n+                    continue;\n+                }\n+                totalBytesRead.addAndGet(bytes.length);\n+                lastSecBytesReadTmp.addAndGet(bytes.length);\n \n-                            totalBytesRead.addAndGet(bytes.length);\n-                            lastSecBytesReadTmp.addAndGet(bytes.length);\n+                final RawMessage rawMessage = new RawMessage(bytes);\n+                input.processRawMessage(rawMessage);\n+            }\n+        }\n \n-                            final RawMessage rawMessage = new RawMessage(bytes);\n+        private Optional<Iterator<ConsumerRecord<byte[], byte[]>>> tryPoll() {\n+            try {\n+                // Workaround https://issues.apache.org/jira/browse/KAFKA-4189 by calling wakeup()\n+                final ScheduledFuture<?> future = scheduler.schedule(consumer::wakeup, 2000, TimeUnit.MILLISECONDS);\n+                final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                future.cancel(true);\n+\n+                return Optional.of(consumerRecords.iterator());\n+            } catch (WakeupException e) {\n+                LOG.error(\"WakeupException in poll. Kafka server is not responding.\");\n+            } catch (InvalidOffsetException | AuthorizationException e) {\n+                LOG.error(\"Exception in poll.\", e);\n+            }\n+            return Optional.empty();\n+        }\n \n-                            input.processRawMessage(rawMessage);\n-                        }\n-                    } catch (Exception e) {\n-                        LOG.error(\"Kafka error in consumer thread.\", e);\n+        @Override\n+        public void run() {\n+            while (!stopped) {\n+                final Optional<Iterator<ConsumerRecord<byte[], byte[]>>> consumerIterator;\n+                try {\n+                    consumerIterator = tryPoll();\n+                    if (! consumerIterator.isPresent()) {\n+                        LOG.error(\"Caught recoverable exception. Retrying\");\n+                        Thread.sleep(2000);\n+                        continue;\n                     }\n+                } catch (KafkaException | InterruptedException e) {\n+                    LOG.error(\"Caught unrecoverable exception in poll. Stopping input\", e);\n+                    stopped = true;\n+                    break;\n                 }\n-                // explicitly commit our offsets when stopping.\n-                // this might trigger a couple of times, but it won't hurt\n-                consumer.commitAsync();\n-                stopLatch.countDown();\n-                consumer.close();\n-            });\n-        });\n+                try {\n+                    consumeRecords(consumerIterator.get());\n+                } catch (Exception e) {\n+                    LOG.error(\"Exception in consumer thread. Continuing\", e);\n+                }\n+            }\n+            // explicitly commit our offsets when stopping.\n+            // this might trigger a couple of times, but it won't hurt\n+            consumer.commitAsync();\n+            stopLatch.countDown();\n+            // TODO once we update our kafka client, we should call this with a timeout\n+            // Otherwise might hang if kafka is not available: https://issues.apache.org/jira/browse/KAFKA-3822\n+            consumer.close();\n+        }\n     }\n \n     private void doLaunchLegacy(final MessageInput input) {\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1OTg4Mw==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384059883", "bodyText": "The consumer.poll() method can throw a few exceptions which we never catch anywhere. This would basically abort the thread without closing any resources. (no consumer.close() call)\nI think we should handle these errors and check what we can do to avoid having a dead input.", "author": "bernd", "createdAt": "2020-02-25T18:56:15Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +187,124 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        final ExecutorService executor = executorService(numThreads);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> {\n+            executor.submit(() -> {\n+                final Properties nprops = (Properties) props.clone();\n+                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n+                final KafkaConsumer<byte[], byte[]> consumer;\n+                try {\n+                    consumer = new KafkaConsumer<>(nprops);\n+                    //noinspection ConstantConditions\n+                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+                } catch (Exception e) {\n+                    LOG.warn(\"Could not create KafkaConsumer\", e);\n+                    throw e;\n+                }\n+\n+                while (!stopped) {\n+                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);", "originalCommit": "a4681b9505cab2f5794cccb60f0c6575edf36641", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDQ5OTk4MQ==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r384499981", "bodyText": "good catch. I found a couple of issues while fixing this.", "author": "mpfz0r", "createdAt": "2020-02-26T13:45:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1OTg4Mw=="}], "type": "inlineReview", "revised_code": {"commit": "51665cec51fffbe0e92d7470452852658a202e87", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex b9b6177648..28fda89c60 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -239,69 +245,103 @@ public class KafkaTransport extends ThrottleableTransport {\n         // this is to avoid yanking away the connection from the consumer runnables\n         stopLatch = new CountDownLatch(numThreads);\n \n-        IntStream.range(0, numThreads).forEach(i -> {\n-            executor.submit(() -> {\n-                final Properties nprops = (Properties) props.clone();\n-                nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + i);\n-                final KafkaConsumer<byte[], byte[]> consumer;\n-                try {\n-                    consumer = new KafkaConsumer<>(nprops);\n-                    //noinspection ConstantConditions\n-                    consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n-                } catch (Exception e) {\n-                    LOG.warn(\"Could not create KafkaConsumer\", e);\n-                    throw e;\n-                }\n+        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n+    }\n \n-                while (!stopped) {\n-                    final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n-                    final Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator = consumerRecords.iterator();\n-                    try {\n-                        // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n-                        // noinspection WhileLoopReplaceableByForEach\n-                        while (consumerIterator.hasNext()) {\n-                            if (paused) {\n-                                // we try not to spin here, so we wait until the lifecycle goes back to running.\n-                                LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n-                                Uninterruptibles.awaitUninterruptibly(pausedLatch);\n-                            }\n-                            // check for being stopped before actually getting the message, otherwise we could end up losing that message\n-                            if (stopped) {\n-                                break;\n-                            }\n-                            if (isThrottled()) {\n-                                blockUntilUnthrottled();\n-                            }\n+    private class ConsumerRunnable implements Runnable {\n+        private final Properties props;\n+        private final MessageInput input;\n+        private final KafkaConsumer<byte[], byte[]> consumer;\n+\n+        public ConsumerRunnable(Properties props, MessageInput input, int threadId) {\n+            this.input = input;\n+            final Properties nprops = (Properties) props.clone();\n+            nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + threadId);\n+            this.props = nprops;\n+            consumer = new KafkaConsumer<>(props);\n+            //noinspection ConstantConditions\n+            consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+        }\n \n-                            // process the message, this will immediately mark the message as having been processed. this gets tricky\n-                            // if we get an exception about processing it down below.\n-                            // final MessageAndMetadata<byte[], byte[]> message = consumerIterator.next();\n+        private void consumeRecords(Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator) {\n+            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+            // noinspection WhileLoopReplaceableByForEach\n+            while (consumerIterator.hasNext()) {\n+                if (paused) {\n+                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                    LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                }\n+                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                if (stopped) {\n+                    break;\n+                }\n+                if (isThrottled()) {\n+                    blockUntilUnthrottled();\n+                }\n \n-                            final byte[] bytes = consumerIterator.next().value();\n+                // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                // if we get an exception about processing it down below.\n+                final byte[] bytes = consumerIterator.next().value();\n \n-                            // it is possible that the message is null\n-                            if (bytes == null) {\n-                                continue;\n-                            }\n+                // it is possible that the message is null\n+                if (bytes == null) {\n+                    continue;\n+                }\n+                totalBytesRead.addAndGet(bytes.length);\n+                lastSecBytesReadTmp.addAndGet(bytes.length);\n \n-                            totalBytesRead.addAndGet(bytes.length);\n-                            lastSecBytesReadTmp.addAndGet(bytes.length);\n+                final RawMessage rawMessage = new RawMessage(bytes);\n+                input.processRawMessage(rawMessage);\n+            }\n+        }\n \n-                            final RawMessage rawMessage = new RawMessage(bytes);\n+        private Optional<Iterator<ConsumerRecord<byte[], byte[]>>> tryPoll() {\n+            try {\n+                // Workaround https://issues.apache.org/jira/browse/KAFKA-4189 by calling wakeup()\n+                final ScheduledFuture<?> future = scheduler.schedule(consumer::wakeup, 2000, TimeUnit.MILLISECONDS);\n+                final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                future.cancel(true);\n+\n+                return Optional.of(consumerRecords.iterator());\n+            } catch (WakeupException e) {\n+                LOG.error(\"WakeupException in poll. Kafka server is not responding.\");\n+            } catch (InvalidOffsetException | AuthorizationException e) {\n+                LOG.error(\"Exception in poll.\", e);\n+            }\n+            return Optional.empty();\n+        }\n \n-                            input.processRawMessage(rawMessage);\n-                        }\n-                    } catch (Exception e) {\n-                        LOG.error(\"Kafka error in consumer thread.\", e);\n+        @Override\n+        public void run() {\n+            while (!stopped) {\n+                final Optional<Iterator<ConsumerRecord<byte[], byte[]>>> consumerIterator;\n+                try {\n+                    consumerIterator = tryPoll();\n+                    if (! consumerIterator.isPresent()) {\n+                        LOG.error(\"Caught recoverable exception. Retrying\");\n+                        Thread.sleep(2000);\n+                        continue;\n                     }\n+                } catch (KafkaException | InterruptedException e) {\n+                    LOG.error(\"Caught unrecoverable exception in poll. Stopping input\", e);\n+                    stopped = true;\n+                    break;\n                 }\n-                // explicitly commit our offsets when stopping.\n-                // this might trigger a couple of times, but it won't hurt\n-                consumer.commitAsync();\n-                stopLatch.countDown();\n-                consumer.close();\n-            });\n-        });\n+                try {\n+                    consumeRecords(consumerIterator.get());\n+                } catch (Exception e) {\n+                    LOG.error(\"Exception in consumer thread. Continuing\", e);\n+                }\n+            }\n+            // explicitly commit our offsets when stopping.\n+            // this might trigger a couple of times, but it won't hurt\n+            consumer.commitAsync();\n+            stopLatch.countDown();\n+            // TODO once we update our kafka client, we should call this with a timeout\n+            // Otherwise might hang if kafka is not available: https://issues.apache.org/jira/browse/KAFKA-3822\n+            consumer.close();\n+        }\n     }\n \n     private void doLaunchLegacy(final MessageInput input) {\n"}}, {"oid": "51665cec51fffbe0e92d7470452852658a202e87", "url": "https://github.com/Graylog2/graylog2-server/commit/51665cec51fffbe0e92d7470452852658a202e87", "message": "Improve Exception handling and refactor code\n\nAvoid resource leaks by handling more exceptions.\nShutdown the input on unrecoverable exceptions.\nRetry on others.\n\nAdd workaround for hanging poll if kafka is down:\n https://issues.apache.org/jira/browse/KAFKA-4189", "committedDate": "2020-02-26T13:36:39Z", "type": "commit"}, {"oid": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b", "url": "https://github.com/Graylog2/graylog2-server/commit/7d6cd47444dee3a119a13db2f7d8d60c8be54d3b", "message": "Restart Input after unrecoverable Kafka error\n\nAlso fix thread leak. We need to shutdown the ExecutorService.", "committedDate": "2020-02-27T16:32:32Z", "type": "commit"}, {"oid": "582d538d3dd5d6157a52f08b83ca3d511b697017", "url": "https://github.com/Graylog2/graylog2-server/commit/582d538d3dd5d6157a52f08b83ca3d511b697017", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer", "committedDate": "2020-02-28T11:01:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODc4MTAyOQ==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r388781029", "bodyText": "This is ugly. Maybe we should just not do this at all.\nI might be misinterpreting the KafkaException being not retryable.\nFurthermore, I don't know how to trigger one, so this isn't a \"real\" problem.", "author": "mpfz0r", "createdAt": "2020-03-06T08:57:06Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -173,17 +197,158 @@ public void setMessageAggregator(CodecAggregator ignored) {\n     }\n \n     @Override\n-    public void doLaunch(final MessageInput input) throws MisfireException {\n-        serverStatus.awaitRunning(new Runnable() {\n-            @Override\n-            public void run() {\n-                lifecycleStateChange(Lifecycle.RUNNING);\n+    public void doLaunch(final MessageInput input) {\n+        final boolean legacyMode = configuration.getBoolean(CK_LEGACY, true);\n+        if (legacyMode) {\n+            final String zooKeper = configuration.getString(CK_ZOOKEEPER);\n+            if (Strings.isNullOrEmpty(zooKeper)) {\n+                throw new IllegalArgumentException(\"ZooKeeper configuration setting cannot be empty\");\n             }\n-        });\n+        } else {\n+            final String bootStrap = configuration.getString(CK_BOOTSTRAP);\n+            if (Strings.isNullOrEmpty(bootStrap)) {\n+                throw new IllegalArgumentException(\"Bootstrap server configuration setting cannot be empty\");\n+            }\n+        }\n \n+        serverStatus.awaitRunning(() -> lifecycleStateChange(Lifecycle.RUNNING));\n         // listen for lifecycle changes\n         serverEventBus.register(this);\n \n+        if (legacyMode) {\n+            doLaunchLegacy(input);\n+        } else {\n+            doLaunchConsumer(input);\n+        }\n+        scheduler.scheduleAtFixedRate(() -> lastSecBytesRead.set(lastSecBytesReadTmp.getAndSet(0)), 1, 1, TimeUnit.SECONDS);\n+    }\n+\n+    private void doLaunchConsumer(final MessageInput input) {\n+        final Properties props = new Properties();\n+\n+        props.put(\"group.id\", configuration.getString(CK_GROUP_ID, DEFAULT_GROUP_ID));\n+        props.put(\"fetch.min.bytes\", String.valueOf(configuration.getInt(CK_FETCH_MIN_BYTES)));\n+        props.put(\"fetch.max.wait.ms\", String.valueOf(configuration.getInt(CK_FETCH_WAIT_MAX)));\n+        //noinspection ConstantConditions\n+        props.put(\"bootstrap.servers\", configuration.getString(CK_BOOTSTRAP));\n+        // Map largest -> latest, smallest -> earliest\n+        final String resetValue = configuration.getString(CK_OFFSET_RESET, DEFAULT_OFFSET_RESET);\n+        props.put(\"auto.offset.reset\", resetValue.equals(\"largest\") ? \"latest\" : \"earliest\");\n+        // Default auto commit interval is 60 seconds. Reduce to 1 second to minimize message duplication\n+        // if something breaks.\n+        props.put(\"auto.commit.interval.ms\", \"1000\");\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+        props.put(org.apache.kafka.clients.consumer.ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class.getName());\n+\n+        insertCustomProperties(props);\n+\n+        final int numThreads = configuration.getInt(CK_THREADS);\n+        // this is being used during shutdown to first stop all submitted jobs before committing the offsets back to zookeeper\n+        // and then shutting down the connection.\n+        // this is to avoid yanking away the connection from the consumer runnables\n+        stopLatch = new CountDownLatch(numThreads);\n+\n+        IntStream.range(0, numThreads).forEach(i -> executor.submit(new ConsumerRunnable(props, input, i)));\n+    }\n+\n+    private class ConsumerRunnable implements Runnable {\n+        private final Properties props;\n+        private final MessageInput input;\n+        private final KafkaConsumer<byte[], byte[]> consumer;\n+\n+        public ConsumerRunnable(Properties props, MessageInput input, int threadId) {\n+            this.input = input;\n+            final Properties nprops = (Properties) props.clone();\n+            nprops.put(\"client.id\", \"gl2-\" + nodeId + \"-\" + input.getId() + \"-\" + threadId);\n+            this.props = nprops;\n+            consumer = new KafkaConsumer<>(props);\n+            //noinspection ConstantConditions\n+            consumer.subscribe(Pattern.compile(configuration.getString(CK_TOPIC_FILTER)), new NoOpConsumerRebalanceListener());\n+        }\n+\n+        private void consumeRecords(Iterator<ConsumerRecord<byte[], byte[]>> consumerIterator) {\n+            // we have to use hasNext() here instead foreach, because next() marks the message as processed immediately\n+            // noinspection WhileLoopReplaceableByForEach\n+            while (consumerIterator.hasNext()) {\n+                if (paused) {\n+                    // we try not to spin here, so we wait until the lifecycle goes back to running.\n+                    LOG.debug(\"Message processing is paused, blocking until message processing is turned back on.\");\n+                    Uninterruptibles.awaitUninterruptibly(pausedLatch);\n+                }\n+                // check for being stopped before actually getting the message, otherwise we could end up losing that message\n+                if (stopped) {\n+                    break;\n+                }\n+                if (isThrottled()) {\n+                    blockUntilUnthrottled();\n+                }\n+\n+                // process the message, this will immediately mark the message as having been processed. this gets tricky\n+                // if we get an exception about processing it down below.\n+                final byte[] bytes = consumerIterator.next().value();\n+\n+                // it is possible that the message is null\n+                if (bytes == null) {\n+                    continue;\n+                }\n+                totalBytesRead.addAndGet(bytes.length);\n+                lastSecBytesReadTmp.addAndGet(bytes.length);\n+\n+                final RawMessage rawMessage = new RawMessage(bytes);\n+                input.processRawMessage(rawMessage);\n+            }\n+        }\n+\n+        private Optional<Iterator<ConsumerRecord<byte[], byte[]>>> tryPoll() {\n+            try {\n+                // Workaround https://issues.apache.org/jira/browse/KAFKA-4189 by calling wakeup()\n+                final ScheduledFuture<?> future = scheduler.schedule(consumer::wakeup, 2000, TimeUnit.MILLISECONDS);\n+                final ConsumerRecords<byte[], byte[]> consumerRecords = consumer.poll(1000);\n+                future.cancel(true);\n+\n+                return Optional.of(consumerRecords.iterator());\n+            } catch (WakeupException e) {\n+                LOG.error(\"WakeupException in poll. Kafka server is not responding.\");\n+            } catch (InvalidOffsetException | AuthorizationException e) {\n+                LOG.error(\"Exception in poll.\", e);\n+            }\n+            return Optional.empty();\n+        }\n+\n+        @Override\n+        public void run() {\n+            while (!stopped) {\n+                final Optional<Iterator<ConsumerRecord<byte[], byte[]>>> consumerIterator;\n+                try {\n+                    consumerIterator = tryPoll();\n+                    if (! consumerIterator.isPresent()) {\n+                        LOG.error(\"Caught recoverable exception. Retrying\");\n+                        Thread.sleep(2000);\n+                        continue;\n+                    }\n+                } catch (KafkaException | InterruptedException e) {\n+                    LOG.error(\"Caught unrecoverable exception in poll. Restarting input\", e);\n+                    // (Ab)use serverEventBus to properly restart the entire input.\n+                    serverEventBus.post(InputCreated.create(input.getId()));", "originalCommit": "7d6cd47444dee3a119a13db2f7d8d60c8be54d3b", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "38de62e01a48a9ec36aacca5ce2f5bbd9cc7b3c9", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex fe6bf17ef3..e15535ea20 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -327,9 +326,8 @@ public class KafkaTransport extends ThrottleableTransport {\n                         continue;\n                     }\n                 } catch (KafkaException | InterruptedException e) {\n-                    LOG.error(\"Caught unrecoverable exception in poll. Restarting input\", e);\n-                    // (Ab)use serverEventBus to properly restart the entire input.\n-                    serverEventBus.post(InputCreated.create(input.getId()));\n+                    LOG.error(\"Caught unrecoverable exception in poll. Stopping input\", e);\n+                    stopped = true;\n                     break;\n                 }\n                 try {\n"}}, {"oid": "38de62e01a48a9ec36aacca5ce2f5bbd9cc7b3c9", "url": "https://github.com/Graylog2/graylog2-server/commit/38de62e01a48a9ec36aacca5ce2f5bbd9cc7b3c9", "message": "Don't restart the Input after a KafkaException\n\nThis reacharound via the eventbus is too ugly\nfor a problem that I can't trigger in practice.\n\nSimply log the exception and stop the transport.", "committedDate": "2020-03-06T09:50:58Z", "type": "commit"}, {"oid": "e10a64f3fd5fb0dac3dbae306f35c7fc9e61fbf1", "url": "https://github.com/Graylog2/graylog2-server/commit/e10a64f3fd5fb0dac3dbae306f35c7fc9e61fbf1", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer", "committedDate": "2020-03-06T09:52:45Z", "type": "commit"}, {"oid": "279cd81cdfc0c079a0c711ec7283f8eb1154d2fa", "url": "https://github.com/Graylog2/graylog2-server/commit/279cd81cdfc0c079a0c711ec7283f8eb1154d2fa", "message": "Merge remote-tracking branch 'origin/master' into kafka-consumer", "committedDate": "2020-03-06T10:00:54Z", "type": "commit"}, {"oid": "b423109e43bae50235f1e9b6026f9d7b4a14b032", "url": "https://github.com/Graylog2/graylog2-server/commit/b423109e43bae50235f1e9b6026f9d7b4a14b032", "message": "Stop transport if we can't process the records\n\nIf our processing throws an exception while\nconsuming the records into our journal,\nit's better to stop the transport early so we\ndon't lose too many messages.\n(enable.auto.commit is our default)", "committedDate": "2020-03-09T12:23:03Z", "type": "commit"}, {"oid": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "url": "https://github.com/Graylog2/graylog2-server/commit/ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "message": "Simplify consumeRecords()\n\nWith the old kafka client, every next() call would mark the offset\nas commited. With the new client, this isn't the case anymore,\nthus we can simplify the code.", "committedDate": "2020-03-09T12:53:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTMwNw==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725307", "bodyText": "I think we should return DEFAULT_POSITION here instead.", "author": "dennisoelkers", "createdAt": "2020-03-09T14:28:36Z", "path": "graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java", "diffHunk": "@@ -42,4 +42,8 @@\n     List<String> getAttributes();\n \n     Map<String, Map<String, String>> getAdditionalInformation();\n+\n+    default int getPosition() {\n+        return 100;", "originalCommit": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzQ0OA==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389753448", "bodyText": "Thanks! that was a sloppy oversight :/", "author": "mpfz0r", "createdAt": "2020-03-09T15:09:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTMwNw=="}], "type": "inlineReview", "revised_code": {"commit": "298a5930989fa6a987f816ff49ed89f9cd1f5782", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java b/graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java\nindex bf4112d625..a36dac2ef8 100644\n--- a/graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java\n+++ b/graylog2-server/src/main/java/org/graylog2/plugin/configuration/fields/ConfigurationField.java\n\n@@ -44,6 +47,6 @@ public interface ConfigurationField {\n     Map<String, Map<String, String>> getAdditionalInformation();\n \n     default int getPosition() {\n-        return 100;\n+        return DEFAULT_POSITION;\n     }\n }\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTk1Mg==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389725952", "bodyText": "Instead of a magic number we should use a constant introduced in ConfigurationField here, maybe something like BELOW_DEFAULT?", "author": "dennisoelkers", "createdAt": "2020-03-09T14:29:30Z", "path": "graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java", "diffHunk": "@@ -389,8 +575,18 @@ public ConfigurationRequest getRequestedConfiguration() {\n                     DEFAULT_GROUP_ID,\n                     \"Name of the consumer group the Kafka input belongs to\",\n                     ConfigurationField.Optional.OPTIONAL));\n+            cr.addField(new TextField(\n+                    CK_CUSTOM_PROPERTIES,\n+                    \"Custom Kafka properties\",\n+                    \"\",\n+                    \"A newline separated list of Kafka properties. (e.g.: \\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\").\",\n+                    ConfigurationField.Optional.OPTIONAL,\n+                    110,", "originalCommit": "ee609ddf8cdb97e052b4e39186c708c4c6d6d280", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzU0Nw==", "url": "https://github.com/Graylog2/graylog2-server/pull/7504#discussion_r389753547", "bodyText": "ack!", "author": "mpfz0r", "createdAt": "2020-03-09T15:09:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTcyNTk1Mg=="}], "type": "inlineReview", "revised_code": {"commit": "298a5930989fa6a987f816ff49ed89f9cd1f5782", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\nindex 227e79789c..e02d7e4685 100644\n--- a/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n+++ b/graylog2-server/src/main/java/org/graylog2/inputs/transports/KafkaTransport.java\n\n@@ -581,7 +581,7 @@ public class KafkaTransport extends ThrottleableTransport {\n                     \"\",\n                     \"A newline separated list of Kafka properties. (e.g.: \\\"ssl.keystore.location=/etc/graylog/server/kafka.keystore.jks\\\").\",\n                     ConfigurationField.Optional.OPTIONAL,\n-                    110,\n+                    ConfigurationField.PLACE_AT_END_POSITION,\n                     TextField.Attribute.TEXTAREA\n                     ));\n \n"}}, {"oid": "298a5930989fa6a987f816ff49ed89f9cd1f5782", "url": "https://github.com/Graylog2/graylog2-server/commit/298a5930989fa6a987f816ff49ed89f9cd1f5782", "message": "Improve position defaults", "committedDate": "2020-03-09T15:12:47Z", "type": "commit"}]}