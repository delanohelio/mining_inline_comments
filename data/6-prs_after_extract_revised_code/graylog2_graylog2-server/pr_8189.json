{"pr_number": 8189, "pr_title": "Extract interface from `Messages` class, isolate ES-specific details", "pr_createdAt": "2020-05-22T12:15:15Z", "pr_url": "https://github.com/Graylog2/graylog2-server/pull/8189", "timeline": [{"oid": "5471be96dcbcab13f031c06154914060f1ea3f22", "url": "https://github.com/Graylog2/graylog2-server/commit/5471be96dcbcab13f031c06154914060f1ea3f22", "message": "Extract interface from `Messages` class, isolate ES-specific details\n\nThis PR is extracting a `MessagesAdapter` interface from the Searches\nclass, that allows us to separate the Elasticsearch-specific\nimplementation details from the consumer of that interface. In order to\nreuse the mostly implementation-independent `MessagesIT` integration test, it\nis made abstract. For every implementation of the `MessagesAdapter`\ninterface, a class extending the `MessagesIT` class should be\ncreated, handling the `MessagesAdapter`'s implementation-specific\ninstantiation details.\n\nThere are some refactoring possibilities here, namely handling of\nsuccess and failures during bulk indexing. There is some domain logic\nleft inside the helper methodes of the `MessagesAdapterES6`, which\nshould be moved to the `Messages` class again.\n\nRefs #8155, #8156.", "committedDate": "2020-05-22T12:26:32Z", "type": "forcePushed"}, {"oid": "0e79dedfbf4ed9d8ae6e551356b43b4e341e96a2", "url": "https://github.com/Graylog2/graylog2-server/commit/0e79dedfbf4ed9d8ae6e551356b43b4e341e96a2", "message": "Adding license headers.", "committedDate": "2020-05-22T12:38:56Z", "type": "forcePushed"}, {"oid": "d50db82a99322f92176e01fea446875e5aa61219", "url": "https://github.com/Graylog2/graylog2-server/commit/d50db82a99322f92176e01fea446875e5aa61219", "message": "Adding test dependencies.", "committedDate": "2020-05-28T11:45:51Z", "type": "forcePushed"}, {"oid": "4d79e771e248f8484cb67546c614f27ba85123fc", "url": "https://github.com/Graylog2/graylog2-server/commit/4d79e771e248f8484cb67546c614f27ba85123fc", "message": "Adjusting tests.", "committedDate": "2020-06-03T15:01:59Z", "type": "forcePushed"}, {"oid": "862d5b33c7a4b6bfc3779fd70931c24e4850f98b", "url": "https://github.com/Graylog2/graylog2-server/commit/862d5b33c7a4b6bfc3779fd70931c24e4850f98b", "message": "Extracting index failure creation helper methods.", "committedDate": "2020-06-04T08:12:02Z", "type": "forcePushed"}, {"oid": "635e26d7f8b5def45e66f80127b6c23353b2a2be", "url": "https://github.com/Graylog2/graylog2-server/commit/635e26d7f8b5def45e66f80127b6c23353b2a2be", "message": "Removing unused method and dependency.", "committedDate": "2020-06-04T09:38:19Z", "type": "forcePushed"}, {"oid": "c70861e6061273cd4d073720393536b5058e6e30", "url": "https://github.com/Graylog2/graylog2-server/commit/c70861e6061273cd4d073720393536b5058e6e30", "message": "Adding test for traffic accounting.", "committedDate": "2020-06-04T12:41:23Z", "type": "forcePushed"}, {"oid": "4da53ca4be395c424ee5b56a297f68cec3da9c02", "url": "https://github.com/Graylog2/graylog2-server/commit/4da53ca4be395c424ee5b56a297f68cec3da9c02", "message": "Adding test dependencies.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "482585c102b52aedba3f16bdfe15c633884eba40", "url": "https://github.com/Graylog2/graylog2-server/commit/482585c102b52aedba3f16bdfe15c633884eba40", "message": "Adding `letterId` field to `IndexFailure`.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "7c564875dd05606415e6a0f2e7cf101939bb8b02", "url": "https://github.com/Graylog2/graylog2-server/commit/7c564875dd05606415e6a0f2e7cf101939bb8b02", "message": "Adding `IndexingRequest` class.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "ad9b710634d7c26e48e3b4f171c0ad7a2af34417", "url": "https://github.com/Graylog2/graylog2-server/commit/ad9b710634d7c26e48e3b4f171c0ad7a2af34417", "message": "Adding `MessagesAdapter` interface.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "e4a57a4ef0efcc38064fd5662b3082a70827e41d", "url": "https://github.com/Graylog2/graylog2-server/commit/e4a57a4ef0efcc38064fd5662b3082a70827e41d", "message": "Adding tests for `Messages`/`MessagesAdapterES6`.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "c1c06f2de362a642ad84cd0fdd464eb24673ee9d", "url": "https://github.com/Graylog2/graylog2-server/commit/c1c06f2de362a642ad84cd0fdd464eb24673ee9d", "message": "Delegating `get`/`analyze` to `MessagesAdapter`.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "12e9c825c0a981ba77041fba5314239d8040e988", "url": "https://github.com/Graylog2/graylog2-server/commit/12e9c825c0a981ba77041fba5314239d8040e988", "message": "Binding implementation for `MessagesAdapter`.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "8540f2941099a8a0a1199845cb184645126a531b", "url": "https://github.com/Graylog2/graylog2-server/commit/8540f2941099a8a0a1199845cb184645126a531b", "message": "Adjusting tests.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "97a9ff80ffdb6e29ad38db6b0d9957430e859c78", "url": "https://github.com/Graylog2/graylog2-server/commit/97a9ff80ffdb6e29ad38db6b0d9957430e859c78", "message": "Adding `MessagesAdapterES6`.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "5ee38bbfcf5b98faa8ff9f2a4dc1d80a2cc0498d", "url": "https://github.com/Graylog2/graylog2-server/commit/5ee38bbfcf5b98faa8ff9f2a4dc1d80a2cc0498d", "message": "Revert \"Adjusting tests.\"\n\nThis reverts commit 23515fc2990bfb78f8c2fc27377da000f75e9533.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "3513f15b8e6ac73955aab52a5bc66dfeaa662272", "url": "https://github.com/Graylog2/graylog2-server/commit/3513f15b8e6ac73955aab52a5bc66dfeaa662272", "message": "Extracting ES-specific code into adapter.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "310a6242bbae0487b47055a4bc0131ddb0c6f6ec", "url": "https://github.com/Graylog2/graylog2-server/commit/310a6242bbae0487b47055a4bc0131ddb0c6f6ec", "message": "Adjusting tests.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "a74263903e366dfd7f26a53fd251eb1c68bdd049", "url": "https://github.com/Graylog2/graylog2-server/commit/a74263903e366dfd7f26a53fd251eb1c68bdd049", "message": "Handle recording processing timestamps in `Messages` class.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "a41f0834bb58d89ed7c40184cda29cb570defe21", "url": "https://github.com/Graylog2/graylog2-server/commit/a41f0834bb58d89ed7c40184cda29cb570defe21", "message": "Minor refactorings.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "5f9d1d1ff267373058b33bebf390cbb705cdf3e7", "url": "https://github.com/Graylog2/graylog2-server/commit/5f9d1d1ff267373058b33bebf390cbb705cdf3e7", "message": "Pulling message size accounting back into `Messages` class.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "eb152564cbb78ab4feebd48c4f06f36b69f5978d", "url": "https://github.com/Graylog2/graylog2-server/commit/eb152564cbb78ab4feebd48c4f06f36b69f5978d", "message": "Extracting index failure creation helper methods.", "committedDate": "2020-06-04T14:28:16Z", "type": "commit"}, {"oid": "2d364f55445b111dd039377e0733a7f3c4a47546", "url": "https://github.com/Graylog2/graylog2-server/commit/2d364f55445b111dd039377e0733a7f3c4a47546", "message": "Formatting fixes.", "committedDate": "2020-06-04T14:28:17Z", "type": "commit"}, {"oid": "5d558af92ccf994aedbc6f97046345210c1bbe16", "url": "https://github.com/Graylog2/graylog2-server/commit/5d558af92ccf994aedbc6f97046345210c1bbe16", "message": "Removing unused method and dependency.", "committedDate": "2020-06-04T14:28:17Z", "type": "commit"}, {"oid": "152b4efec5c6581b36771a05fd88e87b792e5b24", "url": "https://github.com/Graylog2/graylog2-server/commit/152b4efec5c6581b36771a05fd88e87b792e5b24", "message": "Adding test for traffic accounting.", "committedDate": "2020-06-04T14:28:17Z", "type": "commit"}, {"oid": "152b4efec5c6581b36771a05fd88e87b792e5b24", "url": "https://github.com/Graylog2/graylog2-server/commit/152b4efec5c6581b36771a05fd88e87b792e5b24", "message": "Adding test for traffic accounting.", "committedDate": "2020-06-04T14:28:17Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTI0NTY5Mg==", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435245692", "bodyText": "This is probably a stray comment that slipped in when moving the code around. Let's remove it.", "author": "alex-konn", "createdAt": "2020-06-04T13:18:37Z", "path": "graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java", "diffHunk": "@@ -0,0 +1,336 @@\n+package org.graylog.storage.elasticsearch6;\n+\n+import com.codahale.metrics.Meter;\n+import com.codahale.metrics.MetricRegistry;\n+import com.github.joschi.jadconfig.util.Duration;\n+import com.github.rholder.retry.Attempt;\n+import com.github.rholder.retry.RetryException;\n+import com.github.rholder.retry.RetryListener;\n+import com.github.rholder.retry.Retryer;\n+import com.github.rholder.retry.RetryerBuilder;\n+import com.github.rholder.retry.WaitStrategies;\n+import com.github.rholder.retry.WaitStrategy;\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Sets;\n+import io.searchbox.client.JestClient;\n+import io.searchbox.client.JestResult;\n+import io.searchbox.core.Bulk;\n+import io.searchbox.core.BulkResult;\n+import io.searchbox.core.DocumentResult;\n+import io.searchbox.core.Get;\n+import io.searchbox.core.Index;\n+import io.searchbox.indices.Analyze;\n+import org.apache.http.client.config.RequestConfig;\n+import org.graylog2.indexer.ElasticsearchException;\n+import org.graylog2.indexer.IndexFailure;\n+import org.graylog2.indexer.IndexFailureImpl;\n+import org.graylog2.indexer.IndexMapping;\n+import org.graylog2.indexer.cluster.jest.JestUtils;\n+import org.graylog2.indexer.messages.DocumentNotFoundException;\n+import org.graylog2.indexer.messages.IndexBlockRetryAttempt;\n+import org.graylog2.indexer.messages.IndexingRequest;\n+import org.graylog2.indexer.messages.Messages;\n+import org.graylog2.indexer.messages.MessagesAdapter;\n+import org.graylog2.indexer.results.ResultMessage;\n+import org.graylog2.plugin.Message;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import javax.inject.Inject;\n+import javax.inject.Named;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+import static com.codahale.metrics.MetricRegistry.name;\n+\n+public class MessagesAdapterES6 implements MessagesAdapter {\n+    private static final Duration MAX_WAIT_TIME = Duration.seconds(30L);\n+    private static final Logger LOG = LoggerFactory.getLogger(MessagesAdapterES6.class);\n+\n+    @VisibleForTesting\n+    static final WaitStrategy exponentialWaitMilliseconds = WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n+\n+    // the wait strategy uses powers of 2 to compute wait times.\n+    // see https://github.com/rholder/guava-retrying/blob/177b6c9b9f3e7957f404f0bdb8e23374cb1de43f/src/main/java/com/github/rholder/retry/WaitStrategies.java#L304\n+    // using 500 leads to the expected exponential pattern of 1000, 2000, 4000, 8000, ...\n+    private static final int retrySecondsMultiplier = 500;\n+\n+    @VisibleForTesting\n+    static final WaitStrategy exponentialWaitSeconds = WaitStrategies.exponentialWait(retrySecondsMultiplier, MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n+\n+    static final String INDEX_BLOCK_ERROR = \"cluster_block_exception\";\n+    static final String INDEX_BLOCK_REASON = \"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\";\n+\n+    private final JestClient client;\n+    private final boolean useExpectContinue;\n+\n+    private static final Retryer<BulkResult> BULK_REQUEST_RETRYER = RetryerBuilder.<BulkResult>newBuilder()\n+            .retryIfException(t -> t instanceof IOException)\n+            .withWaitStrategy(WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit()))\n+            .withRetryListener(new RetryListener() {\n+                @Override\n+                public <V> void onRetry(Attempt<V> attempt) {\n+                    if (attempt.hasException()) {\n+                        LOG.error(\"Caught exception during bulk indexing: {}, retrying (attempt #{}).\", attempt.getExceptionCause(), attempt.getAttemptNumber());\n+                    } else if (attempt.getAttemptNumber() > 1) {\n+                        LOG.info(\"Bulk indexing finally successful (attempt #{}).\", attempt.getAttemptNumber());\n+                    }\n+                }\n+            })\n+            .build();\n+\n+    private final Meter invalidTimestampMeter;\n+\n+    @Inject\n+    public MessagesAdapterES6(JestClient client,\n+                              @Named(\"elasticsearch_use_expect_continue\") boolean useExpectContinue,\n+                              MetricRegistry metricRegistry) {\n+        this.client = client;\n+        this.useExpectContinue = useExpectContinue;\n+        invalidTimestampMeter = metricRegistry.meter(name(Messages.class, \"invalid-timestamps\"));\n+    }\n+\n+    @Override\n+    public ResultMessage get(String messageId, String index) throws IOException, DocumentNotFoundException {\n+        final Get get = new Get.Builder(index, messageId).type(IndexMapping.TYPE_MESSAGE).build();\n+        final DocumentResult result = client.execute(get);\n+\n+        if (!result.isSucceeded()) {\n+            throw new DocumentNotFoundException(index, messageId);\n+        }\n+\n+        @SuppressWarnings(\"unchecked\") final Map<String, Object> message = (Map<String, Object>) result.getSourceAsObject(Map.class, false);\n+\n+        return ResultMessage.parseFromSource(result.getId(), result.getIndex(), message);\n+    }\n+\n+    @Override\n+    public List<String> analyze(String toAnalyze, String index, String analyzer) throws IOException {\n+        final Analyze analyze = new Analyze.Builder().index(index).analyzer(analyzer).text(toAnalyze).build();\n+        final JestResult result = client.execute(analyze);\n+\n+        @SuppressWarnings(\"unchecked\") final List<Map<String, Object>> tokens = (List<Map<String, Object>>) result.getValue(\"tokens\");\n+        final List<String> terms = new ArrayList<>(tokens.size());\n+        tokens.forEach(token -> terms.add((String) token.get(\"token\")));\n+\n+        return terms;\n+    }\n+\n+    @Override\n+    public List<IndexFailure> bulkIndex(List<IndexingRequest> messageList) {\n+        if (messageList.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n+        int chunkSize = messageList.size();\n+        int offset = 0;\n+        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n+        for (;;) {\n+            try {\n+                final List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, offset, chunkSize);\n+                failedItems.addAll(failures);\n+                break; // on success\n+            } catch (EntityTooLargeException e) {\n+                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n+                if (chunkSize == messageList.size()) {\n+                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n+                }\n+                failedItems.addAll(e.failedItems);\n+                offset += e.indexedSuccessfully;\n+                chunkSize /= 2;\n+            }\n+            if (chunkSize == 0) {\n+                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n+            }\n+        }\n+\n+        return indexFailuresFromMessages(failedItems, messageList);\n+    }\n+\n+    private List<IndexFailure> indexFailuresFromMessages(List<BulkResult.BulkResultItem> failedItems, List<IndexingRequest> messageList) {\n+        if (failedItems.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n+        final Map<String, Message> messageMap = messageList.stream()\n+                .map(IndexingRequest::message)\n+                .distinct()\n+                .collect(Collectors.toMap(Message::getId, Function.identity()));\n+        final List<IndexFailure> indexFailures = new ArrayList<>(failedItems.size());\n+        for (BulkResult.BulkResultItem item : failedItems) {\n+            LOG.warn(\"Failed to index message: index=<{}> id=<{}> error=<{}>\", item.index, item.id, item.error);\n+\n+            // Write failure to index_failures.", "originalCommit": "b3ed13253edc8610310ed1bd2d7a623869344fd2", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "4da53ca4be395c424ee5b56a297f68cec3da9c02", "chunk": "diff --git a/graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java b/graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java\ndeleted file mode 100644\nindex 9cd84d7314..0000000000\n--- a/graylog-storage-elasticsearch6/src/main/java/org/graylog/storage/elasticsearch6/MessagesAdapterES6.java\n+++ /dev/null\n\n@@ -1,336 +0,0 @@\n-package org.graylog.storage.elasticsearch6;\n-\n-import com.codahale.metrics.Meter;\n-import com.codahale.metrics.MetricRegistry;\n-import com.github.joschi.jadconfig.util.Duration;\n-import com.github.rholder.retry.Attempt;\n-import com.github.rholder.retry.RetryException;\n-import com.github.rholder.retry.RetryListener;\n-import com.github.rholder.retry.Retryer;\n-import com.github.rholder.retry.RetryerBuilder;\n-import com.github.rholder.retry.WaitStrategies;\n-import com.github.rholder.retry.WaitStrategy;\n-import com.google.common.annotations.VisibleForTesting;\n-import com.google.common.collect.ImmutableMap;\n-import com.google.common.collect.Iterables;\n-import com.google.common.collect.Sets;\n-import io.searchbox.client.JestClient;\n-import io.searchbox.client.JestResult;\n-import io.searchbox.core.Bulk;\n-import io.searchbox.core.BulkResult;\n-import io.searchbox.core.DocumentResult;\n-import io.searchbox.core.Get;\n-import io.searchbox.core.Index;\n-import io.searchbox.indices.Analyze;\n-import org.apache.http.client.config.RequestConfig;\n-import org.graylog2.indexer.ElasticsearchException;\n-import org.graylog2.indexer.IndexFailure;\n-import org.graylog2.indexer.IndexFailureImpl;\n-import org.graylog2.indexer.IndexMapping;\n-import org.graylog2.indexer.cluster.jest.JestUtils;\n-import org.graylog2.indexer.messages.DocumentNotFoundException;\n-import org.graylog2.indexer.messages.IndexBlockRetryAttempt;\n-import org.graylog2.indexer.messages.IndexingRequest;\n-import org.graylog2.indexer.messages.Messages;\n-import org.graylog2.indexer.messages.MessagesAdapter;\n-import org.graylog2.indexer.results.ResultMessage;\n-import org.graylog2.plugin.Message;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import javax.inject.Inject;\n-import javax.inject.Named;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Locale;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.concurrent.ExecutionException;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-\n-import static com.codahale.metrics.MetricRegistry.name;\n-\n-public class MessagesAdapterES6 implements MessagesAdapter {\n-    private static final Duration MAX_WAIT_TIME = Duration.seconds(30L);\n-    private static final Logger LOG = LoggerFactory.getLogger(MessagesAdapterES6.class);\n-\n-    @VisibleForTesting\n-    static final WaitStrategy exponentialWaitMilliseconds = WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n-\n-    // the wait strategy uses powers of 2 to compute wait times.\n-    // see https://github.com/rholder/guava-retrying/blob/177b6c9b9f3e7957f404f0bdb8e23374cb1de43f/src/main/java/com/github/rholder/retry/WaitStrategies.java#L304\n-    // using 500 leads to the expected exponential pattern of 1000, 2000, 4000, 8000, ...\n-    private static final int retrySecondsMultiplier = 500;\n-\n-    @VisibleForTesting\n-    static final WaitStrategy exponentialWaitSeconds = WaitStrategies.exponentialWait(retrySecondsMultiplier, MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit());\n-\n-    static final String INDEX_BLOCK_ERROR = \"cluster_block_exception\";\n-    static final String INDEX_BLOCK_REASON = \"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\";\n-\n-    private final JestClient client;\n-    private final boolean useExpectContinue;\n-\n-    private static final Retryer<BulkResult> BULK_REQUEST_RETRYER = RetryerBuilder.<BulkResult>newBuilder()\n-            .retryIfException(t -> t instanceof IOException)\n-            .withWaitStrategy(WaitStrategies.exponentialWait(MAX_WAIT_TIME.getQuantity(), MAX_WAIT_TIME.getUnit()))\n-            .withRetryListener(new RetryListener() {\n-                @Override\n-                public <V> void onRetry(Attempt<V> attempt) {\n-                    if (attempt.hasException()) {\n-                        LOG.error(\"Caught exception during bulk indexing: {}, retrying (attempt #{}).\", attempt.getExceptionCause(), attempt.getAttemptNumber());\n-                    } else if (attempt.getAttemptNumber() > 1) {\n-                        LOG.info(\"Bulk indexing finally successful (attempt #{}).\", attempt.getAttemptNumber());\n-                    }\n-                }\n-            })\n-            .build();\n-\n-    private final Meter invalidTimestampMeter;\n-\n-    @Inject\n-    public MessagesAdapterES6(JestClient client,\n-                              @Named(\"elasticsearch_use_expect_continue\") boolean useExpectContinue,\n-                              MetricRegistry metricRegistry) {\n-        this.client = client;\n-        this.useExpectContinue = useExpectContinue;\n-        invalidTimestampMeter = metricRegistry.meter(name(Messages.class, \"invalid-timestamps\"));\n-    }\n-\n-    @Override\n-    public ResultMessage get(String messageId, String index) throws IOException, DocumentNotFoundException {\n-        final Get get = new Get.Builder(index, messageId).type(IndexMapping.TYPE_MESSAGE).build();\n-        final DocumentResult result = client.execute(get);\n-\n-        if (!result.isSucceeded()) {\n-            throw new DocumentNotFoundException(index, messageId);\n-        }\n-\n-        @SuppressWarnings(\"unchecked\") final Map<String, Object> message = (Map<String, Object>) result.getSourceAsObject(Map.class, false);\n-\n-        return ResultMessage.parseFromSource(result.getId(), result.getIndex(), message);\n-    }\n-\n-    @Override\n-    public List<String> analyze(String toAnalyze, String index, String analyzer) throws IOException {\n-        final Analyze analyze = new Analyze.Builder().index(index).analyzer(analyzer).text(toAnalyze).build();\n-        final JestResult result = client.execute(analyze);\n-\n-        @SuppressWarnings(\"unchecked\") final List<Map<String, Object>> tokens = (List<Map<String, Object>>) result.getValue(\"tokens\");\n-        final List<String> terms = new ArrayList<>(tokens.size());\n-        tokens.forEach(token -> terms.add((String) token.get(\"token\")));\n-\n-        return terms;\n-    }\n-\n-    @Override\n-    public List<IndexFailure> bulkIndex(List<IndexingRequest> messageList) {\n-        if (messageList.isEmpty()) {\n-            return Collections.emptyList();\n-        }\n-\n-        int chunkSize = messageList.size();\n-        int offset = 0;\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        for (;;) {\n-            try {\n-                final List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, offset, chunkSize);\n-                failedItems.addAll(failures);\n-                break; // on success\n-            } catch (EntityTooLargeException e) {\n-                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n-                if (chunkSize == messageList.size()) {\n-                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n-                }\n-                failedItems.addAll(e.failedItems);\n-                offset += e.indexedSuccessfully;\n-                chunkSize /= 2;\n-            }\n-            if (chunkSize == 0) {\n-                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n-            }\n-        }\n-\n-        return indexFailuresFromMessages(failedItems, messageList);\n-    }\n-\n-    private List<IndexFailure> indexFailuresFromMessages(List<BulkResult.BulkResultItem> failedItems, List<IndexingRequest> messageList) {\n-        if (failedItems.isEmpty()) {\n-            return Collections.emptyList();\n-        }\n-\n-        final Map<String, Message> messageMap = messageList.stream()\n-                .map(IndexingRequest::message)\n-                .distinct()\n-                .collect(Collectors.toMap(Message::getId, Function.identity()));\n-        final List<IndexFailure> indexFailures = new ArrayList<>(failedItems.size());\n-        for (BulkResult.BulkResultItem item : failedItems) {\n-            LOG.warn(\"Failed to index message: index=<{}> id=<{}> error=<{}>\", item.index, item.id, item.error);\n-\n-            // Write failure to index_failures.\n-            final Message messageEntry = messageMap.get(item.id);\n-\n-            final IndexFailure indexFailure = indexFailureFromResultItem(item, messageEntry);\n-\n-            indexFailures.add(indexFailure);\n-        }\n-\n-        return indexFailures;\n-    }\n-\n-    private IndexFailure indexFailureFromResultItem(BulkResult.BulkResultItem item, Message messageEntry) {\n-        final Map<String, Object> doc = ImmutableMap.<String, Object>builder()\n-                .put(\"letter_id\", item.id)\n-                .put(\"index\", item.index)\n-                .put(\"type\", item.type)\n-                .put(\"message\", item.error)\n-                .put(\"timestamp\", messageEntry.getTimestamp())\n-                .build();\n-\n-        return new IndexFailureImpl(doc);\n-    }\n-\n-    private BulkResult runBulkRequest(final Bulk request, int count) {\n-        try {\n-            if (useExpectContinue) {\n-                // Enable Expect-Continue to catch 413 errors before we send the actual data\n-                final RequestConfig requestConfig = RequestConfig.custom().setExpectContinueEnabled(true).build();\n-                return BULK_REQUEST_RETRYER.call(() -> JestUtils.execute(client, requestConfig, request));\n-            } else {\n-                return BULK_REQUEST_RETRYER.call(() -> client.execute(request));\n-            }\n-        } catch (ExecutionException | RetryException e) {\n-            if (e instanceof RetryException) {\n-                LOG.error(\"Could not bulk index {} messages. Giving up after {} attempts.\", count, ((RetryException) e).getNumberOfFailedAttempts());\n-            } else {\n-                LOG.error(\"Couldn't bulk index \" + count + \" messages.\", e);\n-            }\n-            throw new RuntimeException(e);\n-        }\n-    }\n-\n-\n-    private List<BulkResult.BulkResultItem> bulkIndexChunked(final List<IndexingRequest> messageList, int offset, int chunkSize) throws EntityTooLargeException {\n-        chunkSize = Math.min(messageList.size(), chunkSize);\n-\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        final Iterable<List<IndexingRequest>> chunks = Iterables.partition(messageList.subList(offset, messageList.size()), chunkSize);\n-        int chunkCount = 1;\n-        int indexedSuccessfully = 0;\n-        for (List<IndexingRequest> chunk : chunks) {\n-            final BulkResult result = bulkIndexChunk(chunk);\n-\n-            if (result.getResponseCode() == 413) {\n-                throw new EntityTooLargeException(indexedSuccessfully, failedItems);\n-            }\n-\n-            // TODO should we check result.isSucceeded()?\n-\n-            indexedSuccessfully += chunk.size();\n-\n-            final Set<BulkResult.BulkResultItem> remainingFailures = retryOnlyIndexBlockItemsForever(chunk, result.getFailedItems());\n-\n-            failedItems.addAll(remainingFailures);\n-            if (LOG.isDebugEnabled()) {\n-                String chunkInfo = \"\";\n-                if (chunkSize != messageList.size()) {\n-                    chunkInfo = String.format(Locale.ROOT, \" (chunk %d/%d offset %d)\", chunkCount,\n-                            (int) Math.ceil((double) messageList.size() / chunkSize), offset);\n-                }\n-                LOG.debug(\"Index: Bulk indexed {} messages{}, failures: {}\",\n-                        result.getItems().size(), chunkInfo, failedItems.size());\n-            }\n-            if (!remainingFailures.isEmpty()) {\n-                LOG.error(\"Failed to index [{}] messages. Please check the index error log in your web interface for the reason. Error: {}\",\n-                        remainingFailures.size(), result.getErrorMessage());\n-            }\n-            chunkCount++;\n-        }\n-        return failedItems;\n-    }\n-\n-    private BulkResult bulkIndexChunk(List<IndexingRequest> chunk) {\n-        final Bulk.Builder bulk = new Bulk.Builder();\n-\n-        for (IndexingRequest entry : chunk) {\n-            final Message message = entry.message();\n-\n-            bulk.addAction(new Index.Builder(message.toElasticSearchObject(invalidTimestampMeter))\n-                    .index(entry.indexSet().getWriteIndexAlias())\n-                    .type(IndexMapping.TYPE_MESSAGE)\n-                    .id(message.getId())\n-                    .build());\n-        }\n-\n-        return runBulkRequest(bulk.build(), chunk.size());\n-    }\n-\n-    private Set<BulkResult.BulkResultItem> retryOnlyIndexBlockItemsForever(List<IndexingRequest> chunk, List<BulkResult.BulkResultItem> allFailedItems) {\n-        Set<BulkResult.BulkResultItem> indexBlocks = indexBlocksFrom(allFailedItems);\n-        final Set<BulkResult.BulkResultItem> otherFailures = new HashSet<>(Sets.difference(new HashSet<>(allFailedItems), indexBlocks));\n-        List<IndexingRequest> blockedMessages = messagesForResultItems(chunk, indexBlocks);\n-\n-        if (!indexBlocks.isEmpty()) {\n-            LOG.warn(\"Retrying {} messages, because their indices are blocked with status [read-only / allow delete]\", indexBlocks.size());\n-        }\n-\n-        long attempt = 1;\n-\n-        while (!indexBlocks.isEmpty()) {\n-            waitBeforeRetrying(attempt++);\n-\n-            final BulkResult bulkResult = bulkIndexChunk(blockedMessages);\n-\n-            final List<BulkResult.BulkResultItem> failedItems = bulkResult.getFailedItems();\n-\n-            indexBlocks = indexBlocksFrom(failedItems);\n-            blockedMessages = messagesForResultItems(blockedMessages, indexBlocks);\n-\n-            final Set<BulkResult.BulkResultItem> newOtherFailures = Sets.difference(new HashSet<>(failedItems), indexBlocks);\n-            otherFailures.addAll(newOtherFailures);\n-\n-            if (indexBlocks.isEmpty()) {\n-                LOG.info(\"Retries were successful after {} attempts. Ingestion will continue now.\", attempt);\n-            }\n-        }\n-\n-        return otherFailures;\n-    }\n-\n-    private void waitBeforeRetrying(long attempt) {\n-        try {\n-            final long sleepTime = exponentialWaitSeconds.computeSleepTime(new IndexBlockRetryAttempt(attempt));\n-            Thread.sleep(sleepTime);\n-        } catch (InterruptedException e) {\n-            throw new RuntimeException(e);\n-        }\n-    }\n-\n-    private List<IndexingRequest> messagesForResultItems(List<IndexingRequest> chunk, Set<BulkResult.BulkResultItem> indexBlocks) {\n-        final Set<String> blockedMessageIds = indexBlocks.stream().map(item -> item.id).collect(Collectors.toSet());\n-\n-        return chunk.stream().filter(entry -> blockedMessageIds.contains(entry.message().getId())).collect(Collectors.toList());\n-    }\n-\n-    private Set<BulkResult.BulkResultItem> indexBlocksFrom(List<BulkResult.BulkResultItem> allFailedItems) {\n-        return allFailedItems.stream().filter(this::hasFailedDueToBlockedIndex).collect(Collectors.toSet());\n-    }\n-\n-    private boolean hasFailedDueToBlockedIndex(BulkResult.BulkResultItem item) {\n-        return item.errorType.equals(INDEX_BLOCK_ERROR) && item.errorReason.equals(INDEX_BLOCK_REASON);\n-    }\n-\n-    private static class EntityTooLargeException extends Exception {\n-        private final int indexedSuccessfully;\n-        private final List<BulkResult.BulkResultItem> failedItems;\n-\n-        public EntityTooLargeException(int indexedSuccessfully, List<BulkResult.BulkResultItem> failedItems)  {\n-            this.indexedSuccessfully = indexedSuccessfully;\n-            this.failedItems = failedItems;\n-        }\n-    }\n-}\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjI5Nw==", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435336297", "bodyText": "IntelliJ says this is a typo and should be renamed successfulRequests with one l. I believe it blindly!", "author": "alex-konn", "createdAt": "2020-06-04T15:13:08Z", "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "diffHunk": "@@ -164,210 +75,47 @@ public ResultMessage get(String messageId, String index) throws DocumentNotFound\n             return Collections.emptyList();\n         }\n \n-        int chunkSize = messageList.size();\n-        int offset = 0;\n-        List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        for (;;) {\n-            try {\n-                List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, isSystemTraffic, offset, chunkSize);\n-                failedItems.addAll(failures);\n-                break; // on success\n-            } catch (EntityTooLargeException e) {\n-                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n-                if (chunkSize == messageList.size()) {\n-                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n-                }\n-                failedItems.addAll(e.failedItems);\n-                offset += e.indexedSuccessfully;\n-                chunkSize /= 2;\n-            }\n-            if (chunkSize == 0) {\n-                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n-            }\n-        }\n-\n-        if (!failedItems.isEmpty()) {\n-            final Set<String> failedIds = failedItems.stream().map(item -> item.id).collect(Collectors.toSet());\n-            recordTimestamp(messageList, failedIds);\n-            return propagateFailure(failedItems, messageList);\n-        } else {\n-            recordTimestamp(messageList, Collections.emptySet());\n-            return Collections.emptyList();\n-        }\n-    }\n-\n-    private List<BulkResult.BulkResultItem> bulkIndexChunked(final List<Map.Entry<IndexSet, Message>> messageList, boolean isSystemTraffic, int offset, int chunkSize) throws EntityTooLargeException {\n-        chunkSize = Math.min(messageList.size(), chunkSize);\n-\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        final Iterable<List<Map.Entry<IndexSet, Message>>> chunks = Iterables.partition(messageList.subList(offset, messageList.size()), chunkSize);\n-        int chunkCount = 1;\n-        int indexedSuccessfully = 0;\n-        for (List<Map.Entry<IndexSet, Message>> chunk : chunks) {\n-\n-            long messageSizes = chunk.stream().mapToLong(m -> m.getValue().getSize()).sum();\n-\n-            final BulkResult result = bulkIndexChunk(chunk);\n-\n-            if (result.getResponseCode() == 413) {\n-                throw new EntityTooLargeException(indexedSuccessfully, failedItems);\n-            }\n+        final List<IndexingRequest> indexingRequestList = messageList.stream()\n+                .map(entry -> IndexingRequest.create(entry.getKey(), entry.getValue()))\n+                .collect(Collectors.toList());\n \n-            // TODO should we check result.isSucceeded()?\n+        final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n-            indexedSuccessfully += chunk.size();\n+        final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n+        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()", "originalCommit": "152b4efec5c6581b36771a05fd88e87b792e5b24", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3a8fb5e8a2e8e8980596991bbb92ce4ec82f9122", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java b/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\nindex deb61e4b68..45c69b1d80 100644\n--- a/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\n+++ b/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\n\n@@ -82,18 +82,18 @@ public class Messages {\n         final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n         final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n-        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()\n+        final List<IndexingRequest> successfulRequests = indexingRequestList.stream()\n                 .filter(indexingRequest -> !failedIds.contains(indexingRequest.message().getId()))\n                 .collect(Collectors.toList());\n \n-        recordTimestamp(successfullRequests);\n+        recordTimestamp(successfulRequests);\n         accountTotalMessageSizes(indexingRequestList, isSystemTraffic);\n \n         return propagateFailure(indexFailures);\n     }\n \n-    private void accountTotalMessageSizes(List<IndexingRequest> successfullRequests, boolean isSystemTraffic) {\n-        final long totalSizeOfIndexedMessages = successfullRequests.stream()\n+    private void accountTotalMessageSizes(List<IndexingRequest> requests, boolean isSystemTraffic) {\n+        final long totalSizeOfIndexedMessages = requests.stream()\n                 .map(IndexingRequest::message)\n                 .mapToLong(Message::getSize)\n                 .sum();\n"}}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTMzNjk2OA==", "url": "https://github.com/Graylog2/graylog2-server/pull/8189#discussion_r435336968", "bodyText": "Now that we reproduced the old behavior the parameter should just be named request.", "author": "alex-konn", "createdAt": "2020-06-04T15:14:02Z", "path": "graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java", "diffHunk": "@@ -164,210 +75,47 @@ public ResultMessage get(String messageId, String index) throws DocumentNotFound\n             return Collections.emptyList();\n         }\n \n-        int chunkSize = messageList.size();\n-        int offset = 0;\n-        List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        for (;;) {\n-            try {\n-                List<BulkResult.BulkResultItem> failures = bulkIndexChunked(messageList, isSystemTraffic, offset, chunkSize);\n-                failedItems.addAll(failures);\n-                break; // on success\n-            } catch (EntityTooLargeException e) {\n-                LOG.warn(\"Bulk index failed with 'Request Entity Too Large' error. Retrying by splitting up batch size <{}>.\", chunkSize);\n-                if (chunkSize == messageList.size()) {\n-                    LOG.warn(\"Consider lowering the \\\"output_batch_size\\\" setting.\");\n-                }\n-                failedItems.addAll(e.failedItems);\n-                offset += e.indexedSuccessfully;\n-                chunkSize /= 2;\n-            }\n-            if (chunkSize == 0) {\n-                throw new ElasticsearchException(\"Bulk index cannot split output batch any further.\");\n-            }\n-        }\n-\n-        if (!failedItems.isEmpty()) {\n-            final Set<String> failedIds = failedItems.stream().map(item -> item.id).collect(Collectors.toSet());\n-            recordTimestamp(messageList, failedIds);\n-            return propagateFailure(failedItems, messageList);\n-        } else {\n-            recordTimestamp(messageList, Collections.emptySet());\n-            return Collections.emptyList();\n-        }\n-    }\n-\n-    private List<BulkResult.BulkResultItem> bulkIndexChunked(final List<Map.Entry<IndexSet, Message>> messageList, boolean isSystemTraffic, int offset, int chunkSize) throws EntityTooLargeException {\n-        chunkSize = Math.min(messageList.size(), chunkSize);\n-\n-        final List<BulkResult.BulkResultItem> failedItems = new ArrayList<>();\n-        final Iterable<List<Map.Entry<IndexSet, Message>>> chunks = Iterables.partition(messageList.subList(offset, messageList.size()), chunkSize);\n-        int chunkCount = 1;\n-        int indexedSuccessfully = 0;\n-        for (List<Map.Entry<IndexSet, Message>> chunk : chunks) {\n-\n-            long messageSizes = chunk.stream().mapToLong(m -> m.getValue().getSize()).sum();\n-\n-            final BulkResult result = bulkIndexChunk(chunk);\n-\n-            if (result.getResponseCode() == 413) {\n-                throw new EntityTooLargeException(indexedSuccessfully, failedItems);\n-            }\n+        final List<IndexingRequest> indexingRequestList = messageList.stream()\n+                .map(entry -> IndexingRequest.create(entry.getKey(), entry.getValue()))\n+                .collect(Collectors.toList());\n \n-            // TODO should we check result.isSucceeded()?\n+        final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n-            indexedSuccessfully += chunk.size();\n+        final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n+        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()\n+                .filter(indexingRequest -> !failedIds.contains(indexingRequest.message().getId()))\n+                .collect(Collectors.toList());\n \n-            Set<BulkResult.BulkResultItem> remainingFailures = retryOnlyIndexBlockItemsForever(chunk, result.getFailedItems());\n+        recordTimestamp(successfullRequests);\n+        accountTotalMessageSizes(indexingRequestList, isSystemTraffic);\n \n-            failedItems.addAll(remainingFailures);\n-            if (isSystemTraffic) {\n-                systemTrafficCounter.inc(messageSizes);\n-            } else {\n-                outputByteCounter.inc(messageSizes);\n-            }\n-            if (LOG.isDebugEnabled()) {\n-                String chunkInfo = \"\";\n-                if (chunkSize != messageList.size()) {\n-                    chunkInfo = String.format(Locale.ROOT, \" (chunk %d/%d offset %d)\", chunkCount,\n-                            (int) Math.ceil((double) messageList.size() / chunkSize), offset);\n-                }\n-                LOG.debug(\"Index: Bulk indexed {} messages{}, failures: {}\",\n-                        result.getItems().size(), chunkInfo, failedItems.size());\n-            }\n-            if (!remainingFailures.isEmpty()) {\n-                LOG.error(\"Failed to index [{}] messages. Please check the index error log in your web interface for the reason. Error: {}\",\n-                        remainingFailures.size(), result.getErrorMessage());\n-            }\n-            chunkCount++;\n-        }\n-        return failedItems;\n-    }\n-\n-    private BulkResult bulkIndexChunk(List<Map.Entry<IndexSet, Message>> chunk) {\n-        Bulk.Builder bulk = new Bulk.Builder();\n-\n-        for (Map.Entry<IndexSet, Message> entry : chunk) {\n-            final Message message = entry.getValue();\n-\n-            bulk.addAction(new Index.Builder(message.toElasticSearchObject(invalidTimestampMeter))\n-                    .index(entry.getKey().getWriteIndexAlias())\n-                    .type(IndexMapping.TYPE_MESSAGE)\n-                    .id(message.getId())\n-                    .build());\n-        }\n-\n-        return runBulkRequest(bulk.build(), chunk.size());\n+        return propagateFailure(indexFailures);\n     }\n \n-    private Set<BulkResult.BulkResultItem> retryOnlyIndexBlockItemsForever(List<Map.Entry<IndexSet, Message>> chunk, List<BulkResult.BulkResultItem> allFailedItems) {\n-        Set<BulkResult.BulkResultItem> indexBlocks = indexBlocksFrom(allFailedItems);\n-        final Set<BulkResult.BulkResultItem> otherFailures = new HashSet<>(Sets.difference(new HashSet<>(allFailedItems), indexBlocks));\n-        List<Map.Entry<IndexSet, Message>> blockedMessages = messagesForResultItems(chunk, indexBlocks);\n-\n-        if (!indexBlocks.isEmpty()) {\n-            LOG.warn(\"Retrying {} messages, because their indices are blocked with status [read-only / allow delete]\", indexBlocks.size());\n-        }\n-\n-        long attempt = 1;\n-\n-        while (!indexBlocks.isEmpty()) {\n-            waitBeforeRetrying(attempt++);\n-\n-            final BulkResult bulkResult = bulkIndexChunk(blockedMessages);\n-\n-            final List<BulkResult.BulkResultItem> failedItems = bulkResult.getFailedItems();\n-\n-            indexBlocks = indexBlocksFrom(failedItems);\n-            blockedMessages = messagesForResultItems(blockedMessages, indexBlocks);\n-\n-            final Set<BulkResult.BulkResultItem> newOtherFailures = Sets.difference(new HashSet<>(failedItems), indexBlocks);\n-            otherFailures.addAll(newOtherFailures);\n-\n-            if (indexBlocks.isEmpty()) {\n-                LOG.info(\"Retries were successful after {} attempts. Ingestion will continue now.\", attempt);\n-            }\n-        }\n+    private void accountTotalMessageSizes(List<IndexingRequest> successfullRequests, boolean isSystemTraffic) {", "originalCommit": "152b4efec5c6581b36771a05fd88e87b792e5b24", "replyToReviewId": null, "replies": null, "type": "inlineReview", "revised_code": {"commit": "3a8fb5e8a2e8e8980596991bbb92ce4ec82f9122", "chunk": "diff --git a/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java b/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\nindex deb61e4b68..45c69b1d80 100644\n--- a/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\n+++ b/graylog2-server/src/main/java/org/graylog2/indexer/messages/Messages.java\n\n@@ -82,18 +82,18 @@ public class Messages {\n         final List<IndexFailure> indexFailures = messagesAdapter.bulkIndex(indexingRequestList);\n \n         final Set<String> failedIds = indexFailures.stream().map(IndexFailure::letterId).collect(Collectors.toSet());\n-        final List<IndexingRequest> successfullRequests = indexingRequestList.stream()\n+        final List<IndexingRequest> successfulRequests = indexingRequestList.stream()\n                 .filter(indexingRequest -> !failedIds.contains(indexingRequest.message().getId()))\n                 .collect(Collectors.toList());\n \n-        recordTimestamp(successfullRequests);\n+        recordTimestamp(successfulRequests);\n         accountTotalMessageSizes(indexingRequestList, isSystemTraffic);\n \n         return propagateFailure(indexFailures);\n     }\n \n-    private void accountTotalMessageSizes(List<IndexingRequest> successfullRequests, boolean isSystemTraffic) {\n-        final long totalSizeOfIndexedMessages = successfullRequests.stream()\n+    private void accountTotalMessageSizes(List<IndexingRequest> requests, boolean isSystemTraffic) {\n+        final long totalSizeOfIndexedMessages = requests.stream()\n                 .map(IndexingRequest::message)\n                 .mapToLong(Message::getSize)\n                 .sum();\n"}}, {"oid": "3a8fb5e8a2e8e8980596991bbb92ce4ec82f9122", "url": "https://github.com/Graylog2/graylog2-server/commit/3a8fb5e8a2e8e8980596991bbb92ce4ec82f9122", "message": "Resolving helpful annotations from @alex-konn.", "committedDate": "2020-06-04T15:26:53Z", "type": "commit"}]}