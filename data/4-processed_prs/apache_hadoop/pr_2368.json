{"pr_number": 2368, "pr_title": "HADOOP-17296. ABFS: Force reads to be always of buffer size.", "pr_createdAt": "2020-10-07T22:43:18Z", "pr_url": "https://github.com/apache/hadoop/pull/2368", "timeline": [{"oid": "258f2071dab0026f67e795663382cc778abff0b5", "url": "https://github.com/apache/hadoop/commit/258f2071dab0026f67e795663382cc778abff0b5", "message": "Force full buffer read always as in the case of Gen1\nMake readahead block size and number of readahead buffers configurable\nFixes to RAH", "committedDate": "2020-10-05T09:10:15Z", "type": "commit"}, {"oid": "ab2951b427efbc0550ce0d6d92b569ab2eebf9fc", "url": "https://github.com/apache/hadoop/commit/ab2951b427efbc0550ce0d6d92b569ab2eebf9fc", "message": "Removing minor reduntant changes", "committedDate": "2020-10-05T09:19:28Z", "type": "commit"}, {"oid": "48bee6d5eeaf77ab79eca399ff35f3bc93dc0bc6", "url": "https://github.com/apache/hadoop/commit/48bee6d5eeaf77ab79eca399ff35f3bc93dc0bc6", "message": "updates", "committedDate": "2020-10-05T16:26:23Z", "type": "commit"}, {"oid": "3acfc685303b89d01eb543cde061660f5a72d7eb", "url": "https://github.com/apache/hadoop/commit/3acfc685303b89d01eb543cde061660f5a72d7eb", "message": "updates", "committedDate": "2020-10-05T17:08:53Z", "type": "commit"}, {"oid": "db9b7cf5beedb74af210a6cdeaedb4c2394e6398", "url": "https://github.com/apache/hadoop/commit/db9b7cf5beedb74af210a6cdeaedb4c2394e6398", "message": "minor fixes", "committedDate": "2020-10-05T17:12:05Z", "type": "commit"}, {"oid": "23342cc59da120d76a8bf98a47808641634c32d2", "url": "https://github.com/apache/hadoop/commit/23342cc59da120d76a8bf98a47808641634c32d2", "message": "test updates", "committedDate": "2020-10-05T18:02:48Z", "type": "commit"}, {"oid": "ce5638709e618d617914f64016a1962ac6f296c2", "url": "https://github.com/apache/hadoop/commit/ce5638709e618d617914f64016a1962ac6f296c2", "message": "review comments", "committedDate": "2020-10-05T18:52:37Z", "type": "commit"}, {"oid": "b5a68a1c463cb22e70776fc9950a3e6b6292365b", "url": "https://github.com/apache/hadoop/commit/b5a68a1c463cb22e70776fc9950a3e6b6292365b", "message": "fix config comment", "committedDate": "2020-10-05T20:29:27Z", "type": "commit"}, {"oid": "713bb0229724034ff37d8ab9c00895bba97d2790", "url": "https://github.com/apache/hadoop/commit/713bb0229724034ff37d8ab9c00895bba97d2790", "message": "test updates", "committedDate": "2020-10-07T15:45:34Z", "type": "commit"}, {"oid": "dc45a14910f5ef091b7caec4f7edfb4daca02c70", "url": "https://github.com/apache/hadoop/commit/dc45a14910f5ef091b7caec4f7edfb4daca02c70", "message": "minor updates", "committedDate": "2020-10-07T16:01:17Z", "type": "commit"}, {"oid": "54e07871896feb88fa94f0ef063323fe3dfb1fed", "url": "https://github.com/apache/hadoop/commit/54e07871896feb88fa94f0ef063323fe3dfb1fed", "message": "removing redundant updates", "committedDate": "2020-10-07T16:46:29Z", "type": "commit"}, {"oid": "a1d95d6a31e052b905191681eb356d8915a1a4ea", "url": "https://github.com/apache/hadoop/commit/a1d95d6a31e052b905191681eb356d8915a1a4ea", "message": "checkstyle issues and test fix", "committedDate": "2020-10-07T21:21:50Z", "type": "commit"}, {"oid": "df9359dc9e26a299479b168013949320298596b8", "url": "https://github.com/apache/hadoop/commit/df9359dc9e26a299479b168013949320298596b8", "message": "Findbugs fixes", "committedDate": "2020-10-08T10:58:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDE2Mg==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501750162", "bodyText": "Can this LOG/validation be moved to AbfsInputStreamContext.build() ?", "author": "mukund-thakur", "createdAt": "2020-10-08T14:07:50Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -89,9 +91,24 @@ public AbfsInputStream(\n     this.tolerateOobAppends = abfsInputStreamContext.isTolerateOobAppends();\n     this.eTag = eTag;\n     this.readAheadEnabled = true;\n+    this.alwaysReadBufferSize\n+        = abfsInputStreamContext.shouldReadBufferSizeAlways();\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    readAheadBlockSize = abfsInputStreamContext.getReadAheadBlockSize();\n+    if (this.bufferSize > readAheadBlockSize) {", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY4OTcxMg==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503689712", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-10-13T06:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDE2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDc5Mw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501750793", "bodyText": "nit: typo? initialize it get can set", "author": "mukund-thakur", "createdAt": "2020-10-08T14:08:38Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -89,9 +91,24 @@ public AbfsInputStream(\n     this.tolerateOobAppends = abfsInputStreamContext.isTolerateOobAppends();\n     this.eTag = eTag;\n     this.readAheadEnabled = true;\n+    this.alwaysReadBufferSize\n+        = abfsInputStreamContext.shouldReadBufferSizeAlways();\n     this.cachedSasToken = new CachedSASToken(\n         abfsInputStreamContext.getSasTokenRenewPeriodForStreamsInSeconds());\n     this.streamStatistics = abfsInputStreamContext.getStreamStatistics();\n+    readAheadBlockSize = abfsInputStreamContext.getReadAheadBlockSize();\n+    if (this.bufferSize > readAheadBlockSize) {\n+      LOG.debug(\n+          \"fs.azure.read.request.size[={}] is configured for higher size than \"\n+              + \"fs.azure.read.readahead.blocksize[={}]. Auto-align \"\n+              + \"readAhead block size to be same as readRequestSize.\",\n+          bufferSize, readAheadBlockSize);\n+      readAheadBlockSize = this.bufferSize;\n+    }\n+\n+    // Propagate the config values to ReadBufferManager so that the first instance\n+    // to initialize it get can set the readAheadBlockSize", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY4OTc5OQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503689799", "bodyText": "Fixed", "author": "snvijaya", "createdAt": "2020-10-13T06:13:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1MDc5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1Njg4NQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501756885", "bodyText": "I think putting these config together with DEFAULT_READ_BUFFER_SIZE would make code more readable. Also use 4 * ONE_MB as used above.", "author": "mukund-thakur", "createdAt": "2020-10-08T14:16:31Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -74,6 +74,9 @@\n   public static final String DEFAULT_FS_AZURE_APPEND_BLOB_DIRECTORIES = \"\";\n \n   public static final int DEFAULT_READ_AHEAD_QUEUE_DEPTH = -1;\n+  public static final boolean DEFAULT_ALWAYS_READ_BUFFER_SIZE = false;", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY4OTg1NQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503689855", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-10-13T06:13:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1Njg4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MTY2Mw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501761663", "bodyText": "JIRA and PR description says we are trying to read till bufferSize always rather than just the requested length but as per this line we are enabling the buffer manager readahead as well which is bypassed in random read in gen2 as per line 205 below. PS: I have never seen gen1 code though.", "author": "mukund-thakur", "createdAt": "2020-10-08T14:22:40Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -178,11 +195,15 @@ private int readOneBlock(final byte[] b, final int off, final int len) throws IO\n         buffer = new byte[bufferSize];\n       }\n \n-      // Enable readAhead when reading sequentially\n-      if (-1 == fCursorAfterLastRead || fCursorAfterLastRead == fCursor || b.length >= bufferSize) {\n+      if (alwaysReadBufferSize) {\n         bytesRead = readInternal(fCursor, buffer, 0, bufferSize, false);", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5MTQ2Ng==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503691466", "bodyText": "AlwaysReadBufferSize helped the IO pattern to match the Gen1 run. But to be performant readAhead had to be enabled. For the customer scenario explained in the JIRA , for the small row groups for an overall small parquet file size, reading whole buffer size along with readAhead bought good performance.", "author": "snvijaya", "createdAt": "2020-10-13T06:17:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MTY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501762693", "bodyText": "Would like to understand the reasoning behind this. Thanks.", "author": "mukund-thakur", "createdAt": "2020-10-08T14:23:55Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -223,16 +244,19 @@ private int readInternal(final long position, final byte[] b, final int offset,\n \n       // queue read-aheads\n       int numReadAheads = this.readAheadQueueDepth;\n-      long nextSize;\n       long nextOffset = position;\n+      // First read to queue needs to be of readBufferSize and later", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5MzYzNw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503693637", "bodyText": "Couple of things here:\n\nThe earlier code allowed bufferSize to be configurable whereas ReadAhead buffer size was fixed. And each time loop is done, read issued was always for bufferSize which can lead to gaps/holes in the readAhead range done.\nThere is no validation for 4MB as a fixed size for readAhead is optimal for all sequential reads. Having a higher readAhead range for apps like DFSIO which are guaranteed sequential and doing higher readAhead ranges in background can be performant.\nIn this PR, the bug in point 1 is fixed and also a provision to configure readAhead buffer size is provided.", "author": "snvijaya", "createdAt": "2020-10-13T06:23:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY0Nzc3NA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r507647774", "bodyText": "What do you mean by gaps/holes in the readAhead range done here?\nHave you done any experiments on this readAheadBlockSize config? If so, please share.", "author": "mukund-thakur", "createdAt": "2020-10-19T10:43:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDgwMDAxNQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r510800015", "bodyText": "If the buffer gets overwritten by config to 16MB, the readAhead buffer size will still remain to be 4MB as it was a code static. The loop done will start issuing readAheads in 16 MB buffer sizes, the request to readAhead will be:\noffset=0, Length=16MB\noffset=16MB, Length=32MB\nBut the readAhead buffer size is stuck at 4 MB. so it will read only:\noffset=0 Length=4MB\noffset=16MB Length=4MB\nGap being at 4MB to 16MB here.\nThis bug is getting fixed. Tests for all possible combinations here has been added to the tests of this PR.", "author": "snvijaya", "createdAt": "2020-10-23T10:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjE4NzkyNQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r522187925", "bodyText": "I don't think there is any bug in the current production code as such. As far as I understand the code the change is introduced becuase new config is introduced.\nNow my question is why not use readAheadBlockSize for the first call as well? The calls would be like\noffset=0 Length=4MB\noffset=4MB Length=4MB\nSorry to say this but honestly speaking, introducing so many configs is making the code complex and confusing.", "author": "mukund-thakur", "createdAt": "2020-11-12T15:21:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDc2ODcwNA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r530768704", "bodyText": "Read buffer size config being available to be modified, fixed read ahead buffer size and issuing read aheads by buffer size is the current prod behaviour and will function as the picture attached. This will need fixing.\nAnd as for deprecating read buffer size config and only use the new read ahead buffer size config. The config has been available since GA, and hence deprecating it would not be feasible. (Also for clients who are disabling readAheads to use readahead buffer size for reads might be confusing too).\nAs for the number of different configs present for read, 1 and 2 configs already were present while this PR is introducing 3 and 4. So total of 4 configs.\n\nfs.azure.read.request.size\nfs.azure.readaheadqueue.depth\nfs.azure.read.alwaysReadBufferSize => For Gen1 migrating customers\nfs.azure.read.readahead.blocksize => Was one that needed fixing long back as there is no validation on 4 MB being the right size for all workloads. Just the way read buffer size can be modified.\n\nAll these changes are being added based on various customer issues and experiences that we are dealing with. Instead of spending our time in providing patches that can enable them to test various combinations, having these options over a config for their testing saves our dev time to improve the service. As you can see in the PR, the defaults introduced by these configs will retain the current prod behavior.", "author": "snvijaya", "createdAt": "2020-11-26T04:34:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDc2ODg3Nw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r530768877", "bodyText": "Have synced with @mukund-thakur over mail thread further to clear the understanding.", "author": "snvijaya", "createdAt": "2020-11-26T04:35:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2MjY5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2Mzc5Nw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501763797", "bodyText": "nit: use 4 * ONE_MB consistent as everywhere else.", "author": "mukund-thakur", "createdAt": "2020-10-08T14:25:13Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -37,10 +39,10 @@\n   private static final Logger LOGGER = LoggerFactory.getLogger(ReadBufferManager.class);\n \n   private static final int NUM_BUFFERS = 16;\n-  private static final int BLOCK_SIZE = 4 * 1024 * 1024;\n   private static final int NUM_THREADS = 8;\n   private static final int DEFAULT_THRESHOLD_AGE_MILLISECONDS = 3000; // have to see if 3 seconds is a good threshold\n \n+  private static int blockSize = 4 * 1024 * 1024;", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5MzY5Nw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503693697", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-10-13T06:23:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2Mzc5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NTQ4OA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501795488", "bodyText": "Why all these changes ? Why not just initilize the blockSize in init() ?", "author": "mukund-thakur", "createdAt": "2020-10-08T15:05:14Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -49,21 +51,37 @@\n   private Queue<ReadBuffer> readAheadQueue = new LinkedList<>(); // queue of requests that are not picked up by any worker thread yet\n   private LinkedList<ReadBuffer> inProgressList = new LinkedList<>(); // requests being processed by worker threads\n   private LinkedList<ReadBuffer> completedReadList = new LinkedList<>(); // buffers available for reading\n-  private static final ReadBufferManager BUFFER_MANAGER; // singleton, initialized in static initialization block\n+  private static ReadBufferManager bufferManager; // singleton, initialized in static initialization block\n+  private static final ReentrantLock LOCK = new ReentrantLock();\n \n-  static {\n-    BUFFER_MANAGER = new ReadBufferManager();\n-    BUFFER_MANAGER.init();\n+  static ReadBufferManager getBufferManager() {", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0NzY1OA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504647658", "bodyText": "For singleton classes its a common practice to lock around the new instance creation within the getInstance() method. Also, didnt want to make any changes to init method.", "author": "snvijaya", "createdAt": "2020-10-14T12:47:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDc2OTExNg==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r530769116", "bodyText": "Retaining the change as current change has no functional issues.", "author": "snvijaya", "createdAt": "2020-11-26T04:36:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NTQ4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYyNTM3MA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r531625370", "bodyText": "FWIW, synchronize on the Class object is a standard pattern here, but the lock is fine, just a bit of extra coding", "author": "steveloughran", "createdAt": "2020-11-27T14:14:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NTQ4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NjQ4OA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r501796488", "bodyText": "please add some reasoning/docs around these changes. Thanks.", "author": "mukund-thakur", "createdAt": "2020-10-08T15:06:41Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/ReadBufferManager.java", "diffHunk": "@@ -464,4 +483,53 @@ int getCompletedReadListSize() {\n   void callTryEvict() {\n     tryEvict();\n   }\n+\n+  @VisibleForTesting\n+  void testResetReadBufferManager() {", "originalCommit": "df9359dc9e26a299479b168013949320298596b8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5Mzc2MQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r503693761", "bodyText": "Done", "author": "snvijaya", "createdAt": "2020-10-13T06:23:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc5NjQ4OA=="}], "type": "inlineReview"}, {"oid": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "url": "https://github.com/apache/hadoop/commit/d8664c3fd203a0d72688de9ca93e765c5c096c67", "message": "Incorporate review comments", "committedDate": "2020-10-13T05:16:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMTg4NA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504001884", "bodyText": "AssertJ has rich api's to tackle these kind of assertions. Try that\nexample : Assertions.assertThat(list)\n.hasSameElementsAs(list2)", "author": "mukund-thakur", "createdAt": "2020-10-13T14:32:03Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,", "originalCommit": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0ODEwNA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504648104", "bodyText": "Thanks. Have updated except for the content check assert.", "author": "snvijaya", "createdAt": "2020-10-14T12:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMTg4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMjMzNA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504002334", "bodyText": "Better to use assert equals here inspite of assertTrue no?", "author": "mukund-thakur", "createdAt": "2020-10-13T14:32:40Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,\n+            expectedFirstReadAheadBufferContents));\n+\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+        inputStream.read(secondReadBuffer, 0, readAheadRequestSize) == readAheadRequestSize);\n+    assertTrue(\"Data mismatch found in RAH2\",", "originalCommit": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0OTAxMg==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504649012", "bodyText": ":) Just a force of habit. Will try to use the assertions.\nHave modified in the other test code areas of this PR as well.", "author": "snvijaya", "createdAt": "2020-10-14T12:49:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwMjMzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwOTIxOA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504009218", "bodyText": "See if you can reuse the data generation and new file creation code from ContractTestUtils.dataset() and ContractTestUtils.createFile.", "author": "mukund-thakur", "createdAt": "2020-10-13T14:41:24Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/services/TestAbfsInputStream.java", "diffHunk": "@@ -447,4 +490,168 @@ public void testReadAheadManagerForSuccessfulReadAhead() throws Exception {\n     checkEvictedStatus(inputStream, 0, true);\n   }\n \n+  /**\n+   * Test readahead with different config settings for request request size and\n+   * readAhead block size\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiffReadRequestSizeAndRAHBlockSize() throws Exception {\n+    // Set requestRequestSize = 4MB and readAheadBufferSize=8MB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FOUR_MB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    testReadAheadConfigs(FOUR_MB, TEST_READAHEAD_DEPTH_4, false, EIGHT_MB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(SIXTEEN_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    AbfsInputStream inputStream = testReadAheadConfigs(SIXTEEN_KB,\n+        TEST_READAHEAD_DEPTH_2, true, SIXTEEN_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, SIXTEEN_KB);\n+\n+    // Test for requestRequestSize =16KB and readAheadBufferSize=48KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(SIXTEEN_KB, TEST_READAHEAD_DEPTH_2, true,\n+        FORTY_EIGHT_KB);\n+    testReadAheads(inputStream, SIXTEEN_KB, FORTY_EIGHT_KB);\n+\n+    // Test for requestRequestSize =48KB and readAheadBufferSize=16KB\n+    ReadBufferManager.getBufferManager()\n+        .testResetReadBufferManager(FORTY_EIGHT_KB, INCREASED_READ_BUFFER_AGE_THRESHOLD);\n+    inputStream = testReadAheadConfigs(FORTY_EIGHT_KB, TEST_READAHEAD_DEPTH_2,\n+        true,\n+        SIXTEEN_KB);\n+    testReadAheads(inputStream, FORTY_EIGHT_KB, SIXTEEN_KB);\n+  }\n+\n+\n+  private void testReadAheads(AbfsInputStream inputStream,\n+      int readRequestSize,\n+      int readAheadRequestSize)\n+      throws Exception {\n+    if (readRequestSize > readAheadRequestSize) {\n+      readAheadRequestSize = readRequestSize;\n+    }\n+\n+    byte[] firstReadBuffer = new byte[readRequestSize];\n+    byte[] secondReadBuffer = new byte[readAheadRequestSize];\n+\n+    // get the expected bytes to compare\n+    byte[] expectedFirstReadAheadBufferContents = new byte[readRequestSize];\n+    byte[] expectedSecondReadAheadBufferContents = new byte[readAheadRequestSize];\n+    getExpectedBufferData(0, readRequestSize, expectedFirstReadAheadBufferContents);\n+    getExpectedBufferData(readRequestSize, readAheadRequestSize,\n+        expectedSecondReadAheadBufferContents);\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+              inputStream.read(firstReadBuffer, 0, readRequestSize) == readRequestSize);\n+    assertTrue(\"Data mismatch found in RAH1\",\n+        Arrays.equals(firstReadBuffer,\n+            expectedFirstReadAheadBufferContents));\n+\n+\n+    assertTrue(\"Read should be of exact requested size\",\n+        inputStream.read(secondReadBuffer, 0, readAheadRequestSize) == readAheadRequestSize);\n+    assertTrue(\"Data mismatch found in RAH2\",\n+        Arrays.equals(secondReadBuffer,\n+            expectedSecondReadAheadBufferContents));\n+  }\n+\n+  public AbfsInputStream testReadAheadConfigs(int readRequestSize,\n+      int readAheadQueueDepth,\n+      boolean alwaysReadBufferSizeEnabled,\n+      int readAheadBlockSize) throws Exception {\n+    Configuration\n+        config = new Configuration(\n+        this.getRawConfiguration());\n+    config.set(\"fs.azure.read.request.size\", Integer.toString(readRequestSize));\n+    config.set(\"fs.azure.readaheadqueue.depth\",\n+        Integer.toString(readAheadQueueDepth));\n+    config.set(\"fs.azure.read.alwaysReadBufferSize\",\n+        Boolean.toString(alwaysReadBufferSizeEnabled));\n+    config.set(\"fs.azure.read.readahead.blocksize\",\n+        Integer.toString(readAheadBlockSize));\n+    if (readRequestSize > readAheadBlockSize) {\n+      readAheadBlockSize = readRequestSize;\n+    }\n+\n+    Path testPath = new Path(\n+        \"/testReadAheadConfigs\");\n+    final AzureBlobFileSystem fs = createTestFile(testPath,\n+        ALWAYS_READ_BUFFER_SIZE_TEST_FILE_SIZE, config);\n+    byte[] byteBuffer = new byte[ONE_MB];\n+    AbfsInputStream inputStream = this.getAbfsStore(fs)\n+        .openFileForRead(testPath, null);\n+\n+    assertEquals(\"Unexpected AbfsInputStream buffer size\", readRequestSize,\n+        inputStream.getBufferSize());\n+    assertEquals(\"Unexpected ReadAhead queue depth\", readAheadQueueDepth,\n+        inputStream.getReadAheadQueueDepth());\n+    assertEquals(\"Unexpected AlwaysReadBufferSize settings\",\n+        alwaysReadBufferSizeEnabled,\n+        inputStream.shouldAlwaysReadBufferSize());\n+    assertEquals(\"Unexpected readAhead block size\", readAheadBlockSize,\n+        ReadBufferManager.getBufferManager().getReadAheadBlockSize());\n+\n+    return inputStream;\n+  }\n+\n+  private void getExpectedBufferData(int offset, int length, byte[] b) {\n+    boolean startFillingIn = false;\n+    int indexIntoBuffer = 0;\n+    char character = 'a';\n+\n+    for (int i = 0; i < (offset + length); i++) {\n+      if (i == offset) {\n+        startFillingIn = true;\n+      }\n+\n+      if ((startFillingIn) && (indexIntoBuffer < length)) {\n+        b[indexIntoBuffer] = (byte) character;\n+        indexIntoBuffer++;\n+      }\n+\n+      character = (character == 'z') ? 'a' : (char) ((int) character + 1);\n+    }\n+  }\n+\n+  private AzureBlobFileSystem createTestFile(Path testFilePath, long testFileSize,", "originalCommit": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY1MDgzNw==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504650837", "bodyText": "I need the file created to also have a specific pattern of data ingested and have another method which needs to return the expected data content from offset to range. This is being used to cross check with readahead buffer data. Will retain for these tests. But thanks, hadnt observed the other APIs, will default to them where there isnt a specific needs on the file content.", "author": "snvijaya", "createdAt": "2020-10-14T12:52:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAwOTIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAxODY5MA==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504018690", "bodyText": "Why creating a new method here if we are just doing a passthrough?", "author": "mukund-thakur", "createdAt": "2020-10-13T14:53:08Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRandomRead.java", "diffHunk": "@@ -448,15 +477,119 @@ public void testRandomReadPerformance() throws Exception {\n             ratio < maxAcceptableRatio);\n   }\n \n+  /**\n+   * With this test we should see a full buffer read being triggered in case\n+   * alwaysReadBufferSize is on, else only the requested buffer size.\n+   * Hence a seek done few bytes away from last read position will trigger\n+   * a network read when alwaysReadBufferSize is off, whereas it will return\n+   * from the internal buffer when it is on.\n+   * Reading a full buffer size is the Gen1 behaviour.\n+   * @throws Throwable\n+   */\n+  @Test\n+  public void testAlwaysReadBufferSizeConfig() throws Throwable {\n+    testAlwaysReadBufferSizeConfig(false);\n+    testAlwaysReadBufferSizeConfig(true);\n+  }\n+\n+  private void assertStatistics(AzureBlobFileSystem fs,", "originalCommit": "d8664c3fd203a0d72688de9ca93e765c5c096c67", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY4NTM0OQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r504685349", "bodyText": "Redundant method removed.", "author": "snvijaya", "createdAt": "2020-10-14T13:40:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDAxODY5MA=="}], "type": "inlineReview"}, {"oid": "4eeeca67556fcaf8234b1d85c1b3e5d3b6054d32", "url": "https://github.com/apache/hadoop/commit/4eeeca67556fcaf8234b1d85c1b3e5d3b6054d32", "message": "Review comments", "committedDate": "2020-10-14T11:55:25Z", "type": "commit"}, {"oid": "ac9c464bd28be94d90575973f5edfb5a9c253d46", "url": "https://github.com/apache/hadoop/commit/ac9c464bd28be94d90575973f5edfb5a9c253d46", "message": "Merge from trunk", "committedDate": "2020-10-14T12:20:35Z", "type": "commit"}, {"oid": "dbb42c04b70bbd87d7aee5059da3ae582d899407", "url": "https://github.com/apache/hadoop/commit/dbb42c04b70bbd87d7aee5059da3ae582d899407", "message": "Remove redundant test method", "committedDate": "2020-10-14T13:39:49Z", "type": "commit"}, {"oid": "7a2ec7416f1bb27f3f93f3bdcdb82a27bd3b8636", "url": "https://github.com/apache/hadoop/commit/7a2ec7416f1bb27f3f93f3bdcdb82a27bd3b8636", "message": "Add documentation for the new configs added", "committedDate": "2020-10-22T10:11:49Z", "type": "commit"}, {"oid": "ef2539bbff2578558180a571f82b8eb9b8eedd02", "url": "https://github.com/apache/hadoop/commit/ef2539bbff2578558180a571f82b8eb9b8eedd02", "message": "Merge branch 'trunk' into HADOOP-17296", "committedDate": "2020-10-22T15:20:37Z", "type": "commit"}, {"oid": "c4cb3b23d83a34bbe6aa8ededdfefb4f31f9096a", "url": "https://github.com/apache/hadoop/commit/c4cb3b23d83a34bbe6aa8ededdfefb4f31f9096a", "message": "Fix test generated OOM due to multiple consecutive buffer re-allocations", "committedDate": "2020-10-29T12:08:11Z", "type": "commit"}, {"oid": "6fc3914fd950322db66fe2b0dc710678699839b1", "url": "https://github.com/apache/hadoop/commit/6fc3914fd950322db66fe2b0dc710678699839b1", "message": "Merge branch 'trunk' into HADOOP-17296", "committedDate": "2020-11-25T07:16:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTYyODAzOQ==", "url": "https://github.com/apache/hadoop/pull/2368#discussion_r531628039", "bodyText": "FWIW I use a Junit rule to get the method name, then you can hava a path() method which dynamically creates the unique path, including when you use parameterized tests.", "author": "steveloughran", "createdAt": "2020-11-27T14:19:23Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAzureBlobFileSystemRandomRead.java", "diffHunk": "@@ -99,12 +115,14 @@ public void testBasicRead() throws Exception {\n   public void testRandomRead() throws Exception {\n     Assume.assumeFalse(\"This test does not support namespace enabled account\",\n             this.getFileSystem().getIsNamespaceEnabled());\n-    assumeHugeFileExists();\n+    Path testPath = new Path(TEST_FILE_PREFIX + \"_testRandomRead\");", "originalCommit": "6fc3914fd950322db66fe2b0dc710678699839b1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}