{"pr_number": 2297, "pr_title": "HADOOP-17125. Using snappy-java in SnappyCodec", "pr_createdAt": "2020-09-10T20:01:42Z", "pr_url": "https://github.com/apache/hadoop/pull/2297", "timeline": [{"oid": "23da5134a4a14853491d6674b1bfe848efc12b63", "url": "https://github.com/apache/hadoop/commit/23da5134a4a14853491d6674b1bfe848efc12b63", "message": "SnappyCodec with java-snappy", "committedDate": "2020-09-01T17:57:04Z", "type": "commit"}, {"oid": "3aa3f55a770dc1cf736142786dd791ea78960453", "url": "https://github.com/apache/hadoop/commit/3aa3f55a770dc1cf736142786dd791ea78960453", "message": "rebase master", "committedDate": "2020-09-01T17:57:04Z", "type": "commit"}, {"oid": "65fd9f4f2e40da5f64e46afb072fe70ad2ece6ae", "url": "https://github.com/apache/hadoop/commit/65fd9f4f2e40da5f64e46afb072fe70ad2ece6ae", "message": "Reset compressedDirectBuf and uncompressedDirectBuf.", "committedDate": "2020-09-10T19:35:10Z", "type": "commit"}, {"oid": "9c0f08b274261b00436c50f7fe67f7d46486015c", "url": "https://github.com/apache/hadoop/commit/9c0f08b274261b00436c50f7fe67f7d46486015c", "message": "Revert some debugging code.", "committedDate": "2020-09-10T20:00:44Z", "type": "commit"}, {"oid": "f52dd205707259064247b23f7443f5840bace62f", "url": "https://github.com/apache/hadoop/commit/f52dd205707259064247b23f7443f5840bace62f", "message": "Remove snappy native code.", "committedDate": "2020-09-10T22:52:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486685241", "bodyText": "Per #2201 (comment) Are those native code used in hadoop-mapreduce-client-nativetask? If so, we probably need to keep it now.", "author": "dbtsai", "createdAt": "2020-09-10T23:19:12Z", "path": "hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyCompressor.c", "diffHunk": "@@ -1,166 +0,0 @@\n-/*", "originalCommit": "f52dd205707259064247b23f7443f5840bace62f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NjQ4Mw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486686483", "bodyText": "Hmm, because we remove native method in java files, I think we don't generate .h file needed for compilation: https://ci-hadoop.apache.org/job/hadoop-multibranch/job/PR-2297/1/artifact/out/patch-unit-hadoop-common-project_hadoop-common.txt\n[WARNING] /home/jenkins/jenkins-home/workspace/hadoop-multibranch_PR-2297/src/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.c:32:10: fatal error: org_apache_hadoop_io_compress_snappy_SnappyDecompressor.h: No such file or directory\n[WARNING]  #include \"org_apache_hadoop_io_compress_snappy_SnappyDecompressor.h\"\n[WARNING]           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[WARNING] compilation terminated.", "author": "viirya", "createdAt": "2020-09-10T23:23:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4ODcwMg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r486688702", "bodyText": "Btw, I don't see they are used in hadoop-mapreduce-client-nativetask if I don't miss it. Let's wait the build and test.", "author": "viirya", "createdAt": "2020-09-10T23:30:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjY4NTI0MQ=="}], "type": "inlineReview"}, {"oid": "87903a929342688cc39a512d931441b438b74e72", "url": "https://github.com/apache/hadoop/commit/87903a929342688cc39a512d931441b438b74e72", "message": "Remove snappy compilation.", "committedDate": "2020-09-11T03:40:01Z", "type": "commit"}, {"oid": "5adcf53117d0338910376146766365aa25a13e19", "url": "https://github.com/apache/hadoop/commit/5adcf53117d0338910376146766365aa25a13e19", "message": "Fix limit parameter.", "committedDate": "2020-09-11T17:13:16Z", "type": "commit"}, {"oid": "40cc18a897efd3bbcfbf14bf77babf77890103e8", "url": "https://github.com/apache/hadoop/commit/40cc18a897efd3bbcfbf14bf77babf77890103e8", "message": "Remove require.snappy.", "committedDate": "2020-09-13T22:20:17Z", "type": "commit"}, {"oid": "0e44d4b810fa94253c6eb9b0ad5ea30bc62175ba", "url": "https://github.com/apache/hadoop/commit/0e44d4b810fa94253c6eb9b0ad5ea30bc62175ba", "message": "trigger CI", "committedDate": "2020-09-15T17:55:23Z", "type": "commit"}, {"oid": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "url": "https://github.com/apache/hadoop/commit/666a37bc56d6512fe953e438ad4224e935ea6cd2", "message": "Add compatibility test.", "committedDate": "2020-09-15T21:28:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMDk4OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489420988", "bodyText": "provided", "author": "steveloughran", "createdAt": "2020-09-16T13:06:03Z", "path": "hadoop-common-project/hadoop-common/pom.xml", "diffHunk": "@@ -363,6 +363,10 @@\n       <artifactId>wildfly-openssl-java</artifactId>\n       <scope>provided</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.xerial.snappy</groupId>", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTY2MzI0OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489663248", "bodyText": "We can make it provided, and once we create a hadoop-compression module, we can add back the jar. @viirya since the jar will be provided, we need to check if the class exists so we can log it with right message.", "author": "dbtsai", "createdAt": "2020-09-16T18:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMDk4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422323", "bodyText": "use name of config option which users can tun", "author": "steveloughran", "createdAt": "2020-09-16T13:08:03Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg2MDQ2MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489860460", "bodyText": "I found this check is not needed. Removed.", "author": "viirya", "createdAt": "2020-09-17T01:36:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjMyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489422661", "bodyText": "please stop the IDE removing trailing whitespace on lines which haven't been edited; complicates life", "author": "steveloughran", "createdAt": "2020-09-16T13:08:33Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,13 +258,29 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      // There is compressed input, decompress it now.\n+      int size = Snappy.uncompressedLength((ByteBuffer) compressedDirectBuf);\n+      if (size > uncompressedDirectBuf.remaining()) {\n+        throw new IOException(\"Could not decompress data. \" +\n+          \"uncompressedDirectBuf length is too small.\");\n+      }\n+      size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);\n+      return size;\n+    }\n+  }\n \n-  private native int decompressBytesDirect();\n-  \n   int decompressDirect(ByteBuffer src, ByteBuffer dst) throws IOException {\n     assert (this instanceof SnappyDirectDecompressor);\n-    \n+", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTg1ODA4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489858089", "bodyText": "ok, reverted tailing whitespace.", "author": "viirya", "createdAt": "2020-09-17T01:32:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMjY2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489423811", "bodyText": "what about the others snappylibs?", "author": "steveloughran", "createdAt": "2020-09-16T13:10:12Z", "path": "hadoop-project-dist/pom.xml", "diffHunk": "@@ -341,7 +340,6 @@\n                     <argument>--openssllib=${openssl.lib}</argument>\n                     <argument>--opensslbinbundle=${bundle.openssl.in.bin}</argument>\n                     <argument>--openssllibbundle=${bundle.openssl}</argument>\n-                    <argument>--snappybinbundle=${bundle.snappy.in.bin}</argument>\n                     <argument>--snappylib=${snappy.lib}</argument>\n                     <argument>--snappylibbundle=${bundle.snappy}</argument>", "originalCommit": "666a37bc56d6512fe953e438ad4224e935ea6cd2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTgyOTQxMQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489829411", "bodyText": "we don't touch hadoop-mapreduce-client-nativetask that needs snappy lib still, per #2201 (comment)", "author": "viirya", "createdAt": "2020-09-17T00:43:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk3NzM2NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496977364", "bodyText": "does this mean we don't need the option in dev-support/bin/dist-copynativelibs for snappy?", "author": "sunchao", "createdAt": "2020-09-29T19:13:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5MzY4Mw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497093683", "bodyText": "Hmm, I am not sure about this now. I think it is safer to revert this back? I am not sure if hadoop-mapreduce-client-nativetask uses this.", "author": "viirya", "createdAt": "2020-09-29T22:19:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5OTgxMg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497099812", "bodyText": "Re-checked. hadoop-mapreduce-client-nativetask doesn't use bundle.snappy.in.bin. Only native-win profile of hadoop-common and native-win profile in hadoop-project use bundle.snappy.in.bin. They are added by HADOOP-9802 to support SnappyCodec on Windows. So looks like it is safe to remove.", "author": "viirya", "createdAt": "2020-09-29T22:35:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQyMzgxMQ=="}], "type": "inlineReview"}, {"oid": "9de4712af35830e483d37c50261c36812ed047fa", "url": "https://github.com/apache/hadoop/commit/9de4712af35830e483d37c50261c36812ed047fa", "message": "Check snappy library and remove useless code.", "committedDate": "2020-09-17T01:23:24Z", "type": "commit"}, {"oid": "0ed518d238adfee81c821b74434785afb5684710", "url": "https://github.com/apache/hadoop/commit/0ed518d238adfee81c821b74434785afb5684710", "message": "Revert trailing whitespace.", "committedDate": "2020-09-17T01:34:39Z", "type": "commit"}, {"oid": "0ed518d238adfee81c821b74434785afb5684710", "url": "https://github.com/apache/hadoop/commit/0ed518d238adfee81c821b74434785afb5684710", "message": "Revert trailing whitespace.", "committedDate": "2020-09-17T01:34:39Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489921893", "bodyText": "In the original code, we throw a runtime exception if the native snappy is not found. Should we follow?\n      throw new RuntimeException(\"native snappy library not available: \" +\n          \"SnappyCompressor has not been loaded.\");", "author": "dbtsai", "createdAt": "2020-09-17T03:15:02Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,19 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      LOG.warn(\"Error loading snappy libraries: \" + t);", "originalCommit": "0ed518d238adfee81c821b74434785afb5684710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njg2OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596868", "bodyText": "ok, changed to throw RuntimeException.", "author": "viirya", "createdAt": "2020-09-17T22:28:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMTg5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r489922981", "bodyText": "we need to check if the snappy class is available for SnappyCompressor too.", "author": "dbtsai", "createdAt": "2020-09-17T03:16:50Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,24 +48,6 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;", "originalCommit": "0ed518d238adfee81c821b74434785afb5684710", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5Njc2MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r490596761", "bodyText": "added.", "author": "viirya", "createdAt": "2020-09-17T22:28:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTkyMjk4MQ=="}], "type": "inlineReview"}, {"oid": "712749c041c012bb8eef41f826d1abc8da937a36", "url": "https://github.com/apache/hadoop/commit/712749c041c012bb8eef41f826d1abc8da937a36", "message": "For review comment.", "committedDate": "2020-09-17T22:22:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054048", "bodyText": "Fix this last sentence if you make a new PR", "author": "saintstack", "createdAt": "2020-09-24T05:51:38Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDgyNg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070826", "bodyText": "Oops, thanks.", "author": "viirya", "createdAt": "2020-09-24T06:37:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDA0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494054389", "bodyText": "Is it the 'native snappy library' that is missing or the java-snappy jar?", "author": "saintstack", "createdAt": "2020-09-24T05:52:44Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -48,30 +49,20 @@\n   private long bytesRead = 0L;\n   private long bytesWritten = 0L;\n \n-  private static boolean nativeSnappyLoaded = false;\n-  \n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyCompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyCompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDQ0Mg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070442", "bodyText": "It is java-snappy jar, yeah, I will revise the message.", "author": "viirya", "createdAt": "2020-09-24T06:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NDM4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055344", "bodyText": "s/compressBytesDirect/compressBytesDirectBuf/ ? Or.. why the Bytes... I see none referenced in the method so compressDirectBuf?", "author": "saintstack", "createdAt": "2020-09-24T05:55:41Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +282,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressBytesDirect() throws IOException {", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA3MDE4OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494070189", "bodyText": "This compressBytesDirect and decompressBytesDirect basically are copied from original method names. compressDirectBuf and decompressDirectBuf looks good to me.", "author": "viirya", "createdAt": "2020-09-24T06:35:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NTkwMQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494055901", "bodyText": "ditto... could this message be more informative: i.e. \"hey, operator... you need to add the snappy-java.jar to your CLASSPATH... its not packaged up for you..\"", "author": "saintstack", "createdAt": "2020-09-24T05:57:15Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,20 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if its availability.\n+    try {\n+      SnappyLoader.getVersion();\n+    } catch (Throwable t) {\n+      throw new RuntimeException(\"native snappy library not available: \" +", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NjQyOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494056429", "bodyText": "ditto", "author": "saintstack", "createdAt": "2020-09-24T05:59:04Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +267,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressBytesDirect() throws IOException {", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494057480", "bodyText": "hmm... this is a little anemic. Have you considered adding a data file that is a little more interesting than this?", "author": "saintstack", "createdAt": "2020-09-24T06:02:18Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,43 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07050d06050d\";", "originalCommit": "712749c041c012bb8eef41f826d1abc8da937a36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA2OTU4Ng==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494069586", "bodyText": "String is to make the test as simple as possible. Maybe further shorten the string?", "author": "viirya", "createdAt": "2020-09-24T06:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI1Njg2Ng==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494256866", "bodyText": "should be split across lines, but otherwise fine inline -simpler for tests", "author": "steveloughran", "createdAt": "2020-09-24T12:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzMjIyNg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494532226", "bodyText": "Ok, I split the long string. Thanks.", "author": "viirya", "createdAt": "2020-09-24T18:36:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA1NzQ4MA=="}], "type": "inlineReview"}, {"oid": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "url": "https://github.com/apache/hadoop/commit/2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "message": "Address review comments.", "committedDate": "2020-09-24T18:34:48Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494537597", "bodyText": "nit, uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0); for safety.", "author": "dbtsai", "createdAt": "2020-09-24T18:46:06Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(directBufferSize).position(0);", "originalCommit": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU1MDgwOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494550809", "bodyText": "done. thanks.", "author": "viirya", "createdAt": "2020-09-24T19:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzNzU5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494539014", "bodyText": "Why is this change needed?", "author": "dbtsai", "createdAt": "2020-09-24T18:48:38Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -432,7 +412,11 @@ public void assertCompression(String name, Compressor compressor,\n               joiner.join(name, \"byte arrays not equals error !!!\"),\n               originalRawData, decompressOut.toByteArray());\n         } catch (Exception ex) {\n-          fail(joiner.join(name, ex.getMessage()));\n+          if (ex.getMessage() != null) {\n+            fail(joiner.join(name, ex.getMessage()));\n+          } else {\n+            fail(joiner.join(name, ExceptionUtils.getStackTrace(ex)));", "originalCommit": "2b5a2122d597c06373d242b0cbc3eed8d3fb7aa4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU0MjI0OQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r494542249", "bodyText": "When I first took over this change, the test failed with NPE without any details. It is because the exception thrown returns null from getMessage(). joiner.join(name, null) causes the NPE, so I changed it to print stack trace once getMessage() returns null. It's better for debugging.", "author": "viirya", "createdAt": "2020-09-24T18:54:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTczNjI5NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r499736294", "bodyText": "NPE is why toString() is what new code should do.\nWhy don't we just throw new AssertionError(name +ex, ex). That way, the stack trace doesn't get lost, which is something we never want to have happen,", "author": "steveloughran", "createdAt": "2020-10-05T16:48:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUzOTAxNA=="}], "type": "inlineReview"}, {"oid": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "url": "https://github.com/apache/hadoop/commit/1cb398bbbaf702501f558ce32cda07d1ca7917ca", "message": "Take safer approach.", "committedDate": "2020-09-24T19:09:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158362", "bodyText": "nit: this seems unnecessary as clear is called shortly after at the call site?", "author": "sunchao", "createdAt": "2020-09-25T18:22:07Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java", "diffHunk": "@@ -291,9 +283,17 @@ public long getBytesWritten() {\n   public void end() {\n   }\n \n-  private native static void initIDs();\n-\n-  private native int compressBytesDirect();\n-\n-  public native static String getLibraryName();\n+  private int compressDirectBuf() throws IOException {\n+    if (uncompressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `uncompressedDirectBuf` for reading\n+      uncompressedDirectBuf.limit(uncompressedDirectBufLen).position(0);\n+      int size = Snappy.compress((ByteBuffer) uncompressedDirectBuf,\n+              (ByteBuffer) compressedDirectBuf);\n+      uncompressedDirectBufLen = 0;\n+      uncompressedDirectBuf.limit(uncompressedDirectBuf.capacity()).position(0);", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzE5MQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183191", "bodyText": "Seems so, I remember I added this to fix test failure. It might be SnappyDecompressor, I think, then I copied to SnappyCompressor. Deleted this and see what Jenkins tells.", "author": "viirya", "createdAt": "2020-09-25T19:13:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODM2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495158655", "bodyText": "nit: SnappyLoader is marked as \"internal use-only\" though so not sure if there is better alternative here.", "author": "sunchao", "createdAt": "2020-09-25T18:22:41Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -45,30 +46,21 @@\n   private int userBufOff = 0, userBufLen = 0;\n   private boolean finished;\n \n-  private static boolean nativeSnappyLoaded = false;\n-\n-  static {\n-    if (NativeCodeLoader.isNativeCodeLoaded() &&\n-        NativeCodeLoader.buildSupportsSnappy()) {\n-      try {\n-        initIDs();\n-        nativeSnappyLoaded = true;\n-      } catch (Throwable t) {\n-        LOG.error(\"failed to load SnappyDecompressor\", t);\n-      }\n-    }\n-  }\n-  \n-  public static boolean isNativeCodeLoaded() {\n-    return nativeSnappyLoaded;\n-  }\n-  \n   /**\n    * Creates a new compressor.\n    *\n    * @param directBufferSize size of the direct buffer to be used.\n    */\n   public SnappyDecompressor(int directBufferSize) {\n+    // `snappy-java` is provided scope. We need to check if it is available.\n+    try {\n+      SnappyLoader.getVersion();", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MjMzMg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495182332", "bodyText": "The \"internal user-only\" of SnappyLoader, based on its comment, seems more related to native library loading stuff.\ngetVersion is static method and it doesn't involve loading of native library described in SnappyLoader, so I guess it is fine? Otherwise, I don't find other proper one to check.", "author": "viirya", "createdAt": "2020-09-25T19:11:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE1ODY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495163850", "bodyText": "nit: can we just call compressedDirectBuf.clear()?", "author": "sunchao", "createdAt": "2020-09-25T18:33:04Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java", "diffHunk": "@@ -276,10 +268,20 @@ public void end() {\n     // do nothing\n   }\n \n-  private native static void initIDs();\n+  private int decompressDirectBuf() throws IOException {\n+    if (compressedDirectBufLen == 0) {\n+      return 0;\n+    } else {\n+      // Set the position and limit of `compressedDirectBuf` for reading\n+      compressedDirectBuf.limit(compressedDirectBufLen).position(0);\n+      int size = Snappy.uncompress((ByteBuffer) compressedDirectBuf,\n+              (ByteBuffer) uncompressedDirectBuf);\n+      compressedDirectBufLen = 0;\n+      compressedDirectBuf.limit(compressedDirectBuf.capacity()).position(0);", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4MzI5MA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495183290", "bodyText": "yap", "author": "viirya", "createdAt": "2020-09-25T19:14:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2Mzg1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165398", "bodyText": "nit: unrelated changes :)", "author": "sunchao", "createdAt": "2020-09-25T18:36:13Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/CompressDecompressTester.java", "diffHunk": "@@ -495,19 +479,16 @@ public String getName() {\n     Compressor compressor = pair.compressor;\n \n     if (compressor.getClass().isAssignableFrom(Lz4Compressor.class)\n-            && (NativeCodeLoader.isNativeCodeLoaded()))", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NTkwOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495175908", "bodyText": "Oh, this is from @dbtsai's original change. I think adding curly brackets is better? I can revert this if you think it is necessary.", "author": "viirya", "createdAt": "2020-09-25T18:58:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE3NjkwOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495176908", "bodyText": "Yeah usually it's not recommended to include unrelated changes in Hadoop patch, we may add another refactoring PR later if this is absolutely necessary.", "author": "sunchao", "createdAt": "2020-09-25T19:00:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE5MzMzOA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495193338", "bodyText": "Ok. Reverted the change.", "author": "viirya", "createdAt": "2020-09-25T19:36:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTM5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE2NTkxOQ==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r495165919", "bodyText": "nit: long lines (80 chars).", "author": "sunchao", "createdAt": "2020-09-25T18:37:16Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/snappy/TestSnappyCompressorDecompressor.java", "diffHunk": "@@ -446,4 +442,49 @@ public void doWork() throws Exception {\n \n     ctx.waitFor(60000);\n   }\n+\n+  @Test\n+  public void testSnappyCompatibility() throws Exception {\n+    // HADOOP-17125. Using snappy-java in SnappyCodec. These strings are raw data and compressed data\n+    // using previous native Snappy codec. We use updated Snappy codec to decode it and check if it\n+    // matches.\n+    String rawData = \"010a06030a040a0c0109020c0a010204020d02000b010701080605080b090902060a08050206\" +\n+            \"0a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d060907020a0\" +\n+            \"30a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b060e030e0a07\" +\n+            \"050d06050d\";\n+    String compressed = \"8001f07f010a06030a040a0c0109020c0a010204020d02000b010701080605080b0909020\" +\n+            \"60a080502060a0d06070908080a0c0105030904090d05090800040c090c0d0d0804000d00040b0b0d010d\" +\n+            \"060907020a030a0c0900040905080107040d0c01060a0b09070a04000b01040b09000e0e00020b06050b0\" +\n+            \"60e030e0a07050d06050d\";\n+\n+    byte[] rawDataBytes = Hex.decodeHex(rawData);\n+    byte[] compressedBytes = Hex.decodeHex(compressed);\n+\n+    ByteBuffer inBuf = ByteBuffer.allocateDirect(compressedBytes.length);\n+    inBuf.put(compressedBytes, 0, compressedBytes.length);\n+    inBuf.flip();\n+\n+    ByteBuffer outBuf = ByteBuffer.allocateDirect(rawDataBytes.length);\n+    ByteBuffer expected = ByteBuffer.wrap(rawDataBytes);\n+\n+    SnappyDecompressor.SnappyDirectDecompressor decompressor = new SnappyDecompressor.SnappyDirectDecompressor();", "originalCommit": "1cb398bbbaf702501f558ce32cda07d1ca7917ca", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "aa6a6d550d6c154f072622371263287dc1b34f11", "url": "https://github.com/apache/hadoop/commit/aa6a6d550d6c154f072622371263287dc1b34f11", "message": "For review comments.", "committedDate": "2020-09-25T19:25:02Z", "type": "commit"}, {"oid": "19edba29fa0e6949f11926ada6d606bee39dad67", "url": "https://github.com/apache/hadoop/commit/19edba29fa0e6949f11926ada6d606bee39dad67", "message": "Update BUILDING and NativeLibraries.", "committedDate": "2020-09-25T19:33:35Z", "type": "commit"}, {"oid": "beec93160a3dafbc62637672a70c07f53474e3f9", "url": "https://github.com/apache/hadoop/commit/beec93160a3dafbc62637672a70c07f53474e3f9", "message": "Merge remote-tracking branch 'upstream/trunk' into java-snappy", "committedDate": "2020-09-25T19:35:34Z", "type": "commit"}, {"oid": "fc525faf79b68662f452fc0cd8233194ef9f4555", "url": "https://github.com/apache/hadoop/commit/fc525faf79b68662f452fc0cd8233194ef9f4555", "message": "Make snappy-java as compile scope.", "committedDate": "2020-09-29T17:53:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496960563", "bodyText": "We shouldn't remove this", "author": "sunchao", "createdAt": "2020-09-29T18:45:28Z", "path": "hadoop-common-project/hadoop-common/src/main/native/native.vcxproj", "diffHunk": "@@ -68,30 +68,13 @@\n     <IntDir>..\\..\\..\\target\\native\\$(Configuration)\\</IntDir>\n     <TargetName>hadoop</TargetName>\n   </PropertyGroup>\n-  <PropertyGroup>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.dll')\">$(CustomSnappyPrefix)</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\lib\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\lib</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyPrefix)\\bin\\snappy.dll') And '$(SnappyLib)' == ''\">$(CustomSnappyPrefix)\\bin</SnappyLib>\n-    <SnappyLib Condition=\"Exists('$(CustomSnappyLib)') And '$(SnappyLib)' == ''\">$(CustomSnappyLib)</SnappyLib>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\snappy.h')\">$(CustomSnappyPrefix)</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyPrefix)\\include\\snappy.h') And '$(SnappyInclude)' == ''\">$(CustomSnappyPrefix)\\include</SnappyInclude>\n-    <SnappyInclude Condition=\"Exists('$(CustomSnappyInclude)') And '$(SnappyInclude)' == ''\">$(CustomSnappyInclude)</SnappyInclude>\n-    <SnappyEnabled Condition=\"'$(SnappyLib)' != '' And '$(SnappyInclude)' != ''\">true</SnappyEnabled>\n-    <IncludePath Condition=\"'$(SnappyEnabled)' == 'true'\">$(SnappyInclude);$(IncludePath)</IncludePath>\n-    <IncludePath Condition=\"Exists('$(ZLIB_HOME)')\">$(ZLIB_HOME);$(IncludePath)</IncludePath>", "originalCommit": "fc525faf79b68662f452fc0cd8233194ef9f4555", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk5Nzc3NA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r496997774", "bodyText": "Since we remove snappy native code, why do we need to keep this on windows?", "author": "viirya", "createdAt": "2020-09-29T19:39:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAwOTAyNw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497009027", "bodyText": "I mean the last line, which is about ZLIB.", "author": "sunchao", "createdAt": "2020-09-29T20:00:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA2ODMyNg==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497068326", "bodyText": "I think it is together with snappy stuffs here, no? They are in same PropertyGroup. I think it is used to add zlib home into include paths in the property group.", "author": "viirya", "createdAt": "2020-09-29T21:23:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA3MzI3OA==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497073278", "bodyText": "I think this is for Zlib compressor (see https://issues.apache.org/jira/browse/HADOOP-10450). Yeah it is a bit confusing that it's defined in the same group.", "author": "sunchao", "createdAt": "2020-09-29T21:33:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA3ODEwMw==", "url": "https://github.com/apache/hadoop/pull/2297#discussion_r497078103", "bodyText": "Oh, yeah, will add it back.", "author": "viirya", "createdAt": "2020-09-29T21:44:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njk2MDU2Mw=="}], "type": "inlineReview"}, {"oid": "562b80d036e43dfe4412219b3887f2e557b2d783", "url": "https://github.com/apache/hadoop/commit/562b80d036e43dfe4412219b3887f2e557b2d783", "message": "For review comment.", "committedDate": "2020-09-29T23:06:44Z", "type": "commit"}, {"oid": "0600169d3fa5d082c6a28defc59872b2f485873f", "url": "https://github.com/apache/hadoop/commit/0600169d3fa5d082c6a28defc59872b2f485873f", "message": "Revert Snappy description in BUILDING.txt.", "committedDate": "2020-09-29T23:15:14Z", "type": "commit"}, {"oid": "0600169d3fa5d082c6a28defc59872b2f485873f", "url": "https://github.com/apache/hadoop/commit/0600169d3fa5d082c6a28defc59872b2f485873f", "message": "Revert Snappy description in BUILDING.txt.", "committedDate": "2020-09-29T23:15:14Z", "type": "forcePushed"}, {"oid": "fd71a20df19543a66f23f384c58ad985f0ee2e67", "url": "https://github.com/apache/hadoop/commit/fd71a20df19543a66f23f384c58ad985f0ee2e67", "message": "Fix style issue.", "committedDate": "2020-10-01T17:10:56Z", "type": "commit"}, {"oid": "7dc320ddddeaeac5a4eee36c703fededbccbf6de", "url": "https://github.com/apache/hadoop/commit/7dc320ddddeaeac5a4eee36c703fededbccbf6de", "message": "Fix another style...", "committedDate": "2020-10-02T18:57:22Z", "type": "commit"}, {"oid": "5685b0b7902e7a6c350d3378dea1159cb968cb24", "url": "https://github.com/apache/hadoop/commit/5685b0b7902e7a6c350d3378dea1159cb968cb24", "message": "Address comments: throwing AssertionError and exclude jobTokenPassword for license check.", "committedDate": "2020-10-05T18:07:50Z", "type": "commit"}]}