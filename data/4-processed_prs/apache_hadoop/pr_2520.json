{"pr_number": 2520, "pr_title": "HADOOP-17290. ABFS: Add Identifiers to Client Request Header", "pr_createdAt": "2020-12-04T11:26:11Z", "pr_url": "https://github.com/apache/hadoop/pull/2520", "timeline": [{"oid": "ce05bd0627e1320945067227884fdc22a26a971c", "url": "https://github.com/apache/hadoop/commit/ce05bd0627e1320945067227884fdc22a26a971c", "message": "correlation-id : req-id : retry-count", "committedDate": "2020-09-29T20:09:06Z", "type": "commit"}, {"oid": "da6c02538686015881cbc95f817472d0a01cefc5", "url": "https://github.com/apache/hadoop/commit/da6c02538686015881cbc95f817472d0a01cefc5", "message": "adding IDs", "committedDate": "2020-10-06T08:43:06Z", "type": "commit"}, {"oid": "e21f7c6b1604b515c1359532294b5f49552f36fc", "url": "https://github.com/apache/hadoop/commit/e21f7c6b1604b515c1359532294b5f49552f36fc", "message": "add op id", "committedDate": "2020-10-07T06:57:53Z", "type": "commit"}, {"oid": "67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "url": "https://github.com/apache/hadoop/commit/67c53a3af0827014e4fffc7ae4ffe224edc2b2dd", "message": "undo formatting", "committedDate": "2020-10-07T11:01:35Z", "type": "commit"}, {"oid": "0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "url": "https://github.com/apache/hadoop/commit/0e57f19f90c1b3325a22b8d67ab429ea4ff2f15a", "message": "to pc", "committedDate": "2020-10-07T22:50:04Z", "type": "commit"}, {"oid": "20c916d962fb10e9c9209c332a2aa00818c3f1aa", "url": "https://github.com/apache/hadoop/commit/20c916d962fb10e9c9209c332a2aa00818c3f1aa", "message": "tc -> ops", "committedDate": "2020-10-08T02:15:33Z", "type": "commit"}, {"oid": "5e97c55b77905e441037b0af76db85c69da5f810", "url": "https://github.com/apache/hadoop/commit/5e97c55b77905e441037b0af76db85c69da5f810", "message": "other IDs", "committedDate": "2020-10-08T10:30:11Z", "type": "commit"}, {"oid": "2df54bfb9008324ebee052c052b1c85f88f37417", "url": "https://github.com/apache/hadoop/commit/2df54bfb9008324ebee052c052b1c85f88f37417", "message": "debug", "committedDate": "2020-10-08T16:05:41Z", "type": "commit"}, {"oid": "d3ba55099abebafe7b6624f8aff17710c0b54fe9", "url": "https://github.com/apache/hadoop/commit/d3ba55099abebafe7b6624f8aff17710c0b54fe9", "message": "debug", "committedDate": "2020-10-09T03:18:48Z", "type": "commit"}, {"oid": "70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "url": "https://github.com/apache/hadoop/commit/70e7d0f38fc26e7edd05b2e744390d03ae0ed387", "message": "builds", "committedDate": "2020-10-09T05:31:46Z", "type": "commit"}, {"oid": "3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "url": "https://github.com/apache/hadoop/commit/3cb2ccbacf66a9743019161bc7ead1236d42d0b8", "message": "primary req id", "committedDate": "2020-10-12T04:05:10Z", "type": "commit"}, {"oid": "c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "url": "https://github.com/apache/hadoop/commit/c3ddf824e24998ef0a4d7efd15d032a1bc98ba74", "message": "readahead", "committedDate": "2020-10-12T15:23:11Z", "type": "commit"}, {"oid": "2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "url": "https://github.com/apache/hadoop/commit/2e882d4cfaa531ebfc1922223d60cbde315dfcd9", "message": "dependent & client req id (readaheads)", "committedDate": "2020-10-13T12:24:14Z", "type": "commit"}, {"oid": "eeae13ee9a93dab4ea6713333842b543065529b3", "url": "https://github.com/apache/hadoop/commit/eeae13ee9a93dab4ea6713333842b543065529b3", "message": "liststatus ok", "committedDate": "2020-10-13T13:13:43Z", "type": "commit"}, {"oid": "d4343fae35638e25952a3ce1cc8b472fb726a83b", "url": "https://github.com/apache/hadoop/commit/d4343fae35638e25952a3ce1cc8b472fb726a83b", "message": "create overwrite case ok", "committedDate": "2020-10-13T13:54:26Z", "type": "commit"}, {"oid": "0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "url": "https://github.com/apache/hadoop/commit/0ef9a27aa46e442f52bd57e4a336c94b5ea2e54d", "message": "fixed some errors", "committedDate": "2020-10-15T03:09:17Z", "type": "commit"}, {"oid": "107df060d5bca839cb0e266d6631d862e2fcfe0a", "url": "https://github.com/apache/hadoop/commit/107df060d5bca839cb0e266d6631d862e2fcfe0a", "message": "tc changes", "committedDate": "2020-10-15T19:14:22Z", "type": "commit"}, {"oid": "166f99cd6239cab03076e6dd02192eebd3f79165", "url": "https://github.com/apache/hadoop/commit/166f99cd6239cab03076e6dd02192eebd3f79165", "message": "fix errors", "committedDate": "2020-10-19T01:44:34Z", "type": "commit"}, {"oid": "d762d864206f3b88af01d1558b4101ce8746dedd", "url": "https://github.com/apache/hadoop/commit/d762d864206f3b88af01d1558b4101ce8746dedd", "message": "test", "committedDate": "2020-10-20T10:03:27Z", "type": "commit"}, {"oid": "d905c93c79a3db9d55505bbd022e9d8ae62f8050", "url": "https://github.com/apache/hadoop/commit/d905c93c79a3db9d55505bbd022e9d8ae62f8050", "message": "1 test draft", "committedDate": "2020-10-20T13:47:23Z", "type": "commit"}, {"oid": "56acf8059ff53d9a833df3e45951d2d6e64182db", "url": "https://github.com/apache/hadoop/commit/56acf8059ff53d9a833df3e45951d2d6e64182db", "message": "test IDs", "committedDate": "2020-10-23T05:47:35Z", "type": "commit"}, {"oid": "0a96fbe57d115c80277de7c11510bcbad9de3d55", "url": "https://github.com/apache/hadoop/commit/0a96fbe57d115c80277de7c11510bcbad9de3d55", "message": "clear()", "committedDate": "2020-10-23T07:02:41Z", "type": "commit"}, {"oid": "ba8d98837dc0c5d291a9d1bcde096c09327a024d", "url": "https://github.com/apache/hadoop/commit/ba8d98837dc0c5d291a9d1bcde096c09327a024d", "message": "minor edits", "committedDate": "2020-10-27T17:52:22Z", "type": "commit"}, {"oid": "6be948e986a26c754ba9306a5f6a740e42f25bdd", "url": "https://github.com/apache/hadoop/commit/6be948e986a26c754ba9306a5f6a740e42f25bdd", "message": "minor edits", "committedDate": "2020-10-27T18:27:35Z", "type": "commit"}, {"oid": "5af42f61947953fd879f6a3db96206be88cf9db1", "url": "https://github.com/apache/hadoop/commit/5af42f61947953fd879f6a3db96206be88cf9db1", "message": "minor edits", "committedDate": "2020-10-27T18:31:32Z", "type": "commit"}, {"oid": "077b5bd26503e998071c3b4289d6ddbeab79c63c", "url": "https://github.com/apache/hadoop/commit/077b5bd26503e998071c3b4289d6ddbeab79c63c", "message": "minor edits", "committedDate": "2020-10-27T18:40:31Z", "type": "commit"}, {"oid": "ee07bae1eb71781b11ae52aed473fa22f6095038", "url": "https://github.com/apache/hadoop/commit/ee07bae1eb71781b11ae52aed473fa22f6095038", "message": "minor edits/whitespc", "committedDate": "2020-10-27T19:12:35Z", "type": "commit"}, {"oid": "9457a04dbcac6da42253c4f9bdc0c98758e608a2", "url": "https://github.com/apache/hadoop/commit/9457a04dbcac6da42253c4f9bdc0c98758e608a2", "message": "merge conflict", "committedDate": "2020-11-02T10:48:06Z", "type": "commit"}, {"oid": "16811384244eeb61895c7d04037000b646e5d717", "url": "https://github.com/apache/hadoop/commit/16811384244eeb61895c7d04037000b646e5d717", "message": "pr changes + dummyTC", "committedDate": "2020-11-03T10:32:00Z", "type": "commit"}, {"oid": "a4755553881a1c8d39bdff737bf4a3d273238328", "url": "https://github.com/apache/hadoop/commit/a4755553881a1c8d39bdff737bf4a3d273238328", "message": "test ns() + remove extra changes", "committedDate": "2020-11-03T14:06:56Z", "type": "commit"}, {"oid": "644293231d6eb34245742c1fde75fc022d7dae66", "url": "https://github.com/apache/hadoop/commit/644293231d6eb34245742c1fde75fc022d7dae66", "message": "revert httpop formatting", "committedDate": "2020-11-03T14:25:18Z", "type": "commit"}, {"oid": "ec92f2576698dcb702898f1af61cfb71fb9d54b0", "url": "https://github.com/apache/hadoop/commit/ec92f2576698dcb702898f1af61cfb71fb9d54b0", "message": "revert httpop formatting", "committedDate": "2020-11-03T14:32:59Z", "type": "commit"}, {"oid": "fecc00a2962009f17e4eca57da4b448f25136b4a", "url": "https://github.com/apache/hadoop/commit/fecc00a2962009f17e4eca57da4b448f25136b4a", "message": "move tc init near usage", "committedDate": "2020-11-03T15:08:44Z", "type": "commit"}, {"oid": "1cead5325d825d9076063fef209b07e2f2ae9ca3", "url": "https://github.com/apache/hadoop/commit/1cead5325d825d9076063fef209b07e2f2ae9ca3", "message": "minor change", "committedDate": "2020-11-04T05:52:05Z", "type": "commit"}, {"oid": "5dbc7830221092d91562723320e271901ba4eb82", "url": "https://github.com/apache/hadoop/commit/5dbc7830221092d91562723320e271901ba4eb82", "message": "pr changes", "committedDate": "2020-11-05T02:40:38Z", "type": "commit"}, {"oid": "6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "url": "https://github.com/apache/hadoop/commit/6409e3fde14ea4167d6d6eeacaba2430d9ce15f1", "message": "enum, opnames", "committedDate": "2020-11-05T06:23:28Z", "type": "commit"}, {"oid": "d304123654e1cbe18c6617b317727c7e76a900a5", "url": "https://github.com/apache/hadoop/commit/d304123654e1cbe18c6617b317727c7e76a900a5", "message": "enum", "committedDate": "2020-11-08T08:46:42Z", "type": "commit"}, {"oid": "1e4f46db77273da9ec37770aa2923eb4c8350df2", "url": "https://github.com/apache/hadoop/commit/1e4f46db77273da9ec37770aa2923eb4c8350df2", "message": "format changes", "committedDate": "2020-11-12T04:57:37Z", "type": "commit"}, {"oid": "15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "url": "https://github.com/apache/hadoop/commit/15a22cd4a3f9e57ec35b31d5b786ee19a1213ce8", "message": "test code (#3)\n\n* adding callback structure\r\n\r\n* testListPath correlation header\r\n\r\n* validate IDs; readahead/streamid\r\n\r\n* add common tests + other changes\r\n\r\n* remove stream/extra stuff\r\n\r\n* handle parallel requests\r\n\r\n* clear\r\n\r\n* testTC, retryNum\r\n\r\n* rebase on HADOOP-17290", "committedDate": "2020-11-12T07:35:27Z", "type": "commit"}, {"oid": "e44c64c720485a058d8acd60844848980396ac97", "url": "https://github.com/apache/hadoop/commit/e44c64c720485a058d8acd60844848980396ac97", "message": "other tests (main)", "committedDate": "2020-11-12T12:15:40Z", "type": "commit"}, {"oid": "725c98415030f856ce333b605cbf3de82bb2b028", "url": "https://github.com/apache/hadoop/commit/725c98415030f856ce333b605cbf3de82bb2b028", "message": "all tests", "committedDate": "2020-11-16T05:02:22Z", "type": "commit"}, {"oid": "9b4f55809d54a4f7348859cede4c44a68ccd16ec", "url": "https://github.com/apache/hadoop/commit/9b4f55809d54a4f7348859cede4c44a68ccd16ec", "message": "test tc for appendblob=true", "committedDate": "2020-11-17T07:50:35Z", "type": "commit"}, {"oid": "858695bd6dba12363a7a57e4fe406697f3ce6f8e", "url": "https://github.com/apache/hadoop/commit/858695bd6dba12363a7a57e4fe406697f3ce6f8e", "message": "clean up code", "committedDate": "2020-11-23T05:39:08Z", "type": "commit"}, {"oid": "a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "url": "https://github.com/apache/hadoop/commit/a9a4f425c7b3f97f0ad44426374e3dcfaa34b019", "message": "simplify preq test", "committedDate": "2020-11-23T09:19:05Z", "type": "commit"}, {"oid": "e18bab62749da71b56ecb6a4a3ec86983cdc7139", "url": "https://github.com/apache/hadoop/commit/e18bab62749da71b56ecb6a4a3ec86983cdc7139", "message": "fix matchers error", "committedDate": "2020-11-25T05:45:26Z", "type": "commit"}, {"oid": "1553cf653a5c503aea310e9980c52925be0c05fa", "url": "https://github.com/apache/hadoop/commit/1553cf653a5c503aea310e9980c52925be0c05fa", "message": "merge conflicts", "committedDate": "2020-11-25T06:01:00Z", "type": "commit"}, {"oid": "22c3a8411e623bac3e2909620a13e666184e85cf", "url": "https://github.com/apache/hadoop/commit/22c3a8411e623bac3e2909620a13e666184e85cf", "message": "pr revw changes", "committedDate": "2020-11-25T11:48:45Z", "type": "commit"}, {"oid": "3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "url": "https://github.com/apache/hadoop/commit/3a3a40eab3ff65ef6748c973f0df37e55d2af0ca", "message": "fix some test failures", "committedDate": "2020-11-26T02:49:03Z", "type": "commit"}, {"oid": "472d0903972128c0634702712ab4d7fb84442b3d", "url": "https://github.com/apache/hadoop/commit/472d0903972128c0634702712ab4d7fb84442b3d", "message": "code cleanup", "committedDate": "2020-11-27T04:06:06Z", "type": "commit"}, {"oid": "21e2a8682ca9c9684f68e4c06185f16c991f85d7", "url": "https://github.com/apache/hadoop/commit/21e2a8682ca9c9684f68e4c06185f16c991f85d7", "message": "fix sastoken matcher", "committedDate": "2020-11-27T06:40:32Z", "type": "commit"}, {"oid": "8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "url": "https://github.com/apache/hadoop/commit/8eaad735b62f9a5ae18c6f726f3e58b0c1bce9c3", "message": "access test, formatting", "committedDate": "2020-11-30T09:07:17Z", "type": "commit"}, {"oid": "db8d89586959545ff69e1c9ab95260759c8bf1a4", "url": "https://github.com/apache/hadoop/commit/db8d89586959545ff69e1c9ab95260759c8bf1a4", "message": "format PR diff", "committedDate": "2020-11-30T10:41:03Z", "type": "commit"}, {"oid": "0521969b26ead9397ff6856bba4b7f1feaca5394", "url": "https://github.com/apache/hadoop/commit/0521969b26ead9397ff6856bba4b7f1feaca5394", "message": "more formatting", "committedDate": "2020-11-30T12:20:53Z", "type": "commit"}, {"oid": "907fc1b987d2006ebde2783eca73b0b0391526dc", "url": "https://github.com/apache/hadoop/commit/907fc1b987d2006ebde2783eca73b0b0391526dc", "message": "merge conflict", "committedDate": "2020-12-03T06:35:52Z", "type": "commit"}, {"oid": "f3f91f4390a365f85ed64b34cd667163fa583c1b", "url": "https://github.com/apache/hadoop/commit/f3f91f4390a365f85ed64b34cd667163fa583c1b", "message": "merge conflict", "committedDate": "2020-12-03T06:36:37Z", "type": "commit"}, {"oid": "18ea7f0445555dfbf43aa175a0aa642521ff39f9", "url": "https://github.com/apache/hadoop/commit/18ea7f0445555dfbf43aa175a0aa642521ff39f9", "message": "pr changes", "committedDate": "2020-12-03T07:44:19Z", "type": "commit"}, {"oid": "af74d8e464bfd736614dabbdd54cd37113d40126", "url": "https://github.com/apache/hadoop/commit/af74d8e464bfd736614dabbdd54cd37113d40126", "message": "stream id test", "committedDate": "2020-12-03T09:28:17Z", "type": "commit"}, {"oid": "3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "url": "https://github.com/apache/hadoop/commit/3a4eb41661d9ffc9819a5de66de59ca56c0658d7", "message": "documentation md", "committedDate": "2020-12-04T08:50:24Z", "type": "commit"}, {"oid": "dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "url": "https://github.com/apache/hadoop/commit/dee9ec6970a8898ba9a399e5f5b52ac6140ee250", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2020-12-04T10:26:30Z", "type": "commit"}, {"oid": "f90bbb399437d461a9bc4ee614c27a3d03bae227", "url": "https://github.com/apache/hadoop/commit/f90bbb399437d461a9bc4ee614c27a3d03bae227", "message": "fix yetus bugs", "committedDate": "2020-12-05T06:57:23Z", "type": "commit"}, {"oid": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "url": "https://github.com/apache/hadoop/commit/a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "message": "fix randomread getTC failure", "committedDate": "2020-12-07T10:11:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA4OTQwOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543089409", "bodyText": "This will only be used by test code ? If so, lower accessibility to private and set VisibleForTesting.", "author": "snvijaya", "createdAt": "2020-12-15T06:51:37Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1202,6 +1287,10 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  public void setListenerOperation(String operation) {", "originalCommit": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM4MzE1NQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r548383155", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2020-12-24T05:02:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA4OTQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5ODg5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r543098890", "bodyText": "Same as before. Methods used by test need to be private and annotated with VisibleForTesting", "author": "snvijaya", "createdAt": "2020-12-15T07:11:52Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -107,6 +122,15 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);\n+  }\n+\n+  public void registerListener(Listener listener1) {", "originalCommit": "a5b64ed6af46fca88dbe7eca9c60b318cf8728ba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM4NDUxOA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r548384518", "bodyText": "Cannot make it private since the test classes using it are in a different package. Have moved to the end of file where other VisibleForTesting methods are written (which are also public due to the same reason).", "author": "sumangala-patki", "createdAt": "2020-12-24T05:09:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzA5ODg5MA=="}], "type": "inlineReview"}, {"oid": "6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "url": "https://github.com/apache/hadoop/commit/6a5b51289e90fb5d4f590dc7c1812a22a28a7c24", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2020-12-24T04:50:05Z", "type": "commit"}, {"oid": "d2bf54c1423aea99a21fa8194debea985f6ced01", "url": "https://github.com/apache/hadoop/commit/d2bf54c1423aea99a21fa8194debea985f6ced01", "message": "addressing pr comments", "committedDate": "2020-12-24T04:59:00Z", "type": "commit"}, {"oid": "84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "url": "https://github.com/apache/hadoop/commit/84c620ccf03c0fc37316ac07bc44e60857b1c4d8", "message": "merge conflicts", "committedDate": "2021-02-23T07:09:50Z", "type": "commit"}, {"oid": "98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "url": "https://github.com/apache/hadoop/commit/98e0fb0b0324cd7240cd17e956cdf1919495b8c2", "message": "new file conflicts", "committedDate": "2021-02-23T07:17:35Z", "type": "commit"}, {"oid": "9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "url": "https://github.com/apache/hadoop/commit/9f37b0f2a4cd7429e6ec55d6fac8d7a85e7b0be1", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-02T09:29:59Z", "type": "commit"}, {"oid": "c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "url": "https://github.com/apache/hadoop/commit/c8c1a04eff0f9bb1448b5f142b8b58d7fbd3472e", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-03T04:41:03Z", "type": "commit"}, {"oid": "ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "url": "https://github.com/apache/hadoop/commit/ec869fe0358b07d4548d4e6318c9d0f7d7f1cbd0", "message": "imports", "committedDate": "2021-03-03T05:46:03Z", "type": "commit"}, {"oid": "fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "url": "https://github.com/apache/hadoop/commit/fe3f041b1b39725669fb3301745d3b9cbd1a33aa", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-09T06:14:28Z", "type": "commit"}, {"oid": "22e9a40330fae359dc839b8062efe8f0e841a548", "url": "https://github.com/apache/hadoop/commit/22e9a40330fae359dc839b8062efe8f0e841a548", "message": "import", "committedDate": "2021-03-10T10:24:01Z", "type": "commit"}, {"oid": "29f59f84ee6db71ed342a800282c53035c38b584", "url": "https://github.com/apache/hadoop/commit/29f59f84ee6db71ed342a800282c53035c38b584", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-10T10:25:01Z", "type": "commit"}, {"oid": "a42e5b19a39e6858c790ced0a39edf43a2a262da", "url": "https://github.com/apache/hadoop/commit/a42e5b19a39e6858c790ced0a39edf43a2a262da", "message": "use write code for out", "committedDate": "2021-03-10T10:57:40Z", "type": "commit"}, {"oid": "51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "url": "https://github.com/apache/hadoop/commit/51e1a8ff7bda74d05ba2ccd8e6c497acbfac4ad8", "message": "handle invocation ex + write tests", "committedDate": "2021-03-11T04:34:25Z", "type": "commit"}, {"oid": "4ed465f8ce965343be63194a5034070ef932dfd2", "url": "https://github.com/apache/hadoop/commit/4ed465f8ce965343be63194a5034070ef932dfd2", "message": "checkstyle", "committedDate": "2021-03-11T04:45:47Z", "type": "commit"}, {"oid": "0e9770052dd12cd551c581e1b382b28ec1712147", "url": "https://github.com/apache/hadoop/commit/0e9770052dd12cd551c581e1b382b28ec1712147", "message": "minor chkstyle", "committedDate": "2021-03-11T06:33:05Z", "type": "commit"}, {"oid": "1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "url": "https://github.com/apache/hadoop/commit/1e529f4d5b3d70eae74fbeceb7365925f9c17b68", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-03-15T06:27:54Z", "type": "commit"}, {"oid": "1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "url": "https://github.com/apache/hadoop/commit/1b8f58954d3a1d6c9e89e3d86a0f2eb148a09b9a", "message": "fix merge conflict", "committedDate": "2021-04-11T04:21:35Z", "type": "commit"}, {"oid": "302fc06772362b0d108f3aab42cc7b353d7a7f58", "url": "https://github.com/apache/hadoop/commit/302fc06772362b0d108f3aab42cc7b353d7a7f58", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-15T12:11:31Z", "type": "commit"}, {"oid": "3447977af7444bdd033ba091124bc70b130954e3", "url": "https://github.com/apache/hadoop/commit/3447977af7444bdd033ba091124bc70b130954e3", "message": "lease rm acquire op", "committedDate": "2021-04-15T18:06:00Z", "type": "commit"}, {"oid": "979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "url": "https://github.com/apache/hadoop/commit/979f2b5dd6d23545b04ea00e62b6eaf8f0b93e4b", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-26T06:18:01Z", "type": "commit"}, {"oid": "c9217d9492486f401cbcb9320e8d473f98595c1b", "url": "https://github.com/apache/hadoop/commit/c9217d9492486f401cbcb9320e8d473f98595c1b", "message": "part of merge fix", "committedDate": "2021-04-26T07:01:52Z", "type": "commit"}, {"oid": "fd631219582b5bc6db47b69a4aa496682ac335a4", "url": "https://github.com/apache/hadoop/commit/fd631219582b5bc6db47b69a4aa496682ac335a4", "message": "add active lease fn tests", "committedDate": "2021-04-26T10:05:03Z", "type": "commit"}, {"oid": "78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "url": "https://github.com/apache/hadoop/commit/78a15dc3d6d91a1adc11f6fd46c5aecc839b4105", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-04-26T11:29:10Z", "type": "commit"}, {"oid": "d52ee795da47e98178b2498357fb9f55a4e78ef1", "url": "https://github.com/apache/hadoop/commit/d52ee795da47e98178b2498357fb9f55a4e78ef1", "message": "javadoc", "committedDate": "2021-04-26T11:32:07Z", "type": "commit"}, {"oid": "6966924728e4234af175f839b27535166ab1b6d0", "url": "https://github.com/apache/hadoop/commit/6966924728e4234af175f839b27535166ab1b6d0", "message": "Merge branch 'trunk' into HADOOP-17290", "committedDate": "2021-05-30T09:06:51Z", "type": "commit"}, {"oid": "d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "url": "https://github.com/apache/hadoop/commit/d15d2cf2c07691ebbdbd4450ebe65b2e146fb8ef", "message": "merge", "committedDate": "2021-05-30T09:36:46Z", "type": "commit"}, {"oid": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "url": "https://github.com/apache/hadoop/commit/d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "message": "typo", "committedDate": "2021-05-30T09:40:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NjEwNjI5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r646106290", "bodyText": "Can you give me the example of possible value ?\nWhat if in same jvm two FS objects are there ?", "author": "surendralilhore", "createdAt": "2021-06-06T09:51:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";", "originalCommit": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MzQ2NjMyNg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r653466326", "bodyText": "The client correlation ID can be any client-provided string within the specified length/character constraints (eg, a GUID like \"faaffc18-5a5d-426d-a259-c1a25c442898\", or a simple string such as \"correlation-id\"). If two or more filesystem instances are created using the same Configuration, or using configurations with the same value of fs.azure.client.correlationid, the identifier will be common for those objects.\nThis allows us to correlate requests across different FS and stream instances (for debugging/analysis purposes) as it is passed over a configuration. To filter among filesystem objects, we can use the unique filesystemId (GUID, generated per AzureBlobFileSystem instance)", "author": "sumangala-patki", "createdAt": "2021-06-17T11:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NjEwNjI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MDc4NzA4NQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r650787085", "bodyText": "Why this applicable for only stream id, not for filesystem id ? ?", "author": "surendralilhore", "createdAt": "2021-06-14T09:26:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -142,6 +156,10 @@ public String getPath() {\n     return path;\n   }\n \n+  private String getInputStreamID() {\n+    return StringUtils.right(UUID.randomUUID().toString(), STREAM_ID_LEN);", "originalCommit": "d0b33c151b9331b9f542aa2a94d48b1a25e14d8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MzQ3MDUyNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r653470525", "bodyText": "This was done to minimize length of header wherever possible. FilesystemId is applicable to all requests, and hence needs to be a complete GUID to ensure uniqueness. StreamId, on the other hand, appears only for a fraction of the API called (read/write), and we may not require a full GUID for it to remain unique across different streams", "author": "sumangala-patki", "createdAt": "2021-06-17T11:20:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1MDc4NzA4NQ=="}], "type": "inlineReview"}, {"oid": "6250d04f25d1efed187a0835e70f53415f0e1378", "url": "https://github.com/apache/hadoop/commit/6250d04f25d1efed187a0835e70f53415f0e1378", "message": "fix merge conflict", "committedDate": "2021-06-23T09:14:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3MTA5Mw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657271093", "bodyText": "clientCorrelationId ?   To be similar as 'userAgentId' etc?  And the getter also", "author": "anoopsjohn", "createdAt": "2021-06-23T16:25:18Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -264,6 +266,10 @@\n       DefaultValue = DEFAULT_VALUE_UNKNOWN)\n   private String clusterType;\n \n+  @StringConfigurationValidatorAnnotation(ConfigurationKey = FS_AZURE_CLIENT_CORRELATIONID,\n+          DefaultValue = EMPTY_STRING)\n+  private String clientCorrelationID;", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIwOTIzOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658209239", "bodyText": "renamed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:03:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3MTA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3Njk1OQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657276959", "bodyText": "All places ID to Id?", "author": "anoopsjohn", "createdAt": "2021-06-23T16:32:39Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -111,10 +116,14 @@\n   private Path workingDir;\n   private AzureBlobFileSystemStore abfsStore;\n   private boolean isClosed;\n+  private final String fileSystemID = UUID.randomUUID().toString();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIwOTY1MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658209651", "bodyText": "yes, done", "author": "sumangala-patki", "createdAt": "2021-06-24T19:03:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzI3Njk1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzMzOTgzNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657339834", "bodyText": "Here is a call to getFileStatus(Path) but as part of LISTSTATUS op.  So we should the context created above , during getFileStatus also right?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:00:09Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMDU1Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658210556", "bodyText": "modified getFileStatus to take tracingContext arg", "author": "sumangala-patki", "createdAt": "2021-06-24T19:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzMzOTgzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0MTU3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657341578", "bodyText": "Even this call to FileSystem#listStatusIterator() will create ADL gen2  calls right?  So should there be way for having a single context(above created) to be used for call from there too?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:02:55Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1049,8 +1130,11 @@ public boolean exists(Path f) throws IOException {\n       throws IOException {\n     LOG.debug(\"AzureBlobFileSystem.listStatusIterator path : {}\", path);\n     if (abfsStore.getAbfsConfiguration().enableAbfsListIterator()) {\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.LISTSTATUS, true,\n+          tracingContextFormat, listener);\n       AbfsListStatusRemoteIterator abfsLsItr =\n-          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore);\n+          new AbfsListStatusRemoteIterator(getFileStatus(path), abfsStore, tracingContext);\n       return RemoteIterators.typeCastingRemoteIterator(abfsLsItr);\n     } else {\n       return super.listStatusIterator(path);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMTM0MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658211341", "bodyText": "the else block eventually calls ABFS liststatus method, which generates its own context. It is equivalent to generating and passing object from here as there is no action (request) before/after the super method call", "author": "sumangala-patki", "createdAt": "2021-06-24T19:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0MTU3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NTAyNw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657345027", "bodyText": "Within tryGetFileStatus() there is call to getFileStatus.  We should be using this context created here.\ntryGetFileStatus() been called by createNonRecursive API also.\nHave to handle these.", "author": "anoopsjohn", "createdAt": "2021-06-23T18:07:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -335,7 +361,10 @@ public boolean rename(final Path src, final Path dst) throws IOException {\n     }\n \n     // Non-HNS account need to check dst status on driver side.\n-    if (!abfsStore.getIsNamespaceEnabled() && dstFileStatus == null) {\n+    TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+        fileSystemID, HdfsOperationConstants.RENAME, true, tracingContextFormat,\n+        listener);\n+    if (!abfsStore.getIsNamespaceEnabled(tracingContext) && dstFileStatus == null) {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMTcxNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658211715", "bodyText": "refactored the getFileStatus methods in AzureBlobFileSystem class to accommodate these. Changes done:\n\noverload getFileStatus to use context passed by other methods\ntryGetFileStatus to take context as arg\npvt method createFileSystem signature change to avoid overloading tryGetFileStatus", "author": "sumangala-patki", "createdAt": "2021-06-24T19:07:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NTAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NjMzMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657346332", "bodyText": "GET_FILESTATUS op?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:08:22Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1071,7 +1155,10 @@ private boolean fileSystemExists() throws IOException {\n     LOG.debug(\n             \"AzureBlobFileSystem.fileSystemExists uri: {}\", uri);\n     try {\n-      abfsStore.getFilesystemProperties();\n+      TracingContext tracingContext = new TracingContext(clientCorrelationID,\n+          fileSystemID, HdfsOperationConstants.GET_FILESTATUS,", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjMyNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212324", "bodyText": "This is a private method that does not implement any Hadoop function, and is never used in the Driver. However, we need to pass in a dummy tracing object for syntax. Modified to use test operation type", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NjMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NzQ5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657347490", "bodyText": "What is this operation been set on Listener?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:09:26Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java", "diffHunk": "@@ -1283,6 +1373,11 @@ public String getCanonicalServiceName() {\n     return this.statistics;\n   }\n \n+  @VisibleForTesting\n+  void setListenerOperation(String operation) {\n+    listener.setOperation(operation);", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjUxOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212519", "bodyText": "this is to reset and verify the 2-letter operation type (along with other IDs) when header tests are triggered using callback after header construction", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM0NzQ5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1NTcwMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657355703", "bodyText": "This is the tracing header format right?  Will that be a better name?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:16:59Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -109,6 +109,12 @@\n    *  Default value of this config is true. **/\n   public static final String FS_AZURE_DISABLE_OUTPUTSTREAM_FLUSH = \"fs.azure.disable.outputstream.flush\";\n   public static final String FS_AZURE_USER_AGENT_PREFIX_KEY = \"fs.azure.user.agent.prefix\";\n+  /**\n+   * The client correlation ID provided over config that will be added to\n+   * x-ms-client-request-Id header. Defaults to empty string if the length and\n+   * character constraints are not satisfied. **/\n+  public static final String FS_AZURE_CLIENT_CORRELATIONID = \"fs.azure.client.correlationid\";\n+  public static final String FS_AZURE_TRACINGCONTEXT_FORMAT = \"fs.azure.tracingcontext.format\";", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjU1Mw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212553", "bodyText": "Yes, renamed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1NTcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657359784", "bodyText": "Can be Enum?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:21:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/HdfsOperationConstants.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.constants;\n+\n+public final class HdfsOperationConstants {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzY0MDA3MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657640070", "bodyText": "Can we avoid the hdfs in this ?  Its after all Hadoop common FS API types", "author": "anoopsjohn", "createdAt": "2021-06-24T05:38:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjY5Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212696", "bodyText": "switched to enum; renamed class to FSOperationType", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM1OTc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2NDE1MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657364150", "bodyText": "Only used for tests?  Should be returning List only?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:28:28Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -166,6 +166,11 @@ public String getResponseHeader(String httpHeader) {\n     return connection.getHeaderField(httpHeader);\n   }\n \n+  @VisibleForTesting\n+  public String getRequestHeader(String httpHeader) {\n+    return connection.getRequestProperties().get(httpHeader).toString();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjc3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212778", "bodyText": "method not required, removed", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2NDE1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2ODM3OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657368378", "bodyText": "Can avoid this way of explicit call which for sure should get executed before setting of this header.\nCan we have a better method name than toString() for generation of required header value?  This has to consider the format and generate.", "author": "anoopsjohn", "createdAt": "2021-06-23T18:34:45Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestID();", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMjk1Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658212957", "bodyText": "Generating the clientRequestId in the toString() method of tracingContext is not a good option as it will re-generate it every time the method is called. Also, we need tracingContext to store the id for consistency\nHave renamed toString to constructHeader. Format is passed over config and already available to tracingContext object", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM2ODM3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM3NTUyMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r657375523", "bodyText": "Why passing 'tracingContext' when its set as instance member?", "author": "anoopsjohn", "createdAt": "2021-06-23T18:45:50Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsInputStream.java", "diffHunk": "@@ -451,15 +472,15 @@ private int readInternal(final long position, final byte[] b, final int offset,\n       }\n \n       // got nothing from read-ahead, do our own read now\n-      receivedBytes = readRemote(position, b, offset, length);\n+      receivedBytes = readRemote(position, b, offset, length, new TracingContext(tracingContext));\n       return receivedBytes;\n     } else {\n       LOG.debug(\"read ahead disabled, reading remote\");\n-      return readRemote(position, b, offset, length);\n+      return readRemote(position, b, offset, length, new TracingContext(tracingContext));\n     }\n   }\n \n-  int readRemote(long position, byte[] b, int offset, int length) throws IOException {\n+  int readRemote(long position, byte[] b, int offset, int length, TracingContext tracingContext) throws IOException {", "originalCommit": "6250d04f25d1efed187a0835e70f53415f0e1378", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODIxMzA3NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658213074", "bodyText": "The readRemote function is also used by the readahead worker threads (in ReadBufferWorker). The tracingContext passed from the readahead buffers may correspond to independent read requests", "author": "sumangala-patki", "createdAt": "2021-06-24T19:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1NzM3NTUyMw=="}], "type": "inlineReview"}, {"oid": "1567e3e82240a242a6f11970d6d313db575a3787", "url": "https://github.com/apache/hadoop/commit/1567e3e82240a242a6f11970d6d313db575a3787", "message": "address review comments", "committedDate": "2021-06-24T19:01:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxMzYxMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658613610", "bodyText": "This clone of Context needed here?  What gets changed?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:07:10Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsLease.java", "diffHunk": "@@ -114,13 +119,15 @@ public AbfsLease(AbfsClient client, String path, int acquireMaxRetries,\n     LOG.debug(\"Acquired lease {} on {}\", leaseID, path);\n   }\n \n-  private void acquireLease(RetryPolicy retryPolicy, int numRetries, int retryInterval, long delay)\n+  private void acquireLease(RetryPolicy retryPolicy, int numRetries,\n+      int retryInterval, long delay)\n       throws LeaseException {\n     LOG.debug(\"Attempting to acquire lease on {}, retry {}\", path, numRetries);\n     if (future != null && !future.isDone()) {\n       throw new LeaseException(ERR_LEASE_FUTURE_EXISTS);\n     }\n-    future = client.schedule(() -> client.acquireLease(path, INFINITE_LEASE_DURATION),\n+    future = client.schedule(() -> client.acquireLease(path,\n+        INFINITE_LEASE_DURATION, new TracingContext(tracingContext)),", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4MzkwOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660083909", "bodyText": "not needed; moved to calling method (constructor) where it is cloned only once, instead of per retry", "author": "sumangala-patki", "createdAt": "2021-06-28T20:09:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxMzYxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615501", "bodyText": "getOutputStreamId() and getStreamID() -> Both create some confusion.  Normally the Getter just return a already available value.  getStreamID() make sense.\nYou can use createOutputStreamId() instead?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:10:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -160,6 +170,14 @@ public AbfsOutputStream(\n     if (outputStreamStatistics != null) {\n       this.ioStatistics = outputStreamStatistics.getIOStatistics();\n     }\n+    this.outputStreamId = getOutputStreamId();\n+    this.tracingContext = new TracingContext(tracingContext);\n+    this.tracingContext.setStreamID(outputStreamId);\n+    this.tracingContext.setOperation(FSOperationType.WRITE);\n+  }\n+\n+  private String getOutputStreamId() {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTY5NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658615694", "bodyText": "The same thing in ABFSInputStream also", "author": "anoopsjohn", "createdAt": "2021-06-25T09:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4Mzk3NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660083974", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:09:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNTUwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjMzNw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616337", "bodyText": "Ok the context been passed here might get changed at least wrt the retryCount.  that is why been cloned here?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:11:22Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -385,7 +412,9 @@ private void writeAppendBlobCurrentBufferToService() throws IOException {\n             \"writeCurrentBufferToService\", \"append\")) {\n       AppendRequestParameters reqParams = new AppendRequestParameters(offset, 0,\n           bytesLength, APPEND_MODE, true, leaseId);\n-      AbfsRestOperation op = client.append(path, bytes, reqParams, cachedSasToken.get());\n+      AbfsRestOperation op = client\n+          .append(path, bytes, reqParams, cachedSasToken.get(),\n+              new TracingContext(tracingContext));", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDAwNA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084004", "bodyText": "yes", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxNjY5MA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658616690", "bodyText": "Yaaa here.", "author": "anoopsjohn", "createdAt": "2021-06-25T09:11:54Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -202,9 +206,10 @@ private void completeExecute() throws AzureBlobFileSystemException {\n \n     retryCount = 0;\n     LOG.debug(\"First execution of REST operation - {}\", operationType);\n-    while (!executeHttpOperation(retryCount)) {\n+    while (!executeHttpOperation(retryCount, tracingContext)) {\n       try {\n         ++retryCount;\n+        tracingContext.setRetryCount(retryCount);", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODIzMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618232", "bodyText": "generateClientRequestId() can be done internally within constructHeader()?  Else its upto callee to set it proper before.  Actually the UUID clientReqId generation should be an internal responsibility of this tracingContext right?\nAll other methods are like setter.", "author": "anoopsjohn", "createdAt": "2021-06-25T09:14:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDIzMw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084233", "bodyText": "constructHeader() may be called simply to display IDs, in which case it should not regenerate new clientReqId each time\ncallee setting it would mean adding code to all methods to call this, and retry case would not be handled\nCan be done internally within TC: have a generateClientRequestId function to create guid, and call it in constructor as well as the setRetryCount which is invoked per retry. However, had avoided this since we are populating ID variables only as and when they pass through the corresponding ABFS layers.", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODIzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODg0Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658618847", "bodyText": "Its actually a fresh AbfsHttpOperation operation and here by we set the header.  Can we call it setXXX  than updateXXX?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:15:17Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -221,17 +226,27 @@ private void completeExecute() throws AzureBlobFileSystemException {\n     LOG.trace(\"{} REST operation complete\", operationType);\n   }\n \n+  private void updateClientRequestHeader(AbfsHttpOperation httpOperation,\n+      TracingContext tracingContext) {\n+    tracingContext.generateClientRequestId();\n+    httpOperation.getConnection()\n+        .setRequestProperty(HttpHeaderConfigurations.X_MS_CLIENT_REQUEST_ID,\n+            tracingContext.constructHeader());\n+  }\n+\n   /**\n    * Executes a single HTTP operation to complete the REST operation.  If it\n    * fails, there may be a retry.  The retryCount is incremented with each\n    * attempt.\n    */\n-  private boolean executeHttpOperation(final int retryCount) throws AzureBlobFileSystemException {\n+  private boolean executeHttpOperation(final int retryCount,\n+    TracingContext tracingContext) throws AzureBlobFileSystemException {\n     AbfsHttpOperation httpOperation = null;\n     try {\n       // initialize the HTTP request and open the connection\n       httpOperation = new AbfsHttpOperation(url, method, requestHeaders);\n       incrementCounter(AbfsStatistic.CONNECTIONS_MADE, 1);\n+      updateClientRequestHeader(httpOperation, tracingContext);", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDMwNQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084305", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYxODg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYyMDA0MQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658620041", "bodyText": "This is for testability only?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:17:10Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/Listener.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+\n+/**\n+ * Interface for testing identifiers tracked via TracingContext\n+ * Implemented in TracingHeaderValidator\n+ */\n+\n+public interface Listener {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDQxNg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084416", "bodyText": "yes, it's an interface to trigger header tests through callback when header is constructed. The tests are run only when listener is registered (not null), which is done across existing tests for different methods", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYyMDA0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMDYwMg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658630602", "bodyText": "primaryRequestID will be used in case of Read ahead?  Worth adding come comments about its use here", "author": "anoopsjohn", "createdAt": "2021-06-25T09:34:03Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDUzMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084530", "bodyText": "primaryRequestId is applicable for any method call that triggers more than one http request. For example, methods using continuation logic like listStatus and rename\nHave added comments explaining use of tracingContext and its members at the beginning of file", "author": "sumangala-patki", "createdAt": "2021-06-28T20:10:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMDYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMTEzMQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658631131", "bodyText": "call it opType only?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:34:52Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDU4NA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084584", "bodyText": "done", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMTEzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMzQ1Ng==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658633456", "bodyText": "A regex matching call is not that cheap. We will end up calling this for every object creation of this TracingContext.  Can we limit this check only at the time of FS instantiation?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:38:32Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDY0OA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084648", "bodyText": "true. Changed the validate method to be static, calling from ABFS constructor only now", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODYzMzQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NDAzMA==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658644030", "bodyText": "In any kind of op clientCorrelationID , clientRequestId , fileSystemID , hadoopOpName and retryCount will be present\nOptional things are primaryRequestID and streamID .  Correct?\nWorth detailing in some comments here.\nWhen these 2 are not there it will come like ::::...  ?", "author": "anoopsjohn", "createdAt": "2021-06-25T09:55:21Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingContext.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+import java.util.UUID;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.hadoop.fs.azurebfs.constants.FSOperationType;\n+import org.apache.hadoop.fs.azurebfs.services.AbfsClient;\n+\n+import static org.apache.hadoop.fs.azurebfs.constants.AbfsHttpConstants.EMPTY_STRING;\n+\n+/**\n+ * The TracingContext class to correlate Store requests using unique\n+ * identifiers and resources common to requests (e.g. filesystem, stream)\n+ *\n+ * Implementing new HDFS method:\n+ * Create TracingContext instance in method of outer layer of\n+ * ABFS driver (AzureBlobFileSystem/AbfsInputStream/AbfsOutputStream), to be\n+ * passed through ABFS layers up to AbfsRestOperation.\n+ *\n+ * Add new operations to HdfsOperationConstants file.\n+ *\n+ * PrimaryRequestId can be enabled for individual HDFS API that invoke\n+ * multiple Store calls.\n+ *\n+ * Testing:\n+ * Pass an instance of TracingHeaderValidator to registerListener() of ABFS\n+ * filesystem/stream class before calling the API in tests.\n+ */\n+\n+public class TracingContext {\n+  private final String clientCorrelationID;\n+  private final String fileSystemID;\n+  private String clientRequestId = EMPTY_STRING;\n+  private String primaryRequestID;\n+  private String streamID;\n+  private int retryCount;\n+  private FSOperationType hadoopOpName;\n+  private final TracingHeaderFormat format;\n+  private Listener listener = null;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(AbfsClient.class);\n+  public static final int MAX_CLIENT_CORRELATION_ID_LENGTH = 72;\n+  public static final String CLIENT_CORRELATION_ID_PATTERN = \"[a-zA-Z0-9-]*\";\n+\n+  /**\n+   * Initialize TracingContext\n+   * @param clientCorrelationID Provided over config by client\n+   * @param fileSystemID Unique guid for AzureBlobFileSystem instance\n+   * @param hadoopOpName Code indicating the high-level Hadoop operation that\n+   *                    triggered the current Store request\n+   * @param tracingHeaderFormat Format of IDs to be printed in header and logs\n+   * @param listener Holds instance of TracingHeaderValidator during testing,\n+   *                null otherwise\n+   */\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, TracingHeaderFormat tracingHeaderFormat,\n+      Listener listener) {\n+    this.fileSystemID = fileSystemID;\n+    this.hadoopOpName = hadoopOpName;\n+    this.clientCorrelationID = validateClientCorrelationID(clientCorrelationID);\n+    streamID = EMPTY_STRING;\n+    retryCount = 0;\n+    primaryRequestID = EMPTY_STRING;\n+    format = tracingHeaderFormat;\n+    this.listener = listener;\n+  }\n+\n+  public TracingContext(String clientCorrelationID, String fileSystemID,\n+      FSOperationType hadoopOpName, boolean needsPrimaryReqId,\n+      TracingHeaderFormat tracingHeaderFormat, Listener listener) {\n+    this(clientCorrelationID, fileSystemID, hadoopOpName, tracingHeaderFormat,\n+        listener);\n+    primaryRequestID = needsPrimaryReqId ? UUID.randomUUID().toString() : \"\";\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public TracingContext(TracingContext originalTracingContext) {\n+    this.fileSystemID = originalTracingContext.fileSystemID;\n+    this.streamID = originalTracingContext.streamID;\n+    this.clientCorrelationID = originalTracingContext.clientCorrelationID;\n+    this.hadoopOpName = originalTracingContext.hadoopOpName;\n+    this.retryCount = 0;\n+    this.primaryRequestID = originalTracingContext.primaryRequestID;\n+    this.format = originalTracingContext.format;\n+    if (originalTracingContext.listener != null) {\n+      this.listener = originalTracingContext.listener.getClone();\n+    }\n+  }\n+\n+  public String validateClientCorrelationID(String clientCorrelationID) {\n+    if ((clientCorrelationID.length() > MAX_CLIENT_CORRELATION_ID_LENGTH)\n+        || (!clientCorrelationID.matches(CLIENT_CORRELATION_ID_PATTERN))) {\n+      LOG.debug(\n+          \"Invalid config provided; correlation id not included in header.\");\n+      return EMPTY_STRING;\n+    }\n+    return clientCorrelationID;\n+  }\n+\n+  public void generateClientRequestId() {\n+    clientRequestId = UUID.randomUUID().toString();\n+  }\n+\n+  public void setPrimaryRequestID() {\n+    primaryRequestID = UUID.randomUUID().toString();\n+    if (listener != null) {\n+      listener.updatePrimaryRequestID(primaryRequestID);\n+    }\n+  }\n+\n+  public void setStreamID(String stream) {\n+    streamID = stream;\n+  }\n+\n+  public void setOperation(FSOperationType operation) {\n+    this.hadoopOpName = operation;\n+  }\n+\n+  public void setRetryCount(int retryCount) {\n+    this.retryCount = retryCount;\n+  }\n+\n+  public void setListener(Listener listener) {\n+    this.listener = listener;\n+  }\n+\n+  public String constructHeader() {\n+    String header;\n+    switch (format) {\n+    case ALL_ID_FORMAT:\n+      header =", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDc0Mg==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084742", "bodyText": "Added description to member declaration and added note in the fn\nYes the number of separators (:) is kept constant for parsing", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NDAzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NTEyOQ==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r658645129", "bodyText": "Will the below way will be more consistent?\nSINGLE_ID_FORMAT,  // client-req-id\nTWO_ID_FORMAT; // client-req-id:client-correlation-id\nALL_ID_FORMAT,  // client-req-id:client-correlation-id:filesystem-id:primary-req-id:stream-id:hdfs-operation:retry-count", "author": "anoopsjohn", "createdAt": "2021-06-25T09:57:09Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/utils/TracingHeaderFormat.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.azurebfs.utils;\n+\n+public enum TracingHeaderFormat {", "originalCommit": "1567e3e82240a242a6f11970d6d313db575a3787", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2MDA4NDc4Nw==", "url": "https://github.com/apache/hadoop/pull/2520#discussion_r660084787", "bodyText": "switched enum, but keeping correlationId at the start would be easier to distinguish it as a custom id of variable length", "author": "sumangala-patki", "createdAt": "2021-06-28T20:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY1ODY0NTEyOQ=="}], "type": "inlineReview"}, {"oid": "ecee1809de89ed5b4fc76df79dd64895537aab99", "url": "https://github.com/apache/hadoop/commit/ecee1809de89ed5b4fc76df79dd64895537aab99", "message": "revw comments", "committedDate": "2021-06-28T20:02:10Z", "type": "commit"}, {"oid": "b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "url": "https://github.com/apache/hadoop/commit/b69d9e209e0f6646cd1f1ff6a1a84ac4e7f3bfc8", "message": "correction", "committedDate": "2021-06-30T09:18:17Z", "type": "commit"}, {"oid": "8cf0fe32c89351a55c475cdd91261335591129f8", "url": "https://github.com/apache/hadoop/commit/8cf0fe32c89351a55c475cdd91261335591129f8", "message": "checkstyle", "committedDate": "2021-07-01T09:00:24Z", "type": "commit"}, {"oid": "0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "url": "https://github.com/apache/hadoop/commit/0dc83f4367c6d674a7d8f0bfba0498c26c4551e1", "message": "set header in tc", "committedDate": "2021-07-02T12:03:53Z", "type": "commit"}]}