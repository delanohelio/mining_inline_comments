{"pr_number": 2530, "pr_title": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark", "pr_createdAt": "2020-12-07T20:26:02Z", "pr_url": "https://github.com/apache/hadoop/pull/2530", "timeline": [{"oid": "b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "url": "https://github.com/apache/hadoop/commit/b31a847325a51f5a6bfc878bce4b0f7f70b0e6ff", "message": "HADOOP-17417 ongoing dev\n\nChange-Id: I06fe9ad3ff4e90bf152251e2111946a3ab14e4e8", "committedDate": "2020-12-10T09:21:06Z", "type": "forcePushed"}, {"oid": "d2fd7e3b06bd83712b22fe89b459ffe323d7b85e", "url": "https://github.com/apache/hadoop/commit/d2fd7e3b06bd83712b22fe89b459ffe323d7b85e", "message": "HADOOP-17414. Checkstyle\n\nChange-Id: I60276cf0f7e0e95583d877ecd2e77ff338b9f207", "committedDate": "2020-12-10T20:57:12Z", "type": "forcePushed"}, {"oid": "3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "url": "https://github.com/apache/hadoop/commit/3cf9d2be27bd6a65f14e71ebf433f8a1d6c37423", "message": "HADOOP-17414. Checkstyle and whitespace\n\nChange-Id: Ibceaae5c9cabe15957a8aa50ba429e82854acd01", "committedDate": "2021-01-04T13:45:25Z", "type": "forcePushed"}, {"oid": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "url": "https://github.com/apache/hadoop/commit/732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nThis is a PoC which, having implemented, I don't think is viable.\n\nYes, we can fix up getFileStatus so it reads the header. It even knows\nto always bypass S3Guard (no inconsistencies to worry about any more).\n\nBut: the blast radius of the change is too big. I'm worried about\ndistcp or any other code which goes\nlen =getFileStatus(path).getLen()\nopen(path).readFully(0, len, dest)\n\nYou'll get an EOF here. Find the file through a listing and you'll be OK\nprovided S3Guard isn't updated with that GetFileStatus result, which I\nhave seen.\n\nThe ordering of probes in ITestMagicCommitProtocol.validateTaskAttemptPathAfterWrite\nneed to be list before getFileStatus, so the S3Guard table is updated from\nthe list.\n\noverall: danger. Even without S3Guard there's risk.\n\nAnyway, shown it can be done. And I think there's a merit in a leaner patch\nwhich attaches the marker but doesn't do any fixup. This would let us add\nan API call \"getObjectHeaders(path) -> Future<Map<String, String>> and\nthen use that to do the lookup. We can implement the probe for\nABFS and S3, add a hasPathCapabilities for it as well as an interface\nthe FS can implement (which passthrough filesystems would need to do).\n\nChange-Id: If56213c0c5d8ab696d2d89b48ad52874960b0920", "committedDate": "2021-01-07T11:40:07Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r553870733", "bodyText": "with this change we will enable magic committer for all the components. Maybe point that out in the title of the issue.", "author": "bgaborg", "createdAt": "2021-01-08T10:43:17Z", "path": "hadoop-common-project/hadoop-common/src/main/resources/core-default.xml", "diffHunk": "@@ -1873,11 +1873,9 @@\n \n <property>\n   <name>fs.s3a.committer.magic.enabled</name>\n-  <value>false</value>\n+  <value>true</value>", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI3MDA0MQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554270041", "bodyText": "I also wonder if this (along with the documentation change) is required for this PR.", "author": "sunchao", "createdAt": "2021-01-09T01:26:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc3Nzk1Mg==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554777952", "bodyText": "I'm +1 on enabling this by default as it does not depend on S3Guard any more. I totally agree we should update the JIRA release notes or commit message to indicate this default config change. Or ideally a separate PR.", "author": "liuml07", "createdAt": "2021-01-11T06:14:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA3NTA5Mw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555075093", "bodyText": "I' will add to release notes. This does not enable the committer. All it does is say \"this store has the consistency needed for the committer\".\nNow, we could do a bit more in a separate patch as there's a bit more in settings I'd like to change (we ship in cloudera) where the fs.s3a.buffer.dir is set to something like ${env.LOCAL_DIR:${hadoop.tmp.dir}}/s3a so that incomplete buffered writes (staging committer and blocks for normal files/magic uploads) are always cleaned up. I could make the core-default settings for this committer different", "author": "steveloughran", "createdAt": "2021-01-11T14:15:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzgwMDkyMw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r563800923", "bodyText": "gabor, disabling option is now in HADOOP-17483; #2637 a PR to completely remove the switch entirely. Now we have to decide: do we want that, or would we ever want to turn the s3a client into a dumb client which doesn't do \"magic\" when start writing under certain paths?", "author": "steveloughran", "createdAt": "2021-01-25T15:15:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzg3MDczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTE3NQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269175", "bodyText": "nit: I wonder if some kind of caching will be useful here. We are calling getObjectMetadata for every getXAttr call.", "author": "sunchao", "createdAt": "2021-01-09T01:20:13Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA4MDc0Mg==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555080742", "bodyText": "I would be surprised if this is called any more frequently than getFileStatus; we don't catch that. Well, not now S3Guard is off. And given the inconsistencies it would give rise to: not something I want to replicate.\nWhat I will do is add IOStatistics collection of count & time of the xattr calls. That way we can get the statistics on how common this is.", "author": "steveloughran", "createdAt": "2021-01-11T14:23:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTE3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTIzNQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269235", "bodyText": "nit: to be consistent with the above, should we add comments for the following constants?", "author": "sunchao", "createdAt": "2021-01-09T01:20:47Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTA4MDc5Nw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555080797", "bodyText": "will do", "author": "steveloughran", "createdAt": "2021-01-11T14:24:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTUyMg==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554269522", "bodyText": "hmm why we need a return value if ret is already changed in place?", "author": "sunchao", "createdAt": "2021-01-09T01:22:57Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);\n+      }\n+    }\n+    // missing/empty header or parse failure.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Creates a copy of the passed {@link ObjectMetadata}.\n+   * Does so without using the {@link ObjectMetadata#clone()} method,\n+   * to avoid copying unnecessary headers.\n+   * This operation does not copy the {@code X_HEADER_MAGIC_MARKER}\n+   * header to avoid confusion. If a marker file is renamed,\n+   * it loses information about any remapped file.\n+   * @param source the {@link ObjectMetadata} to copy\n+   * @param ret the metadata to update; this is the return value.\n+   * @return a copy of {@link ObjectMetadata} with only relevant attributes", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI1Mzk2Nw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555253967", "bodyText": "was originally like that, I assume me or somebody else wanted it that way. Will have it return void.", "author": "steveloughran", "createdAt": "2021-01-11T18:29:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI2OTUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI3MDIyMw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554270223", "bodyText": "nit: is \"Created on demand\" accurate? it is created in initialize.", "author": "sunchao", "createdAt": "2021-01-09T01:26:58Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -330,6 +331,11 @@\n    */\n   private DirectoryPolicy directoryPolicy;\n \n+  /**\n+   * Header processing for XAttr. Created on demand.", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI1MjMzNw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555252337", "bodyText": "you are right. I originally had it on demand for XAttr calls, but once I pulled in to the copy operation just made it something created earlier.", "author": "steveloughran", "createdAt": "2021-01-11T18:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDI3MDIyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc3OTU3NQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554779575", "bodyText": "nit: is XA_HEADER_PREFIX a bit clearer name?", "author": "liuml07", "createdAt": "2021-01-11T06:16:34Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/Constants.java", "diffHunk": "@@ -1048,4 +1048,10 @@ private Constants() {\n   public static final String STORE_CAPABILITY_DIRECTORY_MARKER_ACTION_DELETE\n       = \"fs.s3a.capability.directory.marker.action.delete\";\n \n+  /**\n+   * To comply with the XAttr rules, all headers of the object retrieved\n+   * through the getXAttr APIs have the prefix: {@value}.\n+   */\n+  public static final String HEADER_PREFIX = \"header.\";", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTIwNDMwOA==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555204308", "bodyText": "ok", "author": "steveloughran", "createdAt": "2021-01-11T17:08:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc3OTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4NjkxMg==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554786912", "bodyText": "nit: seems better if we move those comments above into the implementation method HeaderProcessing#cloneObjectMetadata\n// This approach may be too brittle, especially if\n// in future there are new attributes added to ObjectMetadata\n// that we do not explicitly call to set here", "author": "liuml07", "createdAt": "2021-01-11T06:26:19Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -4103,56 +4111,7 @@ private ObjectMetadata cloneObjectMetadata(ObjectMetadata source) {\n     // in future there are new attributes added to ObjectMetadata\n     // that we do not explicitly call to set here\n     ObjectMetadata ret = newObjectMetadata(source.getContentLength());\n-\n-    // Possibly null attributes\n-    // Allowing nulls to pass breaks it during later use\n-    if (source.getCacheControl() != null) {\n-      ret.setCacheControl(source.getCacheControl());\n-    }\n-    if (source.getContentDisposition() != null) {\n-      ret.setContentDisposition(source.getContentDisposition());\n-    }\n-    if (source.getContentEncoding() != null) {\n-      ret.setContentEncoding(source.getContentEncoding());\n-    }\n-    if (source.getContentMD5() != null) {\n-      ret.setContentMD5(source.getContentMD5());\n-    }\n-    if (source.getContentType() != null) {\n-      ret.setContentType(source.getContentType());\n-    }\n-    if (source.getExpirationTime() != null) {\n-      ret.setExpirationTime(source.getExpirationTime());\n-    }\n-    if (source.getExpirationTimeRuleId() != null) {\n-      ret.setExpirationTimeRuleId(source.getExpirationTimeRuleId());\n-    }\n-    if (source.getHttpExpiresDate() != null) {\n-      ret.setHttpExpiresDate(source.getHttpExpiresDate());\n-    }\n-    if (source.getLastModified() != null) {\n-      ret.setLastModified(source.getLastModified());\n-    }\n-    if (source.getOngoingRestore() != null) {\n-      ret.setOngoingRestore(source.getOngoingRestore());\n-    }\n-    if (source.getRestoreExpirationTime() != null) {\n-      ret.setRestoreExpirationTime(source.getRestoreExpirationTime());\n-    }\n-    if (source.getSSEAlgorithm() != null) {\n-      ret.setSSEAlgorithm(source.getSSEAlgorithm());\n-    }\n-    if (source.getSSECustomerAlgorithm() != null) {\n-      ret.setSSECustomerAlgorithm(source.getSSECustomerAlgorithm());\n-    }\n-    if (source.getSSECustomerKeyMd5() != null) {\n-      ret.setSSECustomerKeyMd5(source.getSSECustomerKeyMd5());\n-    }\n-\n-    for (Map.Entry<String, String> e : source.getUserMetadata().entrySet()) {\n-      ret.addUserMetadata(e.getKey(), e.getValue());\n-    }\n-    return ret;\n+    return getHeaderProcessing().cloneObjectMetadata(source, ret);", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI0Mjg0OA==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555242848", "bodyText": "will do, especially as its a bit less brittle now", "author": "steveloughran", "createdAt": "2021-01-11T18:09:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4NjkxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4ODk0OA==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554788948", "bodyText": "nit: replace headerProcessing with private getHeaderProcessing() as the getXattrs method?", "author": "liuml07", "createdAt": "2021-01-11T06:28:43Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -4382,6 +4341,37 @@ public EtagChecksum getFileChecksum(Path f, final long length)\n     }\n   }\n \n+  /**\n+   * Get header processing support.\n+   * @return the header processing of this instance.\n+   */\n+  private HeaderProcessing getHeaderProcessing() {\n+    return headerProcessing;\n+  }\n+\n+  @Override\n+  public byte[] getXAttr(final Path path, final String name)\n+      throws IOException {\n+    return getHeaderProcessing().getXAttr(path, name);\n+  }\n+\n+  @Override\n+  public Map<String, byte[]> getXAttrs(final Path path) throws IOException {\n+    return getHeaderProcessing().getXAttrs(path);\n+  }\n+\n+  @Override\n+  public Map<String, byte[]> getXAttrs(final Path path,\n+      final List<String> names)\n+      throws IOException {\n+    return getHeaderProcessing().getXAttrs(path, names);\n+  }\n+\n+  @Override\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return headerProcessing.listXAttrs(path);", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI0Mjk0MA==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555242940", "bodyText": "ok", "author": "steveloughran", "createdAt": "2021-01-11T18:09:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc4ODk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDc5ODQ4OQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554798489", "bodyText": "will never return null, but could return empty byte array?", "author": "liuml07", "createdAt": "2021-01-11T06:41:13Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNjk5Ng==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554816996", "bodyText": "Is it useful to print ex? Or along with the line LOG.debug(\"Fail to parse {} to long with exception\", xAttr, ex)?", "author": "liuml07", "createdAt": "2021-01-11T07:04:45Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2Mjc3MDczNQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r562770735", "bodyText": "done, even though it's not as useful as knowing the actual string.", "author": "steveloughran", "createdAt": "2021-01-22T16:59:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNjk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNzg2Ng==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554817866", "bodyText": "nit: I know this is based on existing code...but Java 8 stream?", "author": "liuml07", "createdAt": "2021-01-11T07:05:52Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/HeaderProcessing.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3a.impl;\n+\n+import java.io.IOException;\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.TreeMap;\n+\n+import com.amazonaws.services.s3.Headers;\n+import com.amazonaws.services.s3.model.ObjectMetadata;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.s3a.Constants.HEADER_PREFIX;\n+import static org.apache.hadoop.fs.s3a.commit.CommitConstants.X_HEADER_MAGIC_MARKER;\n+\n+/**\n+ * Part of the S3A FS where object headers are\n+ * processed.\n+ * Implements all the various XAttr read operations.\n+ * Those APIs all expect byte arrays back.\n+ * Metadata cloning is also implemented here, so as\n+ * to stay in sync with custom header logic.\n+ */\n+public class HeaderProcessing extends AbstractStoreOperation {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(\n+      HeaderProcessing.class);\n+\n+  private static final byte[] EMPTY = new byte[0];\n+\n+  /**\n+   * Length XAttr.\n+   */\n+  public static final String XA_CONTENT_LENGTH =\n+      HEADER_PREFIX + Headers.CONTENT_LENGTH;\n+\n+  /**\n+   * last modified XAttr.\n+   */\n+  public static final String XA_LAST_MODIFIED =\n+      HEADER_PREFIX + Headers.LAST_MODIFIED;\n+\n+  public static final String XA_CONTENT_DISPOSITION =\n+      HEADER_PREFIX + Headers.CONTENT_DISPOSITION;\n+\n+  public static final String XA_CONTENT_ENCODING =\n+      HEADER_PREFIX + Headers.CONTENT_ENCODING;\n+\n+  public static final String XA_CONTENT_LANGUAGE =\n+      HEADER_PREFIX + Headers.CONTENT_LANGUAGE;\n+\n+  public static final String XA_CONTENT_MD5 =\n+      HEADER_PREFIX + Headers.CONTENT_MD5;\n+\n+  public static final String XA_CONTENT_RANGE =\n+      HEADER_PREFIX + Headers.CONTENT_RANGE;\n+\n+  public static final String XA_CONTENT_TYPE =\n+      HEADER_PREFIX + Headers.CONTENT_TYPE;\n+\n+  public static final String XA_ETAG = HEADER_PREFIX + Headers.ETAG;\n+\n+  public HeaderProcessing(final StoreContext storeContext) {\n+    super(storeContext);\n+  }\n+\n+  /**\n+   * Query the store, get all the headers into a map. Each Header\n+   * has the \"header.\" prefix.\n+   * Caller must have read access.\n+   * The value of each header is the string value of the object\n+   * UTF-8 encoded.\n+   * @param path path of object.\n+   * @return the headers\n+   * @throws IOException failure, including file not found.\n+   */\n+  private Map<String, byte[]> retrieveHeaders(Path path) throws IOException {\n+    StoreContext context = getStoreContext();\n+    ObjectMetadata md = context.getContextAccessors()\n+        .getObjectMetadata(path);\n+    Map<String, String> rawHeaders = md.getUserMetadata();\n+    Map<String, byte[]> headers = new TreeMap<>();\n+    rawHeaders.forEach((key, value) ->\n+        headers.put(HEADER_PREFIX + key, encodeBytes(value)));\n+    // and add the usual content length &c, if set\n+    headers.put(XA_CONTENT_DISPOSITION,\n+        encodeBytes(md.getContentDisposition()));\n+    headers.put(XA_CONTENT_ENCODING,\n+        encodeBytes(md.getContentEncoding()));\n+    headers.put(XA_CONTENT_LANGUAGE,\n+        encodeBytes(md.getContentLanguage()));\n+    headers.put(XA_CONTENT_LENGTH,\n+        encodeBytes(md.getContentLength()));\n+    headers.put(\n+        XA_CONTENT_MD5,\n+        encodeBytes(md.getContentMD5()));\n+    headers.put(XA_CONTENT_RANGE,\n+        encodeBytes(md.getContentRange()));\n+    headers.put(XA_CONTENT_TYPE,\n+        encodeBytes(md.getContentType()));\n+    headers.put(XA_ETAG,\n+        encodeBytes(md.getETag()));\n+    headers.put(XA_LAST_MODIFIED,\n+        encodeBytes(md.getLastModified()));\n+    return headers;\n+  }\n+\n+  /**\n+   * Stringify an object and return its bytes in UTF-8 encoding.\n+   * @param s source\n+   * @return encoded object or null\n+   */\n+  public static byte[] encodeBytes(Object s) {\n+    return s == null\n+        ? EMPTY\n+        : s.toString().getBytes(StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get the string value from the bytes.\n+   * if null : return null, otherwise the UTF-8 decoded\n+   * bytes.\n+   * @param bytes source bytes\n+   * @return decoded value\n+   */\n+  public static String decodeBytes(byte[] bytes) {\n+    return bytes == null\n+        ? null\n+        : new String(bytes, StandardCharsets.UTF_8);\n+  }\n+\n+  /**\n+   * Get an XAttr name and value for a file or directory.\n+   * @param path Path to get extended attribute\n+   * @param name XAttr name.\n+   * @return byte[] XAttr value or null\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public byte[] getXAttr(Path path, String name) throws IOException {\n+    return retrieveHeaders(path).get(name);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path}.\n+   *\n+   * @param path Path to get extended attributes\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   * @throws UnsupportedOperationException if the operation is unsupported\n+   *         (default outcome).\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path) throws IOException {\n+    return retrieveHeaders(path);\n+  }\n+\n+  /**\n+   * See {@code FileSystem.listXAttrs(path)}.\n+   * @param path Path to get extended attributes\n+   * @return List of supported XAttrs\n+   * @throws IOException IO failure\n+   */\n+  public List<String> listXAttrs(final Path path) throws IOException {\n+    return new ArrayList<>(retrieveHeaders(path).keySet());\n+  }\n+\n+  /**\n+   * See {@code FileSystem.getXAttrs(path, names}.\n+   * @param path Path to get extended attributes\n+   * @param names XAttr names.\n+   * @return Map describing the XAttrs of the file or directory\n+   * @throws IOException IO failure\n+   */\n+  public Map<String, byte[]> getXAttrs(Path path, List<String> names)\n+      throws IOException {\n+    Map<String, byte[]> headers = retrieveHeaders(path);\n+    Map<String, byte[]> result = new TreeMap<>();\n+    headers.entrySet().stream()\n+        .filter(entry -> names.contains(entry.getKey()))\n+        .forEach(entry -> result.put(entry.getKey(), entry.getValue()));\n+    return result;\n+  }\n+\n+  /**\n+   * Convert an XAttr byte array to a long.\n+   * testability.\n+   * @param data data to parse\n+   * @return either a length or none\n+   */\n+  public static Optional<Long> extractXAttrLongValue(byte[] data) {\n+    String xAttr;\n+    xAttr = HeaderProcessing.decodeBytes(data);\n+    if (StringUtils.isNotEmpty(xAttr)) {\n+      try {\n+        long l = Long.parseLong(xAttr);\n+        if (l >= 0) {\n+          return Optional.of(l);\n+        }\n+      } catch (NumberFormatException ex) {\n+        LOG.warn(\"Not a number: {}\", xAttr);\n+      }\n+    }\n+    // missing/empty header or parse failure.\n+    return Optional.empty();\n+  }\n+\n+  /**\n+   * Creates a copy of the passed {@link ObjectMetadata}.\n+   * Does so without using the {@link ObjectMetadata#clone()} method,\n+   * to avoid copying unnecessary headers.\n+   * This operation does not copy the {@code X_HEADER_MAGIC_MARKER}\n+   * header to avoid confusion. If a marker file is renamed,\n+   * it loses information about any remapped file.\n+   * @param source the {@link ObjectMetadata} to copy\n+   * @param ret the metadata to update; this is the return value.\n+   * @return a copy of {@link ObjectMetadata} with only relevant attributes\n+   */\n+  public ObjectMetadata cloneObjectMetadata(ObjectMetadata source,\n+      ObjectMetadata ret) {\n+\n+    // Possibly null attributes\n+    // Allowing nulls to pass breaks it during later use\n+    if (source.getCacheControl() != null) {\n+      ret.setCacheControl(source.getCacheControl());\n+    }\n+    if (source.getContentDisposition() != null) {\n+      ret.setContentDisposition(source.getContentDisposition());\n+    }\n+    if (source.getContentEncoding() != null) {\n+      ret.setContentEncoding(source.getContentEncoding());\n+    }\n+    if (source.getContentMD5() != null) {\n+      ret.setContentMD5(source.getContentMD5());\n+    }\n+    if (source.getContentType() != null) {\n+      ret.setContentType(source.getContentType());\n+    }\n+    if (source.getExpirationTime() != null) {\n+      ret.setExpirationTime(source.getExpirationTime());\n+    }\n+    if (source.getExpirationTimeRuleId() != null) {\n+      ret.setExpirationTimeRuleId(source.getExpirationTimeRuleId());\n+    }\n+    if (source.getHttpExpiresDate() != null) {\n+      ret.setHttpExpiresDate(source.getHttpExpiresDate());\n+    }\n+    if (source.getLastModified() != null) {\n+      ret.setLastModified(source.getLastModified());\n+    }\n+    if (source.getOngoingRestore() != null) {\n+      ret.setOngoingRestore(source.getOngoingRestore());\n+    }\n+    if (source.getRestoreExpirationTime() != null) {\n+      ret.setRestoreExpirationTime(source.getRestoreExpirationTime());\n+    }\n+    if (source.getSSEAlgorithm() != null) {\n+      ret.setSSEAlgorithm(source.getSSEAlgorithm());\n+    }\n+    if (source.getSSECustomerAlgorithm() != null) {\n+      ret.setSSECustomerAlgorithm(source.getSSECustomerAlgorithm());\n+    }\n+    if (source.getSSECustomerKeyMd5() != null) {\n+      ret.setSSECustomerKeyMd5(source.getSSECustomerKeyMd5());\n+    }\n+\n+    // copy user metadata except the magic marker header.\n+    for (Map.Entry<String, String> e : source.getUserMetadata().entrySet()) {", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI0NTg3MA==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555245870", "bodyText": "yeah, it was just copy and paste. Move to java 8 filtered stream", "author": "steveloughran", "createdAt": "2021-01-11T18:14:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgxNzg2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgyMzgxNQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r554823815", "bodyText": "you mean x-hadoop-s3a-magic-data-length here?", "author": "liuml07", "createdAt": "2021-01-11T07:13:38Z", "path": "hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/committer_architecture.md", "diffHunk": "@@ -1312,6 +1312,16 @@ On `close()`, summary data would be written to the file\n `/results/latest/__magic/job400_1/task_01_01/latest.orc.lzo.pending`.\n This would contain the upload ID and all the parts and etags of uploaded data.\n \n+A marker file is also created, so that code which verifies that a newly created file\n+exists does not fail.\n+1. These marker files are zero bytes long.\n+1. They declare the full length of the final file in the HTTP header\n+   `x-hadoop-s3a-magic-marker`.", "originalCommit": "732cceb01a8c2c5db8ed669bb4cf5086dd101b03", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTI0NzAyNw==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r555247027", "bodyText": "nicely spotted. will fix", "author": "steveloughran", "createdAt": "2021-01-11T18:16:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDgyMzgxNQ=="}], "type": "inlineReview"}, {"oid": "f4f0fb77806eaafcb9388ea31cf19ff136ded02e", "url": "https://github.com/apache/hadoop/commit/f4f0fb77806eaafcb9388ea31cf19ff136ded02e", "message": "HADOOP-17414. Minimising headers returned as XAttrs; directory support\n\n* Only headers returned are served up.\n* Assertions on value of content type for file and /\n* test to verify failure mode when called on missing objects\n* test to call XAttr on a dir -showed that more work was needed\n* cleaned up S3A entry points in porocess of that work\n\nChange-Id: Ib4911240325a56a3853632b411d84c3c99897a64", "committedDate": "2021-01-13T16:14:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzcxNzk5Ng==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r563717996", "bodyText": "I hope removal of this is not a typo.", "author": "mukund-thakur", "createdAt": "2021-01-25T13:22:03Z", "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java", "diffHunk": "@@ -1800,31 +1809,16 @@ public ObjectMetadata getObjectMetadata(Path path) throws IOException {\n    * @return metadata\n    * @throws IOException IO and object access problems.\n    */\n-  @VisibleForTesting\n   @Retries.RetryTranslated\n-  public ObjectMetadata getObjectMetadata(Path path,\n+  private ObjectMetadata getObjectMetadata(Path path,\n       ChangeTracker changeTracker, Invoker changeInvoker, String operation)\n       throws IOException {\n-    checkNotClosed();", "originalCommit": "df719281e19140096206f3281b6b63e4fd7c6562", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzgxMTMyNQ==", "url": "https://github.com/apache/hadoop/pull/2530#discussion_r563811325", "bodyText": "I think I had a good reason for doing this (it was a private method, checks further up, etc). But looking at it now I'm concluding that whatever that reason was -it's not enough. Restoring.", "author": "steveloughran", "createdAt": "2021-01-25T15:27:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzcxNzk5Ng=="}], "type": "inlineReview"}, {"oid": "5e6674829a53d5c5448e7d12500b6470eae3c8a0", "url": "https://github.com/apache/hadoop/commit/5e6674829a53d5c5448e7d12500b6470eae3c8a0", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nSquashed commit.\n\nHADOOP-17414: review feedback, iostatistics and expanded headers\nHADOOP-17414. NPEs on test teardown.\nHADOOP-17414. Minimising headers returned as XAttrs; directory support\nHADOOP-17414. checkstyle and EOF\nHADOOP-17414. include stack trace when number parse fails\nHADOOP-17414. magic file size: revert setting fs.s3a.magic.enabled to true.\n\nReverted changing the default value of fs.s3a.magic.enabled as I've a bigger PR ripping it out entirely\n\nChange-Id: Id79a3c334ca875aab205195285182106d6a6e9e2", "committedDate": "2021-01-25T15:20:19Z", "type": "commit"}, {"oid": "5e6674829a53d5c5448e7d12500b6470eae3c8a0", "url": "https://github.com/apache/hadoop/commit/5e6674829a53d5c5448e7d12500b6470eae3c8a0", "message": "HADOOP-17414. Magic committer files don't have the count of bytes written collected by spark\n\nSquashed commit.\n\nHADOOP-17414: review feedback, iostatistics and expanded headers\nHADOOP-17414. NPEs on test teardown.\nHADOOP-17414. Minimising headers returned as XAttrs; directory support\nHADOOP-17414. checkstyle and EOF\nHADOOP-17414. include stack trace when number parse fails\nHADOOP-17414. magic file size: revert setting fs.s3a.magic.enabled to true.\n\nReverted changing the default value of fs.s3a.magic.enabled as I've a bigger PR ripping it out entirely\n\nChange-Id: Id79a3c334ca875aab205195285182106d6a6e9e2", "committedDate": "2021-01-25T15:20:19Z", "type": "forcePushed"}, {"oid": "e3b8ee303618a774c6b2a85b576220dba4f79141", "url": "https://github.com/apache/hadoop/commit/e3b8ee303618a774c6b2a85b576220dba4f79141", "message": "HADOOP-17414. Restore check on FS being open.\n\nMukund's review comments\n\nChange-Id: I7e31d5dcd336e15a9baf0640d4fa16b0999fb5a1", "committedDate": "2021-01-25T15:29:27Z", "type": "commit"}]}