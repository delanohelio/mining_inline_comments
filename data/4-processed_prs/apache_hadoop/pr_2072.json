{"pr_number": 2072, "pr_title": "HADOOP-17058. ABFS: Support for AppendBlob in Hadoop ABFS Driver", "pr_createdAt": "2020-06-12T17:19:16Z", "pr_url": "https://github.com/apache/hadoop/pull/2072", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846153", "bodyText": "nit: the 2 if here can be combined with &&", "author": "bilaharith", "createdAt": "2020-06-14T16:27:06Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0Njg2Mg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444446862", "bodyText": "code is removed.", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NjIxMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439846213", "bodyText": "nit: Use the constant for forward slash", "author": "bilaharith", "createdAt": "2020-06-14T16:27:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847515", "bodyText": "How about throwing an AzureBlobFileSystemException from here. So that the customer will get to know that the config is not correct.", "author": "bilaharith", "createdAt": "2020-06-14T16:44:08Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -1314,7 +1352,30 @@ private String convertXmsPropertiesToCommaSeparatedString(final Hashtable<String\n     return properties;\n   }\n \n+  private boolean isKeyForPrefixSet(String key, Set<String> dirSet) {\n+\n+    for (String dir : dirSet) {\n+      if (dir.isEmpty() || key.startsWith(dir)) {\n+        return true;\n+      }\n+\n+      try {\n+        URI uri = new URI(dir);\n+        if (null == uri.getAuthority()) {\n+          if (key.startsWith(dir + \"/\")){\n+            return true;\n+          }\n+        }\n+      } catch (URISyntaxException e) {\n+        LOG.info(\"URI syntax error creating URI for {}\", dir);", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMTIwMQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440001201", "bodyText": "this is used for every file being created, returning true or false. Raising an exception can be a problem.", "author": "ishaniahuja", "createdAt": "2020-06-15T08:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA5NTkwMA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440095900", "bodyText": "but what if only one comma separated value is incorrect, we would be throwing an exception adn crashing the app/jvm?", "author": "ishaniahuja", "createdAt": "2020-06-15T10:59:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzUxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439847645", "bodyText": "fs.azure.appendblob.key config name doesn't look good. Wouldsomething like fs.azure.appendblob.directories convey the meaning better? (The way it is named in the FileSystemConfiguarations)", "author": "bilaharith", "createdAt": "2020-06-14T16:45:51Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/ConfigurationKeys.java", "diffHunk": "@@ -59,6 +59,9 @@\n   public static final String FS_AZURE_ENABLE_AUTOTHROTTLING = \"fs.azure.enable.autothrottling\";\n   public static final String FS_AZURE_ALWAYS_USE_HTTPS = \"fs.azure.always.use.https\";\n   public static final String FS_AZURE_ATOMIC_RENAME_KEY = \"fs.azure.atomic.rename.key\";\n+  /** Provides a config to provide comma separated path prefixes on which Appendblob based files are created\n+   *  Default is empty. **/\n+  public static final String FS_AZURE_APPEND_BLOB_KEY = \"fs.azure.appendblob.key\";", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAwMjQ0Ng==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440002446", "bodyText": "I have made is similar to FS_AZURE_ATOMIC_RENAME_KEY which also provide a set of directories. Further please note for appendblob, this is actually a prefix for the path (and not necessarily the directories). This is done so that the test suite (which all runs on a container with a randon guid can run) can run on appendblob based files. Let me know ur comments/thoughts here.", "author": "ishaniahuja", "createdAt": "2020-06-15T08:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzA0Nw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447047", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0NzY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848105", "bodyText": "Could you move this append blob handling to a separate method and call the same from here.", "author": "bilaharith", "createdAt": "2020-06-14T16:51:02Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -323,6 +328,35 @@ private synchronized void writeCurrentBufferToService() throws IOException {\n     final long offset = position;\n     position += bytesLength;\n \n+    if (this.isAppendBlob) {", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzE5OA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r444447198", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-23T19:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r439848257", "bodyText": "nit: Should we have a debug log here?", "author": "bilaharith", "createdAt": "2020-06-14T16:52:43Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +423,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+\n+    // flush is not called for appendblob as is not needed\n+    if (this.isAppendBlob) {\n+      return;", "originalCommit": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA4MTQ0OA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r440081448", "bodyText": "this can lead to frequent log lines, every time a flush(), hflush(), hsync() is called. will that be ok?", "author": "ishaniahuja", "createdAt": "2020-06-15T10:30:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NzQxMg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445367412", "bodyText": "Resolved", "author": "bilaharith", "createdAt": "2020-06-25T07:43:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg0ODI1Nw=="}], "type": "inlineReview"}, {"oid": "d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "url": "https://github.com/apache/hadoop/commit/d1f1fbf0ef8588d0aebc8441e2a4784508042f12", "message": "checkstyle fixes", "committedDate": "2020-06-13T03:11:28Z", "type": "forcePushed"}, {"oid": "2efbe0d0b93255e7cc9e8af771609d99dff8caed", "url": "https://github.com/apache/hadoop/commit/2efbe0d0b93255e7cc9e8af771609d99dff8caed", "message": "changes for appendblo", "committedDate": "2020-06-20T18:31:04Z", "type": "commit"}, {"oid": "cda6d67702d59acff8bca4aa062be108f2f33967", "url": "https://github.com/apache/hadoop/commit/cda6d67702d59acff8bca4aa062be108f2f33967", "message": "yetus fixes", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "89d836deab495b4be559402003c49e795b888e32", "url": "https://github.com/apache/hadoop/commit/89d836deab495b4be559402003c49e795b888e32", "message": "checkstyle fixes", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "url": "https://github.com/apache/hadoop/commit/99f75d8c1dbef131d23eec7bba59dd5dc07c9e5b", "message": "incorporated feedbacks", "committedDate": "2020-06-20T18:31:05Z", "type": "commit"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "commit"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "forcePushed"}, {"oid": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "url": "https://github.com/apache/hadoop/commit/b11ea2d7754561bc5ffa635a3a00284cad58e13c", "message": "AbfsNetworkStats failures fixes for appendblob", "committedDate": "2020-06-21T05:42:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1MzY0OQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445353649", "bodyText": "Minor. Fix comment.", "author": "snvijaya", "createdAt": "2020-06-25T07:16:58Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/constants/FileSystemConfigurations.java", "diffHunk": "@@ -47,6 +47,7 @@\n \n   // Default upload and download buffer size\n   public static final int DEFAULT_WRITE_BUFFER_SIZE = 8 * ONE_MB;  // 8 MB\n+  public static final int APPENDBLOB_MAX_WRITE_BUFFER_SIZE = 4 * ONE_MB;  // 8 MB", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDEyMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354123", "bodyText": "Minor. boolean flag better named as isAppendBlob", "author": "snvijaya", "createdAt": "2020-06-25T07:17:49Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java", "diffHunk": "@@ -272,7 +272,8 @@ public AbfsRestOperation deleteFilesystem() throws AzureBlobFileSystemException\n   }\n \n   public AbfsRestOperation createPath(final String path, final boolean isFile, final boolean overwrite,\n-                                      final String permission, final String umask) throws AzureBlobFileSystemException {\n+                                      final String permission, final String umask,\n+                                      final boolean appendBlob) throws AzureBlobFileSystemException {", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NDY0NA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445354644", "bodyText": "Undo. newline needed after a block.", "author": "snvijaya", "createdAt": "2020-06-25T07:18:50Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsHttpOperation.java", "diffHunk": "@@ -337,7 +344,6 @@ public void processResponse(final byte[] buffer, final int offset, final int len\n     if (this.isTraceEnabled) {\n       startTime = System.nanoTime();\n     }\n-", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTA0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355045", "bodyText": "Undo. new line needed.", "author": "snvijaya", "createdAt": "2020-06-25T07:19:34Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -367,7 +418,6 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n             throw new FileNotFoundException(ex.getMessage());\n           }\n         }\n-", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTIyNQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355225", "bodyText": "unnecessary additional new line.", "author": "snvijaya", "createdAt": "2020-06-25T07:19:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -378,6 +428,7 @@ private synchronized void flushWrittenBytesToService(boolean isClose) throws IOE\n     flushWrittenBytesToServiceInternal(position, false, isClose);\n   }\n \n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTcwNg==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355706", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:20:46Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -389,6 +440,12 @@ private synchronized void flushWrittenBytesToServiceAsync() throws IOException {\n \n   private synchronized void flushWrittenBytesToServiceInternal(final long offset,\n       final boolean retainUncommitedData, final boolean isClose) throws IOException {\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NTgxMQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445355811", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:21:00Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsOutputStream.java", "diffHunk": "@@ -430,10 +487,15 @@ private synchronized void shrinkWriteOperationQueue() throws IOException {\n   }\n \n   private void waitForTaskToComplete() throws IOException {\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NjQxMw==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445356413", "bodyText": "remove newline", "author": "snvijaya", "createdAt": "2020-06-25T07:22:07Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445357550", "bodyText": "Why is it that for Http Status code 400 and above, exception is suppressed ? Can you please add code comments for the reason.", "author": "snvijaya", "createdAt": "2020-06-25T07:24:17Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsRestOperation.java", "diffHunk": "@@ -185,13 +189,18 @@ void execute() throws AzureBlobFileSystemException {\n       try {\n         LOG.debug(\"Retrying REST operation {}. RetryCount = {}\",\n             operationType, retryCount);\n+\n         Thread.sleep(client.getRetryPolicy().getRetryInterval(retryCount));\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n       }\n     }\n \n     if (result.getStatusCode() >= HttpURLConnection.HTTP_BAD_REQUEST) {\n+      if (this.isAppendBlobAppend && retryCount > 0 && result.getStorageErrorCode().equals(\"InvalidQueryParameterValue\")) {", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NDk0NQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r446094945", "bodyText": "This isnt the right place to handle a case specific to appendblob. HttpOperation returned to specific AbfsClient method can take the call on what to return. Similar is done for rename.\nYou can check:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/services/AbfsClient.java#L358", "author": "snvijaya", "createdAt": "2020-06-26T10:11:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM1NzU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445364600", "bodyText": "Would be better to add a new line since the next line is a constructor.", "author": "bilaharith", "createdAt": "2020-06-25T07:37:57Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystemStore.java", "diffHunk": "@@ -144,6 +145,10 @@\n   private final IdentityTransformerInterface identityTransformer;\n   private final AbfsPerfTracker abfsPerfTracker;\n \n+  /**\n+   * The set of directories where we should store files as append blobs.\n+   */\n+  private Set<String> appendBlobDirSet;", "originalCommit": "b11ea2d7754561bc5ffa635a3a00284cad58e13c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc0NTI5MQ==", "url": "https://github.com/apache/hadoop/pull/2072#discussion_r445745291", "bodyText": "done", "author": "ishaniahuja", "createdAt": "2020-06-25T18:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NDYwMA=="}], "type": "inlineReview"}, {"oid": "3b7a65bf066fe41f5a1674584ce3665532fe96ed", "url": "https://github.com/apache/hadoop/commit/3b7a65bf066fe41f5a1674584ce3665532fe96ed", "message": "incorporated feedbacks", "committedDate": "2020-06-25T18:42:45Z", "type": "commit"}, {"oid": "8e089333aa058a09bfcfb402293523057edf013b", "url": "https://github.com/apache/hadoop/commit/8e089333aa058a09bfcfb402293523057edf013b", "message": "support for idempotency check on appendblob append", "committedDate": "2020-06-29T08:47:19Z", "type": "commit"}, {"oid": "8e089333aa058a09bfcfb402293523057edf013b", "url": "https://github.com/apache/hadoop/commit/8e089333aa058a09bfcfb402293523057edf013b", "message": "support for idempotency check on appendblob append", "committedDate": "2020-06-29T08:47:19Z", "type": "forcePushed"}, {"oid": "26c91b3e97ed1b003b405b70653f66796daedb8a", "url": "https://github.com/apache/hadoop/commit/26c91b3e97ed1b003b405b70653f66796daedb8a", "message": "incorporated feedbacks", "committedDate": "2020-06-29T11:55:34Z", "type": "commit"}]}