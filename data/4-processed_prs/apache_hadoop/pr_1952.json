{"pr_number": 1952, "pr_title": "HDFS-1820. FTPFileSystem attempts to close the outputstream even when it is not initialised.", "pr_createdAt": "2020-04-10T20:10:01Z", "pr_url": "https://github.com/apache/hadoop/pull/1952", "timeline": [{"oid": "f33f76f8fb67f9b1c94e65d29bfb415ca826bd2d", "url": "https://github.com/apache/hadoop/commit/f33f76f8fb67f9b1c94e65d29bfb415ca826bd2d", "message": "HDFS-1820 SFTPFilesSystem hangs when a user lacks write permissions", "committedDate": "2020-04-10T16:43:24Z", "type": "commit"}, {"oid": "d879a247ff7498182097aedf4a625ae31dd77ddf", "url": "https://github.com/apache/hadoop/commit/d879a247ff7498182097aedf4a625ae31dd77ddf", "message": "HDFS-1820 fixed code style issues", "committedDate": "2020-04-10T17:57:51Z", "type": "commit"}, {"oid": "bc200c7e2f03b954d8516b4b30e0cf56bd97353d", "url": "https://github.com/apache/hadoop/commit/bc200c7e2f03b954d8516b4b30e0cf56bd97353d", "message": "HDFS-1820 code style issues", "committedDate": "2020-04-10T19:23:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDA5NQ==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407100095", "bodyText": "Can we use setBoolean()?", "author": "goiri", "createdAt": "2020-04-11T19:13:18Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/ftp/TestFTPFileSystem.java", "diffHunk": "@@ -37,9 +54,71 @@\n  */\n public class TestFTPFileSystem {\n \n+  private TestFtpServer server;\n+\n   @Rule\n   public Timeout testTimeout = new Timeout(180000);\n \n+  @Before\n+  public void setUp() throws Exception {\n+    server = new TestFtpServer(GenericTestUtils.getTestDir().toPath()).start();\n+  }\n+\n+  @After\n+  @SuppressWarnings(\"ResultOfMethodCallIgnored\")\n+  public void tearDown() throws Exception {\n+    server.stop();\n+    Files.walk(server.getFtpRoot())\n+        .sorted(Comparator.reverseOrder())\n+        .map(java.nio.file.Path::toFile)\n+        .forEach(File::delete);\n+  }\n+\n+  @Test\n+  public void testCreateWithWritePermissions() throws Exception {\n+    BaseUser user = server.addUser(\"test\", \"password\", new WritePermission());\n+    Configuration configuration = new Configuration();\n+    configuration.set(\"fs.defaultFS\", \"ftp:///\");\n+    configuration.set(\"fs.ftp.host\", \"localhost\");\n+    configuration.setInt(\"fs.ftp.host.port\", server.getPort());\n+    configuration.set(\"fs.ftp.user.localhost\", user.getName());\n+    configuration.set(\"fs.ftp.password.localhost\", user.getPassword());\n+    configuration.set(\"fs.ftp.impl.disable.cache\", \"true\");", "originalCommit": "bc200c7e2f03b954d8516b4b30e0cf56bd97353d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE2NjMzMA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407166330", "bodyText": "indeed! thank you for pointing out", "author": "mpryahin", "createdAt": "2020-04-12T08:36:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDA5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDIwMg==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407100202", "bodyText": "We should use LambdaTestUtils#intercept and make sure that we fail after write()", "author": "goiri", "createdAt": "2020-04-11T19:14:06Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/ftp/TestFTPFileSystem.java", "diffHunk": "@@ -37,9 +54,71 @@\n  */\n public class TestFTPFileSystem {\n \n+  private TestFtpServer server;\n+\n   @Rule\n   public Timeout testTimeout = new Timeout(180000);\n \n+  @Before\n+  public void setUp() throws Exception {\n+    server = new TestFtpServer(GenericTestUtils.getTestDir().toPath()).start();\n+  }\n+\n+  @After\n+  @SuppressWarnings(\"ResultOfMethodCallIgnored\")\n+  public void tearDown() throws Exception {\n+    server.stop();\n+    Files.walk(server.getFtpRoot())\n+        .sorted(Comparator.reverseOrder())\n+        .map(java.nio.file.Path::toFile)\n+        .forEach(File::delete);\n+  }\n+\n+  @Test\n+  public void testCreateWithWritePermissions() throws Exception {\n+    BaseUser user = server.addUser(\"test\", \"password\", new WritePermission());\n+    Configuration configuration = new Configuration();\n+    configuration.set(\"fs.defaultFS\", \"ftp:///\");\n+    configuration.set(\"fs.ftp.host\", \"localhost\");\n+    configuration.setInt(\"fs.ftp.host.port\", server.getPort());\n+    configuration.set(\"fs.ftp.user.localhost\", user.getName());\n+    configuration.set(\"fs.ftp.password.localhost\", user.getPassword());\n+    configuration.set(\"fs.ftp.impl.disable.cache\", \"true\");\n+\n+    FileSystem fs = FileSystem.get(configuration);\n+    byte[] bytesExpected = \"hello world\".getBytes(StandardCharsets.UTF_8);\n+    try (FSDataOutputStream outputStream = fs.create(new Path(\"test1.txt\"))) {\n+      outputStream.write(bytesExpected);\n+    }\n+    try (FSDataInputStream input = fs.open(new Path(\"test1.txt\"))) {\n+      assertThat(bytesExpected, equalTo(IOUtils.readFullyToByteArray(input)));\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateWithoutWritePermissions() throws Exception {\n+    BaseUser user = server.addUser(\"test\", \"password\");\n+    Configuration configuration = new Configuration();\n+    configuration.set(\"fs.defaultFS\", \"ftp:///\");\n+    configuration.set(\"fs.ftp.host\", \"localhost\");\n+    configuration.setInt(\"fs.ftp.host.port\", server.getPort());\n+    configuration.set(\"fs.ftp.user.localhost\", user.getName());\n+    configuration.set(\"fs.ftp.password.localhost\", user.getPassword());\n+    configuration.set(\"fs.ftp.impl.disable.cache\", \"true\");\n+\n+    FileSystem fs = FileSystem.get(configuration);\n+    byte[] bytesExpected = \"hello world\".getBytes(StandardCharsets.UTF_8);\n+\n+    try (FSDataOutputStream outputStream = fs.create(new Path(\"test1.txt\"))) {\n+      outputStream.write(bytesExpected);", "originalCommit": "bc200c7e2f03b954d8516b4b30e0cf56bd97353d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE2NjM3OA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407166378", "bodyText": "fixed, thank you!", "author": "mpryahin", "createdAt": "2020-04-12T08:36:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDQyOA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407100428", "bodyText": "Starting by Test in  this folder I would expect it to have a @test in it.\nTBH, I don't know what better name to give it, FtpTestServer?", "author": "goiri", "createdAt": "2020-04-11T19:16:10Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/ftp/TestFtpServer.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.ftp;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+\n+import org.apache.ftpserver.FtpServer;\n+import org.apache.ftpserver.FtpServerFactory;\n+import org.apache.ftpserver.ftplet.Authority;\n+import org.apache.ftpserver.ftplet.FtpException;\n+import org.apache.ftpserver.ftplet.UserManager;\n+import org.apache.ftpserver.impl.DefaultFtpServer;\n+import org.apache.ftpserver.listener.Listener;\n+import org.apache.ftpserver.listener.ListenerFactory;\n+import org.apache.ftpserver.usermanager.PropertiesUserManagerFactory;\n+import org.apache.ftpserver.usermanager.impl.BaseUser;\n+\n+/**\n+ * Helper class facilitating to manage a local ftp\n+ * server for unit tests purposes only.\n+ */\n+public class TestFtpServer {", "originalCommit": "bc200c7e2f03b954d8516b4b30e0cf56bd97353d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE2NjUwNQ==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r407166505", "bodyText": "Thank you, renamed  the class into FtpTestServer", "author": "mpryahin", "createdAt": "2020-04-12T08:37:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEwMDQyOA=="}], "type": "inlineReview"}, {"oid": "499ecf99180a2f209f97fff6b874c60a69d5fe80", "url": "https://github.com/apache/hadoop/commit/499ecf99180a2f209f97fff6b874c60a69d5fe80", "message": "HDFS-1820 code review improvements", "committedDate": "2020-04-12T08:34:13Z", "type": "commit"}, {"oid": "385ed063d0a9247b73610d4a548200ff4e311b27", "url": "https://github.com/apache/hadoop/commit/385ed063d0a9247b73610d4a548200ff4e311b27", "message": "HDFS-1820 code style issues", "committedDate": "2020-04-12T09:37:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MDk2MA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411270960", "bodyText": "handle case where server==null, i.e. setup failed", "author": "steveloughran", "createdAt": "2020-04-20T10:33:07Z", "path": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/ftp/TestFTPFileSystem.java", "diffHunk": "@@ -37,9 +54,70 @@\n  */\n public class TestFTPFileSystem {\n \n+  private FtpTestServer server;\n+\n   @Rule\n   public Timeout testTimeout = new Timeout(180000);\n \n+  @Before\n+  public void setUp() throws Exception {\n+    server = new FtpTestServer(GenericTestUtils.getTestDir().toPath()).start();\n+  }\n+\n+  @After\n+  @SuppressWarnings(\"ResultOfMethodCallIgnored\")\n+  public void tearDown() throws Exception {\n+    server.stop();", "originalCommit": "385ed063d0a9247b73610d4a548200ff4e311b27", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMwMDU2Mg==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411300562", "bodyText": "thank you, fixed.", "author": "mpryahin", "createdAt": "2020-04-20T11:25:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MDk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MjExNA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411272114", "bodyText": "could this raise an IOE? If so, that disconnect() afterwards still needs to be called, so make close() a catch/log operation. IOUtils.closeStream could do this (and it includes the null check)", "author": "steveloughran", "createdAt": "2020-04-20T10:35:09Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java", "diffHunk": "@@ -340,8 +343,19 @@ public FSDataOutputStream create(Path file, FsPermission permission,\n     // file. The FTP client connection is closed when close() is called on the\n     // FSDataOutputStream.\n     client.changeWorkingDirectory(parent.toUri().getPath());\n-    FSDataOutputStream fos = new FSDataOutputStream(client.storeFileStream(file\n-        .getName()), statistics) {\n+    OutputStream outputStream = client.storeFileStream(file.getName());\n+\n+    if (!FTPReply.isPositivePreliminary(client.getReplyCode())) {\n+      // The ftpClient is an inconsistent state. Must close the stream\n+      // which in turn will logout and disconnect from FTP server\n+      if (outputStream != null) {\n+        outputStream.close();", "originalCommit": "385ed063d0a9247b73610d4a548200ff4e311b27", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMwMDQ1Mw==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411300453", "bodyText": "thank you, fixed.", "author": "mpryahin", "createdAt": "2020-04-20T11:25:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MjExNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MjY0Nw==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411272647", "bodyText": "assuming we have documentation for the FTP connector, you are going to have to document this new option.", "author": "steveloughran", "createdAt": "2020-04-20T10:36:06Z", "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java", "diffHunk": "@@ -110,7 +111,9 @@ public void initialize(URI uri, Configuration conf) throws IOException { // get\n \n     // get port information from uri, (overrides info in conf)\n     int port = uri.getPort();\n-    port = (port == -1) ? FTP.DEFAULT_PORT : port;\n+    if(port == -1){\n+      port = conf.getInt(FS_FTP_HOST_PORT, FTP.DEFAULT_PORT);", "originalCommit": "385ed063d0a9247b73610d4a548200ff4e311b27", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI5OTM4NA==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r411299384", "bodyText": "indeed, but unfortunately there is no documentation for this connector at the moment.", "author": "mpryahin", "createdAt": "2020-04-20T11:23:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MjY0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjMzNjkzNg==", "url": "https://github.com/apache/hadoop/pull/1952#discussion_r412336936", "bodyText": "that's got you out of trouble :)", "author": "steveloughran", "createdAt": "2020-04-21T17:09:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI3MjY0Nw=="}], "type": "inlineReview"}, {"oid": "6ceac7de28f9a9658ef539dd2b157b5963d9599e", "url": "https://github.com/apache/hadoop/commit/6ceac7de28f9a9658ef539dd2b157b5963d9599e", "message": "HDFS-1820 core review enhancements", "committedDate": "2020-04-20T11:24:42Z", "type": "commit"}, {"oid": "3c9b5ad0606371abf04ab08865b4e8f9c75babd9", "url": "https://github.com/apache/hadoop/commit/3c9b5ad0606371abf04ab08865b4e8f9c75babd9", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HDFS-1820", "committedDate": "2020-04-20T13:34:36Z", "type": "commit"}, {"oid": "f46b6514805e3d456a9f708401078b8ce9d85a8e", "url": "https://github.com/apache/hadoop/commit/f46b6514805e3d456a9f708401078b8ce9d85a8e", "message": "Merge branch 'trunk' of https://github.com/apache/hadoop into HDFS-1820", "committedDate": "2020-04-20T16:20:32Z", "type": "commit"}]}