{"pr_number": 1893, "pr_title": "HADOOP-16920 ABFS: Make list page size configurable", "pr_createdAt": "2020-03-11T10:56:09Z", "pr_url": "https://github.com/apache/hadoop/pull/1893", "timeline": [{"oid": "a5bfe135ae3e8dc3a919238d40d4684c584104a6", "url": "https://github.com/apache/hadoop/commit/a5bfe135ae3e8dc3a919238d40d4684c584104a6", "message": "ABFS: Made list page size configurable", "committedDate": "2020-03-11T10:47:37Z", "type": "commit"}, {"oid": "1b21e920537d88243c5a2e9e41dd12f5dd73e5c8", "url": "https://github.com/apache/hadoop/commit/1b21e920537d88243c5a2e9e41dd12f5dd73e5c8", "message": "fixing import order", "committedDate": "2020-03-11T10:54:02Z", "type": "commit"}, {"oid": "1afca492cb4f7c99ca5d1e22c821ed0b115c0eb2", "url": "https://github.com/apache/hadoop/commit/1afca492cb4f7c99ca5d1e22c821ed0b115c0eb2", "message": "Added hamcrest dependancy", "committedDate": "2020-03-11T11:04:59Z", "type": "commit"}, {"oid": "0a19a718cf692c176589e05f0efe6ec99e138feb", "url": "https://github.com/apache/hadoop/commit/0a19a718cf692c176589e05f0efe6ec99e138feb", "message": "Merge branch 'trunk' into HADOOP-16920", "committedDate": "2020-03-12T05:45:57Z", "type": "commit"}, {"oid": "cc791e2297cd37d99b2bdc5b3d6ff181c80c6ca4", "url": "https://github.com/apache/hadoop/commit/cc791e2297cd37d99b2bdc5b3d6ff181c80c6ca4", "message": "Improved test case readability. Added min value for the azure.list.max.results config as 1.", "committedDate": "2020-03-13T07:20:35Z", "type": "commit"}, {"oid": "7291934705c45bf659a5e76a166c291027601819", "url": "https://github.com/apache/hadoop/commit/7291934705c45bf659a5e76a166c291027601819", "message": "Fixing import order", "committedDate": "2020-03-13T07:25:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzI5ODU2OQ==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393298569", "bodyText": "that;s just a lambda expression, you should be able to go es.submit(() -> { touch(filename); return null; }", "author": "steveloughran", "createdAt": "2020-03-16T20:43:38Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsClient.java", "diffHunk": "@@ -75,4 +89,80 @@ public void testUnknownHost() throws Exception {\n             \"UnknownHostException: \" + fakeAccountName,\n             () -> FileSystem.get(conf.getRawConfiguration()));\n   }\n+\n+  @Test\n+  public void testListPathWithValidListMaxResultsValues()\n+      throws IOException, ExecutionException, InterruptedException {\n+    final int fileCount = 10;\n+    final String directory = \"testWithValidListMaxResultsValues\";\n+    createDirectoryWithNFiles(directory, fileCount);\n+    final int[] testData = {fileCount + 100, fileCount + 1, fileCount,\n+        fileCount - 1, 1};\n+    for (int i = 0; i < testData.length; i++) {\n+      int listMaxResults = testData[i];\n+      setListMaxResults(listMaxResults);\n+      int expectedListResultsSize =\n+          listMaxResults > fileCount ? fileCount : listMaxResults;\n+      assertThat(listPath(directory).size(),\n+          is(equalTo(expectedListResultsSize)));\n+    }\n+  }\n+\n+  @Test\n+  public void testListPathWithValueGreaterThanServerMaximum()\n+      throws IOException, ExecutionException, InterruptedException {\n+    setListMaxResults(LIST_MAX_RESULTS_SERVER + 100);\n+    final String directory = \"testWithValueGreaterThanServerMaximum\";\n+    createDirectoryWithNFiles(directory, LIST_MAX_RESULTS_SERVER + 200);\n+    assertThat(listPath(directory).size(),\n+        is(equalTo(LIST_MAX_RESULTS_SERVER)));\n+  }\n+\n+  @Test\n+  public void testListPathWithInvalidListMaxResultsValues() throws Exception {\n+    for (int i = -1; i < 1; i++) {\n+      setListMaxResults(i);\n+      intercept(AbfsRestOperationException.class, \"Operation failed: \\\"One of \"\n+          + \"the query parameters specified in the request URI is outside\" + \" \"\n+          + \"the permissible range.\", () -> listPath(\"directory\"));\n+    }\n+  }\n+\n+  private List<ListResultEntrySchema> listPath(String directory)\n+      throws IOException {\n+    return getFileSystem().getAbfsClient()\n+        .listPath(directory, false, getListMaxResults(), null).getResult()\n+        .getListResultSchema().paths();\n+  }\n+\n+  private int getListMaxResults() throws IOException {\n+    return getFileSystem().getAbfsStore().getAbfsConfiguration()\n+        .getListMaxResults();\n+  }\n+\n+  private void setListMaxResults(int listMaxResults) throws IOException {\n+    getFileSystem().getAbfsStore().getAbfsConfiguration()\n+        .setListMaxResults(listMaxResults);\n+  }\n+\n+  private void createDirectoryWithNFiles(String directory, int n)\n+      throws ExecutionException, InterruptedException {\n+    final List<Future<Void>> tasks = new ArrayList<>();\n+    ExecutorService es = Executors.newFixedThreadPool(10);\n+    for (int i = 0; i < n; i++) {\n+      final Path fileName = new Path(\"/\" + directory + \"/test\" + i);\n+      Callable<Void> callable = new Callable<Void>() {", "originalCommit": "7291934705c45bf659a5e76a166c291027601819", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzgwMDU1MQ==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393800551", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-03-17T16:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzI5ODU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzI5OTM2Ng==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393299366", "bodyText": "use assertJ here and below. It is way better because of the better errors it gives, e.g.\nassertThat(listPath).describedAs(...).hasSize(...)", "author": "steveloughran", "createdAt": "2020-03-16T20:45:30Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsClient.java", "diffHunk": "@@ -75,4 +89,80 @@ public void testUnknownHost() throws Exception {\n             \"UnknownHostException: \" + fakeAccountName,\n             () -> FileSystem.get(conf.getRawConfiguration()));\n   }\n+\n+  @Test\n+  public void testListPathWithValidListMaxResultsValues()\n+      throws IOException, ExecutionException, InterruptedException {\n+    final int fileCount = 10;\n+    final String directory = \"testWithValidListMaxResultsValues\";\n+    createDirectoryWithNFiles(directory, fileCount);\n+    final int[] testData = {fileCount + 100, fileCount + 1, fileCount,\n+        fileCount - 1, 1};\n+    for (int i = 0; i < testData.length; i++) {\n+      int listMaxResults = testData[i];\n+      setListMaxResults(listMaxResults);\n+      int expectedListResultsSize =\n+          listMaxResults > fileCount ? fileCount : listMaxResults;\n+      assertThat(listPath(directory).size(),", "originalCommit": "7291934705c45bf659a5e76a166c291027601819", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzgwMDYzMA==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393800630", "bodyText": "Done", "author": "bilaharith", "createdAt": "2020-03-17T16:16:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzI5OTM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4MjIzNg==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393382236", "bodyText": "Do we need a MaxValue here?", "author": "DadanielZ", "createdAt": "2020-03-17T00:29:18Z", "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java", "diffHunk": "@@ -125,6 +125,11 @@\n       DefaultValue = MAX_CONCURRENT_WRITE_THREADS)\n   private int maxConcurrentWriteThreads;\n \n+  @IntegerConfigurationValidatorAnnotation(ConfigurationKey = AZURE_LIST_MAX_RESULTS,\n+      MinValue = 1,\n+      DefaultValue = DEFAULT_AZURE_LIST_MAX_RESULTS)", "originalCommit": "7291934705c45bf659a5e76a166c291027601819", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzQzOTg5MA==", "url": "https://github.com/apache/hadoop/pull/1893#discussion_r393439890", "bodyText": "Currently server has a maximum limit which is 5000. values >= 5000 will be defaulted to 5000. Did not want to keep max value 5000 here, in case server in future plans making changes to this.", "author": "bilaharith", "createdAt": "2020-03-17T04:33:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzM4MjIzNg=="}], "type": "inlineReview"}, {"oid": "2b0bf2cc75b445d5fd7dc50fa5e9b326f0890780", "url": "https://github.com/apache/hadoop/commit/2b0bf2cc75b445d5fd7dc50fa5e9b326f0890780", "message": "Incorporating review comments", "committedDate": "2020-03-17T16:15:48Z", "type": "commit"}, {"oid": "dbadcfe4521f31664b2b536ee33fd1d2c7da97de", "url": "https://github.com/apache/hadoop/commit/dbadcfe4521f31664b2b536ee33fd1d2c7da97de", "message": "Checkstyle fix", "committedDate": "2020-03-17T17:48:21Z", "type": "commit"}]}