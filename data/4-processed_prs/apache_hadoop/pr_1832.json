{"pr_number": 1832, "pr_title": "HDFS-13989. RBF: Add FSCK to the Router", "pr_createdAt": "2020-02-06T09:16:43Z", "pr_url": "https://github.com/apache/hadoop/pull/1832", "timeline": [{"oid": "f6725bd0569aca087672a98deb61d23ce982bad3", "url": "https://github.com/apache/hadoop/commit/f6725bd0569aca087672a98deb61d23ce982bad3", "message": "HDFS-13989. RBF: Add FSCK to the Router", "committedDate": "2020-02-06T07:21:08Z", "type": "commit"}, {"oid": "e9b41322c0cdec2388efed1b9d7e48a483809fb2", "url": "https://github.com/apache/hadoop/commit/e9b41322c0cdec2388efed1b9d7e48a483809fb2", "message": "Fix compile error", "committedDate": "2020-02-06T07:25:10Z", "type": "commit"}, {"oid": "b023adb3202f8a94d84f976347d44f1302c6e54a", "url": "https://github.com/apache/hadoop/commit/b023adb3202f8a94d84f976347d44f1302c6e54a", "message": "Add unit test", "committedDate": "2020-02-06T08:27:50Z", "type": "commit"}, {"oid": "7aed59de9579fa57c009eaff78bb915e0aedb248", "url": "https://github.com/apache/hadoop/commit/7aed59de9579fa57c009eaff78bb915e0aedb248", "message": "Modified message", "committedDate": "2020-02-06T08:32:04Z", "type": "commit"}, {"oid": "55c79baa490e26281a11f3f85017099621f3c636", "url": "https://github.com/apache/hadoop/commit/55c79baa490e26281a11f3f85017099621f3c636", "message": "Refactor the unit test", "committedDate": "2020-02-06T08:50:24Z", "type": "commit"}, {"oid": "f3b91c44d6a9af7bde68ae7056236fe407c260cc", "url": "https://github.com/apache/hadoop/commit/f3b91c44d6a9af7bde68ae7056236fe407c260cc", "message": "Add Apache license header", "committedDate": "2020-02-06T08:51:19Z", "type": "commit"}, {"oid": "5d6c850163da10cf36185b05b038640ea6802e66", "url": "https://github.com/apache/hadoop/commit/5d6c850163da10cf36185b05b038640ea6802e66", "message": "Undo unnecessary import change", "committedDate": "2020-02-06T08:53:16Z", "type": "commit"}, {"oid": "d697bee4bb006190297e2fa4d918773caac5c93c", "url": "https://github.com/apache/hadoop/commit/d697bee4bb006190297e2fa4d918773caac5c93c", "message": "Add testcase with path argument", "committedDate": "2020-02-06T09:01:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzIwNQ==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093205", "bodyText": "I think we can remove this now.", "author": "goiri", "createdAt": "2020-02-06T21:31:37Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterHttpServer.java", "diffHunk": "@@ -118,10 +118,14 @@ protected void serviceStop() throws Exception {\n \n   private static void setupServlets(\n       HttpServer2 httpServer, Configuration conf) {\n-    // TODO Add servlets for FSCK, etc\n+    // TODO: Add more required servlets", "originalCommit": "d697bee4bb006190297e2fa4d918773caac5c93c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzU1NA==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093554", "bodyText": "Let's make an static import for assertTrue and add a message with the out.", "author": "goiri", "createdAt": "2020-02-06T21:32:24Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.net.InetSocketAddress;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.federation.MiniRouterDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.RouterConfigBuilder;\n+import org.apache.hadoop.hdfs.server.federation.StateStoreDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableManager;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MountTable;\n+import org.apache.http.HttpStatus;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.http.util.EntityUtils;\n+\n+/**\n+ * End-to-end tests for fsck via DFSRouter\n+ */\n+public class TestRouterFsck {\n+\n+  private static StateStoreDFSCluster cluster;\n+  private static MiniRouterDFSCluster.RouterContext routerContext;\n+  private static MountTableResolver mountTable;\n+  private static FileSystem routerFs;\n+  private static InetSocketAddress webAddress;\n+\n+  @BeforeClass\n+  public static void globalSetUp() throws Exception {\n+    // Build and start a federated cluster\n+    cluster = new StateStoreDFSCluster(false, 2);\n+    Configuration conf = new RouterConfigBuilder()\n+        .stateStore()\n+        .admin()\n+        .rpc()\n+        .http()\n+        .build();\n+    cluster.addRouterOverrides(conf);\n+    cluster.startCluster();\n+    cluster.startRouters();\n+    cluster.waitClusterUp();\n+\n+    // Get the end points\n+    routerContext = cluster.getRandomRouter();\n+    routerFs = routerContext.getFileSystem();\n+    Router router = routerContext.getRouter();\n+    mountTable = (MountTableResolver) router.getSubclusterResolver();\n+    webAddress = router.getHttpServerAddress();\n+    Assert.assertNotNull(webAddress);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() {\n+    if (cluster != null) {\n+      cluster.stopRouter(routerContext);\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @After\n+  public void clearMountTable() throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    GetMountTableEntriesRequest req1 =\n+        GetMountTableEntriesRequest.newInstance(\"/\");\n+    GetMountTableEntriesResponse response =\n+        mountTableManager.getMountTableEntries(req1);\n+    for (MountTable entry : response.getEntries()) {\n+      RemoveMountTableEntryRequest req2 =\n+          RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\n+      mountTableManager.removeMountTableEntry(req2);\n+    }\n+  }\n+\n+  private boolean addMountTable(final MountTable entry) throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    AddMountTableEntryRequest addRequest =\n+        AddMountTableEntryRequest.newInstance(entry);\n+    AddMountTableEntryResponse addResponse =\n+        mountTableManager.addMountTableEntry(addRequest);\n+    // Reload the Router cache\n+    mountTable.loadCache(true);\n+    return addResponse.getStatus();\n+  }\n+\n+  @Test\n+  public void testFsck() throws Exception {\n+    MountTable addEntry = MountTable.newInstance(\"/testdir\",\n+        Collections.singletonMap(\"ns0\", \"/testdir\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    addEntry = MountTable.newInstance(\"/testdir2\",\n+        Collections.singletonMap(\"ns1\", \"/testdir2\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    // create 1 file on ns0\n+    routerFs.createNewFile(new Path(\"/testdir/testfile\"));\n+    // create 3 files on ns1\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile2\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile3\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile4\"));\n+\n+    try (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n+      // TODO: support https\n+      HttpGet httpGet = new HttpGet(\"http://\" + webAddress.getHostName() +\n+              \":\" + webAddress.getPort() + \"/fsck\");\n+      try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\n+        Assert.assertEquals(HttpStatus.SC_OK,\n+            httpResponse.getStatusLine().getStatusCode());\n+        String out = EntityUtils.toString(\n+            httpResponse.getEntity(), StandardCharsets.UTF_8);\n+        System.out.println(out);\n+        Assert.assertTrue(out.contains(\"Federated FSCK started\"));", "originalCommit": "d697bee4bb006190297e2fa4d918773caac5c93c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5MzU5Ng==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376093596", "bodyText": "LOG?", "author": "goiri", "createdAt": "2020-02-06T21:32:31Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouterFsck.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.net.InetSocketAddress;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.server.federation.MiniRouterDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.RouterConfigBuilder;\n+import org.apache.hadoop.hdfs.server.federation.StateStoreDFSCluster;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableManager;\n+import org.apache.hadoop.hdfs.server.federation.resolver.MountTableResolver;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.AddMountTableEntryResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetMountTableEntriesResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.RemoveMountTableEntryRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MountTable;\n+import org.apache.http.HttpStatus;\n+import org.apache.http.client.methods.CloseableHttpResponse;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.CloseableHttpClient;\n+import org.apache.http.impl.client.HttpClients;\n+import org.apache.http.util.EntityUtils;\n+\n+/**\n+ * End-to-end tests for fsck via DFSRouter\n+ */\n+public class TestRouterFsck {\n+\n+  private static StateStoreDFSCluster cluster;\n+  private static MiniRouterDFSCluster.RouterContext routerContext;\n+  private static MountTableResolver mountTable;\n+  private static FileSystem routerFs;\n+  private static InetSocketAddress webAddress;\n+\n+  @BeforeClass\n+  public static void globalSetUp() throws Exception {\n+    // Build and start a federated cluster\n+    cluster = new StateStoreDFSCluster(false, 2);\n+    Configuration conf = new RouterConfigBuilder()\n+        .stateStore()\n+        .admin()\n+        .rpc()\n+        .http()\n+        .build();\n+    cluster.addRouterOverrides(conf);\n+    cluster.startCluster();\n+    cluster.startRouters();\n+    cluster.waitClusterUp();\n+\n+    // Get the end points\n+    routerContext = cluster.getRandomRouter();\n+    routerFs = routerContext.getFileSystem();\n+    Router router = routerContext.getRouter();\n+    mountTable = (MountTableResolver) router.getSubclusterResolver();\n+    webAddress = router.getHttpServerAddress();\n+    Assert.assertNotNull(webAddress);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() {\n+    if (cluster != null) {\n+      cluster.stopRouter(routerContext);\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @After\n+  public void clearMountTable() throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    GetMountTableEntriesRequest req1 =\n+        GetMountTableEntriesRequest.newInstance(\"/\");\n+    GetMountTableEntriesResponse response =\n+        mountTableManager.getMountTableEntries(req1);\n+    for (MountTable entry : response.getEntries()) {\n+      RemoveMountTableEntryRequest req2 =\n+          RemoveMountTableEntryRequest.newInstance(entry.getSourcePath());\n+      mountTableManager.removeMountTableEntry(req2);\n+    }\n+  }\n+\n+  private boolean addMountTable(final MountTable entry) throws IOException {\n+    RouterClient client = routerContext.getAdminClient();\n+    MountTableManager mountTableManager = client.getMountTableManager();\n+    AddMountTableEntryRequest addRequest =\n+        AddMountTableEntryRequest.newInstance(entry);\n+    AddMountTableEntryResponse addResponse =\n+        mountTableManager.addMountTableEntry(addRequest);\n+    // Reload the Router cache\n+    mountTable.loadCache(true);\n+    return addResponse.getStatus();\n+  }\n+\n+  @Test\n+  public void testFsck() throws Exception {\n+    MountTable addEntry = MountTable.newInstance(\"/testdir\",\n+        Collections.singletonMap(\"ns0\", \"/testdir\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    addEntry = MountTable.newInstance(\"/testdir2\",\n+        Collections.singletonMap(\"ns1\", \"/testdir2\"));\n+    Assert.assertTrue(addMountTable(addEntry));\n+    // create 1 file on ns0\n+    routerFs.createNewFile(new Path(\"/testdir/testfile\"));\n+    // create 3 files on ns1\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile2\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile3\"));\n+    routerFs.createNewFile(new Path(\"/testdir2/testfile4\"));\n+\n+    try (CloseableHttpClient httpClient = HttpClients.createDefault()) {\n+      // TODO: support https\n+      HttpGet httpGet = new HttpGet(\"http://\" + webAddress.getHostName() +\n+              \":\" + webAddress.getPort() + \"/fsck\");\n+      try (CloseableHttpResponse httpResponse = httpClient.execute(httpGet)) {\n+        Assert.assertEquals(HttpStatus.SC_OK,\n+            httpResponse.getStatusLine().getStatusCode());\n+        String out = EntityUtils.toString(\n+            httpResponse.getEntity(), StandardCharsets.UTF_8);\n+        System.out.println(out);", "originalCommit": "d697bee4bb006190297e2fa4d918773caac5c93c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5NDA5Mg==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376094092", "bodyText": "Can we check for this with the webAddress in the unit test?", "author": "goiri", "createdAt": "2020-02-06T21:33:30Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.net.URL;\n+import java.net.URLConnection;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hdfs.server.federation.resolver.FederationNamenodeServiceState;\n+import org.apache.hadoop.hdfs.server.federation.store.MembershipStore;\n+import org.apache.hadoop.hdfs.server.federation.store.StateStoreService;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MembershipState;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Wrapper for the Router to offer the Namenode FSCK.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsck {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(RouterFsck.class.getName());\n+\n+  private final Router router;\n+  private final InetAddress remoteAddress;\n+  private final PrintWriter out;\n+  private final Map<String, String[]> pmap;\n+\n+  public RouterFsck(Router router, Map<String, String[]> pmap,\n+                    PrintWriter out, InetAddress remoteAddress) {\n+    this.router = router;\n+    this.remoteAddress = remoteAddress;\n+    this.out = out;\n+    this.pmap = pmap;\n+  }\n+\n+  public void fsck() {\n+    final long startTime = Time.monotonicNow();\n+    try {\n+      String msg = \"Federated FSCK started by \" +\n+          UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress +\n+          \" at \" + new Date();\n+      LOG.info(msg);\n+      out.println(msg);\n+\n+      // Check each Namenode in the federation\n+      StateStoreService stateStore = router.getStateStore();\n+      MembershipStore membership =\n+          stateStore.getRegisteredRecordStore(MembershipStore.class);\n+      GetNamenodeRegistrationsRequest request =\n+          GetNamenodeRegistrationsRequest.newInstance();\n+      GetNamenodeRegistrationsResponse response =\n+          membership.getNamenodeRegistrations(request);\n+      List<MembershipState> memberships = response.getNamenodeMemberships();\n+      Collections.sort(memberships);\n+      for (MembershipState nn : memberships) {\n+        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\n+          try {\n+            String webAddress = nn.getWebAddress();\n+            out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");", "originalCommit": "d697bee4bb006190297e2fa4d918773caac5c93c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjIxMzc1Ng==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376213756", "bodyText": "Added the check in the unit test", "author": "aajisaka", "createdAt": "2020-02-07T04:52:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjA5NDA5Mg=="}], "type": "inlineReview"}, {"oid": "0d863df97f67be04bc6200d541ccf85fafc89d2e", "url": "https://github.com/apache/hadoop/commit/0d863df97f67be04bc6200d541ccf85fafc89d2e", "message": "Reflect Inigo's comments.\n\n* Removed unnecessary comment\n* Avoid writing to stdout in the unit test\n* Use static imports for the assertions", "committedDate": "2020-02-07T04:11:00Z", "type": "commit"}, {"oid": "1cf20212e4fb68ad3d70b9994d3408fb063065fe", "url": "https://github.com/apache/hadoop/commit/1cf20212e4fb68ad3d70b9994d3408fb063065fe", "message": "Fix findbugs warning", "committedDate": "2020-02-07T04:13:41Z", "type": "commit"}, {"oid": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa", "url": "https://github.com/apache/hadoop/commit/8d8e25fa7cba84c2b350c4b97f22524b29cee2fa", "message": "Add assertion for NN webaddress", "committedDate": "2020-02-07T04:37:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjQwNw==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562407", "bodyText": "Too long.", "author": "goiri", "createdAt": "2020-02-07T19:19:08Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsck.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.net.URL;\n+import java.net.URLConnection;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.hdfs.server.federation.resolver.FederationNamenodeServiceState;\n+import org.apache.hadoop.hdfs.server.federation.store.MembershipStore;\n+import org.apache.hadoop.hdfs.server.federation.store.StateStoreService;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsRequest;\n+import org.apache.hadoop.hdfs.server.federation.store.protocol.GetNamenodeRegistrationsResponse;\n+import org.apache.hadoop.hdfs.server.federation.store.records.MembershipState;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Wrapper for the Router to offer the Namenode FSCK.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsck {\n+\n+  public static final Logger LOG =\n+      LoggerFactory.getLogger(RouterFsck.class.getName());\n+\n+  private final Router router;\n+  private final InetAddress remoteAddress;\n+  private final PrintWriter out;\n+  private final Map<String, String[]> pmap;\n+\n+  public RouterFsck(Router router, Map<String, String[]> pmap,\n+                    PrintWriter out, InetAddress remoteAddress) {\n+    this.router = router;\n+    this.remoteAddress = remoteAddress;\n+    this.out = out;\n+    this.pmap = pmap;\n+  }\n+\n+  public void fsck() {\n+    final long startTime = Time.monotonicNow();\n+    try {\n+      String msg = \"Federated FSCK started by \" +\n+          UserGroupInformation.getCurrentUser() + \" from \" + remoteAddress +\n+          \" at \" + new Date();\n+      LOG.info(msg);\n+      out.println(msg);\n+\n+      // Check each Namenode in the federation\n+      StateStoreService stateStore = router.getStateStore();\n+      MembershipStore membership =\n+          stateStore.getRegisteredRecordStore(MembershipStore.class);\n+      GetNamenodeRegistrationsRequest request =\n+          GetNamenodeRegistrationsRequest.newInstance();\n+      GetNamenodeRegistrationsResponse response =\n+          membership.getNamenodeRegistrations(request);\n+      List<MembershipState> memberships = response.getNamenodeMemberships();\n+      Collections.sort(memberships);\n+      for (MembershipState nn : memberships) {\n+        if (nn.getState() == FederationNamenodeServiceState.ACTIVE) {\n+          try {\n+            String webAddress = nn.getWebAddress();\n+            out.write(\"Checking \" + nn + \" at \" + webAddress + \"\\n\");\n+            remoteFsck(nn);\n+          } catch (IOException ioe) {\n+            out.println(\"Cannot query \" + nn + \": \" + ioe.getMessage() + \"\\n\");\n+          }\n+        }\n+      }\n+\n+      out.println(\"Federated FSCK ended at \" + new Date() + \" in \"\n+          + (Time.monotonicNow() - startTime + \" milliseconds\"));\n+    } catch (Exception e) {\n+      String errMsg = \"Fsck \" + e.getMessage();\n+      LOG.warn(errMsg, e);\n+      out.println(\"Federated FSCK ended at \" + new Date() + \" in \"\n+          + (Time.monotonicNow() - startTime + \" milliseconds\"));\n+      out.println(e.getMessage());\n+      out.print(\"\\n\\n\" + errMsg);\n+    } finally {\n+      out.close();\n+    }\n+  }\n+\n+  /**\n+   * Perform FSCK in a remote Namenode.\n+   *\n+   * @param nn The state of the remote NameNode\n+   * @throws IOException Failed to fsck in a remote NameNode\n+   */\n+  private void remoteFsck(MembershipState nn) throws IOException {\n+    final String scheme = nn.getWebScheme();\n+    final String webAddress = nn.getWebAddress();\n+    final String args = getURLArguments(pmap);\n+    final URL url = new URL(scheme + \"://\" + webAddress + \"/fsck?\" + args);\n+\n+    // Connect to the Namenode and output\n+    final URLConnection conn = url.openConnection();\n+    try (InputStream is = conn.getInputStream();\n+         InputStreamReader isr = new InputStreamReader(is, StandardCharsets.UTF_8);", "originalCommit": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjUxNw==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562517", "bodyText": "First sentence should end with a period. [JavadocStyle]", "author": "goiri", "createdAt": "2020-02-07T19:19:24Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable */", "originalCommit": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjU2MjcyNw==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r376562727", "bodyText": "Bad indent.\nCheck other checkstyles:\nhttps://builds.apache.org/job/hadoop-multibranch/job/PR-1832/2/artifact/out/diff-checkstyle-hadoop-hdfs-project.txt", "author": "goiri", "createdAt": "2020-02-07T19:19:52Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable */\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final String SERVLET_NAME = \"fsck\";\n+  public static final String PATH_SPEC = \"/fsck\";\n+\n+  /** Handle fsck request */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response\n+      ) throws IOException {", "originalCommit": "8d8e25fa7cba84c2b350c4b97f22524b29cee2fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "699ac37e698ef96062e5012b6b1301f8d1f53c81", "url": "https://github.com/apache/hadoop/commit/699ac37e698ef96062e5012b6b1301f8d1f53c81", "message": "Fix checkstyle warnings", "committedDate": "2020-02-10T04:07:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNjg0OQ==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r377216849", "bodyText": "HttpURLConnection.HTTP_BAD_REQUEST in java.net.", "author": "goiri", "createdAt": "2020-02-10T17:46:09Z", "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterFsckServlet.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.federation.router;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.InetAddress;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.Map;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.server.common.JspHelper;\n+import org.apache.hadoop.security.UserGroupInformation;\n+\n+/**\n+ * This class is used in Namesystem's web server to do fsck on namenode.\n+ */\n+@InterfaceAudience.Private\n+public class RouterFsckServlet extends HttpServlet {\n+  /** for java.io.Serializable. */\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final String SERVLET_NAME = \"fsck\";\n+  public static final String PATH_SPEC = \"/fsck\";\n+\n+  /** Handle fsck request. */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response)\n+      throws IOException {\n+    final Map<String, String[]> pmap = request.getParameterMap();\n+    final PrintWriter out = response.getWriter();\n+    final InetAddress remoteAddress =\n+        InetAddress.getByName(request.getRemoteAddr());\n+    final ServletContext context = getServletContext();\n+    final Configuration conf = RouterHttpServer.getConfFromContext(context);\n+    final UserGroupInformation ugi = getUGI(request, conf);\n+    try {\n+      ugi.doAs((PrivilegedExceptionAction<Object>) () -> {\n+        Router router = RouterHttpServer.getRouterFromContext(context);\n+        new RouterFsck(router, pmap, out, remoteAddress).fsck();\n+        return null;\n+      });\n+    } catch (InterruptedException e) {\n+      response.sendError(400, e.getMessage());", "originalCommit": "699ac37e698ef96062e5012b6b1301f8d1f53c81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNzQ2OA==", "url": "https://github.com/apache/hadoop/pull/1832#discussion_r377217468", "bodyText": "Or HttpStatus.SC_BAD_REQUEST if you want to follow the same as the unit test.", "author": "goiri", "createdAt": "2020-02-10T17:47:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzIxNjg0OQ=="}], "type": "inlineReview"}, {"oid": "6680f58d3507d5c02c789fbd6ef4b7853d47caf3", "url": "https://github.com/apache/hadoop/commit/6680f58d3507d5c02c789fbd6ef4b7853d47caf3", "message": "Replace 400 with HttpURLConnection.HTTP_BAD_REQUEST", "committedDate": "2020-02-12T01:27:03Z", "type": "commit"}]}