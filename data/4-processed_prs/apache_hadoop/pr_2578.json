{"pr_number": 2578, "pr_title": "HDFS-15754. Add DataNode packet metrics", "pr_createdAt": "2020-12-29T22:59:46Z", "pr_url": "https://github.com/apache/hadoop/pull/2578", "timeline": [{"oid": "1da0b9f849271744214d9474f6f0a514085dd36b", "url": "https://github.com/apache/hadoop/commit/1da0b9f849271744214d9474f6f0a514085dd36b", "message": "[HDFS-15754] Add DataNode packet metrics", "committedDate": "2020-12-29T22:58:56Z", "type": "commit"}, {"oid": "1da0b9f849271744214d9474f6f0a514085dd36b", "url": "https://github.com/apache/hadoop/commit/1da0b9f849271744214d9474f6f0a514085dd36b", "message": "[HDFS-15754] Add DataNode packet metrics", "committedDate": "2020-12-29T22:58:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzA4MA==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r549883080", "bodyText": "We'll need to add these new metrics to here right?", "author": "sunchao", "createdAt": "2020-12-29T23:11:50Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -183,6 +183,11 @@\n   @Metric private MutableRate checkAndUpdateOp;\n   @Metric private MutableRate updateReplicaUnderRecoveryOp;\n \n+  @Metric MutableCounterLong totalPacketsReceived;", "originalCommit": "1da0b9f849271744214d9474f6f0a514085dd36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTE0OTgwMw==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r551149803", "bodyText": "Done", "author": "fengnanli", "createdAt": "2021-01-04T07:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzE1OA==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r549883158", "bodyText": "nit: code style", "author": "sunchao", "createdAt": "2020-12-29T23:12:09Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).stopSendingPacketDownstream(Mockito.anyString());\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).delayWriteToOsCache();\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      }).when(injector).delayWriteToDisk();\n+      DataNodeFaultInjector.set(injector);\n+      Path testFile = new Path(\"/testFlushNanosMetric.txt\");\n+      FSDataOutputStream fout = fs.create(testFile);\n+      fout.write(new byte[1]);\n+      fout.hsync();\n+      fout.close();\n+      List<DataNode> datanodes = cluster.getDataNodes();\n+      DataNode datanode = datanodes.get(0);\n+      MetricsRecordBuilder dnMetrics = getMetrics(datanode.getMetrics().name());\n+      assertTrue(\"More than 1 packet received\",\n+          getLongCounter(\"TotalPacketsReceived\", dnMetrics) > 1L);\n+      assertTrue(\"More than 1 slow packet to mirror\",\n+          getLongCounter(\"TotalPacketsSlowWriteToMirror\", dnMetrics) > 1L);\n+      assertCounter(\"TotalPacketsSlowWriteToDisk\", 1L, dnMetrics);\n+      assertCounter(\"TotalPacketsSlowWriteOsCache\", 0L, dnMetrics);\n+    } finally {\n+      if (cluster != null) {cluster.shutdown();}", "originalCommit": "1da0b9f849271744214d9474f6f0a514085dd36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTE0OTg2MA==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r551149860", "bodyText": "Done", "author": "fengnanli", "createdAt": "2021-01-04T07:19:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTg4MzE1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3NzczMA==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550277730", "bodyText": "As we are at it, let's use the logger format. We still need the Arrays toString so we need the isWarnEnabled though.", "author": "goiri", "createdAt": "2020-12-30T17:55:47Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java", "diffHunk": "@@ -603,12 +604,15 @@ private int receivePacket() throws IOException {\n             mirrorAddr,\n             duration);\n         trackSendPacketToLastNodeInPipeline(duration);\n-        if (duration > datanodeSlowLogThresholdMs && LOG.isWarnEnabled()) {\n-          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n-              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms), \"\n-              + \"downstream DNs=\" + Arrays.toString(downstreamDNs)\n-              + \", blockId=\" + replicaInfo.getBlockId()\n-              + \", seqno=\" + seqno);\n+        if (duration > datanodeSlowLogThresholdMs) {\n+          datanode.metrics.incrPacketSlowWriteToMirror();\n+          if (LOG.isWarnEnabled()) {\n+            LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n+                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms), \"\n+                + \"downstream DNs=\" + Arrays.toString(downstreamDNs)\n+                + \", blockId=\" + replicaInfo.getBlockId()", "originalCommit": "1da0b9f849271744214d9474f6f0a514085dd36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTE1MjM2MQ==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r551152361", "bodyText": "Done", "author": "fengnanli", "createdAt": "2021-01-04T07:28:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3NzczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODA2Mw==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550278063", "bodyText": "setInt", "author": "goiri", "createdAt": "2020-12-30T17:56:48Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);", "originalCommit": "1da0b9f849271744214d9474f6f0a514085dd36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTE1MjUwMQ==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r551152501", "bodyText": "Done", "author": "fengnanli", "createdAt": "2021-01-04T07:28:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODI0Ng==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r550278246", "bodyText": "Extract the sleeping answer and return for each?", "author": "goiri", "createdAt": "2020-12-30T17:57:25Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,65 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.set(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, \"\" + interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Mockito.doAnswer(new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)", "originalCommit": "1da0b9f849271744214d9474f6f0a514085dd36b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTE1MjUzNQ==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r551152535", "bodyText": "Done", "author": "fengnanli", "createdAt": "2021-01-04T07:28:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3ODI0Ng=="}], "type": "inlineReview"}, {"oid": "05f04b1ca4ced0849b1fc0685077f88db2a47027", "url": "https://github.com/apache/hadoop/commit/05f04b1ca4ced0849b1fc0685077f88db2a47027", "message": "Address comments", "committedDate": "2021-01-04T07:30:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzMzNg==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r552933336", "bodyText": "nit: name this to incrPacketsSlowWriteToOsCache?", "author": "sunchao", "createdAt": "2021-01-06T19:57:51Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -690,4 +695,20 @@ public void addCheckAndUpdateOp(long latency) {\n   public void addUpdateReplicaUnderRecoveryOp(long latency) {\n     updateReplicaUnderRecoveryOp.add(latency);\n   }\n+\n+  public void incrPacketsReceived() {\n+    packetsReceived.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteToMirror() {\n+    packetsSlowWriteToMirror.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteToDisk() {\n+    packetsSlowWriteToDisk.incr();\n+  }\n+\n+  public void incrPacketsSlowWriteOsCache() {", "originalCommit": "05f04b1ca4ced0849b1fc0685077f88db2a47027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzQ0Mg==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r552933442", "bodyText": "ditto", "author": "sunchao", "createdAt": "2021-01-06T19:58:05Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/metrics/DataNodeMetrics.java", "diffHunk": "@@ -183,6 +183,11 @@\n   @Metric private MutableRate checkAndUpdateOp;\n   @Metric private MutableRate updateReplicaUnderRecoveryOp;\n \n+  @Metric MutableCounterLong packetsReceived;\n+  @Metric MutableCounterLong packetsSlowWriteToMirror;\n+  @Metric MutableCounterLong packetsSlowWriteToDisk;\n+  @Metric MutableCounterLong packetsSlowWriteOsCache;", "originalCommit": "05f04b1ca4ced0849b1fc0685077f88db2a47027", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3be5d26cec4c93b5b47ef795ee320b38e5a9356d", "url": "https://github.com/apache/hadoop/commit/3be5d26cec4c93b5b47ef795ee320b38e5a9356d", "message": "Rename", "committedDate": "2021-01-07T00:37:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzEyODgzNA==", "url": "https://github.com/apache/hadoop/pull/2578#discussion_r553128834", "bodyText": "I think this also needs update.", "author": "sunchao", "createdAt": "2021-01-07T06:14:10Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeMetrics.java", "diffHunk": "@@ -161,6 +163,53 @@ public void testReceivePacketMetrics() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testReceivePacketSlowMetrics() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    final int interval = 1;\n+    conf.setInt(DFSConfigKeys.DFS_METRICS_PERCENTILES_INTERVALS_KEY, interval);\n+    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(3).build();\n+    try {\n+      cluster.waitActive();\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      final DataNodeFaultInjector injector =\n+          Mockito.mock(DataNodeFaultInjector.class);\n+      Answer answer = new Answer() {\n+        @Override\n+        public Object answer(InvocationOnMock invocationOnMock)\n+            throws Throwable {\n+          // make the op taking longer time\n+          Thread.sleep(1000);\n+          return null;\n+        }\n+      };\n+      Mockito.doAnswer(answer).when(injector).\n+          stopSendingPacketDownstream(Mockito.anyString());\n+      Mockito.doAnswer(answer).when(injector).delayWriteToOsCache();\n+      Mockito.doAnswer(answer).when(injector).delayWriteToDisk();\n+      DataNodeFaultInjector.set(injector);\n+      Path testFile = new Path(\"/testFlushNanosMetric.txt\");\n+      FSDataOutputStream fout = fs.create(testFile);\n+      fout.write(new byte[1]);\n+      fout.hsync();\n+      fout.close();\n+      List<DataNode> datanodes = cluster.getDataNodes();\n+      DataNode datanode = datanodes.get(0);\n+      MetricsRecordBuilder dnMetrics = getMetrics(datanode.getMetrics().name());\n+      assertTrue(\"More than 1 packet received\",\n+          getLongCounter(\"TotalPacketsReceived\", dnMetrics) > 1L);\n+      assertTrue(\"More than 1 slow packet to mirror\",\n+          getLongCounter(\"TotalPacketsSlowWriteToMirror\", dnMetrics) > 1L);\n+      assertCounter(\"TotalPacketsSlowWriteToDisk\", 1L, dnMetrics);\n+      assertCounter(\"TotalPacketsSlowWriteOsCache\", 0L, dnMetrics);", "originalCommit": "3be5d26cec4c93b5b47ef795ee320b38e5a9356d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7fa02862558ce5ffb3e4b5f20e1f7506730339e6", "url": "https://github.com/apache/hadoop/commit/7fa02862558ce5ffb3e4b5f20e1f7506730339e6", "message": "Fix test", "committedDate": "2021-01-07T07:32:11Z", "type": "commit"}]}