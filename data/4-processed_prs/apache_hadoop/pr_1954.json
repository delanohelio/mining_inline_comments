{"pr_number": 1954, "pr_title": "HDFS-15217 Add more information to longest write/read lock held log", "pr_createdAt": "2020-04-11T14:04:59Z", "pr_url": "https://github.com/apache/hadoop/pull/1954", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODc3NA==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407098774", "bodyText": "Can we just keep the old constructor with the old parameters that pass the null to the new one instead of changing everywhere?", "author": "goiri", "createdAt": "2020-04-11T19:00:12Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -106,8 +107,8 @@ public Long initialValue() {\n    * lock was held since the last report.\n    */\n   private final AtomicReference<LockHeldInfo> longestReadLockHeldInfo =\n-      new AtomicReference<>(new LockHeldInfo(0, 0, null));\n-  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null);\n+      new AtomicReference<>(new LockHeldInfo(0, 0, null, null, null));\n+  private LockHeldInfo longestWriteLockHeldInfo = new LockHeldInfo(0, 0, null, null, null);", "originalCommit": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE0MTQ4OA==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407141488", "bodyText": "Maybe it's better to have default constructor (without any parameters) of LockHeldInfo for those initial instances? I will try to do that.", "author": "brfrn169", "createdAt": "2020-04-12T03:48:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODc3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5ODk0OQ==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407098949", "bodyText": "As we are touching this, can we make this into a regular:\nboolean done = false;\nwhile (!done) {\n...\n} else {\ndone = true;\n}\n}", "author": "goiri", "createdAt": "2020-04-11T19:02:08Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -176,13 +181,23 @@ public void readUnlock(String opName) {\n     final long readLockIntervalMs =\n         TimeUnit.NANOSECONDS.toMillis(readLockIntervalNanos);\n     if (needReport && readLockIntervalMs >= this.readLockReportingThresholdMs) {\n-      LockHeldInfo localLockHeldInfo;\n+      String lockReportInfo = null;\n       do {\n-        localLockHeldInfo = longestReadLockHeldInfo.get();\n-      } while (localLockHeldInfo.getIntervalMs() - readLockIntervalMs < 0 &&\n-          !longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n-              new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n-                  StringUtils.getStackTrace(Thread.currentThread()))));\n+        LockHeldInfo localLockHeldInfo = longestReadLockHeldInfo.get();\n+        if (localLockHeldInfo.getIntervalMs() <= readLockIntervalMs) {\n+          if (lockReportInfo == null) {\n+            lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+                lockReportInfoSupplier.get() + \")\" : \"\";\n+          }\n+          if (longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n+            new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n+              StringUtils.getStackTrace(Thread.currentThread()), opName, lockReportInfo))) {\n+            break;\n+          }\n+        } else {\n+          break;\n+        }\n+      } while (true);", "originalCommit": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTAwMQ==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099001", "bodyText": "Can we extract some of this? At least the stack trace.", "author": "goiri", "createdAt": "2020-04-11T19:02:46Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -176,13 +181,23 @@ public void readUnlock(String opName) {\n     final long readLockIntervalMs =\n         TimeUnit.NANOSECONDS.toMillis(readLockIntervalNanos);\n     if (needReport && readLockIntervalMs >= this.readLockReportingThresholdMs) {\n-      LockHeldInfo localLockHeldInfo;\n+      String lockReportInfo = null;\n       do {\n-        localLockHeldInfo = longestReadLockHeldInfo.get();\n-      } while (localLockHeldInfo.getIntervalMs() - readLockIntervalMs < 0 &&\n-          !longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n-              new LockHeldInfo(currentTimeMs, readLockIntervalMs,\n-                  StringUtils.getStackTrace(Thread.currentThread()))));\n+        LockHeldInfo localLockHeldInfo = longestReadLockHeldInfo.get();\n+        if (localLockHeldInfo.getIntervalMs() <= readLockIntervalMs) {\n+          if (lockReportInfo == null) {\n+            lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+                lockReportInfoSupplier.get() + \")\" : \"\";\n+          }\n+          if (longestReadLockHeldInfo.compareAndSet(localLockHeldInfo,\n+            new LockHeldInfo(currentTimeMs, readLockIntervalMs,", "originalCommit": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTEwOA==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099108", "bodyText": "Extract the trace at least.", "author": "goiri", "createdAt": "2020-04-11T19:03:45Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -253,10 +294,12 @@ public void writeUnlock(String opName, boolean suppressWriteLockReport) {\n     LogAction logAction = LogThrottlingHelper.DO_NOT_LOG;\n     if (needReport &&\n         writeLockIntervalMs >= this.writeLockReportingThresholdMs) {\n-      if (longestWriteLockHeldInfo.getIntervalMs() < writeLockIntervalMs) {\n+      if (longestWriteLockHeldInfo.getIntervalMs() <= writeLockIntervalMs) {\n+        String lockReportInfo = lockReportInfoSupplier != null ? \" (\" +\n+            lockReportInfoSupplier.get() + \")\" : \"\";\n         longestWriteLockHeldInfo =\n             new LockHeldInfo(currentTimeMs, writeLockIntervalMs,\n-                StringUtils.getStackTrace(Thread.currentThread()));\n+                StringUtils.getStackTrace(Thread.currentThread()), opName, lockReportInfo);", "originalCommit": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTU5Mw==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407099593", "bodyText": "I'm having a hard time finding who uses this Supplier. Where are we using this?", "author": "goiri", "createdAt": "2020-04-11T19:08:25Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java", "diffHunk": "@@ -159,10 +160,14 @@ public void readLockInterruptibly() throws InterruptedException {\n   }\n \n   public void readUnlock() {\n-    readUnlock(OP_NAME_OTHER);\n+    readUnlock(OP_NAME_OTHER, null);\n   }\n \n   public void readUnlock(String opName) {\n+    readUnlock(opName, null);\n+  }\n+\n+  public void readUnlock(String opName, Supplier<String> lockReportInfoSupplier) {", "originalCommit": "4197ce4f6d4456f2fe1fee6b4de240f773b0db90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzE1MjQ1OA==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407152458", "bodyText": "I use this Supplier in the following places:\nhttps://github.com/brfrn169/hadoop/blob/HDFS-15217/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java#L189-L190\nhttps://github.com/brfrn169/hadoop/blob/HDFS-15217/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystemLock.java#L298-L299\nThe reason why I use Supplier is that we don't always print the lock report, only when the lock interval is more than the threshold (dfs.namenode.write-lock-reporting-threshold-ms or dfs.namenode.read-lock-reporting-threshold-ms). We can do lazy building additional information with the lockReportInfoSupplier.", "author": "brfrn169", "createdAt": "2020-04-12T06:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5OTU5Mw=="}], "type": "inlineReview"}, {"oid": "07e50509932eb5565f6ff4e937116b932ac05998", "url": "https://github.com/apache/hadoop/commit/07e50509932eb5565f6ff4e937116b932ac05998", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-12T07:01:54Z", "type": "forcePushed"}, {"oid": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "url": "https://github.com/apache/hadoop/commit/ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-12T13:13:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTgxMw==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407235813", "bodyText": "Add a comment with the example of the actual line.", "author": "goiri", "createdAt": "2020-04-12T18:31:10Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemLockReport.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.LoggerFactory;\n+\n+import java.security.PrivilegedExceptionAction;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestFSNamesystemLockReport {\n+\n+  @FunctionalInterface\n+  private interface SupplierWithException<T> {\n+    T get() throws Exception;\n+  }\n+\n+  @FunctionalInterface\n+  private interface Procedure {\n+    void invoke() throws Exception;\n+  }\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FileSystem fs;\n+  private UserGroupInformation userGroupInfo;\n+  private GenericTestUtils.LogCapturer logs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new HdfsConfiguration();\n+    conf.set(DFS_PERMISSIONS_SUPERUSERGROUP_KEY, \"hadoop\");\n+\n+    // Make the lock report always shown\n+    conf.setLong(DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY, 0);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\n+    fs = cluster.getFileSystem();\n+\n+    userGroupInfo = UserGroupInformation.createUserForTesting(\"bob\",\n+        new String[] {\"hadoop\"});\n+\n+    logs = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.LOG);\n+    GenericTestUtils\n+        .setLogLevel(LoggerFactory.getLogger(FSNamesystem.class.getName()),\n+        org.slf4j.event.Level.INFO);\n+  }\n+\n+  @After\n+  public void cleanUp() throws Exception {\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    FileSystem userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);\n+\n+    FSDataOutputStream os = testLockReport(() ->", "originalCommit": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI5NDQ5MA==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407294490", "bodyText": "I posted the example in the Jira:\nhttps://issues.apache.org/jira/browse/HDFS-15217\nDo we still need to put it here?", "author": "brfrn169", "createdAt": "2020-04-13T02:57:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTgxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzYwNTQzNw==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407605437", "bodyText": "I think it would help for each of the lines we check what would be an example line.", "author": "goiri", "createdAt": "2020-04-13T17:31:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNjQzMw==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r407236433", "bodyText": "This is the same in the previous method. Extract?", "author": "goiri", "createdAt": "2020-04-12T18:36:26Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFSNamesystemLockReport.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.slf4j.LoggerFactory;\n+\n+import java.security.PrivilegedExceptionAction;\n+\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_PERMISSIONS_SUPERUSERGROUP_KEY;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestFSNamesystemLockReport {\n+\n+  @FunctionalInterface\n+  private interface SupplierWithException<T> {\n+    T get() throws Exception;\n+  }\n+\n+  @FunctionalInterface\n+  private interface Procedure {\n+    void invoke() throws Exception;\n+  }\n+\n+  private Configuration conf;\n+  private MiniDFSCluster cluster;\n+  private FileSystem fs;\n+  private UserGroupInformation userGroupInfo;\n+  private GenericTestUtils.LogCapturer logs;\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    conf = new HdfsConfiguration();\n+    conf.set(DFS_PERMISSIONS_SUPERUSERGROUP_KEY, \"hadoop\");\n+\n+    // Make the lock report always shown\n+    conf.setLong(DFS_NAMENODE_READ_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_NAMENODE_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY, 0);\n+    conf.setLong(DFS_LOCK_SUPPRESS_WARNING_INTERVAL_KEY, 0);\n+\n+    cluster = new MiniDFSCluster.Builder(conf).numDataNodes(4).build();\n+    fs = cluster.getFileSystem();\n+\n+    userGroupInfo = UserGroupInformation.createUserForTesting(\"bob\",\n+        new String[] {\"hadoop\"});\n+\n+    logs = GenericTestUtils.LogCapturer.captureLogs(FSNamesystem.LOG);\n+    GenericTestUtils\n+        .setLogLevel(LoggerFactory.getLogger(FSNamesystem.class.getName()),\n+        org.slf4j.event.Level.INFO);\n+  }\n+\n+  @After\n+  public void cleanUp() throws Exception {\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+    if (cluster != null) {\n+      cluster.shutdown();\n+      cluster = null;\n+    }\n+  }\n+\n+  @Test\n+  public void test() throws Exception {\n+    FileSystem userfs = DFSTestUtil.getFileSystemAs(userGroupInfo, conf);\n+\n+    FSDataOutputStream os = testLockReport(() ->\n+        userfs.create(new Path(\"/file\")),\n+        \".* by create \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=bob:hadoop:rw-r--r--\\\\) .*\");\n+    os.close();\n+\n+    FSDataInputStream is = testLockReport(() -> userfs.open(new Path(\"/file\")),\n+        \".* by open \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+    is.close();\n+\n+    testLockReport(() ->\n+        userfs.setPermission(new Path(\"/file\"), new FsPermission(644)),\n+        \".* by setPermission \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=bob:hadoop:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.setOwner(new Path(\"/file\"), \"alice\", \"group1\"),\n+        \".* by setOwner \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=alice:group1:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.listStatus(new Path(\"/\")),\n+        \".* by listStatus \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+\n+    testLockReport(() -> userfs.getFileStatus(new Path(\"/file\")),\n+        \".* by getfileinfo \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+\n+    testLockReport(() -> userfs.mkdirs(new Path(\"/dir\")),\n+        \".* by mkdirs \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/dir,dst=null,\" +\n+        \"perm=bob:hadoop:rwxr-xr-x\\\\) .*\");\n+\n+    testLockReport(() -> userfs.rename(new Path(\"/file\"), new Path(\"/file2\")),\n+        \".* by rename \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file,dst=/file2,\" +\n+        \"perm=alice:group1:-w----r-T\\\\) .*\");\n+\n+    testLockReport(() -> userfs.delete(new Path(\"/file2\"), false),\n+        \".* by delete \\\\(ugi=bob \\\\(auth:SIMPLE\\\\),\" +\n+        \"ip=/\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3},src=/file2,dst=null,\" +\n+        \"perm=null\\\\) .*\");\n+  }\n+\n+  private void testLockReport(Procedure procedure,\n+      String expectedLockReportRegex) throws Exception {\n+    logs.clearOutput();\n+    userGroupInfo.doAs((PrivilegedExceptionAction<Void>) () -> {\n+      procedure.invoke();\n+      return null;\n+    });\n+\n+    boolean matches = false;\n+    for (String line : logs.getOutput().split(System.lineSeparator())) {\n+      if (line.matches(expectedLockReportRegex)) {\n+        matches = true;\n+        break;\n+      }\n+    }\n+    assertTrue(matches);\n+  }\n+\n+  private <T> T testLockReport(SupplierWithException<T> supplier,\n+      String expectedLockReportRegex) throws Exception {\n+    logs.clearOutput();\n+    T ret = userGroupInfo.doAs((PrivilegedExceptionAction<T>) supplier::get);\n+\n+    boolean matches = false;", "originalCommit": "ef6802b3b05ba4f1ba92c89c2ddb6a6e031dd80c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "855db207d0bd75142a44433016f5a26962cf6d26", "url": "https://github.com/apache/hadoop/commit/855db207d0bd75142a44433016f5a26962cf6d26", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-13T03:01:18Z", "type": "forcePushed"}, {"oid": "16ad10709dc319b740371af2ecbb22c836d4b45c", "url": "https://github.com/apache/hadoop/commit/16ad10709dc319b740371af2ecbb22c836d4b45c", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-14T02:27:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzMjY5OQ==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r409032699", "bodyText": "I think you need to rebase, right?", "author": "goiri", "createdAt": "2020-04-15T18:02:56Z", "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "diffHunk": "@@ -3204,11 +3235,12 @@ void renameTo(final String src, final String dst,\n         res = FSDirRenameOp.renameToInt(dir, pc, src, dst, logRetryCache,\n             options);\n       } finally {\n-        writeUnlock(operationName);\n+        FileStatus status = res != null ? res.auditStat : null;\n+        writeUnlock(operationName,\n+            getLockReportInfoSupplier(src, dst, status));\n       }\n     } catch (AccessControlException e) {\n-      logAuditEvent(false, operationName + \" (options=\" +\n-          Arrays.toString(options) + \")\", src, dst, null);\n+      logAuditEvent(false, operationName, src, dst, null);", "originalCommit": "16ad10709dc319b740371af2ecbb22c836d4b45c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwMzQ5Mg==", "url": "https://github.com/apache/hadoop/pull/1954#discussion_r409103492", "bodyText": "Thank you for pointing out this. I modified the patch for the review.", "author": "brfrn169", "createdAt": "2020-04-15T20:08:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTAzMjY5OQ=="}], "type": "inlineReview"}, {"oid": "190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "url": "https://github.com/apache/hadoop/commit/190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-15T20:06:11Z", "type": "commit"}, {"oid": "190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "url": "https://github.com/apache/hadoop/commit/190bdd3d2a57fddae04b4d9b1504c9d1fc7295e5", "message": "HDFS-15217 Add more information to longest write/read lock held log", "committedDate": "2020-04-15T20:06:11Z", "type": "forcePushed"}]}