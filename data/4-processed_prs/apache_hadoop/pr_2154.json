{"pr_number": 2154, "pr_title": "HADOOP-17113. Adding ReadAhead Counters in ABFS", "pr_createdAt": "2020-07-20T15:46:41Z", "pr_url": "https://github.com/apache/hadoop/pull/2154", "timeline": [{"oid": "ba2869a0866f40d918e817b7e6119c2f65c71acb", "url": "https://github.com/apache/hadoop/commit/ba2869a0866f40d918e817b7e6119c2f65c71acb", "message": "HADOOP-17113. Adding ReadAhead Counters in ABFS", "committedDate": "2020-07-20T15:43:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTI2Nw==", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655267", "bodyText": "nice explanation", "author": "steveloughran", "createdAt": "2020-07-20T19:54:52Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.", "originalCommit": "ba2869a0866f40d918e817b7e6119c2f65c71acb", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTY0Mg==", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457655642", "bodyText": "Try using AssertJ.assertThat here, it lets you declare the specific \"isGreaterThan\" assertion; it's describedAs() does the string formatting too.", "author": "steveloughran", "createdAt": "2020-07-20T19:55:36Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +291,98 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      assertTrue(String.format(\"actual value of %d is not greater than or \"", "originalCommit": "ba2869a0866f40d918e817b7e6119c2f65c71acb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0MTIyMA==", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457841220", "bodyText": "It would be good to add it in this patch. Thanks for the tip.", "author": "mehakmeet", "createdAt": "2020-07-21T05:15:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY1NTY0Mg=="}], "type": "inlineReview"}, {"oid": "2f3763d5fb11897063b1571cb546734d7b6d8c85", "url": "https://github.com/apache/hadoop/commit/2f3763d5fb11897063b1571cb546734d7b6d8c85", "message": "HADOOP-17113. Adding assertThat", "committedDate": "2020-07-21T05:14:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk5MTEwOA==", "url": "https://github.com/apache/hadoop/pull/2154#discussion_r457991108", "bodyText": "can't we just throw this? If not, at least use LOG", "author": "steveloughran", "createdAt": "2020-07-21T10:19:03Z", "path": "hadoop-tools/hadoop-azure/src/test/java/org/apache/hadoop/fs/azurebfs/ITestAbfsInputStreamStatistics.java", "diffHunk": "@@ -285,6 +292,96 @@ public void testWithNullStreamStatistics() throws IOException {\n     }\n   }\n \n+  /**\n+   * Testing readAhead counters in AbfsInputStream with 30 seconds timeout.\n+   */\n+  @Test(timeout = TIMEOUT_30_SECONDS)\n+  public void testReadAheadCounters() throws IOException {\n+    describe(\"Test to check correct values for readAhead counters in \"\n+        + \"AbfsInputStream\");\n+\n+    AzureBlobFileSystem fs = getFileSystem();\n+    AzureBlobFileSystemStore abfss = fs.getAbfsStore();\n+    Path readAheadCountersPath = path(getMethodName());\n+\n+    /*\n+     * Setting the block size for readAhead as 4KB.\n+     */\n+    abfss.getAbfsConfiguration().setReadBufferSize(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+    AbfsOutputStream out = null;\n+    AbfsInputStream in = null;\n+\n+    try {\n+\n+      /*\n+       * Creating a file of 1MB size.\n+       */\n+      out = createAbfsOutputStreamWithFlushEnabled(fs, readAheadCountersPath);\n+      out.write(defBuffer);\n+      out.close();\n+\n+      in = abfss.openFileForRead(readAheadCountersPath, fs.getFsStatistics());\n+\n+      /*\n+       * Reading 1KB after each i * KB positions. Hence the reads are from 0\n+       * to 1KB, 1KB to 2KB, and so on.. for 5 operations.\n+       */\n+      for (int i = 0; i < 5; i++) {\n+        in.seek(ONE_KB * i);\n+        in.read(defBuffer, ONE_KB * i, ONE_KB);\n+      }\n+      AbfsInputStreamStatisticsImpl stats =\n+          (AbfsInputStreamStatisticsImpl) in.getStreamStatistics();\n+\n+      /*\n+       * Since, readAhead is done in background threads. Sometimes, the\n+       * threads aren't finished in the background and could result in\n+       * inaccurate results. So, we wait till we have the accurate values\n+       * with a limit of 30 seconds as that's when the test times out.\n+       *\n+       */\n+      while (stats.getRemoteBytesRead() < CUSTOM_READ_AHEAD_BUFFER_SIZE\n+          || stats.getReadAheadBytesRead() < CUSTOM_BLOCK_BUFFER_SIZE) {\n+        Thread.sleep(THREAD_SLEEP_10_SECONDS);\n+      }\n+\n+      /*\n+       * Verifying the counter values of readAheadBytesRead and remoteBytesRead.\n+       *\n+       * readAheadBytesRead : Since, we read 1KBs 5 times, that means we go\n+       * from 0 to 5KB in the file. The bufferSize is set to 4KB, and since\n+       * we have 8 blocks of readAhead buffer. We would have 8 blocks of 4KB\n+       * buffer. Our read is till 5KB, hence readAhead would ideally read 2\n+       * blocks of 4KB which is equal to 8KB. But, sometimes to get more than\n+       * one block from readAhead buffer we might have to wait for background\n+       * threads to fill the buffer and hence we might do remote read which\n+       * would be faster. Therefore, readAheadBytesRead would be equal to or\n+       * greater than 4KB.\n+       *\n+       * remoteBytesRead : Since, the bufferSize is set to 4KB and the number\n+       * of blocks or readAheadQueueDepth is equal to 8. We would read 8 * 4\n+       * KB buffer on the first read, which is equal to 32KB. But, if we are not\n+       * able to read some bytes that were in the buffer after doing\n+       * readAhead, we might use remote read again. Thus, the bytes read\n+       * remotely could also be greater than 32Kb.\n+       *\n+       */\n+      Assertions.assertThat(stats.getReadAheadBytesRead()).describedAs(\n+          \"Mismatch in readAheadBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_BLOCK_BUFFER_SIZE);\n+\n+      Assertions.assertThat(stats.getRemoteBytesRead()).describedAs(\n+          \"Mismatch in remoteBytesRead counter value\")\n+          .isGreaterThanOrEqualTo(CUSTOM_READ_AHEAD_BUFFER_SIZE);\n+\n+    } catch (InterruptedException e) {\n+      e.printStackTrace();", "originalCommit": "2f3763d5fb11897063b1571cb546734d7b6d8c85", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "84b282c2ceb2602590c938a28db7e2f996c84963", "url": "https://github.com/apache/hadoop/commit/84b282c2ceb2602590c938a28db7e2f996c84963", "message": "HADOOP-17113. adding exception to method", "committedDate": "2020-07-21T11:22:13Z", "type": "commit"}]}