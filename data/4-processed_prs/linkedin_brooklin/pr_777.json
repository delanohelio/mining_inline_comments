{"pr_number": 777, "pr_title": "Handle Exception in seekToLastCheckpoint in AbstractKafkaMirrorMakerConnectorTask", "pr_createdAt": "2020-11-05T18:27:23Z", "pr_url": "https://github.com/linkedin/brooklin/pull/777", "timeline": [{"oid": "c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "url": "https://github.com/linkedin/brooklin/commit/c31cd4a15cc8dd69b0a653dbe4201de934ea6d65", "message": "Merge pull request #1 from linkedin/master\n\nPull latest", "committedDate": "2019-11-18T20:06:44Z", "type": "commit"}, {"oid": "79ffa91188aa16ebcb2e9c1e55d4b9a8911b9c36", "url": "https://github.com/linkedin/brooklin/commit/79ffa91188aa16ebcb2e9c1e55d4b9a8911b9c36", "message": "Merge branch 'master' of github.com:linkedin/brooklin", "committedDate": "2020-11-02T20:44:50Z", "type": "commit"}, {"oid": "2244a93bc0c319c099301feec5d3f0f79297e3c1", "url": "https://github.com/linkedin/brooklin/commit/2244a93bc0c319c099301feec5d3f0f79297e3c1", "message": "Handle WakupException in seekToLastCheckpoint", "committedDate": "2020-11-05T07:16:21Z", "type": "commit"}, {"oid": "5b3ca1a8e9e0b71463d2becd6113502888f36387", "url": "https://github.com/linkedin/brooklin/commit/5b3ca1a8e9e0b71463d2becd6113502888f36387", "message": "Handle exception for flushless producer", "committedDate": "2020-11-05T18:25:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI3OTcwOQ==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518279709", "bodyText": "Please catch only the WakeupException here to commit safe offsets to ensure we only commit here on the shutdown path. I want to make sure that we understand any other exceptions thrown before we add code to catch those and commit safe offsets.", "author": "somandal", "createdAt": "2020-11-05T18:42:40Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTask.java", "diffHunk": "@@ -485,6 +483,30 @@ public KafkaDatastreamStatesResponse getKafkaDatastreamStatesResponse() {\n         _isFlushlessModeEnabled ? _flushlessProducer.getInFlightMessagesCounts() : Collections.emptyMap());\n   }\n \n+  @VisibleForTesting\n+  protected void seekToLastCheckpoint(Set<TopicPartition> topicPartitions) {\n+    try {\n+      super.seekToLastCheckpoint(topicPartitions);\n+    } catch (Exception e) {", "originalCommit": "5b3ca1a8e9e0b71463d2becd6113502888f36387", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxNjMzNw==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518416337", "bodyText": "The reason to catch any exception is, it does not really matter. safeOffset should be good to commit irrespective of the exception type.", "author": "vmaheshw", "createdAt": "2020-11-05T22:46:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI3OTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI3OTkzOA==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518279938", "bodyText": "nit: add a space after \"//\"\nreword: Flushless mode tracks the successfully received acks, so it is safe to commit offsets even if flush throws an exception. Commit the safe offsets to reduce send duplication.", "author": "somandal", "createdAt": "2020-11-05T18:43:04Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTask.java", "diffHunk": "@@ -376,14 +376,12 @@ protected void maybeCommitOffsets(Consumer<?, ?> consumer, boolean hardCommit) {\n     if (_isFlushlessModeEnabled) {\n       if (hardCommit) { // hard commit (flush and commit checkpoints)\n         LOG.info(\"Calling flush on the producer.\");\n-        _datastreamTask.getEventProducer().flush();\n-        // Flush may succeed even though some of the records received send failures. Flush only guarantees that all\n-        // outstanding send() calls have completed, without providing any guarantees about their successful completion.\n-        // Thus it is possible that some send callbacks returned an exception and such TopicPartitions must be rewound\n-        // to their last committed offset to avoid data loss.\n-        rewindAndPausePartitionsOnSendException();\n-        commitSafeOffsets(consumer);\n-\n+        try {\n+          _datastreamTask.getEventProducer().flush();\n+        } finally {\n+          //committing the safe offsets will reduce the send duplication.", "originalCommit": "5b3ca1a8e9e0b71463d2becd6113502888f36387", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI4MTc4NA==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518281784", "bodyText": "Can you add a comment explaining why you aren't just calling super.getLastCheckpointToSeekTo(lastCheckpoint, tpWithNoCommits, tp);", "author": "somandal", "createdAt": "2020-11-05T18:46:18Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/mirrormaker/KafkaMirrorMakerConnectorTask.java", "diffHunk": "@@ -485,6 +483,30 @@ public KafkaDatastreamStatesResponse getKafkaDatastreamStatesResponse() {\n         _isFlushlessModeEnabled ? _flushlessProducer.getInFlightMessagesCounts() : Collections.emptyMap());\n   }\n \n+  @VisibleForTesting\n+  protected void seekToLastCheckpoint(Set<TopicPartition> topicPartitions) {\n+    try {\n+      super.seekToLastCheckpoint(topicPartitions);\n+    } catch (Exception e) {\n+      commitSafeOffsets(_consumer);\n+      throw e;\n+    }\n+  }\n+\n+  @Override\n+  protected void getLastCheckpointToSeekTo(Map<TopicPartition, OffsetAndMetadata> lastCheckpoint,\n+      Set<TopicPartition> tpWithNoCommits, TopicPartition tp) {\n+    if (_isFlushlessModeEnabled) {", "originalCommit": "5b3ca1a8e9e0b71463d2becd6113502888f36387", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI4OTM4MQ==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518289381", "bodyText": "was removing this error log intended? if so, why?", "author": "somandal", "createdAt": "2020-11-05T18:59:14Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaBasedConnectorTask.java", "diffHunk": "@@ -274,7 +274,6 @@ protected void rewindAndPausePartitionOnException(TopicPartition srcTopicPartiti\n       // Seek to last checkpoint failed. Throw an exception to avoid any data loss scenarios where the consumed\n       // offset can be committed even though the send for that offset has failed.\n       String errorMessage = String.format(\"Partition rewind for %s failed due to \", srcTopicPartition);\n-      _logger.error(errorMessage, e);", "originalCommit": "5b3ca1a8e9e0b71463d2becd6113502888f36387", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQxODI0Mw==", "url": "https://github.com/linkedin/brooklin/pull/777#discussion_r518418243", "bodyText": "This error was duplicate in the logs. The caller of this method prints the exception, or there is a print when the thread dies with the exception.", "author": "vmaheshw", "createdAt": "2020-11-05T22:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODI4OTM4MQ=="}], "type": "inlineReview"}, {"oid": "f46a88a97951f917809616bae8ca0450f6cc1533", "url": "https://github.com/linkedin/brooklin/commit/f46a88a97951f917809616bae8ca0450f6cc1533", "message": "Address comments", "committedDate": "2020-11-05T22:53:25Z", "type": "commit"}]}