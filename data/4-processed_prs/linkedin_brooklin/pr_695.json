{"pr_number": 695, "pr_title": "Don't reuse the same producer on interrupted flush in KafkaProducerWrapper", "pr_createdAt": "2020-03-16T17:04:38Z", "pr_url": "https://github.com/linkedin/brooklin/pull/695", "timeline": [{"oid": "7f3e9b290c0620e3d3b55483de39837143666bea", "url": "https://github.com/linkedin/brooklin/commit/7f3e9b290c0620e3d3b55483de39837143666bea", "message": "Don't reuse the same producer on interrupted flush in KafkaProducerWrapper", "committedDate": "2020-03-16T16:26:48Z", "type": "commit"}, {"oid": "4330544489adea7a7a166f52325aae765f283596", "url": "https://github.com/linkedin/brooklin/commit/4330544489adea7a7a166f52325aae765f283596", "message": "Initial test", "committedDate": "2020-03-19T01:44:02Z", "type": "commit"}, {"oid": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "url": "https://github.com/linkedin/brooklin/commit/f7163bc64d5c3197252f71eb11b35013f22b9e5b", "message": "Fix up test", "committedDate": "2020-03-19T03:28:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MTY3NQ==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395091675", "bodyText": "Please, add a comment explaining why executing this flush() call on a separate thread was necessary.\n\n\nFeel free to ignore this comment: not that it matters much, but would it be slightly more appropriate to use Executors.newSingleThreadExecutor() or even just new Thread().start()?", "author": "ahmedahamid", "createdAt": "2020-03-19T14:59:23Z", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();", "originalCommit": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExODg2NA==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395118864", "bodyText": "done", "author": "somandal", "createdAt": "2020-03-19T15:35:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MTY3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MzY3NA==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395093674", "bodyText": "nit: prefix field names with an underscore", "author": "ahmedahamid", "createdAt": "2020-03-19T15:02:02Z", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;", "originalCommit": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExODk1OA==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395118958", "bodyText": "woops, done!", "author": "somandal", "createdAt": "2020-03-19T15:35:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5MzY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTcwMg==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395095702", "bodyText": "Just to make it super simple for anyone reading this test, please add a comment summarizing your intent (e.g. calling flush() on the first created producer will throw).", "author": "ahmedahamid", "createdAt": "2020-03-19T15:04:39Z", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.anyInt;\n+import static org.mockito.Matchers.anyObject;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper producerWrapper =\n+        new MockKafkaProducerWrapper(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    ExecutorService executorService = Executors.newCachedThreadPool();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocations count\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Second producer's flush() has not been mocked to throw exceptions, this should not throw\n+    producerWrapper.flush();\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 2);\n+\n+    // Send should reuse the older producer and the counts should not be reset\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+\n+    // Closing the producer's task, and since this is the only task, the producer should be closed\n+    producerWrapper.close(task);\n+    producerWrapper.verifySend(2);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+    Assert.assertEquals(producerWrapper.numCreateKafkaProducerCalls, 2);\n+  }\n+\n+  private static class MockKafkaProducerWrapper extends KafkaProducerWrapper<byte[], byte[]> {\n+    private boolean createKafkaProducerCalled;\n+    private int numCreateKafkaProducerCalls;\n+    private Producer<byte[], byte[]> mockProducer;\n+\n+    MockKafkaProducerWrapper(String logSuffix, Properties props, String metricsNamesPrefix) {\n+      super(logSuffix, props, metricsNamesPrefix);\n+    }\n+\n+    @Override\n+    Producer<byte[], byte[]> createKafkaProducer() {\n+      @SuppressWarnings(\"unchecked\")\n+      Producer<byte[], byte[]> producer = (Producer<byte[], byte[]>) mock(Producer.class);\n+      if (!createKafkaProducerCalled) {", "originalCommit": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExOTA5OQ==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395119099", "bodyText": "done", "author": "somandal", "createdAt": "2020-03-19T15:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NTcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwNjA1NA==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395106054", "bodyText": "@VisibleForTesting", "author": "ahmedahamid", "createdAt": "2020-03-19T15:18:08Z", "path": "datastream-kafka/src/main/java/com/linkedin/datastream/kafka/KafkaProducerWrapper.java", "diffHunk": "@@ -173,13 +174,17 @@ int getTasksSize() {\n     } else {\n       if (_kafkaProducer == null) {\n         _rateLimiter.acquire();\n-        _kafkaProducer = _producerFactory.createProducer(_props);\n+        _kafkaProducer = createKafkaProducer();\n         NUM_PRODUCERS.incrementAndGet();\n       }\n     }\n     return _kafkaProducer;\n   }\n \n+  Producer<K, V> createKafkaProducer() {", "originalCommit": "f7163bc64d5c3197252f71eb11b35013f22b9e5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTExOTE5MQ==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r395119191", "bodyText": "done", "author": "somandal", "createdAt": "2020-03-19T15:35:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTEwNjA1NA=="}], "type": "inlineReview"}, {"oid": "c3570fd4ece623e66257fb40476f4350638c7933", "url": "https://github.com/linkedin/brooklin/commit/c3570fd4ece623e66257fb40476f4350638c7933", "message": "Address review comments", "committedDate": "2020-03-19T15:38:39Z", "type": "commit"}, {"oid": "24683c0dced218313e15c41780025ab7d0a594a6", "url": "https://github.com/linkedin/brooklin/commit/24683c0dced218313e15c41780025ab7d0a594a6", "message": "Fix typo", "committedDate": "2020-03-19T15:44:42Z", "type": "commit"}, {"oid": "4ba5846f9086a5932a3c435fccb88d6fc3cac607", "url": "https://github.com/linkedin/brooklin/commit/4ba5846f9086a5932a3c435fccb88d6fc3cac607", "message": "Fixes", "committedDate": "2020-03-19T16:05:47Z", "type": "commit"}, {"oid": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b", "url": "https://github.com/linkedin/brooklin/commit/d01ad1bc5aafbf6893bb3a8fed285004dd374f0b", "message": "Fixes", "committedDate": "2020-03-19T16:06:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzNzg3MA==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r397237870", "bodyText": "nit: resetting send, flush and close (for completeness)", "author": "DEEPTHIKORAT", "createdAt": "2020-03-24T15:22:21Z", "path": "datastream-kafka/src/test/java/com/linkedin/datastream/kafka/TestKafkaProducerWrapper.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.kafka;\n+\n+import java.util.Collections;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.kafka.clients.producer.Callback;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.errors.InterruptException;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.codahale.metrics.MetricRegistry;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.metrics.DynamicMetricsManager;\n+import com.linkedin.datastream.server.DatastreamTask;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.DatastreamTestUtils;\n+\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+\n+\n+/**\n+ * Tests for {@link KafkaProducerWrapper}\n+ */\n+@Test\n+public class TestKafkaProducerWrapper {\n+\n+  @Test\n+  public void testFlushInterrupt() throws Exception {\n+    DynamicMetricsManager.createInstance(new MetricRegistry(), getClass().getSimpleName());\n+    Properties transportProviderProperties = new Properties();\n+    transportProviderProperties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:1234\");\n+    transportProviderProperties.put(ProducerConfig.CLIENT_ID_CONFIG, \"testClient\");\n+    transportProviderProperties.put(KafkaTransportProviderAdmin.ZK_CONNECT_STRING_CONFIG, \"zk-connect-string\");\n+\n+    String topicName = \"random-topic-42\";\n+\n+    MockKafkaProducerWrapper<byte[], byte[]> producerWrapper =\n+        new MockKafkaProducerWrapper<>(\"log-suffix\", transportProviderProperties, \"metrics\");\n+\n+    String destinationUri = \"localhost:1234/\" + topicName;\n+    Datastream ds = DatastreamTestUtils.createDatastream(\"test\", \"ds1\", \"source\", destinationUri, 1);\n+\n+    DatastreamTask task = new DatastreamTaskImpl(Collections.singletonList(ds));\n+    ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(topicName, null, null);\n+    producerWrapper.assignTask(task);\n+\n+    // Sending first event, send should pass, none of the other methods on the producer should have been called\n+    producerWrapper.send(task, producerRecord, null);\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(0);\n+    producerWrapper.verifyClose(0);\n+    Assert.assertEquals(producerWrapper.getNumCreateKafkaProducerCalls(), 1);\n+\n+    // Calling the first flush() on a separate thread because the InterruptException calls Thread interrupt() on the\n+    // currently running thread. If not run on a separate thread, the test thread itself will be interrupted.\n+    ExecutorService executorService = Executors.newSingleThreadExecutor();\n+    executorService.submit(() -> {\n+      // Flush has been mocked to throw an InterruptException\n+      Assert.assertThrows(InterruptException.class, producerWrapper::flush);\n+    }).get();\n+\n+    producerWrapper.verifySend(1);\n+    producerWrapper.verifyFlush(1);\n+    producerWrapper.verifyClose(1);\n+\n+    // Second send should create a new producer, resetting flush() and close() invocation counts", "originalCommit": "d01ad1bc5aafbf6893bb3a8fed285004dd374f0b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzI1NzQ5Ng==", "url": "https://github.com/linkedin/brooklin/pull/695#discussion_r397257496", "bodyText": "As discussed offline, I'll take care of this on the next review. :)", "author": "somandal", "createdAt": "2020-03-24T15:47:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIzNzg3MA=="}], "type": "inlineReview"}]}