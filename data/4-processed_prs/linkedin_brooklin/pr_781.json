{"pr_number": 781, "pr_title": "Pull in some Kafka consumer metrics for the AbstractKafkaBasedConnectorTask", "pr_createdAt": "2020-11-13T04:14:11Z", "pr_url": "https://github.com/linkedin/brooklin/pull/781", "timeline": [{"oid": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "url": "https://github.com/linkedin/brooklin/commit/e3caf82fafa2ef47e2cbba09ca9131c498767eab", "message": "Pull in some Kafka consumer metrics for the AbstractKafkaBasedConnectorTask", "committedDate": "2020-11-13T03:56:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ0NTA2OA==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524445068", "bodyText": "I don't have a strong opinion on the matter but unregisterMetric no-ops if the metric hasn't been registered. This may spare you having to maintain and check _kafkaConsumerMetricsRegistered.", "author": "ahmedahamid", "createdAt": "2020-11-16T17:27:36Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaBasedConnectorTaskMetrics.java", "diffHunk": "@@ -151,6 +201,11 @@ public void deregisterMetrics() {\n       DYNAMIC_METRICS_MANAGER.unregisterMetric(_className, _key, TIME_SPENT_BETWEEN_POLLS_MS);\n       DYNAMIC_METRICS_MANAGER.unregisterMetric(_className, _key, PER_EVENT_PROCESSING_TIME_NANOS);\n     }\n+\n+    if (_kafkaConsumerMetricsRegistered) {", "originalCommit": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg0MTMxNw==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524841317", "bodyText": "Removed the extra check and removed the boolean", "author": "somandal", "createdAt": "2020-11-17T02:18:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ0NTA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ0OTg0MA==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524449840", "bodyText": "nit: remove empty line", "author": "ahmedahamid", "createdAt": "2020-11-16T17:34:28Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaBasedConnectorTaskMetrics.java", "diffHunk": "@@ -130,6 +143,43 @@\n     DYNAMIC_METRICS_MANAGER.registerGauge(_className, AGGREGATE, NUM_TOPICS, aggNumTopics::get);\n   }\n \n+  /**\n+   * Register some of the Kafka consumer metrics of interest. This cannot be done as part of the constructor\n+   * as the Kafka consumer may be created at a later time than this object.\n+   * @param consumer the Kafka consumer for which to register the metrics\n+   * @param clientId the Kafka consumer's client.id\n+   */\n+  public void registerKafkaConsumerMetrics(Consumer<?, ?> consumer, String clientId) {\n+    if (consumer == null || StringUtils.isBlank(clientId)) {\n+      _errorLogger.warn(\"Cannot register the Kafka consumer metrics, either the consumer is null or the client.id is blank\");\n+      return;\n+    }\n+\n+    Supplier<Double> watermarkSpanSupplier = () -> getConsumerOffsetWatermarkSpanMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_OFFSET_WATERMARK_SPAN, watermarkSpanSupplier);\n+\n+    Supplier<Double> offsetResetSupplier = () -> getConsumerLiclosestDataLossEstimationMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_LICLOSEST_DATA_LOSS_ESTIMATION, offsetResetSupplier);\n+\n+    _kafkaConsumerMetricsRegistered = true;\n+  }\n+\n+  private double getConsumerOffsetWatermarkSpanMetric(Consumer<?, ?> consumer, String clientId) {\n+    return getConsumerMetricValue(consumer, clientId, \"consumer-offset-watermark-span\");\n+", "originalCommit": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg0MDI3MA==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524840270", "bodyText": "oops, done", "author": "somandal", "createdAt": "2020-11-17T02:15:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ0OTg0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5NTA5NQ==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524895095", "bodyText": "Thank you", "author": "ahmedahamid", "createdAt": "2020-11-17T05:36:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ0OTg0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1MjU3NQ==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524452575", "bodyText": "Why not throw on null consumer (and possibly clientId as well)?", "author": "ahmedahamid", "createdAt": "2020-11-16T17:38:47Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaBasedConnectorTaskMetrics.java", "diffHunk": "@@ -130,6 +143,43 @@\n     DYNAMIC_METRICS_MANAGER.registerGauge(_className, AGGREGATE, NUM_TOPICS, aggNumTopics::get);\n   }\n \n+  /**\n+   * Register some of the Kafka consumer metrics of interest. This cannot be done as part of the constructor\n+   * as the Kafka consumer may be created at a later time than this object.\n+   * @param consumer the Kafka consumer for which to register the metrics\n+   * @param clientId the Kafka consumer's client.id\n+   */\n+  public void registerKafkaConsumerMetrics(Consumer<?, ?> consumer, String clientId) {\n+    if (consumer == null || StringUtils.isBlank(clientId)) {", "originalCommit": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg0MDIyNw==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524840227", "bodyText": "I've removed the consumer check, as if the consumer is null, we should throw. I do believe that clientId isn't a mandatory consumer config, due to which I'd like to ensure that we don't throw an exception if this is blank, so keeping this one around.", "author": "somandal", "createdAt": "2020-11-17T02:14:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1MjU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg5NTc1Mg==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524895752", "bodyText": "Makes sense. Feel free to add a validate.notnull for consumer, too.", "author": "ahmedahamid", "createdAt": "2020-11-17T05:39:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1MjU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1NjQyOQ==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524456429", "bodyText": "I don't think consumer can be null since you already guard against that prior to calling getConsumerMetricValue(). The suppliers/lambdas are bound to the consumer param.", "author": "ahmedahamid", "createdAt": "2020-11-16T17:44:33Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaBasedConnectorTaskMetrics.java", "diffHunk": "@@ -130,6 +143,43 @@\n     DYNAMIC_METRICS_MANAGER.registerGauge(_className, AGGREGATE, NUM_TOPICS, aggNumTopics::get);\n   }\n \n+  /**\n+   * Register some of the Kafka consumer metrics of interest. This cannot be done as part of the constructor\n+   * as the Kafka consumer may be created at a later time than this object.\n+   * @param consumer the Kafka consumer for which to register the metrics\n+   * @param clientId the Kafka consumer's client.id\n+   */\n+  public void registerKafkaConsumerMetrics(Consumer<?, ?> consumer, String clientId) {\n+    if (consumer == null || StringUtils.isBlank(clientId)) {\n+      _errorLogger.warn(\"Cannot register the Kafka consumer metrics, either the consumer is null or the client.id is blank\");\n+      return;\n+    }\n+\n+    Supplier<Double> watermarkSpanSupplier = () -> getConsumerOffsetWatermarkSpanMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_OFFSET_WATERMARK_SPAN, watermarkSpanSupplier);\n+\n+    Supplier<Double> offsetResetSupplier = () -> getConsumerLiclosestDataLossEstimationMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_LICLOSEST_DATA_LOSS_ESTIMATION, offsetResetSupplier);\n+\n+    _kafkaConsumerMetricsRegistered = true;\n+  }\n+\n+  private double getConsumerOffsetWatermarkSpanMetric(Consumer<?, ?> consumer, String clientId) {\n+    return getConsumerMetricValue(consumer, clientId, \"consumer-offset-watermark-span\");\n+\n+  }\n+\n+  private double getConsumerLiclosestDataLossEstimationMetric(Consumer<?, ?> consumer, String clientId) {\n+    return getConsumerMetricValue(consumer, clientId, \"consumer-liclosest-data-loss-estimation\");\n+  }\n+\n+  private double getConsumerMetricValue(Consumer<?, ?> consumer, String clientId, String metricName) {\n+    Map<String, String> tags = new HashMap<>(2);\n+    tags.put(\"client-id\", clientId);\n+    MetricName name = new MetricName(metricName, \"lnkd\", \"\", tags);\n+    return Optional.ofNullable(consumer).map(c -> c.metrics().get(name)).map(Metric::value).orElse(0.0);", "originalCommit": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg0MDM1Mw==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524840353", "bodyText": "Yeah good point. Have refactored this bit.", "author": "somandal", "createdAt": "2020-11-17T02:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1NjQyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1OTAzMw==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524459033", "bodyText": "Are you deliberately using the deprecated Metric::value instead of Metric.metricValue() + casting?", "author": "ahmedahamid", "createdAt": "2020-11-16T17:48:48Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaBasedConnectorTaskMetrics.java", "diffHunk": "@@ -130,6 +143,43 @@\n     DYNAMIC_METRICS_MANAGER.registerGauge(_className, AGGREGATE, NUM_TOPICS, aggNumTopics::get);\n   }\n \n+  /**\n+   * Register some of the Kafka consumer metrics of interest. This cannot be done as part of the constructor\n+   * as the Kafka consumer may be created at a later time than this object.\n+   * @param consumer the Kafka consumer for which to register the metrics\n+   * @param clientId the Kafka consumer's client.id\n+   */\n+  public void registerKafkaConsumerMetrics(Consumer<?, ?> consumer, String clientId) {\n+    if (consumer == null || StringUtils.isBlank(clientId)) {\n+      _errorLogger.warn(\"Cannot register the Kafka consumer metrics, either the consumer is null or the client.id is blank\");\n+      return;\n+    }\n+\n+    Supplier<Double> watermarkSpanSupplier = () -> getConsumerOffsetWatermarkSpanMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_OFFSET_WATERMARK_SPAN, watermarkSpanSupplier);\n+\n+    Supplier<Double> offsetResetSupplier = () -> getConsumerLiclosestDataLossEstimationMetric(consumer, clientId);\n+    DYNAMIC_METRICS_MANAGER.registerGauge(_className, _key, CONSUMER_LICLOSEST_DATA_LOSS_ESTIMATION, offsetResetSupplier);\n+\n+    _kafkaConsumerMetricsRegistered = true;\n+  }\n+\n+  private double getConsumerOffsetWatermarkSpanMetric(Consumer<?, ?> consumer, String clientId) {\n+    return getConsumerMetricValue(consumer, clientId, \"consumer-offset-watermark-span\");\n+\n+  }\n+\n+  private double getConsumerLiclosestDataLossEstimationMetric(Consumer<?, ?> consumer, String clientId) {\n+    return getConsumerMetricValue(consumer, clientId, \"consumer-liclosest-data-loss-estimation\");\n+  }\n+\n+  private double getConsumerMetricValue(Consumer<?, ?> consumer, String clientId, String metricName) {\n+    Map<String, String> tags = new HashMap<>(2);\n+    tags.put(\"client-id\", clientId);\n+    MetricName name = new MetricName(metricName, \"lnkd\", \"\", tags);\n+    return Optional.ofNullable(consumer).map(c -> c.metrics().get(name)).map(Metric::value).orElse(0.0);", "originalCommit": "e3caf82fafa2ef47e2cbba09ca9131c498767eab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg0MTIzNA==", "url": "https://github.com/linkedin/brooklin/pull/781#discussion_r524841234", "bodyText": "As discussed offline, my usage of the deprecated Metric::value was indeed deliberate. For using Metric.metricValue() + casting, you need to know the metric type, and unfortunately if Kafka changes the metric type underneath us, it can cause us to not emit the metric and there is no easy way to know why. On the other hand, if the deprecated method is removed or if the behavior is buggy, we have a problem too. Decided to go with the Metric.metricValue() + casting", "author": "somandal", "createdAt": "2020-11-17T02:18:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1OTAzMw=="}], "type": "inlineReview"}, {"oid": "438959a931e69c38c47a26bee179b56d3c5b8a1c", "url": "https://github.com/linkedin/brooklin/commit/438959a931e69c38c47a26bee179b56d3c5b8a1c", "message": "Address review comments", "committedDate": "2020-11-17T02:21:11Z", "type": "commit"}]}