{"pr_number": 735, "pr_title": "Added diag endpoint for consumer offsets for bmm", "pr_createdAt": "2020-07-22T23:18:32Z", "pr_url": "https://github.com/linkedin/brooklin/pull/735", "timeline": [{"oid": "946ba805a6edb96b1373308743deade37abff2fe", "url": "https://github.com/linkedin/brooklin/commit/946ba805a6edb96b1373308743deade37abff2fe", "message": "Added diag endpoint for consumer offsets for bmm", "committedDate": "2020-07-22T23:13:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzNzM4NQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r459137385", "bodyText": "Here I'm making an assumption that record collection is ordered by offset, and also that it can't be empty. Need to ensure it's true.", "author": "jzakaryan", "createdAt": "2020-07-22T23:20:52Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -68,12 +74,48 @@ public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartiti\n         }\n       }\n     });\n+\n+    // Remove consumer offsets for partitions that have been revoked\n+    topicPartitions.forEach(topicPartition -> {\n+      Map<Integer, Long> partitions = _consumerOffsets.get(topicPartition.topic());\n+      if (partitions != null) {\n+        partitions.remove(topicPartition.partition());\n+        if (partitions.isEmpty()) {\n+          _consumerOffsets.remove(topicPartition.topic());\n+        }\n+      }\n+    });\n+  }\n+\n+  /**\n+   * Updates consumer offsets for partitions that have been polled\n+   * @param consumerRecords consumer records that have been the result of the poll\n+   */\n+  public void onPartitionsPolled(@NotNull ConsumerRecords<?, ?> consumerRecords) {\n+    Collection<TopicPartition> topicPartitions = consumerRecords.partitions();\n+\n+    topicPartitions.forEach(topicPartition -> {\n+      List<? extends ConsumerRecord<?, ?>> partitionRecords = consumerRecords.records(topicPartition);\n+      ConsumerRecord<?, ?> lastRecord = partitionRecords.get(partitionRecords.size() - 1);\n+\n+      Map<Integer, Long> partitionOffsetMap = _consumerOffsets.putIfAbsent(topicPartition.topic(),\n+          new ConcurrentHashMap<>());\n+      partitionOffsetMap.put(topicPartition.partition(), lastRecord.offset());", "originalCommit": "946ba805a6edb96b1373308743deade37abff2fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE0OTE4Mg==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460149182", "bodyText": "How are we testing these changes?", "author": "vishwajith-s", "createdAt": "2020-07-24T16:08:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzNzM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3MDY5MQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460170691", "bodyText": "public ConsumerRecords<K,V> poll\u200b(java.time.Duration timeout)\nFetch data for the topics or partitions specified using one of the subscribe/assign APIs. It is an error to not have subscribed to any topics or partitions before polling for data.\nOn each poll, consumer will try to use the last consumed offset as the starting offset and fetch sequentially. The last consumed offset can be manually set through seek(TopicPartition, long) or automatically set as the last committed offset for the subscribed list of partitions\nThis method returns immediately if there are records available. Otherwise, it will await the passed timeout. If the timeout expires, an empty record set will be returned. Note that this method may block beyond the timeout in order to execute custom ConsumerRebalanceListener callbacks.\n\nSo from the documentation above, it does look like we can expect the offsets to be in order\nWhat about unit tests though? I recall seeing us call some of the other APIs in unit tests (but I could be wrong). Should we add some unit tests for this?", "author": "somandal", "createdAt": "2020-07-24T16:48:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzNzM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMDkxOA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r478600918", "bodyText": "I added unit tests.", "author": "jzakaryan", "createdAt": "2020-08-27T18:03:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTEzNzM4NQ=="}], "type": "inlineReview"}, {"oid": "23afccba98529c20a7364b614df28ca40bf6eefa", "url": "https://github.com/linkedin/brooklin/commit/23afccba98529c20a7364b614df28ca40bf6eefa", "message": "Bug fix", "committedDate": "2020-07-23T18:13:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE0Mjc3MQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460142771", "bodyText": "Aren't we supposed to return an empty String? Does this compile?", "author": "vishwajith-s", "createdAt": "2020-07-24T15:57:34Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaConnectorDiagUtils.java", "diffHunk": "@@ -61,4 +61,41 @@ public static String reduceTopicPartitionStatsResponses(Map<String, String> resp\n \n     return JsonUtils.toJson(result.values());\n   }\n+\n+  /**\n+   * Reduce/Merge the KafkaConsumerOffsetsResponse responses of a collection of hosts/instances into one response\n+   */\n+  public static String reduceConsumerOffsetsResponses(Map<String, String> responses, Logger logger) {\n+    Map<String, KafkaConsumerOffsetsResponse> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaConsumerOffsetsResponse> responseList;\n+      try {\n+        responseList = JsonUtils.fromJson(json, new TypeReference<List<KafkaConsumerOffsetsResponse>>() {\n+        });\n+      } catch (Exception e) {\n+        logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE2OTgwOA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460169808", "bodyText": "+1, and the same for the remaining returns in this function.", "author": "somandal", "createdAt": "2020-07-24T16:47:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE0Mjc3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjA1NzU1NQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r466057555", "bodyText": "Surprisingly, trying to return any String doesn't compile here. That's probably Java related trickery. Not sure why.", "author": "jzakaryan", "createdAt": "2020-08-05T23:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE0Mjc3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE2OTQwMA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460169400", "bodyText": "nit:  since each 'if' returns, it's okay to make them all if blocks. Free free to ignore this, using \"else if\" is fine too.", "author": "somandal", "createdAt": "2020-07-24T16:46:34Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -491,10 +511,12 @@ public String reduce(String query, Map<String, String> responses) {\n       if (path != null\n           && (path.equalsIgnoreCase(DiagnosticsRequestType.DATASTREAM_STATE.toString()))) {\n         return JsonUtils.toJson(responses);\n-      }\n-      if (path != null\n+      } else if (path != null", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3MjE0NQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460172145", "bodyText": "nit: remove empty line?", "author": "somandal", "createdAt": "2020-07-24T16:51:32Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaConsumerOffsetsResponse.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+/**\n+ * Response structure used for consumer offsets diagnostics-endpoint requests\n+ */\n+public class KafkaConsumerOffsetsResponse {\n+  private final String _consumerGroupId;\n+  private final Map<String, Map<Integer, Long>> _consumerOffsets;\n+\n+  /**\n+   * Constructor for {@link KafkaConsumerOffsetsResponse}\n+   * @param consumerOffsets Consumer offsets for all topic partitions\n+   * @param consumerGroupId Consumer group ID\n+   */\n+  public KafkaConsumerOffsetsResponse(Map<String, Map<Integer, Long>> consumerOffsets, String consumerGroupId) {\n+    _consumerGroupId = consumerGroupId;\n+    _consumerOffsets = consumerOffsets;\n+  }\n+\n+  /**\n+   * Constructor for {@link KafkaConsumerOffsetsResponse}\n+   * @param consumerGroupId Consumer group ID\n+   */\n+  public KafkaConsumerOffsetsResponse(String consumerGroupId) {\n+    this(new HashMap<>(), consumerGroupId);\n+  }\n+\n+  public Map<String, Map<Integer, Long>> getConsumerOffsets() {\n+    return _consumerOffsets;\n+  }\n+\n+  public String getConsumerGroupId() {\n+    return _consumerGroupId;\n+  }\n+", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NDc2NA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460174764", "bodyText": "Do we not need to update the _consumerOffsets here? Guess you are relying on poll() instead to keep track of valid TopicPartitions in _consumerOffsets, right?\nMight be nice to add a comment somewhere which explains this. Since we don't do symmetric handling, it may become confusing for a future person reading the code, as to why are we removing TopicPartitions on revoke, but  not adding them on assign?", "author": "somandal", "createdAt": "2020-07-24T16:56:21Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -46,6 +51,7 @@ public KafkaTopicPartitionTracker(String consumerGroupId) {\n    * @param topicPartitions the topic partitions which have been assigned\n    */\n   public void onPartitionsAssigned(@NotNull Collection<TopicPartition> topicPartitions) {", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODcxNTE4MQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r478715181", "bodyText": "Added a comment explaining why I remove offsets when partitions are revoked.", "author": "jzakaryan", "createdAt": "2020-08-27T21:49:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3NDc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MzE2OA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460183168", "bodyText": "I think this code won't be called if _enablePartitionAssignment is true (since this doesn't use consumer group management). These onPartitionAssigned()/onPartitionRevoked() callbacks are Kafka callbacks invoked specifically for consumer group management only. For onPartitionAssigned(), we manually set up some of the state in consumerSubscribe() function for _enablePartitionAssignment. I don't see any handling there to revoke TopicPartitions which are no longer assigned for the partition tracker. We need to fix this for the topic manager related stuff in that function too.\nFeel free to open a separate bug to handle this correctly (and feel free to take it up or assign it to me). Wanted to capture this potential gap that we may have (needs proper investigation to ensure there is no other code path which is taking care of this already) so that we don't forget to look into it.", "author": "somandal", "createdAt": "2020-07-24T17:13:39Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaTopicPartitionTracker.java", "diffHunk": "@@ -68,12 +74,48 @@ public void onPartitionsRevoked(@NotNull Collection<TopicPartition> topicPartiti\n         }\n       }\n     });\n+\n+    // Remove consumer offsets for partitions that have been revoked", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODYwMDM3MQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r478600371", "bodyText": "Will open a ticket to handle that case", "author": "jzakaryan", "createdAt": "2020-08-27T18:02:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4MzE2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE4NjYzMQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r460186631", "bodyText": "This may be more of a question for @vishwajith-s and @ahmedahamid. What's the recommendation for reusing an existing DiagnosticsRequestType vs. adding a new one? I see that some request types return more than one piece of information (such as returning all partitions, paused partitions, auto-paused partitions, and inflight messages as part of a single request).", "author": "somandal", "createdAt": "2020-07-24T17:20:05Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/AbstractKafkaConnector.java", "diffHunk": "@@ -101,6 +101,7 @@ public Thread newThread(@NotNull Runnable r) {\n   enum DiagnosticsRequestType {\n     DATASTREAM_STATE,\n     PARTITIONS,\n+    CONSUMER_OFFSETS", "originalCommit": "23afccba98529c20a7364b614df28ca40bf6eefa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e230cd0be0a38118027022315f57d65d9678f066", "url": "https://github.com/linkedin/brooklin/commit/e230cd0be0a38118027022315f57d65d9678f066", "message": "Fixed json serialization issue. Added test for consumer offsets diag endpoint", "committedDate": "2020-08-26T21:04:47Z", "type": "commit"}, {"oid": "20ef41ed0869462b7dd83fd436c15558e013a275", "url": "https://github.com/linkedin/brooklin/commit/20ef41ed0869462b7dd83fd436c15558e013a275", "message": "Added test for consumer offset reducer", "committedDate": "2020-08-26T22:55:49Z", "type": "commit"}, {"oid": "fe52fcf876d7c5d8ac3a283cb03a451bb417bff6", "url": "https://github.com/linkedin/brooklin/commit/fe52fcf876d7c5d8ac3a283cb03a451bb417bff6", "message": "Checkstyle fixes", "committedDate": "2020-08-26T23:32:34Z", "type": "commit"}, {"oid": "a704ac5f792fb6e1bc18f84f92d14bdaaed9a738", "url": "https://github.com/linkedin/brooklin/commit/a704ac5f792fb6e1bc18f84f92d14bdaaed9a738", "message": "Fixed flakiness of consumer offsets test", "committedDate": "2020-08-27T17:48:27Z", "type": "commit"}, {"oid": "cbebf01ada209cb6c3ff5648bdbad605b1b088ab", "url": "https://github.com/linkedin/brooklin/commit/cbebf01ada209cb6c3ff5648bdbad605b1b088ab", "message": "Added comments. Minor improvements to test code", "committedDate": "2020-08-27T21:48:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM5MjYyNw==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r481392627", "bodyText": "Feel free to ignore this suggestion, but can we take TOPIC_COUNT, PARTITION_COUNT, and PARTITION_MESSAGE_COUNT as input parameters? Reason I ask is that we may want to add more tests in this area in the future, and it'll be nice to make this a future proof utility.", "author": "somandal", "createdAt": "2020-09-01T19:51:10Z", "path": "datastream-kafka-connector/src/test/java/com/linkedin/datastream/connectors/kafka/mirrormaker/TestKafkaConsumerOffsets.java", "diffHunk": "@@ -0,0 +1,208 @@\n+/**\n+ *  Copyright 2020 LinkedIn Corporation. All rights reserved.\n+ *  Licensed under the BSD 2-Clause License. See the LICENSE file in the project root for license information.\n+ *  See the NOTICE file in the project root for additional information regarding copyright ownership.\n+ */\n+package com.linkedin.datastream.connectors.kafka.mirrormaker;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.IntStream;\n+\n+import org.codehaus.jackson.type.TypeReference;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.testng.Assert;\n+import org.testng.annotations.Test;\n+\n+import com.linkedin.datastream.common.Datastream;\n+import com.linkedin.datastream.common.JsonUtils;\n+import com.linkedin.datastream.common.PollUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaConnectorDiagUtils;\n+import com.linkedin.datastream.connectors.kafka.KafkaConsumerOffsetsResponse;\n+import com.linkedin.datastream.connectors.kafka.MockDatastreamEventProducer;\n+import com.linkedin.datastream.server.DatastreamTaskImpl;\n+import com.linkedin.datastream.testutil.BaseKafkaZkTest;\n+\n+\n+/**\n+ * Tests for kafka consumer offsets diagnostic endpoint\n+ */\n+@Test\n+public class TestKafkaConsumerOffsets extends BaseKafkaZkTest {\n+\n+  private static final int TOPIC_COUNT = 2;\n+  private static final int PARTITION_COUNT = 2;\n+  private static final int PARTITION_MESSAGE_COUNT = 10;\n+  private static final String CONSUMER_OFFSETS = \"/consumer_offsets\";\n+  private static final Logger LOG = LoggerFactory.getLogger(TestKafkaConsumerOffsets.class);\n+\n+  @Test\n+  public void testConsumerOffsetsDiagEndpoint() {\n+    // create topics\n+    List<String> topics = new ArrayList<>();\n+    IntStream.range(0, TOPIC_COUNT).forEach(i -> topics.add(\"topic\" + i));\n+    topics.forEach(topic -> createTopic(_zkUtils, topic, PARTITION_COUNT));\n+\n+    // setup datastream and connector\n+    Datastream datastream = KafkaMirrorMakerConnectorTestUtils.createDatastream(\"topicStream\", _broker, \"topic\\\\d+\");\n+    DatastreamTaskImpl task = new DatastreamTaskImpl(Collections.singletonList(datastream));\n+    MockDatastreamEventProducer datastreamProducer = new MockDatastreamEventProducer();\n+    task.setEventProducer(datastreamProducer);\n+\n+    KafkaMirrorMakerConnector connector = new KafkaMirrorMakerConnector(\"MirrorMakerConnector\",\n+        KafkaMirrorMakerConnectorTestUtils.getDefaultConfig(Optional.empty()), \"testCluster\");\n+    connector.start(null);\n+\n+    // produce messages to each topic partition\n+    topics.forEach(topic -> IntStream.range(0, PARTITION_COUNT).forEach(partition ->\n+        KafkaMirrorMakerConnectorTestUtils.produceEventsToPartition(topic, partition, PARTITION_MESSAGE_COUNT, _kafkaCluster)));\n+\n+    connector.onAssignmentChange(Collections.singletonList(task));\n+\n+    // wait until the consumer offsets are updated for each topic partition\n+    if (!PollUtils.poll(() -> testConsumerOffsetsAreUpdated(connector),\n+        KafkaMirrorMakerConnectorTestUtils.POLL_PERIOD_MS, KafkaMirrorMakerConnectorTestUtils.POLL_TIMEOUT_MS)) {\n+      Assert.fail(\"Consumer offsets were not updated correctly\");\n+    }\n+\n+    // shutdown\n+    connector.stop();\n+  }\n+\n+  @Test\n+  public void testConsumerOffsetsReducer() {\n+    String topic1 = \"topic1\";\n+    String topic2 = \"topic2\";\n+\n+    String consumerGroup1 = \"cg1\";\n+    String consumerGroup2 = \"cg2\";\n+    String consumerGroup3 = \"cg3\";\n+\n+    String instance1 = \"i1\";\n+    String instance2 = \"i2\";\n+\n+    // constructing instance1 consumer offsets\n+    List<KafkaConsumerOffsetsResponse> responseList1 = new ArrayList<>();\n+\n+    // instance 1 consumer group 1\n+    Map<String, Map<Integer, Long>> topicPartitionOffsets1 = new HashMap<>();\n+\n+    Map<Integer, Long> partitionOffsets1 = new HashMap<>();\n+    partitionOffsets1.put(0, 10L);\n+    partitionOffsets1.put(1, 10L);\n+    topicPartitionOffsets1.put(topic1, partitionOffsets1);\n+\n+    Map<Integer, Long> partitionOffsets2 = new HashMap<>();\n+    partitionOffsets2.put(0, 10L);\n+    partitionOffsets2.put(1, 10L);\n+    topicPartitionOffsets1.put(topic2, partitionOffsets2);\n+\n+    responseList1.add(new KafkaConsumerOffsetsResponse(topicPartitionOffsets1, consumerGroup1));\n+\n+    // instance 1 consumer group 2\n+    Map<String, Map<Integer, Long>> topicPartitionOffsets2 = new HashMap<>();\n+\n+    Map<Integer, Long> partitionOffsets3 = new HashMap<>();\n+    partitionOffsets3.put(0, 20L);\n+    partitionOffsets3.put(1, 20L);\n+    topicPartitionOffsets2.put(topic1, partitionOffsets3);\n+\n+    Map<Integer, Long> partitionOffsets4 = new HashMap<>();\n+    partitionOffsets4.put(0, 20L);\n+    partitionOffsets4.put(1, 20L);\n+    topicPartitionOffsets2.put(topic2, partitionOffsets4);\n+\n+    responseList1.add(new KafkaConsumerOffsetsResponse(topicPartitionOffsets2, consumerGroup2));\n+\n+    // constructing instance2 consumer offsets\n+    List<KafkaConsumerOffsetsResponse> responseList2 = new ArrayList<>();\n+\n+    // instance 2 consumer group 1\n+    Map<String, Map<Integer, Long>> topicPartitionOffsets3 = new HashMap<>();\n+\n+    Map<Integer, Long> partitionOffsets5 = new HashMap<>();\n+    partitionOffsets5.put(2, 10L);\n+    partitionOffsets5.put(3, 10L);\n+    topicPartitionOffsets3.put(topic1, partitionOffsets5);\n+\n+    responseList2.add(new KafkaConsumerOffsetsResponse(topicPartitionOffsets3, consumerGroup1));\n+\n+    // instance 2 consumer group 3\n+    Map<String, Map<Integer, Long>> topicPartitionOffsets4 = new HashMap<>();\n+\n+    Map<Integer, Long> partitionOffsets6 = new HashMap<>();\n+    partitionOffsets6.put(0, 30L);\n+    topicPartitionOffsets4.put(topic2, partitionOffsets6);\n+    responseList2.add(new KafkaConsumerOffsetsResponse(topicPartitionOffsets4, consumerGroup3));\n+\n+    // reducing responses and asserting correctness\n+    Map<String, String> responseMap = new HashMap<>();\n+    responseMap.put(instance1, JsonUtils.toJson(responseList1));\n+    responseMap.put(instance2, JsonUtils.toJson(responseList2));\n+\n+    String reducedMapJson = KafkaConnectorDiagUtils.reduceConsumerOffsetsResponses(responseMap, LOG);\n+    List<KafkaConsumerOffsetsResponse> responseList =\n+        JsonUtils.fromJson(reducedMapJson, new TypeReference<List<KafkaConsumerOffsetsResponse>>() { });\n+\n+    Assert.assertEquals(responseList.size(), 3); // 3 consumer groups\n+\n+    KafkaConsumerOffsetsResponse cg1Response = responseList.stream().\n+        filter(r -> r.getConsumerGroupId().equals(consumerGroup1)).findAny().orElse(null);\n+    Assert.assertNotNull(cg1Response);\n+    Assert.assertEquals(cg1Response.getConsumerOffsets().keySet().size(), 2); // cg1 consumes both topics\n+    Assert.assertEquals(cg1Response.getConsumerOffsets().get(topic1).keySet().size(), 4); // cg1 consumes 4 partitions for topic 1\n+    Assert.assertEquals(cg1Response.getConsumerOffsets().get(topic2).keySet().size(), 2); // cg1 consumes 2 partitions for topic 2\n+\n+    KafkaConsumerOffsetsResponse cg2Response = responseList.stream().\n+        filter(r -> r.getConsumerGroupId().equals(consumerGroup2)).findAny().orElse(null);\n+    Assert.assertNotNull(cg2Response);\n+    Assert.assertEquals(cg2Response.getConsumerOffsets().keySet().size(), 2); // cg2 consumers both topics\n+    Assert.assertEquals(cg2Response.getConsumerOffsets().get(topic1).keySet().size(), 2); // cg2 consumes 2 partitions for topic 1\n+    Assert.assertEquals(cg2Response.getConsumerOffsets().get(topic2).keySet().size(), 2); // cg2 consumes 2 partitions for topic 2\n+\n+    KafkaConsumerOffsetsResponse cg3Response = responseList.stream().\n+        filter(r -> r.getConsumerGroupId().equals(consumerGroup3)).findAny().orElse(null);\n+    Assert.assertNotNull(cg3Response);\n+    Assert.assertEquals(cg3Response.getConsumerOffsets().keySet().size(), 1); // cg3 consumes only topic 2\n+    Assert.assertEquals(cg3Response.getConsumerOffsets().get(topic2).size(), 1); // cg3 consumes 1 partition for topic 2\n+  }\n+\n+  private boolean testConsumerOffsetsAreUpdated(KafkaMirrorMakerConnector connector) {", "originalCommit": "cbebf01ada209cb6c3ff5648bdbad605b1b088ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MzA1MQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r481453051", "bodyText": "For now that can easily be configured by changing the values of constants and rebuilding. If we see the need to add more tests here we can easily parameterize those.", "author": "jzakaryan", "createdAt": "2020-09-01T21:52:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTM5MjYyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM4NTMyOQ==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r482385329", "bodyText": "Should we ignore all the responses from an instance if one result is bad?", "author": "vishwajith-s", "createdAt": "2020-09-02T20:03:04Z", "path": "datastream-kafka-connector/src/main/java/com/linkedin/datastream/connectors/kafka/KafkaConnectorDiagUtils.java", "diffHunk": "@@ -61,4 +61,41 @@ public static String reduceTopicPartitionStatsResponses(Map<String, String> resp\n \n     return JsonUtils.toJson(result.values());\n   }\n+\n+  /**\n+   * Reduce/Merge the KafkaConsumerOffsetsResponse responses of a collection of hosts/instances into one response\n+   */\n+  public static String reduceConsumerOffsetsResponses(Map<String, String> responses, Logger logger) {\n+    Map<String, KafkaConsumerOffsetsResponse> result = new HashMap<>();\n+\n+    responses.forEach((instance, json) -> {\n+      List<KafkaConsumerOffsetsResponse> responseList;\n+      try {\n+        responseList = JsonUtils.fromJson(json, new TypeReference<List<KafkaConsumerOffsetsResponse>>() {\n+        });\n+      } catch (Exception e) {\n+        logger.error(\"Invalid response {} from instance {}\", json, instance);\n+        return;\n+      }\n+\n+      responseList.forEach(response -> {\n+        if (response.getConsumerOffsets() == null || StringUtils.isBlank(response.getConsumerGroupId())) {", "originalCommit": "cbebf01ada209cb6c3ff5648bdbad605b1b088ab", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NjczNA==", "url": "https://github.com/linkedin/brooklin/pull/735#discussion_r482596734", "bodyText": "I think you're right @vishwajith-s . When a host returns empty consumer offsets we can ignore that but when a host is, let's say, down, or the diag endpoint is not responsive, we can declare failure/throw exception. I'll make necessary changes and update the PR.", "author": "jzakaryan", "createdAt": "2020-09-02T23:25:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjM4NTMyOQ=="}], "type": "inlineReview"}, {"oid": "428d53da8111362ab7368779486f19879edcf3ed", "url": "https://github.com/linkedin/brooklin/commit/428d53da8111362ab7368779486f19879edcf3ed", "message": "Better handling of the case when offsets are empty", "committedDate": "2020-09-03T17:06:11Z", "type": "commit"}]}