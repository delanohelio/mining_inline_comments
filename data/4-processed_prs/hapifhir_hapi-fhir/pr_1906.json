{"pr_number": 1906, "pr_title": "Spring batch integration", "pr_createdAt": "2020-06-09T00:11:48Z", "pr_url": "https://github.com/hapifhir/hapi-fhir/pull/1906", "timeline": [{"oid": "35b1533db1aa6b3d293736dbe41bf20fbf8a0fa2", "url": "https://github.com/hapifhir/hapi-fhir/commit/35b1533db1aa6b3d293736dbe41bf20fbf8a0fa2", "message": "Add new project for spring batch", "committedDate": "2020-05-26T05:32:00Z", "type": "commit"}, {"oid": "48fcf8a39da192883193d8423504c4b6060b13f1", "url": "https://github.com/hapifhir/hapi-fhir/commit/48fcf8a39da192883193d8423504c4b6060b13f1", "message": "wip", "committedDate": "2020-05-26T19:08:00Z", "type": "commit"}, {"oid": "81d5ff8f5a0722ec2468da13116d04f2e1142608", "url": "https://github.com/hapifhir/hapi-fhir/commit/81d5ff8f5a0722ec2468da13116d04f2e1142608", "message": "Add schema initialization", "committedDate": "2020-05-28T00:06:52Z", "type": "commit"}, {"oid": "a30682562552871603975689ef149606ba326bca", "url": "https://github.com/hapifhir/hapi-fhir/commit/a30682562552871603975689ef149606ba326bca", "message": "moving dependencies around. Get non-persisted batch working", "committedDate": "2020-05-28T19:10:52Z", "type": "commit"}, {"oid": "aa7d1cbcb7f9454173ebf6df7f53b95d5f34adeb", "url": "https://github.com/hapifhir/hapi-fhir/commit/aa7d1cbcb7f9454173ebf6df7f53b95d5f34adeb", "message": "Wip getting tasks built", "committedDate": "2020-06-01T18:32:36Z", "type": "commit"}, {"oid": "0fc2e04e650f9ee850767716c1c42da2498918d6", "url": "https://github.com/hapifhir/hapi-fhir/commit/0fc2e04e650f9ee850767716c1c42da2498918d6", "message": "wip", "committedDate": "2020-06-01T19:13:55Z", "type": "commit"}, {"oid": "f8d699e13b07756079eb93bfb34a34d1a04af4b6", "url": "https://github.com/hapifhir/hapi-fhir/commit/f8d699e13b07756079eb93bfb34a34d1a04af4b6", "message": "Initial working commit passing jobs to spring batch", "committedDate": "2020-06-01T22:51:09Z", "type": "commit"}, {"oid": "bb863f26fab72566042c0e446abc484d1e2ae546", "url": "https://github.com/hapifhir/hapi-fhir/commit/bb863f26fab72566042c0e446abc484d1e2ae546", "message": "Initial run through the first part of batch integration", "committedDate": "2020-06-02T20:44:23Z", "type": "commit"}, {"oid": "31b7e862d43728f0553ee946df2f648381bd83fa", "url": "https://github.com/hapifhir/hapi-fhir/commit/31b7e862d43728f0553ee946df2f648381bd83fa", "message": "Remove extra beans", "committedDate": "2020-06-02T22:00:56Z", "type": "commit"}, {"oid": "e8970bce465603e1a1fe641875c03d9d2a98bc8b", "url": "https://github.com/hapifhir/hapi-fhir/commit/e8970bce465603e1a1fe641875c03d9d2a98bc8b", "message": "Running batch job", "committedDate": "2020-06-02T23:08:19Z", "type": "commit"}, {"oid": "7d46b3cc10122f4d2db2d6a349c70198f941719c", "url": "https://github.com/hapifhir/hapi-fhir/commit/7d46b3cc10122f4d2db2d6a349c70198f941719c", "message": "Partitioning Based on resource type complete. Still no threading", "committedDate": "2020-06-03T21:58:42Z", "type": "commit"}, {"oid": "7ed65b0080bed4d498b99b0757b819257110959d", "url": "https://github.com/hapifhir/hapi-fhir/commit/7ed65b0080bed4d498b99b0757b819257110959d", "message": "partially done writer, working out size-based chunking", "committedDate": "2020-06-05T03:58:06Z", "type": "commit"}, {"oid": "4405e50db9e8bb17f2ef87ee44dd969857bced29", "url": "https://github.com/hapifhir/hapi-fhir/commit/4405e50db9e8bb17f2ef87ee44dd969857bced29", "message": "wip", "committedDate": "2020-06-05T22:53:06Z", "type": "commit"}, {"oid": "44aa688a20cd216560c164a32c6c81962da6be47", "url": "https://github.com/hapifhir/hapi-fhir/commit/44aa688a20cd216560c164a32c6c81962da6be47", "message": "Final config layout of Bulk Export Batch job", "committedDate": "2020-06-08T22:59:08Z", "type": "commit"}, {"oid": "864c9c2cb81ec736aa334f353bcca519c7dc81e4", "url": "https://github.com/hapifhir/hapi-fhir/commit/864c9c2cb81ec736aa334f353bcca519c7dc81e4", "message": "Add task executor to partition step", "committedDate": "2020-06-08T23:35:15Z", "type": "commit"}, {"oid": "25fa0377df579fa64163474ca657fa59accc3cd0", "url": "https://github.com/hapifhir/hapi-fhir/commit/25fa0377df579fa64163474ca657fa59accc3cd0", "message": "tidying", "committedDate": "2020-06-09T00:02:37Z", "type": "commit"}, {"oid": "278a1070aeac0d82175c629a08b93a272a36c611", "url": "https://github.com/hapifhir/hapi-fhir/commit/278a1070aeac0d82175c629a08b93a272a36c611", "message": ":Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-09T00:16:11Z", "type": "commit"}, {"oid": "8b995f8be7700da93de43631308e6602f5d9a202", "url": "https://github.com/hapifhir/hapi-fhir/commit/8b995f8be7700da93de43631308e6602f5d9a202", "message": "Remove dead test", "committedDate": "2020-06-09T00:16:22Z", "type": "commit"}, {"oid": "9c153527dd862d64bc4d363379df894b5d80596f", "url": "https://github.com/hapifhir/hapi-fhir/commit/9c153527dd862d64bc4d363379df894b5d80596f", "message": "remove dead imports", "committedDate": "2020-06-09T16:03:56Z", "type": "commit"}, {"oid": "7273457ea76aa4ee482c126d2408721567aa7ae6", "url": "https://github.com/hapifhir/hapi-fhir/commit/7273457ea76aa4ee482c126d2408721567aa7ae6", "message": "Rework to read and process larger chunks", "committedDate": "2020-06-09T16:55:10Z", "type": "commit"}, {"oid": "4f59c38692fd9650c196150a035917d702e9c0c8", "url": "https://github.com/hapifhir/hapi-fhir/commit/4f59c38692fd9650c196150a035917d702e9c0c8", "message": "Remove comment", "committedDate": "2020-06-09T17:20:41Z", "type": "commit"}, {"oid": "a25a1064f843a13e0d1c7c07d779c688ceb1942f", "url": "https://github.com/hapifhir/hapi-fhir/commit/a25a1064f843a13e0d1c7c07d779c688ceb1942f", "message": "remove constant", "committedDate": "2020-06-09T17:22:31Z", "type": "commit"}, {"oid": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "url": "https://github.com/hapifhir/hapi-fhir/commit/b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "message": "Add invalid parameter validator", "committedDate": "2020-06-09T18:02:37Z", "type": "commit"}, {"oid": "57f2002b841b81edce2f4d254e9a673a1bbca616", "url": "https://github.com/hapifhir/hapi-fhir/commit/57f2002b841b81edce2f4d254e9a673a1bbca616", "message": "organize imports", "committedDate": "2020-06-09T19:49:11Z", "type": "commit"}, {"oid": "7fdd0f17531bff1323e81303888a7584638e1e10", "url": "https://github.com/hapifhir/hapi-fhir/commit/7fdd0f17531bff1323e81303888a7584638e1e10", "message": "Merge remote-tracking branch 'remotes/origin/master' into spring-batch-integration", "committedDate": "2020-06-09T19:52:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NDkwNQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437674905", "bodyText": "Javadoc on interfaces", "author": "fil512", "createdAt": "2020-06-09T19:43:28Z", "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/api/IBatchJobSubmitter.java", "diffHunk": "@@ -0,0 +1,11 @@\n+package ca.uhn.fhir.jpa.batch.api;\n+\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+\n+public interface IBatchJobSubmitter {", "originalCommit": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NjM0MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437766340", "bodyText": "\ud83d\udc4d", "author": "tadgh", "createdAt": "2020-06-09T22:55:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NDkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NjcwNw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437676707", "bodyText": "This claims to be a Spring Config but doesn't contain any beans?", "author": "fil512", "createdAt": "2020-06-09T19:46:53Z", "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/config/InMemoryJobRepositoryBatchConfig.java", "diffHunk": "@@ -0,0 +1,66 @@\n+package ca.uhn.fhir.jpa.batch.config;\n+\n+import org.springframework.batch.core.configuration.annotation.BatchConfigurer;\n+import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.explore.JobExplorer;\n+import org.springframework.batch.core.explore.support.MapJobExplorerFactoryBean;\n+import org.springframework.batch.core.launch.JobLauncher;\n+import org.springframework.batch.core.launch.support.SimpleJobLauncher;\n+import org.springframework.batch.core.repository.JobRepository;\n+import org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean;\n+import org.springframework.batch.support.transaction.ResourcelessTransactionManager;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.transaction.PlatformTransactionManager;\n+\n+import javax.annotation.PostConstruct;\n+\n+@Configuration", "originalCommit": "b1d2ab7619e86d84952b69bb0ff16e6af1b6513c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3MTg4OQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437771889", "bodyText": "@EnableBatchProcessing does, and the @configuration is required on it.", "author": "tadgh", "createdAt": "2020-06-09T23:11:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY3NjcwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4Mzc3OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437683778", "bodyText": "move to common?", "author": "fil512", "createdAt": "2020-06-09T20:00:09Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1MTA0MQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437751041", "bodyText": "This is specifically for bulk export, so I don't think it should live in common", "author": "tadgh", "createdAt": "2020-06-09T22:11:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4Mzc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4MzkwOQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437683909", "bodyText": "rename", "author": "fil512", "createdAt": "2020-06-09T20:00:22Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3NzM3Nw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437777377", "bodyText": "done", "author": "tadgh", "createdAt": "2020-06-09T23:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4MzkwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDA0NA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437684044", "bodyText": "move to common?", "author": "fil512", "createdAt": "2020-06-09T20:00:40Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {\n+\t\treturn myStepBuilderFactory.get(\"partitionStep\")\n+\t\t\t.partitioner(\"bulkExportGenerateResourceFilesStep\", partitioner(null))\n+\t\t\t.step(bulkExportGenerateResourceFilesStep())\n+\t\t\t.taskExecutor(myTaskExecutor)\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic BulkItemReader bulkItemReader(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\tBulkItemReader bulkItemReader = new BulkItemReader();\n+\t\tbulkItemReader.setJobUUID(theJobUUID);\n+\t\treturn bulkItemReader;\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic ResourceTypePartitioner partitioner(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcxMTk3MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437711970", "bodyText": "It isn't really common, since this step actually comes a BulkExportEntity job UUID, it isn't really reusable. Maybe It's possible to refactor such that it doesn't need it directly though", "author": "tadgh", "createdAt": "2020-06-09T20:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDA0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDM5OQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437684399", "bodyText": "move to common?", "author": "fil512", "createdAt": "2020-06-09T20:01:16Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobConfig.java", "diffHunk": "@@ -0,0 +1,101 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.batch.processors.PidToIBaseResourceProcessor;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.JobScope;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.core.task.TaskExecutor;\n+\n+import java.util.List;\n+\n+/**\n+ * Spring batch Job configuration file. Contains all necessary plumbing to run a\n+ * Bulk Export job.\n+ */\n+@Configuration\n+public class BulkExportJobConfig {\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate PidToIBaseResourceProcessor myPidToIBaseResourceProcessor;\n+\n+\t@Autowired\n+\tprivate TaskExecutor myTaskExecutor;\n+\n+\t@Bean\n+\tpublic Job bulkExportJob() {\n+\t\treturn myJobBuilderFactory.get(\"bulkExportJob\")\n+\t\t\t.validator(jobExistsValidator())\n+\t\t\t.start(partitionStep())\n+\t\t\t.listener(bulkExportJobCompletionListener())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\tpublic JobParametersValidator jobExistsValidator() {\n+\t\treturn new JobExistsParameterValidator();\n+\t}\n+\n+\n+\t@Bean\n+\tpublic Step bulkExportGenerateResourceFilesStep() {\n+\t\treturn myStepBuilderFactory.get(\"bulkExportGenerateResourceFilesStep\")\n+\t\t\t.<List<ResourcePersistentId>, List<IBaseResource>> chunk(100) //1000 resources per generated file, as the reader returns 10 resources at a time.\n+\t\t\t.reader(bulkItemReader(null))\n+\t\t\t.processor(myPidToIBaseResourceProcessor)\n+\t\t\t.writer(resourceToFileWriter())\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic BulkExportJobStatusChangeListener bulkExportJobCompletionListener() {\n+\t\treturn new BulkExportJobStatusChangeListener();\n+\t}\n+\n+\t@Bean\n+\tpublic Step partitionStep() {\n+\t\treturn myStepBuilderFactory.get(\"partitionStep\")\n+\t\t\t.partitioner(\"bulkExportGenerateResourceFilesStep\", partitioner(null))\n+\t\t\t.step(bulkExportGenerateResourceFilesStep())\n+\t\t\t.taskExecutor(myTaskExecutor)\n+\t\t\t.build();\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic BulkItemReader bulkItemReader(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\tBulkItemReader bulkItemReader = new BulkItemReader();\n+\t\tbulkItemReader.setJobUUID(theJobUUID);\n+\t\treturn bulkItemReader;\n+\t}\n+\n+\t@Bean\n+\t@JobScope\n+\tpublic ResourceTypePartitioner partitioner(@Value(\"#{jobParameters['jobUUID']}\") String theJobUUID) {\n+\t\treturn new ResourceTypePartitioner(theJobUUID);\n+\t}\n+\n+\t@Bean\n+\t@StepScope\n+\tpublic ItemWriter<List<IBaseResource>> resourceToFileWriter() {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1MTE3Mg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437751172", "bodyText": "Also relies on specific bulk export stuff", "author": "tadgh", "createdAt": "2020-06-09T22:11:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NDM5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTE2NA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437685164", "bodyText": "Won't it just keep failing endlessly?  I wonder if failed jobs should move to an error state with maybe a way to retry them?", "author": "fil512", "createdAt": "2020-06-09T20:02:45Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportJobStatusChangeListener.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.springframework.batch.core.BatchStatus;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobExecutionListener;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+/**\n+ * Will run before and after a job to set the status to whatever is appropriate.\n+ */\n+public class BulkExportJobStatusChangeListener implements JobExecutionListener {\n+\n+\t@Value(\"#{jobParameters['jobUUID']}\")\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\t@Override\n+\tpublic void beforeJob(JobExecution theJobExecution) {\n+\t\tif (theJobExecution.getStatus() == BatchStatus.STARTING) {\n+\t\t\tmyBulkExportDaoSvc.setJobToStatus(myJobUUID, BulkJobStatusEnum.BUILDING);\n+\t\t}\n+\n+\t}\n+\n+\t@Override\n+\tpublic void afterJob(JobExecution theJobExecution) {\n+\t\tif (theJobExecution.getStatus() == BatchStatus.COMPLETED) {\n+\t\t\tmyBulkExportDaoSvc.setJobToStatus(myJobUUID, BulkJobStatusEnum.COMPLETE);\n+\t\t} else {\n+\t\t\t//If the job didn't complete successfully, just set it back to submitted so that it gets picked up again by the scheduler.", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2MzA3Mg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437763072", "bodyText": "Good point, I've changed this to set it to ERROR state instead. It's probably worth looking into a restart policy. James said for now it's fine to just fail out of it with an error state.", "author": "tadgh", "createdAt": "2020-06-09T22:45:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTkwMQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437685901", "bodyText": "should this be configurable?  (e.g. one of the job params...?)", "author": "fil512", "createdAt": "2020-06-09T20:04:20Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc1MTM1Nw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437751357", "bodyText": "Yeah makes sense for it to be. I just copied what was hardcoded in the original implementation.", "author": "tadgh", "createdAt": "2020-06-09T22:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NTkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NjM0MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437686340", "bodyText": "warn", "author": "fil512", "createdAt": "2020-06-09T20:05:09Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;\n+\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate SearchBuilderFactory mySearchBuilderFactory;\n+\n+\tprivate BulkExportJobEntity myJobEntity;\n+\n+\tprivate String myJobUUID;\n+\n+\t@Value(\"#{stepExecutionContext['resourceType']}\")\n+\tprivate String myResourceType;\n+\n+\tIterator<ResourcePersistentId> myPidIterator;\n+\n+\tprivate void loadResourcePids() {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(myJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NzAzMw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437687033", "bodyText": "feels like we'll probably want a separate logger for batch jobs so they don't get swamped by fhir request logs", "author": "fil512", "createdAt": "2020-06-09T20:06:32Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -0,0 +1,106 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.interceptor.model.RequestPartitionId;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.dao.IResultIterator;\n+import ca.uhn.fhir.jpa.dao.ISearchBuilder;\n+import ca.uhn.fhir.jpa.dao.SearchBuilderFactory;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n+import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n+import ca.uhn.fhir.rest.param.DateRangeParam;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.NonTransientResourceException;\n+import org.springframework.batch.item.ParseException;\n+import org.springframework.batch.item.UnexpectedInputException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n+\tprivate static final Logger ourLog = LoggerFactory.getLogger(BulkItemReader.class);\n+\n+\tprivate static final int READ_CHUNK_SIZE = 10;\n+\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate SearchBuilderFactory mySearchBuilderFactory;\n+\n+\tprivate BulkExportJobEntity myJobEntity;\n+\n+\tprivate String myJobUUID;\n+\n+\t@Value(\"#{stepExecutionContext['resourceType']}\")\n+\tprivate String myResourceType;\n+\n+\tIterator<ResourcePersistentId> myPidIterator;\n+\n+\tprivate void loadResourcePids() {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(myJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");\n+\t\t\treturn;\n+\t\t}\n+\t\tmyJobEntity = jobOpt.get();\n+\t\tourLog.info(\"Bulk export starting generation for batch export job: {}\", myJobEntity);", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc2NTQ1OQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437765459", "bodyText": "Good call. I've duplicated the work done in EMPI for batch", "author": "tadgh", "createdAt": "2020-06-09T22:52:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4NzAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4ODkxOA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437688918", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * This class will prevent job running if the UUID is found to be non-existent, or invalid.\n          \n          \n            \n             * This class will prevent a job from running if the UUID does not exist or is invalid.", "author": "fil512", "createdAt": "2020-06-09T20:10:10Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.apache.commons.lang3.StringUtils;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.Optional;\n+\n+/**\n+ * This class will prevent job running if the UUID is found to be non-existent, or invalid.", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY4OTU0Ng==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437689546", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\t\tthrow new JobParametersInvalidException(\"You did not pass a jobUUID to this job!\");\n          \n          \n            \n            \t\t\tthrow new JobParametersInvalidException(\"Missing jobUUID job parameter\");", "author": "fil512", "createdAt": "2020-06-09T20:11:25Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.apache.commons.lang3.StringUtils;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.JobParametersValidator;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.Optional;\n+\n+/**\n+ * This class will prevent job running if the UUID is found to be non-existent, or invalid.\n+ */\n+public class JobExistsParameterValidator implements JobParametersValidator {\n+\t@Autowired\n+\tprivate IBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Override\n+\tpublic void validate(JobParameters theJobParameters) throws JobParametersInvalidException {\n+\t\tString jobUUID = theJobParameters.getString(\"jobUUID\");\n+\t\tif (StringUtils.isBlank(jobUUID)) {\n+\t\t\tthrow new JobParametersInvalidException(\"You did not pass a jobUUID to this job!\");", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MDA5MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437690090", "bodyText": "I like the way the steps are separated out by Spring Batch.  Feels like it cleans up and organizes our batch jobs--plus provides for better re-use", "author": "fil512", "createdAt": "2020-06-09T20:12:29Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MDk3Ng==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437690976", "bodyText": "nitpick: there are so many contexts, I like to call these myFhirContext", "author": "fil512", "createdAt": "2020-06-09T20:14:11Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {\n+\tprivate static final Logger ourLog = getLogger(ResourceToFileWriter.class);\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MjY2Nw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437692667", "bodyText": "nitpick: simple if statements are more approachable to junior developers", "author": "fil512", "createdAt": "2020-06-09T20:17:19Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceToFileWriter.java", "diffHunk": "@@ -0,0 +1,103 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.context.FhirContext;\n+import ca.uhn.fhir.jpa.api.dao.DaoRegistry;\n+import ca.uhn.fhir.jpa.api.dao.IFhirResourceDao;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.parser.IParser;\n+import ca.uhn.fhir.rest.api.Constants;\n+import ca.uhn.fhir.util.BinaryUtil;\n+import org.hl7.fhir.instance.model.api.IBaseBinary;\n+import org.hl7.fhir.instance.model.api.IBaseResource;\n+import org.hl7.fhir.instance.model.api.IIdType;\n+import org.slf4j.Logger;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.beans.factory.annotation.Value;\n+\n+import javax.annotation.PostConstruct;\n+import java.io.ByteArrayOutputStream;\n+import java.io.OutputStreamWriter;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceToFileWriter implements ItemWriter<List<IBaseResource>> {\n+\tprivate static final Logger ourLog = getLogger(ResourceToFileWriter.class);\n+\n+\t@Autowired\n+\tprivate FhirContext myContext;\n+\n+\t@Autowired\n+\tprivate DaoRegistry myDaoRegistry;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tprivate ByteArrayOutputStream myOutputStream;\n+\tprivate OutputStreamWriter myWriter;\n+\tprivate IParser myParser;\n+\n+\t@Value(\"#{stepExecutionContext['bulkExportCollectionEntityId']}\")\n+\tprivate Long myBulkExportCollectionEntityId;\n+\n+\tprivate IFhirResourceDao<IBaseBinary> myBinaryDao;\n+\n+\n+\tpublic ResourceToFileWriter() {\n+\t\tmyOutputStream = new ByteArrayOutputStream();\n+\t\tmyWriter = new OutputStreamWriter(myOutputStream, Constants.CHARSET_UTF8);\n+\t}\n+\n+\t@PostConstruct\n+\tpublic void start() {\n+\t\tmyParser = myContext.newJsonParser().setPrettyPrint(false);\n+\t\tmyBinaryDao = getBinaryDao();\n+\t}\n+\n+\tprivate Optional<IIdType> flushToFiles() {\n+\t\tif (myOutputStream.size() > 0) {\n+\t\t\tIIdType createdId = createBinaryFromOutputStream();\n+\t\t\tBulkExportCollectionFileEntity file = new BulkExportCollectionFileEntity();\n+\t\t\tfile.setResource(createdId.getIdPart());\n+\n+\t\t\tmyBulkExportDaoSvc.addFileToCollectionWithId(myBulkExportCollectionEntityId, file);\n+\n+\t\t\tmyOutputStream.reset();\n+\n+\t\t\treturn Optional.of(createdId);\n+\t\t}\n+\n+\t\treturn Optional.empty();\n+\t}\n+\n+\tprivate IIdType createBinaryFromOutputStream() {\n+\t\tIBaseBinary binary = BinaryUtil.newBinary(myContext);\n+\t\tbinary.setContentType(Constants.CT_FHIR_NDJSON);\n+\t\tbinary.setContent(myOutputStream.toByteArray());\n+\n+\t\treturn myBinaryDao.create(binary).getResource().getIdElement();\n+\t}\n+\n+\n+\t@SuppressWarnings(\"unchecked\")\n+\tprivate IFhirResourceDao<IBaseBinary> getBinaryDao() {\n+\t\treturn myDaoRegistry.getResourceDao(\"Binary\");\n+\t}\n+\n+\t@Override\n+\tpublic void write(List<? extends List<IBaseResource>> theList) throws Exception {\n+\n+\t\tfor (List<IBaseResource> resourceList : theList) {\n+\t\t\tfor (IBaseResource nextFileResource : resourceList) {\n+\t\t\t\tmyParser.encodeResourceToWriter(nextFileResource, myWriter);\n+\t\t\t\tmyWriter.append(\"\\n\");\n+\t\t\t}\n+\t\t}\n+\n+\t\tOptional<IIdType> createdId = flushToFiles();\n+\t\tcreatedId.ifPresent(theIIdType -> ourLog.warn(\"Created resources for bulk export file containing {} resources of type \", theIIdType.toUnqualifiedVersionless().getValue()));", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MzA3MQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437693071", "bodyText": "commented out code", "author": "fil512", "createdAt": "2020-06-09T20:18:07Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceTypePartitioner.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.partition.support.Partitioner;\n+import org.springframework.batch.item.ExecutionContext;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceTypePartitioner implements Partitioner {\n+\tprivate static final Logger ourLog = getLogger(ResourceTypePartitioner.class);\n+\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tpublic ResourceTypePartitioner(String theJobUUID) {\n+\t\tmyJobUUID = theJobUUID;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, ExecutionContext> partition(int gridSize) {\n+\t\tMap<String, ExecutionContext> partitionContextMap = new HashMap<>();\n+\n+\t\tMap<Long, String> idToResourceType = myBulkExportDaoSvc.getBulkJobCollectionIdToResourceTypeMap(\tmyJobUUID);\n+\t\t//observation -> obs1.json, obs2.json, obs3.json BulkJobCollectionEntity", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5MzU4Ng==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437693586", "bodyText": "nitpick: replace large block with method reference", "author": "fil512", "createdAt": "2020-06-09T20:19:13Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/ResourceTypePartitioner.java", "diffHunk": "@@ -0,0 +1,60 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.partition.support.Partitioner;\n+import org.springframework.batch.item.ExecutionContext;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class ResourceTypePartitioner implements Partitioner {\n+\tprivate static final Logger ourLog = getLogger(ResourceTypePartitioner.class);\n+\n+\tprivate String myJobUUID;\n+\n+\t@Autowired\n+\tprivate BulkExportDaoSvc myBulkExportDaoSvc;\n+\n+\tpublic ResourceTypePartitioner(String theJobUUID) {\n+\t\tmyJobUUID = theJobUUID;\n+\t}\n+\n+\t@Override\n+\tpublic Map<String, ExecutionContext> partition(int gridSize) {\n+\t\tMap<String, ExecutionContext> partitionContextMap = new HashMap<>();\n+\n+\t\tMap<Long, String> idToResourceType = myBulkExportDaoSvc.getBulkJobCollectionIdToResourceTypeMap(\tmyJobUUID);\n+\t\t//observation -> obs1.json, obs2.json, obs3.json BulkJobCollectionEntity\n+\t\t//bulk Collection Entity ID -> patient\n+\n+\t\t// 123123-> Patient\n+\t\t// 91876389126-> Observation\n+\t\tidToResourceType.entrySet().stream()\n+\t\t\t.forEach(entry -> {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NDQzNw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437694437", "bodyText": "nice to see this class shrink in size!  :-)", "author": "fil512", "createdAt": "2020-06-09T20:20:46Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkDataExportSvcImpl.java", "diffHunk": "@@ -429,7 +320,7 @@ private void updateExpiry(BulkExportJobEntity theJob) {\n \n \t@Transactional\n \t@Override\n-\tpublic JobInfo getJobStatusOrThrowResourceNotFound(String theJobId) {\n+\tpublic JobInfo getJobInfoOrThrowResourceNotFound(String theJobId) {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTI1MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695250", "bodyText": "warn", "author": "fil512", "createdAt": "2020-06-09T20:22:10Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTMzNQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695335", "bodyText": "and name the uuid", "author": "fil512", "createdAt": "2020-06-09T20:22:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTUyMA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437695520", "bodyText": "warn and name it.", "author": "fil512", "createdAt": "2020-06-09T20:22:43Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -0,0 +1,81 @@\n+package ca.uhn.fhir.jpa.bulk.svc;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportCollectionFileDao;\n+import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportCollectionFileEntity;\n+import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n+import org.slf4j.Logger;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Service;\n+\n+import javax.transaction.Transactional;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+@Service\n+public class BulkExportDaoSvc {\n+\tprivate static final Logger ourLog = getLogger(BulkExportDaoSvc.class);\n+\n+\t@Autowired\n+\tIBulkExportJobDao myBulkExportJobDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionDao myBulkExportCollectionDao;\n+\n+\t@Autowired\n+\tIBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n+\n+\t@Transactional\n+\tpublic void addFileToCollectionWithId(Long theCollectionEntityId, BulkExportCollectionFileEntity theFile) {\n+\t\tOptional<BulkExportCollectionEntity> byId = myBulkExportCollectionDao.findById(theCollectionEntityId);\n+\t\tif (byId.isPresent()) {\n+\t\t\tBulkExportCollectionEntity exportCollectionEntity = byId.get();\n+\t\t\ttheFile.setCollection(exportCollectionEntity);;\n+\t\t\tmyBulkExportCollectionFileDao.saveAndFlush(theFile);\n+\t\t\tmyBulkExportCollectionDao.saveAndFlush(exportCollectionEntity);\n+\t\t}\n+\n+\t}\n+\n+\t@Transactional\n+\tpublic Map<Long, String> getBulkJobCollectionIdToResourceTypeMap(String theJobUUID) {\n+\t\tBulkExportJobEntity bulkExportJobEntity = loadJob(theJobUUID);\n+\t\tCollection<BulkExportCollectionEntity> collections = bulkExportJobEntity.getCollections();\n+\t\treturn collections.stream()\n+\t\t\t.collect(Collectors.toMap(\n+\t\t\t\tBulkExportCollectionEntity::getId,\n+\t\t\t\tBulkExportCollectionEntity::getResourceType\n+\t\t\t));\n+\t}\n+\n+\tprivate BulkExportJobEntity loadJob(String theJobUUID) {\n+\t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!jobOpt.isPresent()) {\n+\t\t\tourLog.info(\"Job appears to be deleted\");\n+\t\t\treturn null;\n+\t\t}\n+\t\treturn jobOpt.get();\n+\t}\n+\n+\t@Transactional\n+\tpublic void setJobToStatus(String theJobUUID, BulkJobStatusEnum theStatus) {\n+\t\tOptional<BulkExportJobEntity> oJob = myBulkExportJobDao.findByJobId(theJobUUID);\n+\t\tif (!oJob.isPresent()) {\n+\t\t\tourLog.error(\"Job doesn't exist!\");", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NjE2NA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437696164", "bodyText": "nitpick: I'd be inclined to return here and drop the else", "author": "fil512", "createdAt": "2020-06-09T20:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMjc0MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r438302740", "bodyText": "Done", "author": "tadgh", "createdAt": "2020-06-10T17:46:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NTUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5Njk1NA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437696954", "bodyText": "commented code", "author": "fil512", "createdAt": "2020-06-09T20:25:32Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/dao/data/IBulkExportCollectionDao.java", "diffHunk": "@@ -36,4 +36,6 @@\n \t@Query(\"DELETE FROM BulkExportCollectionEntity t WHERE t.myId = :pid\")\n \tvoid deleteByPid(@Param(\"pid\") Long theId);\n \n+//\t@Query(\"SELECT BulkExportCollectionEntity \")\n+//\tvoid findByJobId(Long theId);", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NzQ4NA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437697484", "bodyText": "Would it make sense for us to subclass Job and autowire by type?  (Would save the @Qualifier)", "author": "fil512", "createdAt": "2020-06-09T20:26:37Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -41,6 +52,12 @@\n \tprivate IBulkExportCollectionFileDao myBulkExportCollectionFileDao;\n \t@Autowired\n \tprivate IBulkDataExportSvc myBulkDataExportSvc;\n+\t@Autowired\n+\tprivate IBatchJobSubmitter myBatchJobSubmitter;\n+\n+\t@Autowired\n+\t@Qualifier(\"bulkExportJob\")\n+\tprivate Job myBulkJob;", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyNTM3OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437825378", "bodyText": "Maybe? I'll look into this. JobBuilderFactory returns a Job specifically, so I'll see if theres a way to do this without the builder.", "author": "tadgh", "createdAt": "2020-06-10T02:30:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzY5NzQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMDA2Nw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437700067", "bodyText": "This is a nice test.  Demonstrates how simple and clean it is to create and submit a job.", "author": "fil512", "createdAt": "2020-06-09T20:31:42Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());\n+\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMDM0OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437700348", "bodyText": "Feels like needing to create a uuid is such a common thing all the services are going to want to do, I wonder if we should hide this by default and just do it for them.", "author": "fil512", "createdAt": "2020-06-09T20:32:15Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyMTQzMw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437821433", "bodyText": "So this is a thing Specific (i believe) to bulk export. When you ask for a bulk export, it creates a BulkExportJobEntity which has a UUID. When the job runs, all the generated files end up pointing to this entity, via BulkExportCollectionEntity which each contain a set of BulkExportCollectionFileEntity The UUID is the UUID of this persisted job. I suppose we could remove the UUID from there, and just have a foreign reference to the existing Spring Batch job, but thats probably a separate discussion. By default, each Spring batch Job execution has a job execution ID, it just isn't shown here.", "author": "tadgh", "createdAt": "2020-06-10T02:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMDM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMTI5Mw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437701293", "bodyText": "await().until(() -> myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(theJobId).getStatus() == BulkJobStatusEnum.COMPLETE)", "author": "fil512", "createdAt": "2020-06-09T20:34:10Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -234,7 +251,45 @@ public void testSubmitReusesExisting() {\n \t}\n \n \n-\t\t@Test\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", jobDetails.getJobId());\n+\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\n+\t\tIBulkDataExportSvc.JobInfo jobInfo = awaitJobCompletion(jobDetails.getJobId());\n+\t\tassertThat(jobInfo.getStatus(), equalTo(BulkJobStatusEnum.COMPLETE));\n+\t}\n+\n+\t@Test\n+\tpublic void testJobParametersValidatorRejectsInvalidParameters() {\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", \"I'm not real!\");\n+\t\ttry {\n+\t\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\t\t\tfail(\"Should have had invalid parameter execption!\");\n+\t\t} catch (JobParametersInvalidException e) {\n+\n+\t\t}\n+\n+\t}\n+\n+\tpublic IBulkDataExportSvc.JobInfo awaitJobCompletion(String theJobId) throws InterruptedException {\n+\t\twhile(true) {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjYyMg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437702622", "bodyText": "Change this comment so it makes sense outside the context of this commit:\nPlease do not rename this bean to \"transactionManager\" as that will conflict with the Spring Batch transactionManager.", "author": "fil512", "createdAt": "2020-06-09T20:36:51Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/config/TestJPAConfig.java", "diffHunk": "@@ -40,8 +40,12 @@ public ModelConfig modelConfig() {\n \t\treturn daoConfig().getModelConfig();\n \t}\n \n+\t/*\n+\tI had to rename this bean as it was clashing with Spring Batch `transactionManager` in SimpleBatchConfiguration", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjc4MQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437702781", "bodyText": "or something like that", "author": "fil512", "createdAt": "2020-06-09T20:37:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjYyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyMjA3MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437822070", "bodyText": "Sounds good!", "author": "tadgh", "createdAt": "2020-06-10T02:17:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMjYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzA2MQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437703061", "bodyText": "is it possible to call it jpaTransactionManager ?", "author": "fil512", "createdAt": "2020-06-09T20:37:39Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/config/TestJPAConfig.java", "diffHunk": "@@ -40,8 +40,12 @@ public ModelConfig modelConfig() {\n \t\treturn daoConfig().getModelConfig();\n \t}\n \n+\t/*\n+\tI had to rename this bean as it was clashing with Spring Batch `transactionManager` in SimpleBatchConfiguration\n+\t */\n \t@Bean\n-\tpublic JpaTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {\n+\t@Primary\n+\tpublic JpaTransactionManager hapiTransactionManager(EntityManagerFactory entityManagerFactory) {", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcxMjM4Nw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437712387", "bodyText": "When I tried that, it conflicted with the one provided with @EnableBatchProcessing", "author": "tadgh", "createdAt": "2020-06-09T20:51:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzA2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzc2OQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437703769", "bodyText": "add another version where JobParameters is optional and we auto-create a uuid for the caller?", "author": "fil512", "createdAt": "2020-06-09T20:38:56Z", "path": "hapi-fhir-jpaserver-batch/src/main/java/ca/uhn/fhir/jpa/batch/svc/BatchJobSubmitterImpl.java", "diffHunk": "@@ -0,0 +1,40 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.api.IBatchJobSubmitter;\n+import org.slf4j.Logger;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.JobExecution;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.launch.JobLauncher;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRepository;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import static org.slf4j.LoggerFactory.getLogger;\n+\n+public class BatchJobSubmitterImpl implements IBatchJobSubmitter {\n+\n+\tprivate static final Logger ourLog = getLogger(BatchJobSubmitterImpl.class);\n+\n+\t@Autowired\n+\tprivate JobLauncher myJobLauncher;\n+\n+\t@Autowired\n+\tprivate JobRepository myJobRepository;\n+\n+\t@Override\n+\tpublic JobExecution runJob(Job theJob, JobParameters theJobParameters) throws JobParametersInvalidException{", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3ODcxMg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437778712", "bodyText": "We actually are relying on an existing UUID from a BulkExportJobEntity.", "author": "tadgh", "createdAt": "2020-06-09T23:33:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwMzc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNTQ5MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437705490", "bodyText": "this latch is never used...?", "author": "fil512", "createdAt": "2020-06-09T20:42:13Z", "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/config/BatchJobConfig.java", "diffHunk": "@@ -0,0 +1,84 @@\n+package ca.uhn.fhir.jpa.batch.config;\n+\n+import ca.uhn.fhir.interceptor.api.HookParams;\n+import ca.uhn.test.concurrency.IPointcutLatch;\n+import ca.uhn.test.concurrency.PointcutLatch;\n+import org.springframework.batch.core.Job;\n+import org.springframework.batch.core.Step;\n+import org.springframework.batch.core.configuration.annotation.JobBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepBuilderFactory;\n+import org.springframework.batch.core.configuration.annotation.StepScope;\n+import org.springframework.batch.core.step.tasklet.Tasklet;\n+import org.springframework.batch.item.ItemReader;\n+import org.springframework.batch.item.ItemWriter;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+\n+import java.util.List;\n+\n+@Configuration\n+public class BatchJobConfig implements IPointcutLatch {\n+\n+\t@Autowired\n+\tprivate JobBuilderFactory myJobBuilderFactory;\n+\n+\t@Autowired\n+\tprivate StepBuilderFactory myStepBuilderFactory;\n+", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3ODYwNg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437778606", "bodyText": "Right you are", "author": "tadgh", "createdAt": "2020-06-09T23:32:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNTQ5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNzIyMg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437707222", "bodyText": "This job appears to run forever", "author": "fil512", "createdAt": "2020-06-09T20:45:20Z", "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/svc/BatchSvcTest.java", "diffHunk": "@@ -0,0 +1,24 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.BaseBatchR4Test;\n+import ca.uhn.fhir.jpa.batch.config.BatchJobConfig;\n+import org.junit.Test;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+public class BatchSvcTest extends BaseBatchR4Test {\n+\t@Autowired\n+\tprivate BatchJobConfig myBatchJobConfig;\n+\n+\t@Test\n+\tpublic void testApplicationContextLoads() throws JobParametersInvalidException, JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException, InterruptedException {\n+\t\tmyBatchJobConfig.setExpectedCount(1);\n+\t\tmyJobLauncher.run(myJob, new JobParameters());", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzc3ODc2Ng==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437778766", "bodyText": "Right you are. FIxed", "author": "tadgh", "createdAt": "2020-06-09T23:33:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNzIyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzcwNzM3NQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/1906#discussion_r437707375", "bodyText": "I don't see how this could ever happen...?", "author": "fil512", "createdAt": "2020-06-09T20:45:32Z", "path": "hapi-fhir-jpaserver-batch/src/test/java/ca/uhn/fhir/jpa/batch/svc/BatchSvcTest.java", "diffHunk": "@@ -0,0 +1,24 @@\n+package ca.uhn.fhir.jpa.batch.svc;\n+\n+import ca.uhn.fhir.jpa.batch.BaseBatchR4Test;\n+import ca.uhn.fhir.jpa.batch.config.BatchJobConfig;\n+import org.junit.Test;\n+import org.springframework.batch.core.JobParameters;\n+import org.springframework.batch.core.JobParametersInvalidException;\n+import org.springframework.batch.core.repository.JobExecutionAlreadyRunningException;\n+import org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException;\n+import org.springframework.batch.core.repository.JobRestartException;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+public class BatchSvcTest extends BaseBatchR4Test {\n+\t@Autowired\n+\tprivate BatchJobConfig myBatchJobConfig;\n+\n+\t@Test\n+\tpublic void testApplicationContextLoads() throws JobParametersInvalidException, JobExecutionAlreadyRunningException, JobRestartException, JobInstanceAlreadyCompleteException, InterruptedException {\n+\t\tmyBatchJobConfig.setExpectedCount(1);\n+\t\tmyJobLauncher.run(myJob, new JobParameters());\n+\t\tmyBatchJobConfig.awaitExpected();", "originalCommit": "7fdd0f17531bff1323e81303888a7584638e1e10", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b7cea688f6acd1ccd514fae4838db628338b4171", "url": "https://github.com/hapifhir/hapi-fhir/commit/b7cea688f6acd1ccd514fae4838db628338b4171", "message": "Update hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java\n\nCo-authored-by: Ken Stevens <khstevens@gmail.com>", "committedDate": "2020-06-09T22:52:37Z", "type": "commit"}, {"oid": "d90987368a9dfab00435a1303d5d09df15380b70", "url": "https://github.com/hapifhir/hapi-fhir/commit/d90987368a9dfab00435a1303d5d09df15380b70", "message": "Update hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/JobExistsParameterValidator.java\n\nCo-authored-by: Ken Stevens <khstevens@gmail.com>", "committedDate": "2020-06-09T23:29:15Z", "type": "commit"}, {"oid": "942fc313ec19635fb69635ec73f01d3fd1d6416a", "url": "https://github.com/hapifhir/hapi-fhir/commit/942fc313ec19635fb69635ec73f01d3fd1d6416a", "message": "Code review comments", "committedDate": "2020-06-10T02:00:41Z", "type": "commit"}, {"oid": "c36e1d28995da74200b39d56aef009211fbd5c0c", "url": "https://github.com/hapifhir/hapi-fhir/commit/c36e1d28995da74200b39d56aef009211fbd5c0c", "message": "More code review comments", "committedDate": "2020-06-10T02:18:06Z", "type": "commit"}, {"oid": "6fc5b753e5d36f1d9277373b6bd5f5d1295e2d22", "url": "https://github.com/hapifhir/hapi-fhir/commit/6fc5b753e5d36f1d9277373b6bd5f5d1295e2d22", "message": "Readding imports", "committedDate": "2020-06-10T02:20:43Z", "type": "commit"}, {"oid": "6e43c5348c99f759f1c6e33eba3368229f8f189e", "url": "https://github.com/hapifhir/hapi-fhir/commit/6e43c5348c99f759f1c6e33eba3368229f8f189e", "message": "Add missing imports from import reorganization", "committedDate": "2020-06-10T02:27:16Z", "type": "commit"}, {"oid": "d698ad71801945ef83eba5579092d85bd7115632", "url": "https://github.com/hapifhir/hapi-fhir/commit/d698ad71801945ef83eba5579092d85bd7115632", "message": "Code review comments", "committedDate": "2020-06-10T17:46:28Z", "type": "commit"}, {"oid": "4e42a59ad4b3ebe0a37426db2b916111585fc9e9", "url": "https://github.com/hapifhir/hapi-fhir/commit/4e42a59ad4b3ebe0a37426db2b916111585fc9e9", "message": "Add licenses", "committedDate": "2020-06-10T18:52:45Z", "type": "commit"}, {"oid": "a9c704c06e34c66b007a326147a5a802127988d1", "url": "https://github.com/hapifhir/hapi-fhir/commit/a9c704c06e34c66b007a326147a5a802127988d1", "message": "Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-10T19:24:19Z", "type": "commit"}, {"oid": "0f469c1a56c4196f3fcd1d91d7f1744604d3f5a8", "url": "https://github.com/hapifhir/hapi-fhir/commit/0f469c1a56c4196f3fcd1d91d7f1744604d3f5a8", "message": "Rework to allow for job creation inside of batch", "committedDate": "2020-06-12T21:20:50Z", "type": "commit"}, {"oid": "928aafba1e406c0e912d7c9508ec442cdf0da312", "url": "https://github.com/hapifhir/hapi-fhir/commit/928aafba1e406c0e912d7c9508ec442cdf0da312", "message": "Rename bean", "committedDate": "2020-06-12T23:18:04Z", "type": "commit"}, {"oid": "4270636d434ee60e97404156ad17e85959002ac6", "url": "https://github.com/hapifhir/hapi-fhir/commit/4270636d434ee60e97404156ad17e85959002ac6", "message": "Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-12T23:20:54Z", "type": "commit"}, {"oid": "798fdb9cc2196f30407666a6bf23d29266da844f", "url": "https://github.com/hapifhir/hapi-fhir/commit/798fdb9cc2196f30407666a6bf23d29266da844f", "message": "Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-15T16:10:41Z", "type": "commit"}, {"oid": "eccd280d711856de69e297462fa08ce27d6ce3b7", "url": "https://github.com/hapifhir/hapi-fhir/commit/eccd280d711856de69e297462fa08ce27d6ce3b7", "message": "Fix test error", "committedDate": "2020-06-15T18:24:39Z", "type": "commit"}, {"oid": "9222fa3f59b22ecee2884e61928f2f82df4ed38a", "url": "https://github.com/hapifhir/hapi-fhir/commit/9222fa3f59b22ecee2884e61928f2f82df4ed38a", "message": "change transaction call", "committedDate": "2020-06-15T22:17:25Z", "type": "commit"}, {"oid": "d32ad744021b34af102aa01068a88687decd0403", "url": "https://github.com/hapifhir/hapi-fhir/commit/d32ad744021b34af102aa01068a88687decd0403", "message": "some renaming", "committedDate": "2020-06-16T04:49:34Z", "type": "commit"}, {"oid": "0e86732599c178ebb2400c85df1b70273c4ac579", "url": "https://github.com/hapifhir/hapi-fhir/commit/0e86732599c178ebb2400c85df1b70273c4ac579", "message": "Add changelog", "committedDate": "2020-06-19T21:03:22Z", "type": "commit"}, {"oid": "bb7a773f41378c690a4d427e4970541df50688f9", "url": "https://github.com/hapifhir/hapi-fhir/commit/bb7a773f41378c690a4d427e4970541df50688f9", "message": "Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-23T23:02:41Z", "type": "commit"}, {"oid": "4cccc9804bd411893d64f060919db61316b9a697", "url": "https://github.com/hapifhir/hapi-fhir/commit/4cccc9804bd411893d64f060919db61316b9a697", "message": "nop for CI", "committedDate": "2020-06-23T23:29:36Z", "type": "commit"}, {"oid": "2bd17806c48f0ad756d639f26ac6e22be23f092e", "url": "https://github.com/hapifhir/hapi-fhir/commit/2bd17806c48f0ad756d639f26ac6e22be23f092e", "message": "Update changelog", "committedDate": "2020-06-24T16:46:56Z", "type": "commit"}, {"oid": "59659b280245da10edd1d8d3aff55b0dba48c48d", "url": "https://github.com/hapifhir/hapi-fhir/commit/59659b280245da10edd1d8d3aff55b0dba48c48d", "message": "Move order of modules", "committedDate": "2020-06-24T17:11:30Z", "type": "commit"}, {"oid": "a7740922f9ecf0142cdddd6387afcd7a4d99bd6f", "url": "https://github.com/hapifhir/hapi-fhir/commit/a7740922f9ecf0142cdddd6387afcd7a4d99bd6f", "message": "Specify version", "committedDate": "2020-06-24T18:02:52Z", "type": "commit"}, {"oid": "99fe66bb589507236a51c37a97af7073f3754fb1", "url": "https://github.com/hapifhir/hapi-fhir/commit/99fe66bb589507236a51c37a97af7073f3754fb1", "message": "Fix pom to use project.version", "committedDate": "2020-06-24T18:52:05Z", "type": "commit"}, {"oid": "64dbd038c96dd45292f44691051650a030b39972", "url": "https://github.com/hapifhir/hapi-fhir/commit/64dbd038c96dd45292f44691051650a030b39972", "message": "Increase min thread count for batch. Add new  which waits for all bulkexport jobs to be done", "committedDate": "2020-06-24T20:16:11Z", "type": "commit"}, {"oid": "571589d00de013712b24e25fbdf7b1adfb221b24", "url": "https://github.com/hapifhir/hapi-fhir/commit/571589d00de013712b24e25fbdf7b1adfb221b24", "message": "Merge branch 'master' into spring-batch-integration", "committedDate": "2020-06-24T20:46:13Z", "type": "commit"}, {"oid": "a69c1478a234f31723323d3b613341ded2535e2b", "url": "https://github.com/hapifhir/hapi-fhir/commit/a69c1478a234f31723323d3b613341ded2535e2b", "message": "Merge remote-tracking branch 'remotes/origin/master' into spring-batch-integration", "committedDate": "2020-06-25T02:07:38Z", "type": "commit"}, {"oid": "3da4ad280bda3d89d3041ad670cefb3525a00cd8", "url": "https://github.com/hapifhir/hapi-fhir/commit/3da4ad280bda3d89d3041ad670cefb3525a00cd8", "message": "undoes merge issues", "committedDate": "2020-06-25T03:00:52Z", "type": "commit"}, {"oid": "6875740178255d1b71fea76bb4bbd77c58e35ce5", "url": "https://github.com/hapifhir/hapi-fhir/commit/6875740178255d1b71fea76bb4bbd77c58e35ce5", "message": "Merge branch 'spring-batch-integration' of github.com:jamesagnew/hapi-fhir into spring-batch-integration", "committedDate": "2020-06-25T03:15:47Z", "type": "commit"}, {"oid": "06e1368388a72615a392f92d2c90b635fdad18bc", "url": "https://github.com/hapifhir/hapi-fhir/commit/06e1368388a72615a392f92d2c90b635fdad18bc", "message": "Re-add comment", "committedDate": "2020-06-25T04:00:45Z", "type": "commit"}, {"oid": "e51c5b52630f11298112bde7f721ca80c0cc5e96", "url": "https://github.com/hapifhir/hapi-fhir/commit/e51c5b52630f11298112bde7f721ca80c0cc5e96", "message": "change transactionmanager name so it doesnt shadow the batchprocessing one", "committedDate": "2020-06-25T04:03:02Z", "type": "commit"}, {"oid": "93da042d19138c328006f04b8534338ef6f05b1f", "url": "https://github.com/hapifhir/hapi-fhir/commit/93da042d19138c328006f04b8534338ef6f05b1f", "message": "fix empi tests so they work with Spring Batch\n(all but one test fixed)", "committedDate": "2020-06-25T15:14:42Z", "type": "commit"}, {"oid": "1d9641a7361178d1c72cf37e6aa78da662ffaed3", "url": "https://github.com/hapifhir/hapi-fhir/commit/1d9641a7361178d1c72cf37e6aa78da662ffaed3", "message": "fix empilinks", "committedDate": "2020-06-25T17:28:03Z", "type": "commit"}, {"oid": "ed155e7657ad108b09e38f967fad2cddbabc7082", "url": "https://github.com/hapifhir/hapi-fhir/commit/ed155e7657ad108b09e38f967fad2cddbabc7082", "message": "wip", "committedDate": "2020-06-25T17:37:02Z", "type": "commit"}, {"oid": "21567fdc43955c1d60eb68cd7ffa4203e1574806", "url": "https://github.com/hapifhir/hapi-fhir/commit/21567fdc43955c1d60eb68cd7ffa4203e1574806", "message": "Tidy await", "committedDate": "2020-06-25T18:42:17Z", "type": "commit"}, {"oid": "c11b6eeab6274cf9830ff432c19e05598070c39d", "url": "https://github.com/hapifhir/hapi-fhir/commit/c11b6eeab6274cf9830ff432c19e05598070c39d", "message": "back out Tx wrap\nuse qualifier", "committedDate": "2020-06-25T19:27:31Z", "type": "commit"}, {"oid": "7322760f5d09a19bbd1aeff975193db328012e28", "url": "https://github.com/hapifhir/hapi-fhir/commit/7322760f5d09a19bbd1aeff975193db328012e28", "message": "Merge remote-tracking branch 'origin/spring-batch-integration' into spring-batch-integration", "committedDate": "2020-06-25T19:27:46Z", "type": "commit"}, {"oid": "484f390db07bc87ffb5df7c270e2893fc0e4735e", "url": "https://github.com/hapifhir/hapi-fhir/commit/484f390db07bc87ffb5df7c270e2893fc0e4735e", "message": "Fix batch test", "committedDate": "2020-06-25T20:00:59Z", "type": "commit"}, {"oid": "13bbdde45890fab645c96d2e921b24e47482f2dc", "url": "https://github.com/hapifhir/hapi-fhir/commit/13bbdde45890fab645c96d2e921b24e47482f2dc", "message": "pipeline fun", "committedDate": "2020-06-25T22:38:15Z", "type": "commit"}, {"oid": "0b622597b4c6fccb3422f5643e5f9235c5638fe3", "url": "https://github.com/hapifhir/hapi-fhir/commit/0b622597b4c6fccb3422f5643e5f9235c5638fe3", "message": "Muck with initial loading to allow EMF and platformtransactionmanager to be available", "committedDate": "2020-06-26T17:13:03Z", "type": "commit"}, {"oid": "b31b6652a956d18687fe3a104486a54d9f02b09f", "url": "https://github.com/hapifhir/hapi-fhir/commit/b31b6652a956d18687fe3a104486a54d9f02b09f", "message": "Add async to the job launcher, fix test to reflect this", "committedDate": "2020-06-26T18:51:01Z", "type": "commit"}, {"oid": "8f6aee64b132853d72cf6a0c4036289fd6dc6fea", "url": "https://github.com/hapifhir/hapi-fhir/commit/8f6aee64b132853d72cf6a0c4036289fd6dc6fea", "message": "Fix batch project to now use an async task launcher", "committedDate": "2020-06-26T19:22:23Z", "type": "commit"}, {"oid": "16039af110ca3cf84fb569c2a4d5a02a79a212bb", "url": "https://github.com/hapifhir/hapi-fhir/commit/16039af110ca3cf84fb569c2a4d5a02a79a212bb", "message": "Fix up samlpe starters", "committedDate": "2020-06-26T20:15:31Z", "type": "commit"}, {"oid": "be23eb3cfc58f72df1822ee5fe00f9affa348cd7", "url": "https://github.com/hapifhir/hapi-fhir/commit/be23eb3cfc58f72df1822ee5fe00f9affa348cd7", "message": "Move the flush into the existing transaction", "committedDate": "2020-06-27T22:06:37Z", "type": "commit"}]}