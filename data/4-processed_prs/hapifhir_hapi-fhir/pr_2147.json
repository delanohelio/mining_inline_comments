{"pr_number": 2147, "pr_title": "Suport typeFilter parameter for bulk export", "pr_createdAt": "2020-10-26T14:38:04Z", "pr_url": "https://github.com/hapifhir/hapi-fhir/pull/2147", "timeline": [{"oid": "36ee21c34d006426d56a8080144d2d7543d1504b", "url": "https://github.com/hapifhir/hapi-fhir/commit/36ee21c34d006426d56a8080144d2d7543d1504b", "message": "Cleanup", "committedDate": "2020-10-25T21:26:56Z", "type": "commit"}, {"oid": "9e7315c8ebff9650cfbb5e0454419c1e136147af", "url": "https://github.com/hapifhir/hapi-fhir/commit/9e7315c8ebff9650cfbb5e0454419c1e136147af", "message": "Fix tests", "committedDate": "2020-10-25T22:02:30Z", "type": "commit"}, {"oid": "0c8ff33fdb46d1aec370e157ace0104f6d8dbb11", "url": "https://github.com/hapifhir/hapi-fhir/commit/0c8ff33fdb46d1aec370e157ace0104f6d8dbb11", "message": "Work on bulk export", "committedDate": "2020-10-25T23:24:18Z", "type": "commit"}, {"oid": "cb320496e0ff8e5f65a4c0abf536387ef88f2711", "url": "https://github.com/hapifhir/hapi-fhir/commit/cb320496e0ff8e5f65a4c0abf536387ef88f2711", "message": "Work on export", "committedDate": "2020-10-26T09:24:58Z", "type": "commit"}, {"oid": "6296b88a12a19912d379724cb747ee53ebcf990d", "url": "https://github.com/hapifhir/hapi-fhir/commit/6296b88a12a19912d379724cb747ee53ebcf990d", "message": "Merge branch 'master' into ja_20201025_bulk_export_cleanup", "committedDate": "2020-10-26T13:06:10Z", "type": "commit"}, {"oid": "24cf3f126af77014a8a9479b6d4f9b97fc6f64da", "url": "https://github.com/hapifhir/hapi-fhir/commit/24cf3f126af77014a8a9479b6d4f9b97fc6f64da", "message": "Test fixes", "committedDate": "2020-10-26T14:37:31Z", "type": "commit"}, {"oid": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "url": "https://github.com/hapifhir/hapi-fhir/commit/0323cfe91c80e7167c01afab2d75b4b9042d2885", "message": "Add changelog", "committedDate": "2020-10-26T14:38:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMzcwMw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512013703", "bodyText": "Genrate -> Generate", "author": "tadgh", "createdAt": "2020-10-26T14:42:28Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkExportGenrateResourceFilesStepListener.java", "diffHunk": "@@ -0,0 +1,34 @@\n+package ca.uhn.fhir.jpa.bulk.job;\n+\n+import ca.uhn.fhir.jpa.bulk.model.BulkJobStatusEnum;\n+import ca.uhn.fhir.jpa.bulk.svc.BulkExportDaoSvc;\n+import org.springframework.batch.core.ExitStatus;\n+import org.springframework.batch.core.StepExecution;\n+import org.springframework.batch.core.StepExecutionListener;\n+import org.springframework.beans.factory.annotation.Autowired;\n+\n+import javax.annotation.Nonnull;\n+\n+import static org.apache.commons.lang3.StringUtils.isNotBlank;\n+\n+public class BulkExportGenrateResourceFilesStepListener  implements StepExecutionListener {", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNDYyMg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512014622", "bodyText": "Maybe add a small class javadoc indicating that this class is meant to set correct failure state on the entity. It's obvious enough reading the afterStep, but a small docstring could help", "author": "tadgh", "createdAt": "2020-10-26T14:43:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMzcwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzNjc4MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512036780", "bodyText": "Done", "author": "jamesagnew", "createdAt": "2020-10-26T15:11:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMzcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNzQ1OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512017458", "bodyText": "This logical block could be ripped out into a helper function for conciseness.", "author": "tadgh", "createdAt": "2020-10-26T14:47:12Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/job/BulkItemReader.java", "diffHunk": "@@ -31,73 +32,78 @@\n import ca.uhn.fhir.jpa.dao.data.IBulkExportJobDao;\n import ca.uhn.fhir.jpa.entity.BulkExportJobEntity;\n import ca.uhn.fhir.jpa.model.search.SearchRuntimeDetails;\n+import ca.uhn.fhir.jpa.model.util.JpaConstants;\n+import ca.uhn.fhir.jpa.searchparam.MatchUrlService;\n import ca.uhn.fhir.jpa.searchparam.SearchParameterMap;\n import ca.uhn.fhir.rest.api.server.storage.ResourcePersistentId;\n import ca.uhn.fhir.rest.param.DateRangeParam;\n+import ca.uhn.fhir.util.UrlUtil;\n import org.hl7.fhir.instance.model.api.IBaseResource;\n import org.slf4j.Logger;\n import org.springframework.batch.item.ItemReader;\n-import org.springframework.batch.item.NonTransientResourceException;\n-import org.springframework.batch.item.ParseException;\n-import org.springframework.batch.item.UnexpectedInputException;\n import org.springframework.beans.factory.annotation.Autowired;\n import org.springframework.beans.factory.annotation.Value;\n \n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n import java.util.Optional;\n \n public class BulkItemReader implements ItemReader<List<ResourcePersistentId>> {\n \tprivate static final Logger ourLog = Logs.getBatchTroubleshootingLog();\n-\n+\tIterator<ResourcePersistentId> myPidIterator;\n \t@Value(\"#{jobParameters['readChunkSize']}\")\n \tprivate Long READ_CHUNK_SIZE;\n-\n \t@Autowired\n \tprivate IBulkExportJobDao myBulkExportJobDao;\n-\n \t@Autowired\n \tprivate DaoRegistry myDaoRegistry;\n-\n \t@Autowired\n \tprivate FhirContext myContext;\n-\n \t@Autowired\n \tprivate SearchBuilderFactory mySearchBuilderFactory;\n-\n-\tprivate BulkExportJobEntity myJobEntity;\n-\n \t@Value(\"#{jobExecutionContext['jobUUID']}\")\n \tprivate String myJobUUID;\n-\n \t@Value(\"#{stepExecutionContext['resourceType']}\")\n \tprivate String myResourceType;\n-\n-\tIterator<ResourcePersistentId> myPidIterator;\n+\t@Autowired\n+\tprivate MatchUrlService myMatchUrlService;\n \n \tprivate void loadResourcePids() {\n \t\tOptional<BulkExportJobEntity> jobOpt = myBulkExportJobDao.findByJobId(myJobUUID);\n \t\tif (!jobOpt.isPresent()) {\n \t\t\tourLog.warn(\"Job appears to be deleted\");\n \t\t\treturn;\n \t\t}\n-\t\tmyJobEntity = jobOpt.get();\n-\t\tourLog.info(\"Bulk export starting generation for batch export job: {}\", myJobEntity);\n+\t\tBulkExportJobEntity jobEntity = jobOpt.get();\n+\t\tourLog.info(\"Bulk export starting generation for batch export job: {}\", jobEntity);\n \n-\t\tIFhirResourceDao dao = myDaoRegistry.getResourceDao(myResourceType);\n+\t\tIFhirResourceDao<?> dao = myDaoRegistry.getResourceDao(myResourceType);\n \n \t\tourLog.info(\"Bulk export assembling export of type {} for job {}\", myResourceType, myJobUUID);\n \n-\t\tClass<? extends IBaseResource> nextTypeClass = myContext.getResourceDefinition(myResourceType).getImplementingClass();\n+\t\tRuntimeResourceDefinition def = myContext.getResourceDefinition(myResourceType);\n+\t\tClass<? extends IBaseResource> nextTypeClass = def.getImplementingClass();\n \t\tISearchBuilder sb = mySearchBuilderFactory.newSearchBuilder(dao, myResourceType, nextTypeClass);\n \n \t\tSearchParameterMap map = new SearchParameterMap();\n-\t\tmap.setLoadSynchronous(true);\n-\t\tif (myJobEntity.getSince() != null) {\n-\t\t\tmap.setLastUpdated(new DateRangeParam(myJobEntity.getSince(), null));\n+\t\tMap<String, String[]> requestUrl = UrlUtil.parseQueryStrings(jobEntity.getRequest());\n+\t\tString[] typeFilters = requestUrl.get(JpaConstants.PARAM_EXPORT_TYPE_FILTER);\n+\t\tif (typeFilters != null) {\n+\t\t\tOptional<String> filter = Arrays.stream(typeFilters).filter(t -> t.startsWith(myResourceType + \"?\")).findFirst();\n+\t\t\tif (filter.isPresent()) {\n+\t\t\t\tString matchUrl = filter.get();\n+\t\t\t\tmap = myMatchUrlService.translateMatchUrl(matchUrl, def);\n+\t\t\t}\n+\t\t}\n+", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzNzg4Mg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512037882", "bodyText": "done", "author": "jamesagnew", "createdAt": "2020-10-26T15:12:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNzQ1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMDk5MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512020990", "bodyText": "Another good candidate for helper extraction", "author": "tadgh", "createdAt": "2020-10-26T14:51:28Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkDataExportSvcImpl.java", "diffHunk": "@@ -290,14 +284,35 @@ public JobInfo submitJob(String theOutputFormat, Set<String> theResourceTypes, D\n \t\tjob.setCreated(new Date());\n \t\tjob.setRequest(request);\n \n-\t\tupdateExpiry(job);\n-\t\tmyBulkExportJobDao.save(job);\n-\n+\t\t// Validate types\n \t\tfor (String nextType : resourceTypes) {\n \t\t\tif (!myDaoRegistry.isResourceTypeSupported(nextType)) {\n \t\t\t\tString msg = myContext.getLocalizer().getMessage(BulkDataExportSvcImpl.class, \"unknownResourceType\", nextType);\n \t\t\t\tthrow new InvalidRequestException(msg);\n \t\t\t}\n+\t\t}\n+\n+\t\t// Validate type filter\n+\t\tif (theFilters != null) {\n+\t\t\tSet<String> types = new HashSet<>();\n+\t\t\tfor (String next : theFilters) {\n+\t\t\t\tif (!next.contains(\"?\")) {\n+\t\t\t\t\tthrow new InvalidRequestException(\"Invalid \" + JpaConstants.PARAM_EXPORT_TYPE_FILTER + \" value \\\"\" + next + \"\\\". Must be in the form [ResourceType]?[params]\");\n+\t\t\t\t}\n+\t\t\t\tString resourceType = next.substring(0, next.indexOf(\"?\"));\n+\t\t\t\tif (!resourceTypes.contains(resourceType)) {\n+\t\t\t\t\tthrow new InvalidRequestException(\"Invalid \" + JpaConstants.PARAM_EXPORT_TYPE_FILTER + \" value \\\"\" + next + \"\\\". Resource type does not appear in \" + JpaConstants.PARAM_EXPORT_TYPE + \" list\");\n+\t\t\t\t}\n+\t\t\t\tif (!types.add(resourceType)) {\n+\t\t\t\t\tthrow new InvalidRequestException(\"Invalid \" + JpaConstants.PARAM_EXPORT_TYPE_FILTER + \" value \\\"\" + next + \"\\\". Multiple filters found for type \" + resourceType);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzNzk3NQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512037975", "bodyText": "done", "author": "jamesagnew", "createdAt": "2020-10-26T15:12:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMDk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMTczOQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512021739", "bodyText": "Do we still need this? Trying to recall where this is used", "author": "tadgh", "createdAt": "2020-10-26T14:52:26Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkDataExportSvcImpl.java", "diffHunk": "@@ -374,4 +388,14 @@ public synchronized void cancelAndPurgeAllJobs() {\n \t\t\treturn null;\n \t\t});\n \t}\n+\n+\tpublic static class Job implements HapiJob {\n+\t\t@Autowired\n+\t\tprivate IBulkDataExportSvc myTarget;\n+\n+\t\t@Override\n+\t\tpublic void execute(JobExecutionContext theContext) {\n+\t\t\tmyTarget.buildExportFiles();\n+\t\t}\n+\t}", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzODY3OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512038678", "bodyText": "I believe it's needed.. this is the scheduled job that looks for job entities that aren't in the job executor yet and requests the batch job to start", "author": "jamesagnew", "createdAt": "2020-10-26T15:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMTczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMjgxMg==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512022812", "bodyText": "This was all just the old method right?", "author": "tadgh", "createdAt": "2020-10-26T14:53:48Z", "path": "hapi-fhir-jpaserver-base/src/main/java/ca/uhn/fhir/jpa/bulk/svc/BulkExportDaoSvc.java", "diffHunk": "@@ -120,96 +104,9 @@ public void setJobToStatus(String theJobUUID, BulkJobStatusEnum theStatus) {\n \t\tourLog.info(\"Setting job with UUID {} to {}\", theJobUUID, theStatus);\n \t\tBulkExportJobEntity bulkExportJobEntity = oJob.get();\n \t\tbulkExportJobEntity.setStatus(theStatus);\n+\t\tbulkExportJobEntity.setStatusMessage(theStatusMessage);\n \t\tmyBulkExportJobDao.save(bulkExportJobEntity);\n \n \t}\n \n-\tpublic IBulkDataExportSvc.JobInfo submitJob(String theOutputFormat, Set<String> theResourceTypes, Date theSince, Set<String> theFilters, int theReuseMillis) {\n-\t\tString outputFormat = Constants.CT_FHIR_NDJSON;\n-\t\tif (isNotBlank(theOutputFormat)) {\n-\t\t\toutputFormat = theOutputFormat;\n-\t\t}\n-\t\tif (!Constants.CTS_NDJSON.contains(outputFormat)) {\n-\t\t\tthrow new InvalidRequestException(\"Invalid output format: \" + theOutputFormat);\n-\t\t}\n-\n-\t\tStringBuilder requestBuilder = new StringBuilder();\n-\t\trequestBuilder.append(\"/\").append(JpaConstants.OPERATION_EXPORT);\n-\t\trequestBuilder.append(\"?\").append(JpaConstants.PARAM_EXPORT_OUTPUT_FORMAT).append(\"=\").append(escapeUrlParam(outputFormat));\n-\t\tSet<String> resourceTypes = theResourceTypes;\n-\t\tif (resourceTypes != null) {\n-\t\t\trequestBuilder.append(\"&\").append(JpaConstants.PARAM_EXPORT_TYPE).append(\"=\").append(String.join(\",\", resourceTypes));\n-\t\t}\n-\t\tDate since = theSince;\n-\t\tif (since != null) {\n-\t\t\trequestBuilder.append(\"&\").append(JpaConstants.PARAM_EXPORT_SINCE).append(\"=\").append(new InstantType(since).setTimeZoneZulu(true).getValueAsString());\n-\t\t}\n-\t\tif (theFilters != null && theFilters.size() > 0) {\n-\t\t\trequestBuilder.append(\"&\").append(JpaConstants.PARAM_EXPORT_TYPE_FILTER).append(\"=\").append(String.join(\",\", theFilters));\n-\t\t}\n-\t\tString request = requestBuilder.toString();\n-\n-\t\tDate cutoff = DateUtils.addMilliseconds(new Date(), -theReuseMillis);\n-\t\tPageable page = PageRequest.of(0, 10);\n-\t\tSlice<BulkExportJobEntity> existing = myBulkExportJobDao.findExistingJob(page, request, cutoff, BulkJobStatusEnum.ERROR);\n-\t\tif (!existing.isEmpty()) {\n-\t\t\treturn toSubmittedJobInfo(existing.iterator().next());\n-\t\t}\n-\n-\t\tif (resourceTypes != null && resourceTypes.contains(\"Binary\")) {\n-\t\t\tString msg = myFhirContext.getLocalizer().getMessage(BulkDataExportSvcImpl.class, \"onlyBinarySelected\");\n-\t\t\tthrow new InvalidRequestException(msg);\n-\t\t}\n-\n-\t\tif (resourceTypes == null || resourceTypes.isEmpty()) {\n-\t\t\t// This is probably not a useful default, but having the default be \"download the whole\n-\t\t\t// server\" seems like a risky default too. We'll deal with that by having the default involve\n-\t\t\t// only returning a small time span\n-\t\t\tresourceTypes = myFhirContext.getResourceTypes();\n-\t\t\tif (since == null) {\n-\t\t\t\tsince = DateUtils.addDays(new Date(), -1);\n-\t\t\t}\n-\t\t}\n-\n-\t\tresourceTypes =\n-\t\t\tresourceTypes\n-\t\t\t\t.stream()\n-\t\t\t\t.filter(t -> !\"Binary\".equals(t))\n-\t\t\t\t.collect(Collectors.toSet());\n-\n-\t\tBulkExportJobEntity job = new BulkExportJobEntity();\n-\t\tjob.setJobId(UUID.randomUUID().toString());\n-\t\tjob.setStatus(BulkJobStatusEnum.SUBMITTED);\n-\t\tjob.setSince(since);\n-\t\tjob.setCreated(new Date());\n-\t\tjob.setRequest(request);\n-\n-\t\tupdateExpiry(job);\n-\t\tmyBulkExportJobDao.save(job);\n-\n-\t\tfor (String nextType : resourceTypes) {\n-\t\t\tif (!myDaoRegistry.isResourceTypeSupported(nextType)) {\n-\t\t\t\tString msg = myFhirContext.getLocalizer().getMessage(BulkDataExportSvcImpl.class, \"unknownResourceType\", nextType);\n-\t\t\t\tthrow new InvalidRequestException(msg);\n-\t\t\t}\n-\n-\t\t\tBulkExportCollectionEntity collection = new BulkExportCollectionEntity();\n-\t\t\tcollection.setJob(job);\n-\t\t\tcollection.setResourceType(nextType);\n-\t\t\tjob.getCollections().add(collection);\n-\t\t\tmyBulkExportCollectionDao.save(collection);\n-\t\t}\n-\n-\t\tourLog.info(\"Bulk export job submitted: {}\", job.toString());\n-\n-\t\treturn toSubmittedJobInfo(job);\n-\t}\n-\n-\tprivate void updateExpiry(BulkExportJobEntity theJob) {\n-\t\ttheJob.setExpiry(DateUtils.addMilliseconds(new Date(), myRetentionPeriod));\n-\t}\n-\tprivate IBulkDataExportSvc.JobInfo toSubmittedJobInfo(BulkExportJobEntity theJob) {\n-\t\treturn new IBulkDataExportSvc.JobInfo().setJobId(theJob.getJobId());\n-\t}\n-", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzODkyNw==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512038927", "bodyText": "yeah", "author": "jamesagnew", "createdAt": "2020-10-26T15:14:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMjgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNDk3OA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512024978", "bodyText": "This is an awesome test", "author": "tadgh", "createdAt": "2020-10-26T14:56:28Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -156,7 +162,89 @@ public void testSubmit_InvalidResourceTypes() {\n \t}\n \n \t@Test\n-\tpublic void testSubmitForSpecificResources() {\n+\tpublic void testSubmit_MultipleTypeFiltersForSameType() {\n+\t\ttry {\n+\t\t\tmyBulkDataExportSvc.submitJob(Constants.CT_FHIR_NDJSON, Sets.newHashSet(\"Patient\"), null, Sets.newHashSet(\"Patient?name=a\", \"Patient?active=true\"));\n+\t\t\tfail();\n+\t\t} catch (InvalidRequestException e) {\n+\t\t\tassertEquals(\"Invalid _typeFilter value \\\"Patient?name=a\\\". Multiple filters found for type Patient\", e.getMessage());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testSubmit_TypeFilterForNonSelectedType() {\n+\t\ttry {\n+\t\t\tmyBulkDataExportSvc.submitJob(Constants.CT_FHIR_NDJSON, Sets.newHashSet(\"Patient\"), null, Sets.newHashSet(\"Observation?code=123\"));\n+\t\t\tfail();\n+\t\t} catch (InvalidRequestException e) {\n+\t\t\tassertEquals(\"Invalid _typeFilter value \\\"Observation?code=123\\\". Resource type does not appear in _type list\", e.getMessage());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testSubmit_TypeFilterInvalid() {\n+\t\ttry {\n+\t\t\tmyBulkDataExportSvc.submitJob(Constants.CT_FHIR_NDJSON, Sets.newHashSet(\"Patient\"), null, Sets.newHashSet(\"Hello\"));\n+\t\t\tfail();\n+\t\t} catch (InvalidRequestException e) {\n+\t\t\tassertEquals(\"Invalid _typeFilter value \\\"Hello\\\". Must be in the form [ResourceType]?[params]\", e.getMessage());\n+\t\t}\n+\t}\n+\n+\t@Test\n+\tpublic void testSubmit_ReusesExisting() {\n+\n+\t\t// Submit\n+\t\tIBulkDataExportSvc.JobInfo jobDetails1 = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\t\tassertNotNull(jobDetails1.getJobId());\n+\n+\t\t// Submit again\n+\t\tIBulkDataExportSvc.JobInfo jobDetails2 = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\t\tassertNotNull(jobDetails2.getJobId());\n+\n+\t\tassertEquals(jobDetails1.getJobId(), jobDetails2.getJobId());\n+\t}\n+\n+\t@Test\n+\tpublic void testGenerateBulkExport_FailureDuringGeneration() {\n+\n+\t\t// Register an interceptor that will force the resource search to fail unexpectedly\n+\t\tIAnonymousInterceptor interceptor = (pointcut, args) -> {\n+\t\t\tthrow new NullPointerException(\"help i'm a bug\");\n+\t\t};\n+\t\tmyInterceptorRegistry.registerAnonymousInterceptor(Pointcut.JPA_PERFTRACE_SEARCH_SELECT_COMPLETE, interceptor);\n+\n+\t\ttry {\n+\n+\t\t\t// Create some resources to load\n+\t\t\tcreateResources();\n+\n+\t\t\t// Create a bulk job\n+\t\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\"), null, null);\n+\t\t\tassertNotNull(jobDetails.getJobId());\n+\n+\t\t\t// Check the status\n+\t\t\tIBulkDataExportSvc.JobInfo status = myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(jobDetails.getJobId());\n+\t\t\tassertEquals(BulkJobStatusEnum.SUBMITTED, status.getStatus());\n+\n+\t\t\t// Run a scheduled pass to build the export\n+\t\t\tmyBulkDataExportSvc.buildExportFiles();\n+\n+\t\t\tawaitAllBulkJobCompletions();\n+\n+\t\t\t// Fetch the job again\n+\t\t\tstatus = myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(jobDetails.getJobId());\n+\t\t\tassertEquals(BulkJobStatusEnum.ERROR, status.getStatus());\n+\t\t\tassertThat(status.getStatusMessage(), containsString(\"help i'm a bug\"));\n+\n+\t\t} finally {", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAzOTk5MA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512039990", "bodyText": "", "author": "jamesagnew", "createdAt": "2020-10-26T15:15:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNDk3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNTkxMQ==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512025911", "bodyText": "Did I just never complete this comment?!", "author": "tadgh", "createdAt": "2020-10-26T14:57:39Z", "path": "hapi-fhir-jpaserver-base/src/test/java/ca/uhn/fhir/jpa/bulk/BulkDataExportSvcImplR4Test.java", "diffHunk": "@@ -400,15 +437,95 @@ public void testSubmit_WithSince() throws InterruptedException {\n \t\t}\n \t}\n \n+\t@Test\n+\tpublic void testBatchJobIsCapableOfCreatingAnExportEntityIfNoJobIsProvided() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t//Add the UUID to the job\n+\t\tBulkExportJobParametersBuilder paramBuilder = new BulkExportJobParametersBuilder();\n+\t\tparamBuilder\n+\t\t\t.setReadChunkSize(100L)\n+\t\t\t.setOutputFormat(Constants.CT_FHIR_NDJSON)\n+\t\t\t.setResourceTypes(Arrays.asList(\"Patient\", \"Observation\"));\n+\n+\t\tJobExecution jobExecution = myBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\n+\t\tawaitJobCompletion(jobExecution);\n+\t\tString jobUUID = (String) jobExecution.getExecutionContext().get(\"jobUUID\");\n+\t\tIBulkDataExportSvc.JobInfo jobInfo = myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(jobUUID);\n+\n+\t\tassertThat(jobInfo.getStatus(), equalTo(BulkJobStatusEnum.COMPLETE));\n+\t\tassertThat(jobInfo.getFiles().size(), equalTo(2));\n+\t}\n+\n+\tpublic void awaitAllBulkJobCompletions() {\n+\t\tList<JobInstance> bulkExport = myJobExplorer.findJobInstancesByJobName(\"bulkExportJob\", 0, 100);\n+\t\tif (bulkExport.isEmpty()) {\n+\t\t\tfail(\"There are no bulk export jobs running!\");\n+\t\t}\n+\t\tList<JobExecution> bulkExportExecutions = bulkExport.stream().flatMap(jobInstance -> myJobExplorer.getJobExecutions(jobInstance).stream()).collect(Collectors.toList());\n+\t\tawaitJobCompletions(bulkExportExecutions);\n+\t}\n+\n+\tpublic void awaitJobCompletions(Collection<JobExecution> theJobs) {\n+\t\ttheJobs.forEach(jobExecution -> awaitJobCompletion(jobExecution));\n+\t}\n+\n+\t@Test\n+\tpublic void testBatchJobSubmitsAndRuns() throws Exception {\n+\t\tcreateResources();\n+\n+\t\t// Create a bulk job\n+\t\tIBulkDataExportSvc.JobInfo jobDetails = myBulkDataExportSvc.submitJob(null, Sets.newHashSet(\"Patient\", \"Observation\"), null, null);\n+\n+\t\t//Add the UUID to the job\n+\t\tBulkExportJobParametersBuilder paramBuilder = new BulkExportJobParametersBuilder()\n+\t\t\t.setJobUUID(jobDetails.getJobId())\n+\t\t\t.setReadChunkSize(10L);\n+\n+\t\tJobExecution jobExecution = myBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\n+\t\tawaitJobCompletion(jobExecution);\n+\t\tIBulkDataExportSvc.JobInfo jobInfo = myBulkDataExportSvc.getJobInfoOrThrowResourceNotFound(jobDetails.getJobId());\n+\n+\t\tassertThat(jobInfo.getStatus(), equalTo(BulkJobStatusEnum.COMPLETE));\n+\t\tassertThat(jobInfo.getFiles().size(), equalTo(2));\n+\t}\n+\n+\t@Test\n+\tpublic void testJobParametersValidatorRejectsInvalidParameters() {\n+\t\tJobParametersBuilder paramBuilder = new JobParametersBuilder().addString(\"jobUUID\", \"I'm not real!\");\n+\t\ttry {\n+\t\t\tmyBatchJobSubmitter.runJob(myBulkJob, paramBuilder.toJobParameters());\n+\t\t\tfail(\"Should have had invalid parameter execption!\");\n+\t\t} catch (JobParametersInvalidException e) {\n+\t\t\t// good\n+\t\t}\n+\n+\t}\n+\t//Note that if the job is generated, and doesnt rely on an existed persisted BulkExportJobEntity, it will need to\n+\t//create one itself, which means that its jobUUID isnt known until it starts. to get around this, we move\n+", "originalCommit": "0323cfe91c80e7167c01afab2d75b4b9042d2885", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA0MDcxOA==", "url": "https://github.com/hapifhir/hapi-fhir/pull/2147#discussion_r512040718", "bodyText": "heh looks like. i'll complete it", "author": "jamesagnew", "createdAt": "2020-10-26T15:16:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNTkxMQ=="}], "type": "inlineReview"}, {"oid": "9a8d1309007d8396ee67016f3c48d15479b582d3", "url": "https://github.com/hapifhir/hapi-fhir/commit/9a8d1309007d8396ee67016f3c48d15479b582d3", "message": "Address review comments", "committedDate": "2020-10-26T15:16:48Z", "type": "commit"}, {"oid": "27060b6b87aea8c6dd1e2355db8f30fc87fb096c", "url": "https://github.com/hapifhir/hapi-fhir/commit/27060b6b87aea8c6dd1e2355db8f30fc87fb096c", "message": "Address review comments", "committedDate": "2020-10-26T15:19:28Z", "type": "commit"}]}