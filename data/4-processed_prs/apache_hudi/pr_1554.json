{"pr_number": 1554, "pr_title": "[HUDI-704]Add test for RepairsCommand", "pr_createdAt": "2020-04-23T02:23:26Z", "pr_url": "https://github.com/apache/hudi/pull/1554", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTA2OQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281069", "bodyText": "Let us use the constant from HoodieTestDataGenerator rather than using this string when running actual command.", "author": "pratyakshsharma", "createdAt": "2020-04-26T10:55:38Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTcyMQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281721", "bodyText": "This seems a bit misleading. If the file has time of 20160401010101, how can the records have time of 20160401010202? Rather we should have the file as 3_0_20160401010202.parquet and generate one more .commit file in meta folder for this.\nPlease correct me if I am missing something.", "author": "pratyakshsharma", "createdAt": "2020-04-26T10:58:59Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4MzE4MQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r416283181", "bodyText": "This seems a bit misleading. If the file has time of 20160401010101, how can the records have time of 20160401010202? Rather we should have the file as 3_0_20160401010202.parquet and generate one more .commit file in meta folder for this.\nPlease correct me if I am missing something.\n\nYes, you are right, it maybe cause misleading, had address it, thanks.", "author": "hddong", "createdAt": "2020-04-28T02:36:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTcyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTg4MA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281880", "bodyText": "same here as well. Let us use the constant.", "author": "pratyakshsharma", "createdAt": "2020-04-26T10:59:52Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MjkwMw==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415282903", "bodyText": "Any specific reason for having a separate class for RepairsCommand#deduplicate method?", "author": "pratyakshsharma", "createdAt": "2020-04-26T11:05:09Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI4NTYwNw==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r416285607", "bodyText": "Any specific reason for having a separate class for RepairsCommand#deduplicate method?\n\ndeduplicate is integration test(unit tests do not meet the requirements), SparkLuncher need load jars under lib which generate during mvn package.", "author": "hddong", "createdAt": "2020-04-28T02:45:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MjkwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMzQwMA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418433400", "bodyText": "It would be better to add the description you mentioned into the Javadoc of this class?", "author": "yanghua", "createdAt": "2020-05-01T06:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MjkwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMDMwNA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418430304", "bodyText": "Use Files.createFile()?", "author": "yanghua", "createdAt": "2020-05-01T06:10:13Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMDUyNw==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418430527", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-01T06:11:38Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTE3Ng==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431176", "bodyText": "Multiple methods have this code snippet. Can we extract it into init method? Or Judge exists before create?", "author": "yanghua", "createdAt": "2020-05-01T06:15:11Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTQ4NQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431485", "bodyText": "metaclient -> meta client?", "author": "yanghua", "createdAt": "2020-05-01T06:16:46Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta --dryrun false\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    List<String> paths = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath);\n+    // after dry run, the action will be 'Repaired'\n+    String[][] rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"No\", \"Repaired\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+\n+    cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+\n+    // after real run, Metadata is present now.\n+    rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"Yes\", \"None\"})\n+        .toArray(String[][]::new);\n+    expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair overwrite-hoodie-props'.\n+   */\n+  @Test\n+  public void testOverwriteHoodieProperties() throws IOException {\n+    URL newProps = this.getClass().getClassLoader().getResource(\"table-config.properties\");\n+    assertNotNull(\"New property file must exist\", newProps);\n+\n+    CommandResult cr = getShell().executeCommand(\"repair overwrite-hoodie-props --new-props-file \" + newProps.getPath());\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    Map<String, String> oldProps = HoodieCLI.getTableMetaClient().getTableConfig().getProps();\n+\n+    // after overwrite, the stored value in .hoodie is equals to which read from properties.\n+    Map<String, String> result = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient()).getTableConfig().getProps();\n+    Properties expectProps = new Properties();\n+    expectProps.load(new FileInputStream(new File(newProps.getPath())));\n+\n+    Map<String, String> expected = expectProps.entrySet().stream()\n+        .collect(Collectors.toMap(e -> String.valueOf(e.getKey()), e -> String.valueOf(e.getValue())));\n+    Assert.assertEquals(expected, result);\n+\n+    // check result\n+    List<String> allPropsStr = Arrays.asList(\"hoodie.table.name\", \"hoodie.table.type\",\n+        \"hoodie.archivelog.folder\", \"hoodie.timeline.layout.version\");\n+    String[][] rows = allPropsStr.stream().sorted().map(key -> new String[]{key,\n+        oldProps.getOrDefault(key, null), result.getOrDefault(key, null)})\n+        .toArray(String[][]::new);\n+    String expect = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_HOODIE_PROPERTY,\n+        HoodieTableHeaderFields.HEADER_OLD_VALUE, HoodieTableHeaderFields.HEADER_NEW_VALUE}, rows);\n+\n+    Assert.assertEquals(expect, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair corrupted clean files'.\n+   */\n+  @Test\n+  public void testRemoveCorruptedPendingCleanAction() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+\n+    // Create four requested files\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Write corrupted requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionRequestedFile(tablePath, timestamp, conf);\n+    }\n+\n+    // reload metaclient\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // first, there are four instants\n+    assertEquals(4, metaClient.getActiveTimeline().filterInflightsAndRequested().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"repair corrupted clean files\");\n+    assertTrue(cr.isSuccess());\n+\n+    // reload metaclient", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTU0MQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431541", "bodyText": "metaclient -> meta client?", "author": "yanghua", "createdAt": "2020-05-01T06:17:04Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta --dryrun false\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    List<String> paths = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath);\n+    // after dry run, the action will be 'Repaired'\n+    String[][] rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"No\", \"Repaired\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+\n+    cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+\n+    // after real run, Metadata is present now.\n+    rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"Yes\", \"None\"})\n+        .toArray(String[][]::new);\n+    expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair overwrite-hoodie-props'.\n+   */\n+  @Test\n+  public void testOverwriteHoodieProperties() throws IOException {\n+    URL newProps = this.getClass().getClassLoader().getResource(\"table-config.properties\");\n+    assertNotNull(\"New property file must exist\", newProps);\n+\n+    CommandResult cr = getShell().executeCommand(\"repair overwrite-hoodie-props --new-props-file \" + newProps.getPath());\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    Map<String, String> oldProps = HoodieCLI.getTableMetaClient().getTableConfig().getProps();\n+\n+    // after overwrite, the stored value in .hoodie is equals to which read from properties.\n+    Map<String, String> result = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient()).getTableConfig().getProps();\n+    Properties expectProps = new Properties();\n+    expectProps.load(new FileInputStream(new File(newProps.getPath())));\n+\n+    Map<String, String> expected = expectProps.entrySet().stream()\n+        .collect(Collectors.toMap(e -> String.valueOf(e.getKey()), e -> String.valueOf(e.getValue())));\n+    Assert.assertEquals(expected, result);\n+\n+    // check result\n+    List<String> allPropsStr = Arrays.asList(\"hoodie.table.name\", \"hoodie.table.type\",\n+        \"hoodie.archivelog.folder\", \"hoodie.timeline.layout.version\");\n+    String[][] rows = allPropsStr.stream().sorted().map(key -> new String[]{key,\n+        oldProps.getOrDefault(key, null), result.getOrDefault(key, null)})\n+        .toArray(String[][]::new);\n+    String expect = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_HOODIE_PROPERTY,\n+        HoodieTableHeaderFields.HEADER_OLD_VALUE, HoodieTableHeaderFields.HEADER_NEW_VALUE}, rows);\n+\n+    Assert.assertEquals(expect, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair corrupted clean files'.\n+   */\n+  @Test\n+  public void testRemoveCorruptedPendingCleanAction() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+\n+    // Create four requested files\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Write corrupted requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionRequestedFile(tablePath, timestamp, conf);\n+    }\n+\n+    // reload metaclient", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMzYzOQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418433639", "bodyText": "add an empty line before these two lines and mark them private.", "author": "yanghua", "createdAt": "2020-05-01T06:26:22Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDAxNw==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434017", "bodyText": "Files.createFile", "author": "yanghua", "createdAt": "2020-05-01T06:28:08Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDA1MA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434050", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-01T06:28:21Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDExNw==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434117", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-01T06:28:40Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDMxNQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434315", "bodyText": "map(f -> f.getPath()) -> map(HoodieBaseFile::getPath)", "author": "yanghua", "createdAt": "2020-05-01T06:29:36Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDcxNA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434714", "bodyText": "We do not need to specify the size for toArray(). new String[0] is OK.", "author": "yanghua", "createdAt": "2020-05-01T06:31:47Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDc5OQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434799", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-01T06:32:13Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDgxNg==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434816", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-01T06:32:20Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "22955d0e28152138d32be764265fbbd227a4122f", "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "message": "Add test for RepairsCommand rebase to master", "committedDate": "2020-05-04T02:31:50Z", "type": "commit"}, {"oid": "22955d0e28152138d32be764265fbbd227a4122f", "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "message": "Add test for RepairsCommand rebase to master", "committedDate": "2020-05-04T02:31:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NDkyMA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420784920", "bodyText": "\"Spark Master \" -> \"Spark Master\"?", "author": "yanghua", "createdAt": "2020-05-06T13:22:01Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420785759", "bodyText": "The same suggestion, we should try to define a data structure? We can refactor it later.", "author": "yanghua", "createdAt": "2020-05-06T13:23:10Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,\n+      @CliOption(key = \"sparkMemory\", unspecifiedDefaultValue = \"4G\",\n+          help = \"Spark executor memory\") final String sparkMemory,\n+      @CliOption(key = {\"dryrun\"},\n+          help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n       throws Exception {\n+    if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n+      sparkPropertiesPath =\n+          Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n+    }\n+\n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n-    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), duplicatedPartitionPath, repairedOutputPath,\n-        HoodieCLI.getTableMetaClient().getBasePath());\n+    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM4NzU4OA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r421387588", "bodyText": "The same suggestion, we should try to define a data structure? We can refactor it later.\n\nWe can focus on PR(#1174), but it was left behind for too long.", "author": "hddong", "createdAt": "2020-05-07T10:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNDQxNA==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r421734414", "bodyText": "@hddong Yeah its been open for some time now. The work was mostly done, I was stuck at fixing test cases. Will take a look at it soon. :)", "author": "pratyakshsharma", "createdAt": "2020-05-07T19:15:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjgzMQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420786831", "bodyText": "IMHO, we also need to refactor the arg parse. But not in this PR.", "author": "yanghua", "createdAt": "2020-05-06T13:24:39Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -73,8 +73,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[1], args[2]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 4);\n-        returnCode = deduplicatePartitionPath(jsc, args[1], args[2], args[3]);\n+        assert (args.length == 7);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDQ0Ng==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790446", "bodyText": "Can we use String.format(xxx) here?", "author": "yanghua", "createdAt": "2020-05-06T13:29:33Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDU5MQ==", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790591", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-05-06T13:29:44Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "originalCommit": "22955d0e28152138d32be764265fbbd227a4122f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "95b9e09a708f778262ba0040d651f408e9d6a3d1", "url": "https://github.com/apache/hudi/commit/95b9e09a708f778262ba0040d651f408e9d6a3d1", "message": "fix", "committedDate": "2020-05-07T09:58:47Z", "type": "commit"}]}