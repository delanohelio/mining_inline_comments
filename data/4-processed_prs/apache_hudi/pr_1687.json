{"pr_number": 1687, "pr_title": " [HUDI-684] Introduced abstraction for writing and reading different types of base file formats.", "pr_createdAt": "2020-05-29T21:40:18Z", "pr_url": "https://github.com/apache/hudi/pull/1687", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NTYwNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434265606", "bodyText": "Can we push the sorting to the spark shuffle machinery? repartitionAndSortWithinPartitions(). It cheap and practically free", "author": "vinothchandar", "createdAt": "2020-06-03T01:51:48Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -133,6 +136,12 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n       // Create the writer for writing the new version file\n       storageWriter =\n           HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+\n+      if (hoodieTable.requireSortedRecords()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5Mzc5Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435493796", "bodyText": "We already sort using RDD.sortBy()\n(see WriteHandle.java)\ntaggedRecords.sortBy(r -> r.getRecordKey(), true, taggedRecords.getNumPartitions());\nMerging has some additional complication:\nDuring merging, we are reading existing records (which should be sorted in HFile) and updating a \"few\" of them. Then we are writing all of them back to a new HFile. So effectively, we have two sorted list of records:\n\nThe records being read from existing HFile (sorted in last write)\nThe records being updated (sorted in WriteHandle code)\n\nWe can do it in three steps but this will require a large amount of memory or an ExternalSpillableMap:\n\nRead all records from existing HFile\nApply updates in-memory\nWrite to new HFile\n\nThe way I have implemented is like a merge-sort which does not require reading all the records from the HFile before applying updates.", "author": "prashantwason", "createdAt": "2020-06-04T19:17:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NTYwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2MTQ3Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440461476", "bodyText": "Can we have HashBasedJoin and SortMergeJoin as first level abstractions which both taken in 2 streams of records to be merged ? We can then plugin the algorithm depending on storage preference.", "author": "bvaradar", "createdAt": "2020-06-15T21:37:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NTYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2Nzc2NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434267764", "bodyText": "If we push sorting to spark, then all the iterators fed to create/merge/append handle will sort records by records key.. for merge handle we can do a simple merge sort style sort merge (instead of hash merge). We just assert that both existing files and incoming records are sorted", "author": "vinothchandar", "createdAt": "2020-06-03T02:01:09Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -214,6 +223,36 @@ private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord>\n    */\n   public void write(GenericRecord oldRecord) {\n     String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    if (hoodieTable.requireSortedRecords()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MzEwNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434273106", "bodyText": "The advantage is more modularity as well as low memory overhead to merge..\nNote that we don\u2019t do this for data today because we may not necessarily want the data Sorted by key", "author": "vinothchandar", "createdAt": "2020-06-03T02:24:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2Nzc2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NDkwMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435494902", "bodyText": "In my changes:\n\nSpark based sorting is used (see WriteHandle.java)\nSorting is ONLY done if format requires it (so there is no change to Parquet format)\nMerge style sort is implemented\n\n\n\nThe advantage is more modularity as well as low memory overhead to merge..\nYes, that was my vision too.", "author": "prashantwason", "createdAt": "2020-06-04T19:19:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2Nzc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MzkzNQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434273935", "bodyText": "I did not review this block too closely. Let\u2019s settle on the high level approach first", "author": "vinothchandar", "createdAt": "2020-06-03T02:28:38Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -214,6 +223,36 @@ private boolean writeRecord(HoodieRecord<T> hoodieRecord, Option<IndexedRecord>\n    */\n   public void write(GenericRecord oldRecord) {\n     String key = oldRecord.get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+\n+    if (hoodieTable.requireSortedRecords()) {\n+      // To maintain overall sorted order across updates and inserts, write any new inserts whose keys are less than\n+      // the oldRecord's key.\n+      while (!newRecordKeysSorted.isEmpty() && newRecordKeysSorted.peek().compareTo(key) <= 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5NTMxNQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435495315", "bodyText": "Sure. BTW this is the merge sort style merge you pointed in the previous comment.", "author": "prashantwason", "createdAt": "2020-06-04T19:20:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MzkzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434274651", "bodyText": "HFilev itself contains bloom filters right.. why do we need them outside?", "author": "vinothchandar", "createdAt": "2020-06-03T02:31:54Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieStorageWriterFactory.java", "diffHunk": "@@ -66,4 +67,21 @@\n \n     return new HoodieParquetWriter<>(instantTime, path, parquetConfig, schema, sparkTaskContextSupplier);\n   }\n+\n+  private static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> newHFileStorageWriter(\n+      String instantTime, Path path, HoodieWriteConfig config, Schema schema, HoodieTable hoodieTable,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    BloomFilter filter = createBloomFilter(config);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTQ5OTU5OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435499599", "bodyText": "HFile allows adding BloomFilters using an interface. They are not always present (i.e. there is no AvroWriteSupport kind of implementation).\nI think there are some benefits to using Hoodie's BloomFilter:\n\nIf we use HBase's implementation of BloomFilter, we will have to convert it to HoodieBloom filter as Hoodie code uses HoodieBloomFilter in functions\nAdvances in Hoodie's Bloom filter (compression, dynamic sizing) etc will not be available to HFile format\nCommon interface across all base file formats so easier to maintain in the long run and easier to update", "author": "prashantwason", "createdAt": "2020-06-04T19:29:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg0NjM2Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r437846366", "bodyText": "okay. I was thinking about dynamic sizing as well . 2.\nSo IIUC, HFile does natively support serializing a bloom filter with the file? and thats why we take advantage of?", "author": "vinothchandar", "createdAt": "2020-06-10T03:59:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwNjQxNw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440406417", "bodyText": "Nope. We are serializing the HoodieBloomFilter to bytes and saving this as a metadata block in HFile. HFile has support for adding custom named blocks of data which are loaded on demand (unlike Parquet Footer which is always read).\nHFile has its own interface BloomFilters which can also be implemented by HoodieBloomFilter. But that sounds overkill to me.\nAs more base file formats are added, I think we should focus on common functionality so that we prevent code duplication.", "author": "prashantwason", "createdAt": "2020-06-15T19:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzNzAzNw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441937037", "bodyText": "I think the dynamic bloom is reason enough. So good here", "author": "vinothchandar", "createdAt": "2020-06-18T02:44:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NTE1OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434275159", "bodyText": "StorageWriter is not a great name really.. let\u2019s rename consistently to HoodieFileWriter/HoodieFileWriterFactory?", "author": "vinothchandar", "createdAt": "2020-06-03T02:33:48Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieStorageWriterFactory.java", "diffHunk": "@@ -34,29 +35,29 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n public class HoodieStorageWriterFactory {\n \n   public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n       String instantTime, Path path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema,\n       SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n-    final String name = path.getName();\n-    final String extension = FSUtils.isLogFile(path) ? HOODIE_LOG.getFileExtension() : FSUtils.getFileExtension(name);\n+    final HoodieTableConfig tableConfig = hoodieTable.getMetaClient().getTableConfig();\n+    final String extension = FSUtils.isLogFile(path) ? tableConfig.getLogFileFormat().getFileExtension() : tableConfig.getBaseFileFormat().getFileExtension();\n     if (PARQUET.getFileExtension().equals(extension)) {\n       return newParquetStorageWriter(instantTime, path, config, schema, hoodieTable, sparkTaskContextSupplier);\n     }\n+    if (HFILE.getFileExtension().equals(extension)) {\n+      return newHFileStorageWriter(instantTime, path, config, schema, hoodieTable, sparkTaskContextSupplier);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTUwMTU5Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435501593", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-06-04T19:33:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NTE1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434282887", "bodyText": "Assume this actually pushed the projection predicates down??", "author": "vinothchandar", "createdAt": "2020-06-03T03:07:09Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -89,11 +87,12 @@ public CommitActionExecutor(JavaSparkContext jsc,\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(table.getHadoopConf(), upsertHandle.getWriterSchema());\n       BoundedInMemoryExecutor<GenericRecord, GenericRecord, Void> wrapper = null;\n-      try (ParquetReader<IndexedRecord> reader =\n-          AvroParquetReader.<IndexedRecord>builder(upsertHandle.getOldFilePath()).withConf(table.getHadoopConf()).build()) {\n-        wrapper = new SparkBoundedInMemoryExecutor(config, new ParquetReaderIterator(reader),\n+      try {\n+        HoodieStorageReader<IndexedRecord> storageReader =\n+            HoodieStorageReaderFactory.getStorageReader(table.getHadoopConf(), upsertHandle.getOldFilePath());\n+        wrapper =\n+            new SparkBoundedInMemoryExecutor(config, storageReader.getRecordIterator(upsertHandle.getWriterSchema()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYyODA0Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435628047", "bodyText": "This is just creating a ParquetReader and getting an iterator to read all the records. The entire record will need to be read here as we are merging.\nI didn't understand how predicates are applicable here. Are they not in the InputFormat?", "author": "prashantwason", "createdAt": "2020-06-05T00:28:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzODIyMQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441938221", "bodyText": "No we specify a projection schema even without input format being involved in other places. E.g: in fetching keys for indexing", "author": "vinothchandar", "createdAt": "2020-06-18T02:48:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2ODY3Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443268672", "bodyText": "@vinothchandar :  Guess its taken care of. Here is the getRecordIterator for Parquet.\npublic Iterator<R> getRecordIterator(Schema schema) throws IOException {\n    AvroReadSupport.setAvroReadSchema(conf, schema);\n    ParquetReader<IndexedRecord> reader = AvroParquetReader.<IndexedRecord>builder(path).withConf(conf).build();\n    return new ParquetReaderIterator(reader);\n  }\n\nWe set the scheme before instantiating the reader.\nLet me know is my understanding is wrong.", "author": "nsivabalan", "createdAt": "2020-06-21T23:47:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4Mjg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434283607", "bodyText": "Have you tried to fish out all such occurrences in this pr. This is great!", "author": "vinothchandar", "createdAt": "2020-06-03T03:10:12Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/rollback/RollbackHelper.java", "diffHunk": "@@ -71,8 +71,9 @@ public RollbackHelper(HoodieTableMetaClient metaClient, HoodieWriteConfig config\n    */\n   public List<HoodieRollbackStat> performRollback(JavaSparkContext jsc, HoodieInstant instantToRollback, List<RollbackRequest> rollbackRequests) {\n \n+    String basefileExtension = metaClient.getTableConfig().getBaseFileFormat().getFileExtension();\n     SerializablePathFilter filter = (path) -> {\n-      if (path.toString().contains(\".parquet\")) {\n+      if (path.toString().contains(basefileExtension)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTUxMjg2Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435512867", "bodyText": "Except for tests I have removed all occurrences. Left a few TODOs for test functions which can be removed.", "author": "prashantwason", "createdAt": "2020-06-04T19:54:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzNzMwOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441937309", "bodyText": "Can we also get rid of those in this pr itself", "author": "vinothchandar", "createdAt": "2020-06-18T02:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTc4OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444689789", "bodyText": "@prashantwason : Can you confirm if all occurrences (including tests) are taken care of ?", "author": "bvaradar", "createdAt": "2020-06-24T07:11:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk5NTI2MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445995261", "bodyText": "Created Jira to tack this https://issues.apache.org/jira/browse/HUDI-1055", "author": "bvaradar", "createdAt": "2020-06-26T06:38:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MzYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMzMQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284331", "bodyText": "hudi-common has Scala now? Why do we need this", "author": "vinothchandar", "createdAt": "2020-06-03T03:13:28Z", "path": "hudi-common/pom.xml", "diffHunk": "@@ -78,6 +92,38 @@\n           </imports>\n         </configuration>\n       </plugin>\n+      <plugin>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3OTIyMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440479220", "bodyText": "+1", "author": "bvaradar", "createdAt": "2020-06-15T22:21:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUwOTk2Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440509967", "bodyText": "Removed.", "author": "prashantwason", "createdAt": "2020-06-15T23:58:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDQ3OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284479", "bodyText": "Please pull this into a variable in parent pom", "author": "vinothchandar", "createdAt": "2020-06-03T03:14:10Z", "path": "hudi-common/pom.xml", "diffHunk": "@@ -201,7 +247,26 @@\n       <groupId>org.apache.hbase</groupId>\n       <artifactId>hbase-server</artifactId>\n       <version>${hbase.version}</version>\n-      <scope>test</scope>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- Spark -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- spark-avro -->\n+    <dependency>\n+      <groupId>com.databricks</groupId>\n+      <artifactId>spark-avro_${scala.binary.version}</artifactId>\n+      <version>4.0.0</version>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxMDA4Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440510082", "bodyText": "Moved.", "author": "prashantwason", "createdAt": "2020-06-15T23:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDU5Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434284593", "bodyText": "Why do we need this? hudi-common having spark is an anti pattern", "author": "vinothchandar", "createdAt": "2020-06-03T03:14:43Z", "path": "hudi-common/pom.xml", "diffHunk": "@@ -201,7 +247,26 @@\n       <groupId>org.apache.hbase</groupId>\n       <artifactId>hbase-server</artifactId>\n       <version>${hbase.version}</version>\n-      <scope>test</scope>\n+      <scope>provided</scope>\n+    </dependency>\n+\n+    <!-- Spark -->\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-core_${scala.binary.version}</artifactId>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUyMjk5Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440522997", "bodyText": "Removed.", "author": "prashantwason", "createdAt": "2020-06-16T00:44:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDU5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTM4NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434285384", "bodyText": "Let\u2019s rename HoodieStorageReader to HoodieFileReader (also the factory)", "author": "vinothchandar", "createdAt": "2020-06-03T03:17:50Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import scala.Tuple2;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieStorageReader {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxMzM3OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440513378", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-06-16T00:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTM4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NTYzMQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434285631", "bodyText": "Use of raw type", "author": "vinothchandar", "createdAt": "2020-06-03T03:19:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/io/storage/HoodieHFileReader.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.Cell;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileScanner;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import scala.Tuple2;\n+\n+public class HoodieHFileReader<R extends IndexedRecord> implements HoodieStorageReader {\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileReader.class);\n+  private Path path;\n+  private Configuration conf;\n+  private HFile.Reader reader;\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  public HoodieHFileReader(Configuration configuration, Path path, CacheConfig cacheConfig) throws IOException {\n+    this.conf = configuration;\n+    this.path = path;\n+    this.reader = HFile.createReader(FSUtils.getFs(path.toString(), configuration), path, cacheConfig, conf);\n+  }\n+\n+  @Override\n+  public String[] readMinMaxRecordKeys() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      return new String[] { new String(fileInfo.get(KEY_MIN_RECORD.getBytes())),\n+          new String(fileInfo.get(KEY_MAX_RECORD.getBytes()))};\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read min/max record key out of file information block correctly from path\", e);\n+    }\n+  }\n+\n+  @Override\n+  public Schema getSchema() {\n+    try {\n+      Map<byte[], byte[]> fileInfo = reader.loadFileInfo();\n+      return new Schema.Parser().parse(new String(fileInfo.get(KEY_SCHEMA.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read schema of file from path\", e);\n+    }\n+\n+  }\n+\n+  @Override\n+  public BloomFilter readBloomFilter() {\n+    Map<byte[], byte[]> fileInfo;\n+    try {\n+      fileInfo = reader.loadFileInfo();\n+      ByteBuffer serializedFilter = reader.getMetaBlock(KEY_BLOOM_FILTER_META_BLOCK, false);\n+      byte[] filterBytes = new byte[serializedFilter.remaining()];\n+      serializedFilter.get(filterBytes); // read the bytes that were written\n+      return BloomFilterFactory.fromString(new String(filterBytes),\n+          new String(fileInfo.get(KEY_BLOOM_FILTER_TYPE_CODE.getBytes())));\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Could not read bloom filter from \" + path, e);\n+    }\n+  }\n+\n+  @Override\n+  public Set<String> filterRowKeys(Set candidateRowKeys) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjE1MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286150", "bodyText": "Introducing scala into hudi-common is a no-go.. there are non spark query bundles built off this, which will all need different scala  version artifacts", "author": "vinothchandar", "createdAt": "2020-06-03T03:21:18Z", "path": "hudi-common/src/main/scala/com/databricks/spark/avro/HoodieAvroSchemaConversion.scala", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.databricks.spark.avro\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.GenericRecord\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.types.DataType\n+\n+/**\n+ * This helper class is required since SchemaConverters.createConverterToSQL is currently private.\n+ */\n+object HoodieAvroSchemaConversion {\n+  def createConverterToSQL(avroSchema: Schema, sparkSchema: DataType): (GenericRecord) => Row =", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NDU3Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440484572", "bodyText": "+1", "author": "bvaradar", "createdAt": "2020-06-15T22:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjE1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNDY4Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440514687", "bodyText": "Removed.", "author": "prashantwason", "createdAt": "2020-06-16T00:14:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjE1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjM5NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286394", "bodyText": "Why do we need this?", "author": "vinothchandar", "createdAt": "2020-06-03T03:22:26Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+\n+import java.io.IOException;\n+\n+/**\n+ * HoodieInputFormat for HUDI datasets which store data in HFile base file format.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNTA1Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440515057", "bodyText": "So we can query metadata table from Hive.\nSo we can keep the same unit tests across all base file formats.\n\nUnit tests for Table validate data by reading incrementally using InputFormats.", "author": "prashantwason", "createdAt": "2020-06-16T00:15:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzODAzOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441938039", "bodyText": "We can do this in a follow on IMO.. unless you can verify this works across engines as is in a short amount of time", "author": "vinothchandar", "createdAt": "2020-06-18T02:48:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkzMQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434286931", "bodyText": "This code is being actively refactored by @garyli1019  and @bhasudha  as well. Gary\u2019s pr will land soon I believe..", "author": "vinothchandar", "createdAt": "2020-06-03T03:25:00Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -18,339 +18,14 @@\n \n package org.apache.hudi.hadoop;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n-import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hudi.common.model.HoodieBaseFile;\n-import org.apache.hudi.common.model.HoodieCommitMetadata;\n-import org.apache.hudi.common.model.HoodiePartitionMetadata;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n- * HoodieInputFormat which understands the Hoodie File Structure and filters files based on the Hoodie Mode. If paths\n- * that does not correspond to a hoodie table then they are passed in as is (as what FileInputFormat.listStatus()\n- * would do). The JobConf could have paths from multipe Hoodie/Non-Hoodie tables\n+ * HoodieInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetInputFormat extends MapredParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetInputFormat.class);\n-\n-  protected Configuration conf;\n-\n-  @Override\n-  public FileStatus[] listStatus(JobConf job) throws IOException {\n-    // Segregate inputPaths[] to incremental, snapshot and non hoodie paths\n-    List<String> incrementalTables = HoodieHiveUtil.getIncrementalTableNames(Job.getInstance(job));\n-    InputPathHandler inputPathHandler = new InputPathHandler(conf, getInputPaths(job), incrementalTables);\n-    List<FileStatus> returns = new ArrayList<>();\n-\n-    Map<String, HoodieTableMetaClient> tableMetaClientMap = inputPathHandler.getTableMetaClientMap();\n-    // process incremental pulls first\n-    for (String table : incrementalTables) {\n-      HoodieTableMetaClient metaClient = tableMetaClientMap.get(table);\n-      if (metaClient == null) {\n-        /* This can happen when the INCREMENTAL mode is set for a table but there were no InputPaths\n-         * in the jobConf\n-         */\n-        continue;\n-      }\n-      List<Path> inputPaths = inputPathHandler.getGroupedIncrementalPaths().get(metaClient);\n-      List<FileStatus> result = listStatusForIncrementalMode(job, metaClient, inputPaths);\n-      if (result != null) {\n-        returns.addAll(result);\n-      }\n-    }\n-\n-    // process non hoodie Paths next.\n-    List<Path> nonHoodiePaths = inputPathHandler.getNonHoodieInputPaths();\n-    if (nonHoodiePaths.size() > 0) {\n-      setInputPaths(job, nonHoodiePaths.toArray(new Path[nonHoodiePaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      returns.addAll(Arrays.asList(fileStatuses));\n-    }\n-\n-    // process snapshot queries next.\n-    List<Path> snapshotPaths = inputPathHandler.getSnapshotPaths();\n-    if (snapshotPaths.size() > 0) {\n-      setInputPaths(job, snapshotPaths.toArray(new Path[snapshotPaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      Map<HoodieTableMetaClient, List<FileStatus>> groupedFileStatus =\n-          groupFileStatusForSnapshotPaths(fileStatuses, tableMetaClientMap.values());\n-      LOG.info(\"Found a total of \" + groupedFileStatus.size() + \" groups\");\n-      for (Map.Entry<HoodieTableMetaClient, List<FileStatus>> entry : groupedFileStatus.entrySet()) {\n-        List<FileStatus> result = filterFileStatusForSnapshotMode(entry.getKey(), entry.getValue());\n-        if (result != null) {\n-          returns.addAll(result);\n-        }\n-      }\n-    }\n-    return returns.toArray(new FileStatus[returns.size()]);\n-  }\n-\n-  /**\n-   * Filter any specific instants that we do not want to process.\n-   * example timeline:\n-   *\n-   * t0 -> create bucket1.parquet\n-   * t1 -> create and append updates bucket1.log\n-   * t2 -> request compaction\n-   * t3 -> create bucket2.parquet\n-   *\n-   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n-   *\n-   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n-   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n-   */\n-  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n-    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n-    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline.filterPendingCompactionTimeline().firstInstant();\n-    if (pendingCompactionInstant.isPresent()) {\n-      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline.findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n-      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n-          - instantsTimeline.getCommitsTimeline().countInstants();\n-      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n-              + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n-\n-      return instantsTimeline;\n-    } else {\n-      return timeline;\n-    }\n-  }\n-\n-  /**\n-   * Achieves listStatus functionality for an incrementally queried table. Instead of listing all\n-   * partitions and then filtering based on the commits of interest, this logic first extracts the\n-   * partitions touched by the desired commits and then lists only those partitions.\n-   */\n-  private List<FileStatus> listStatusForIncrementalMode(\n-      JobConf job, HoodieTableMetaClient tableMetaClient, List<Path> inputPaths) throws IOException {\n-    String tableName = tableMetaClient.getTableConfig().getTableName();\n-    Job jobContext = Job.getInstance(job);\n-    HoodieDefaultTimeline baseTimeline;\n-    if (HoodieHiveUtil.stopAtCompaction(jobContext, tableName)) {\n-      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n-    } else {\n-      baseTimeline = tableMetaClient.getActiveTimeline();\n-    }\n-\n-    HoodieTimeline timeline = baseTimeline.getCommitsTimeline().filterCompletedInstants();\n-    String lastIncrementalTs = HoodieHiveUtil.readStartCommitTime(jobContext, tableName);\n-    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n-    Integer maxCommits = HoodieHiveUtil.readMaxCommits(jobContext, tableName);\n-    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n-    List<HoodieInstant> commitsToCheck = timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n-        .getInstants().collect(Collectors.toList());\n-    // Extract partitions touched by the commitsToCheck\n-    Set<String> partitionsToList = new HashSet<>();\n-    for (HoodieInstant commit : commitsToCheck) {\n-      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n-          HoodieCommitMetadata.class);\n-      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n-    }\n-    if (partitionsToList.isEmpty()) {\n-      return null;\n-    }\n-    String incrementalInputPaths = partitionsToList.stream()\n-        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n-        .filter(s -> {\n-          /*\n-           * Ensure to return only results from the original input path that has incremental changes\n-           * This check is needed for the following corner case -  When the caller invokes\n-           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n-           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n-           * accidentally return all incremental changes for the entire table in every listStatus()\n-           * call. This will create redundant splits. Instead we only want to return the incremental\n-           * changes (if so any) in that batch of input paths.\n-           *\n-           * NOTE on Hive queries that are executed using Fetch task:\n-           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n-           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n-           * disable fetch tasks using the hive session property for incremental queries:\n-           * `set hive.fetch.task.conversion=none;`\n-           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n-           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n-           * those partitions.\n-           */\n-          for (Path path : inputPaths) {\n-            if (path.toString().contains(s)) {\n-              return true;\n-            }\n-          }\n-          return false;\n-        })\n-        .collect(Collectors.joining(\",\"));\n-    if (StringUtils.isNullOrEmpty(incrementalInputPaths)) {\n-      return null;\n-    }\n-    // Mutate the JobConf to set the input paths to only partitions touched by incremental pull.\n-    setInputPaths(job, incrementalInputPaths);\n-    FileStatus[] fileStatuses = super.listStatus(job);\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n-    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n-    return returns;\n-  }\n-\n-  /**\n-   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n-   * based on given table metadata.\n-   * @param fileStatuses\n-   * @param metaClientList\n-   * @return\n-   * @throws IOException\n-   */\n-  private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n-      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n-    // This assumes the paths for different tables are grouped together\n-    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n-    HoodieTableMetaClient metadata = null;\n-    for (FileStatus status : fileStatuses) {\n-      Path inputPath = status.getPath();\n-      if (!inputPath.getName().endsWith(\".parquet\")) {\n-        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n-        // with \".\"\n-        continue;\n-      }\n-      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n-        for (HoodieTableMetaClient metaClient : metaClientList) {\n-          if (inputPath.toString().contains(metaClient.getBasePath())) {\n-            metadata = metaClient;\n-            if (!grouped.containsKey(metadata)) {\n-              grouped.put(metadata, new ArrayList<>());\n-            }\n-            break;\n-          }\n-        }\n-      }\n-      grouped.get(metadata).add(status);\n-    }\n-    return grouped;\n-  }\n-\n-  /**\n-   * Filters data files for a snapshot queried table.\n-   */\n-  private List<FileStatus> filterFileStatusForSnapshotMode(\n-      HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n-    }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n-      }\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    return returns;\n-  }\n-\n-  /**\n-   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n-   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n-   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n-   */\n-  private HoodieBaseFile checkFileStatus(HoodieBaseFile dataFile) {\n-    Path dataPath = dataFile.getFileStatus().getPath();\n-    try {\n-      if (dataFile.getFileSize() == 0) {\n-        FileSystem fs = dataPath.getFileSystem(conf);\n-        LOG.info(\"Refreshing file status \" + dataFile.getPath());\n-        return new HoodieBaseFile(fs.getFileStatus(dataPath));\n-      }\n-      return dataFile;\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Could not get FileStatus on path \" + dataPath);\n-    }\n-  }\n-\n-  public void setConf(Configuration conf) {\n-    this.conf = conf;\n-  }\n-\n-  @Override\n-  public Configuration getConf() {\n-    return conf;\n-  }\n-\n-  @Override\n-  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf job,\n-      final Reporter reporter) throws IOException {\n-    // TODO enable automatic predicate pushdown after fixing issues\n-    // FileSplit fileSplit = (FileSplit) split;\n-    // HoodieTableMetadata metadata = getTableMetadata(fileSplit.getPath().getParent());\n-    // String tableName = metadata.getTableName();\n-    // String mode = HoodieHiveUtil.readMode(job, tableName);\n-\n-    // if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {\n-    // FilterPredicate predicate = constructHoodiePredicate(job, tableName, split);\n-    // LOG.info(\"Setting parquet predicate push down as \" + predicate);\n-    // ParquetInputFormat.setFilterPredicate(job, predicate);\n-    // clearOutExistingPredicate(job);\n-    // }\n-    return super.getRecordReader(split, job, reporter);\n-  }\n-\n-  /**\n-   * Read the table metadata from a data path. This assumes certain hierarchy of files which should be changed once a\n-   * better way is figured out to pass in the hoodie meta directory\n-   */\n-  protected static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n-    int levels = HoodieHiveUtil.DEFAULT_LEVELS_TO_BASEPATH;\n-    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n-      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n-      metadata.readFromFS();\n-      levels = metadata.getPartitionDepth();\n-    }\n-    Path baseDir = HoodieHiveUtil.getNthParent(dataPath, levels);\n-    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n-    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+public class HoodieParquetInputFormat extends HoodieInputFormat {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTUzNDI2Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r435534263", "bodyText": "Yes, the static methods in this class will be moved to a utils class, so we can use it from different FileFormat based on the query engines. Spark Datasource uses its own FileFormat other than mapreduce.FileFormat", "author": "garyli1019", "createdAt": "2020-06-04T20:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNTU1Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440515553", "bodyText": "I have merged my changed while maintaining the changes in HUDI-822. This still leaves a lot of possibility of reducing code duplication across InputFormats.", "author": "prashantwason", "createdAt": "2020-06-16T00:17:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NjkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzI1NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287254", "bodyText": "Is this change strictly necessary for this pr", "author": "vinothchandar", "createdAt": "2020-06-03T03:26:17Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/hive/HoodieCombineHiveInputFormat.java", "diffHunk": "@@ -872,7 +873,7 @@ public void createPool(JobConf conf, PathFilter... filters) {\n         job.set(\"hudi.hive.realtime\", \"true\");\n         InputSplit[] splits;\n         if (hoodieFilter) {\n-          HoodieParquetInputFormat input = new HoodieParquetRealtimeInputFormat();\n+          HoodieParquetRealtimeInputFormat input = new HoodieParquetRealtimeInputFormat();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzQ2Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287466", "bodyText": "This is a behavior change.. ccc @n3nash  to confirm if this is ok", "author": "vinothchandar", "createdAt": "2020-06-03T03:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzI1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUyODEyMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440528122", "bodyText": "Reverted.", "author": "prashantwason", "createdAt": "2020-06-16T01:03:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzgyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434287826", "bodyText": "Same thing here. This code is already changing momentarily", "author": "vinothchandar", "createdAt": "2020-06-03T03:28:48Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -18,250 +18,23 @@\n \n package org.apache.hudi.hadoop.realtime;\n \n-import org.apache.hudi.common.fs.FSUtils;\n-import org.apache.hudi.common.model.FileSlice;\n-import org.apache.hudi.common.model.HoodieLogFile;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.util.CollectionUtils;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.ValidationUtils;\n-import org.apache.hudi.exception.HoodieException;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n import org.apache.hudi.hadoop.UseFileSplitsFromInputFormat;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.FileSplit;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n import org.apache.hudi.hadoop.UseRecordReaderFromInputFormat;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.function.Function;\n-import java.util.stream.Collectors;\n-import java.util.stream.Stream;\n \n /**\n- * Input Format, that provides a real-time view of data in a Hoodie table.\n+ * HoodieRealtimeInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseRecordReaderFromInputFormat\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetRealtimeInputFormat extends HoodieParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetRealtimeInputFormat.class);\n-\n-  // These positions have to be deterministic across all tables\n-  public static final int HOODIE_COMMIT_TIME_COL_POS = 0;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNTg0OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440515848", "bodyText": "I have merged my change on top of HUDI-822.", "author": "prashantwason", "createdAt": "2020-06-16T00:18:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NzgyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTA3Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434289072", "bodyText": "AFAIK hive will check in places whether the registered input format is instanceof ParquetInputFormat.. for applying certain optimizations. It\u2019s probably better to not change the class hierarchy, but try to abstract by using more modular classes", "author": "vinothchandar", "createdAt": "2020-06-03T03:34:17Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -18,339 +18,14 @@\n \n package org.apache.hudi.hadoop;\n \n-import org.apache.hadoop.conf.Configurable;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileStatus;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n-import org.apache.hadoop.io.ArrayWritable;\n-import org.apache.hadoop.io.NullWritable;\n-import org.apache.hadoop.mapred.InputSplit;\n-import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapred.RecordReader;\n-import org.apache.hadoop.mapred.Reporter;\n-import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hudi.common.model.HoodieBaseFile;\n-import org.apache.hudi.common.model.HoodieCommitMetadata;\n-import org.apache.hudi.common.model.HoodiePartitionMetadata;\n-import org.apache.hudi.common.table.HoodieTableMetaClient;\n-import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n-import org.apache.hudi.common.table.timeline.HoodieInstant;\n-import org.apache.hudi.common.table.timeline.HoodieTimeline;\n-import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n-import org.apache.hudi.common.table.view.TableFileSystemView.BaseFileOnlyView;\n-import org.apache.hudi.common.util.Option;\n-import org.apache.hudi.common.util.StringUtils;\n-import org.apache.hudi.exception.HoodieIOException;\n-import org.apache.log4j.LogManager;\n-import org.apache.log4j.Logger;\n-\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.stream.Collectors;\n \n /**\n- * HoodieInputFormat which understands the Hoodie File Structure and filters files based on the Hoodie Mode. If paths\n- * that does not correspond to a hoodie table then they are passed in as is (as what FileInputFormat.listStatus()\n- * would do). The JobConf could have paths from multipe Hoodie/Non-Hoodie tables\n+ * HoodieInputFormat for HUDI datasets which store data in Parquet base file format.\n  */\n @UseFileSplitsFromInputFormat\n-public class HoodieParquetInputFormat extends MapredParquetInputFormat implements Configurable {\n-\n-  private static final Logger LOG = LogManager.getLogger(HoodieParquetInputFormat.class);\n-\n-  protected Configuration conf;\n-\n-  @Override\n-  public FileStatus[] listStatus(JobConf job) throws IOException {\n-    // Segregate inputPaths[] to incremental, snapshot and non hoodie paths\n-    List<String> incrementalTables = HoodieHiveUtil.getIncrementalTableNames(Job.getInstance(job));\n-    InputPathHandler inputPathHandler = new InputPathHandler(conf, getInputPaths(job), incrementalTables);\n-    List<FileStatus> returns = new ArrayList<>();\n-\n-    Map<String, HoodieTableMetaClient> tableMetaClientMap = inputPathHandler.getTableMetaClientMap();\n-    // process incremental pulls first\n-    for (String table : incrementalTables) {\n-      HoodieTableMetaClient metaClient = tableMetaClientMap.get(table);\n-      if (metaClient == null) {\n-        /* This can happen when the INCREMENTAL mode is set for a table but there were no InputPaths\n-         * in the jobConf\n-         */\n-        continue;\n-      }\n-      List<Path> inputPaths = inputPathHandler.getGroupedIncrementalPaths().get(metaClient);\n-      List<FileStatus> result = listStatusForIncrementalMode(job, metaClient, inputPaths);\n-      if (result != null) {\n-        returns.addAll(result);\n-      }\n-    }\n-\n-    // process non hoodie Paths next.\n-    List<Path> nonHoodiePaths = inputPathHandler.getNonHoodieInputPaths();\n-    if (nonHoodiePaths.size() > 0) {\n-      setInputPaths(job, nonHoodiePaths.toArray(new Path[nonHoodiePaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      returns.addAll(Arrays.asList(fileStatuses));\n-    }\n-\n-    // process snapshot queries next.\n-    List<Path> snapshotPaths = inputPathHandler.getSnapshotPaths();\n-    if (snapshotPaths.size() > 0) {\n-      setInputPaths(job, snapshotPaths.toArray(new Path[snapshotPaths.size()]));\n-      FileStatus[] fileStatuses = super.listStatus(job);\n-      Map<HoodieTableMetaClient, List<FileStatus>> groupedFileStatus =\n-          groupFileStatusForSnapshotPaths(fileStatuses, tableMetaClientMap.values());\n-      LOG.info(\"Found a total of \" + groupedFileStatus.size() + \" groups\");\n-      for (Map.Entry<HoodieTableMetaClient, List<FileStatus>> entry : groupedFileStatus.entrySet()) {\n-        List<FileStatus> result = filterFileStatusForSnapshotMode(entry.getKey(), entry.getValue());\n-        if (result != null) {\n-          returns.addAll(result);\n-        }\n-      }\n-    }\n-    return returns.toArray(new FileStatus[returns.size()]);\n-  }\n-\n-  /**\n-   * Filter any specific instants that we do not want to process.\n-   * example timeline:\n-   *\n-   * t0 -> create bucket1.parquet\n-   * t1 -> create and append updates bucket1.log\n-   * t2 -> request compaction\n-   * t3 -> create bucket2.parquet\n-   *\n-   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n-   *\n-   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n-   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n-   */\n-  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n-    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n-    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline.filterPendingCompactionTimeline().firstInstant();\n-    if (pendingCompactionInstant.isPresent()) {\n-      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline.findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n-      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n-          - instantsTimeline.getCommitsTimeline().countInstants();\n-      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n-              + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n-\n-      return instantsTimeline;\n-    } else {\n-      return timeline;\n-    }\n-  }\n-\n-  /**\n-   * Achieves listStatus functionality for an incrementally queried table. Instead of listing all\n-   * partitions and then filtering based on the commits of interest, this logic first extracts the\n-   * partitions touched by the desired commits and then lists only those partitions.\n-   */\n-  private List<FileStatus> listStatusForIncrementalMode(\n-      JobConf job, HoodieTableMetaClient tableMetaClient, List<Path> inputPaths) throws IOException {\n-    String tableName = tableMetaClient.getTableConfig().getTableName();\n-    Job jobContext = Job.getInstance(job);\n-    HoodieDefaultTimeline baseTimeline;\n-    if (HoodieHiveUtil.stopAtCompaction(jobContext, tableName)) {\n-      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n-    } else {\n-      baseTimeline = tableMetaClient.getActiveTimeline();\n-    }\n-\n-    HoodieTimeline timeline = baseTimeline.getCommitsTimeline().filterCompletedInstants();\n-    String lastIncrementalTs = HoodieHiveUtil.readStartCommitTime(jobContext, tableName);\n-    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n-    Integer maxCommits = HoodieHiveUtil.readMaxCommits(jobContext, tableName);\n-    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n-    List<HoodieInstant> commitsToCheck = timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n-        .getInstants().collect(Collectors.toList());\n-    // Extract partitions touched by the commitsToCheck\n-    Set<String> partitionsToList = new HashSet<>();\n-    for (HoodieInstant commit : commitsToCheck) {\n-      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n-          HoodieCommitMetadata.class);\n-      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n-    }\n-    if (partitionsToList.isEmpty()) {\n-      return null;\n-    }\n-    String incrementalInputPaths = partitionsToList.stream()\n-        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n-        .filter(s -> {\n-          /*\n-           * Ensure to return only results from the original input path that has incremental changes\n-           * This check is needed for the following corner case -  When the caller invokes\n-           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n-           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n-           * accidentally return all incremental changes for the entire table in every listStatus()\n-           * call. This will create redundant splits. Instead we only want to return the incremental\n-           * changes (if so any) in that batch of input paths.\n-           *\n-           * NOTE on Hive queries that are executed using Fetch task:\n-           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n-           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n-           * disable fetch tasks using the hive session property for incremental queries:\n-           * `set hive.fetch.task.conversion=none;`\n-           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n-           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n-           * those partitions.\n-           */\n-          for (Path path : inputPaths) {\n-            if (path.toString().contains(s)) {\n-              return true;\n-            }\n-          }\n-          return false;\n-        })\n-        .collect(Collectors.joining(\",\"));\n-    if (StringUtils.isNullOrEmpty(incrementalInputPaths)) {\n-      return null;\n-    }\n-    // Mutate the JobConf to set the input paths to only partitions touched by incremental pull.\n-    setInputPaths(job, incrementalInputPaths);\n-    FileStatus[] fileStatuses = super.listStatus(job);\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n-    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n-    return returns;\n-  }\n-\n-  /**\n-   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n-   * based on given table metadata.\n-   * @param fileStatuses\n-   * @param metaClientList\n-   * @return\n-   * @throws IOException\n-   */\n-  private Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n-      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n-    // This assumes the paths for different tables are grouped together\n-    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n-    HoodieTableMetaClient metadata = null;\n-    for (FileStatus status : fileStatuses) {\n-      Path inputPath = status.getPath();\n-      if (!inputPath.getName().endsWith(\".parquet\")) {\n-        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n-        // with \".\"\n-        continue;\n-      }\n-      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n-        for (HoodieTableMetaClient metaClient : metaClientList) {\n-          if (inputPath.toString().contains(metaClient.getBasePath())) {\n-            metadata = metaClient;\n-            if (!grouped.containsKey(metadata)) {\n-              grouped.put(metadata, new ArrayList<>());\n-            }\n-            break;\n-          }\n-        }\n-      }\n-      grouped.get(metadata).add(status);\n-    }\n-    return grouped;\n-  }\n-\n-  /**\n-   * Filters data files for a snapshot queried table.\n-   */\n-  private List<FileStatus> filterFileStatusForSnapshotMode(\n-      HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n-    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n-    }\n-    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n-    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n-    BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n-    // filter files on the latest commit found\n-    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n-    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n-    List<FileStatus> returns = new ArrayList<>();\n-    for (HoodieBaseFile filteredFile : filteredFiles) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n-      }\n-      filteredFile = checkFileStatus(filteredFile);\n-      returns.add(filteredFile.getFileStatus());\n-    }\n-    return returns;\n-  }\n-\n-  /**\n-   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n-   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n-   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n-   */\n-  private HoodieBaseFile checkFileStatus(HoodieBaseFile dataFile) {\n-    Path dataPath = dataFile.getFileStatus().getPath();\n-    try {\n-      if (dataFile.getFileSize() == 0) {\n-        FileSystem fs = dataPath.getFileSystem(conf);\n-        LOG.info(\"Refreshing file status \" + dataFile.getPath());\n-        return new HoodieBaseFile(fs.getFileStatus(dataPath));\n-      }\n-      return dataFile;\n-    } catch (IOException e) {\n-      throw new HoodieIOException(\"Could not get FileStatus on path \" + dataPath);\n-    }\n-  }\n-\n-  public void setConf(Configuration conf) {\n-    this.conf = conf;\n-  }\n-\n-  @Override\n-  public Configuration getConf() {\n-    return conf;\n-  }\n-\n-  @Override\n-  public RecordReader<NullWritable, ArrayWritable> getRecordReader(final InputSplit split, final JobConf job,\n-      final Reporter reporter) throws IOException {\n-    // TODO enable automatic predicate pushdown after fixing issues\n-    // FileSplit fileSplit = (FileSplit) split;\n-    // HoodieTableMetadata metadata = getTableMetadata(fileSplit.getPath().getParent());\n-    // String tableName = metadata.getTableName();\n-    // String mode = HoodieHiveUtil.readMode(job, tableName);\n-\n-    // if (HoodieHiveUtil.INCREMENTAL_SCAN_MODE.equals(mode)) {\n-    // FilterPredicate predicate = constructHoodiePredicate(job, tableName, split);\n-    // LOG.info(\"Setting parquet predicate push down as \" + predicate);\n-    // ParquetInputFormat.setFilterPredicate(job, predicate);\n-    // clearOutExistingPredicate(job);\n-    // }\n-    return super.getRecordReader(split, job, reporter);\n-  }\n-\n-  /**\n-   * Read the table metadata from a data path. This assumes certain hierarchy of files which should be changed once a\n-   * better way is figured out to pass in the hoodie meta directory\n-   */\n-  protected static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n-    int levels = HoodieHiveUtil.DEFAULT_LEVELS_TO_BASEPATH;\n-    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n-      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n-      metadata.readFromFS();\n-      levels = metadata.getPartitionDepth();\n-    }\n-    Path baseDir = HoodieHiveUtil.getNthParent(dataPath, levels);\n-    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n-    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+public class HoodieParquetInputFormat extends HoodieInputFormat {\n+  public HoodieParquetInputFormat() {\n+    super(new MapredParquetInputFormat());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNTk0MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440515941", "bodyText": "Reverted.\nBut if Hive has checks for instanceof then does it mean Hive does not support any other InputFormat except Parquet? Or you mean Hoodie code which interfaces with Hive?", "author": "prashantwason", "createdAt": "2020-06-16T00:19:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTQ1MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r434289450", "bodyText": "Why does checkstyle complain", "author": "vinothchandar", "createdAt": "2020-06-03T03:35:48Z", "path": "style/checkstyle-suppressions.xml", "diffHunk": "@@ -26,4 +26,5 @@\n   <!-- Member Names expected to start with \"_\"  -->\n   <suppress checks=\"naming\" files=\"TestRecord.java\" lines=\"1-9999\"/>\n   <suppress checks=\"IllegalImport\" files=\"Option.java\" />\n+  <suppress checks=\"naming\" files=\"HoodieInputFormat.java\" lines=\"73\"/>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDUxNjMzMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440516332", "bodyText": "Code removed.", "author": "prashantwason", "createdAt": "2020-06-16T00:20:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4OTQ1MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ1NDA1OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440454058", "bodyText": "Is this only needed on the write side ?", "author": "bvaradar", "createdAt": "2020-06-15T21:20:57Z", "path": "hudi-client/pom.xml", "diffHunk": "@@ -185,6 +185,12 @@\n       <artifactId>hbase-client</artifactId>\n       <version>${hbase.version}</version>\n     </dependency>\n+    <dependency>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU1ODU3Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440558577", "bodyText": "For both reader and writer side of HFile format.", "author": "prashantwason", "createdAt": "2020-06-16T03:01:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ1NDA1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440465555", "bodyText": "We have a family of things that needs to be constructed:\n\nStorageWriter\nStorageReader\nMergeAlgorithm\n\nIn this case, AbstractFactoryPattern would be useful  https://en.wikipedia.org/wiki/Abstract_factory_pattern#/media/File:Abstract_factory_UML.svg  pattern would be ideal for this case.\nThis is more or less similar to what has been done here. Currently, there is a separate Writer and Reader Factory. What do you think about creating an AbstractFactory and have one concrete implementation for Parquet and another for HFile  where all the above objects are constructed ?", "author": "bvaradar", "createdAt": "2020-06-15T21:47:13Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAwMjA2NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441002064", "bodyText": "I have created the new HoodieSortedMergeHandle which extends the HoodieMergeHandle. This way, the sort functionality is moved to the HoodieSortedMergeHandle. The correct handle will be initialized and used within CommitActionExecutor.\nThis was cleaner than an external merge Algorithm to be plugged into the HoodieMergeHandle because:\n\nSort in MergeHandle is closely tied to reading and writing the records. The algorithm is better suited to cases where all records are already available in memory.\nCleaner code as the intention is evident from the handle name.\nRemoves code duplication\nNo change for non-hfile base formats (the existing HoodieMergeHandle will be used).\n\nPlease take a look.", "author": "prashantwason", "createdAt": "2020-06-16T16:55:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAwNTQ4Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441005487", "bodyText": "Regarding AbstractFactoryPattern:\nI can group the two reader -writer factories into one single factory:\nclass HoodieFIleFormatFactory:\n    public getFileWriter()\n    public getFileReader()\n\nstatic HoodieFIleFormatFactory getFileFormatFactory()\n\nIMO, this does not change much but makes it a bit complicated as:\n\nFile writers require more configuration (e.g. compression algorithm etc)\nFile readers only require file path\n\nIn case only a reader is required - initializing a common factory class will require passing parameters (common with writer).", "author": "prashantwason", "createdAt": "2020-06-16T17:01:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NzMxOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443267319", "bodyText": "I did go thru the AbstractFactoryPattern. Is this what you are insinuating @bvaradar\nclass FileStorageFactoryProducer {\n\n\tFileStorageFactory getFileStorageFactory(Config config){\n\t\tif(config.getFileStorageType() == PARQUET) {\n\t\t\treturn new ParquetFileStorageFactory(....)\n\t\t} else if(config.getFileStorageType() == HFile) {\n\t\t\treturn new HFileFileStorageFactory(....)\n\t\t} else {\n\t\t   // throw exception\n\t\t}\n\t}  \n}\n\n\ninterface FileStorageFactory {\n\t\n\tpublic static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(String instantTime, \nPath path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema, \nSparkTaskContextSupplier sparkTaskContextSupplier) throws IOException;\n\n        public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileReader<R> getFileReader(\nConfiguration conf, Path path) throws IOException;\n\n}\n\n\nclass ParquetFileStorageFactory implements FileStorageFactory {\n\t\n\t\t// exposes Writer and Reader for Parquet\n\n}\n\n\nclass HFileFileStorageFactory implements FileStorageFactory {\n\t\t// exposes Writer and Reader for HFile\n}", "author": "nsivabalan", "createdAt": "2020-06-21T23:31:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ1ODYwMw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444458603", "bodyText": "@nsivabalan : Yes, this is what I was referring to but it is not specific to Storage alone. Other parts like the merge algorithm that needs to be configured per storage-type is also created here.\n@prashantwason : Let's revisit this in subsequent PR where we parameterize merge algorithm based on storage types. It is ok to keep reader and writer factory separate.", "author": "bvaradar", "createdAt": "2020-06-23T19:31:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTU1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2NTc1Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440465756", "bodyText": "Please see above for AbstractFactory suggestion.", "author": "bvaradar", "createdAt": "2020-06-15T21:47:45Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriterFactory.java", "diffHunk": "@@ -34,29 +34,28 @@\n \n import java.io.IOException;\n \n-import static org.apache.hudi.common.model.HoodieFileFormat.HOODIE_LOG;\n import static org.apache.hudi.common.model.HoodieFileFormat.PARQUET;\n+import static org.apache.hudi.common.model.HoodieFileFormat.HFILE;\n \n-public class HoodieStorageWriterFactory {\n+public class HoodieFileWriterFactory {\n \n-  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieStorageWriter<R> getStorageWriter(\n+  public static <T extends HoodieRecordPayload, R extends IndexedRecord> HoodieFileWriter<R> getFileWriter(\n       String instantTime, Path path, HoodieTable<T> hoodieTable, HoodieWriteConfig config, Schema schema,\n       SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n-    final String name = path.getName();\n-    final String extension = FSUtils.isLogFile(path) ? HOODIE_LOG.getFileExtension() : FSUtils.getFileExtension(name);\n+    final String extension = FSUtils.getFileExtension(path.getName());\n     if (PARQUET.getFileExtension().equals(extension)) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2ODI3MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440468270", "bodyText": "Looks like this is duplicated. Can you refactor to reuse this code.", "author": "bvaradar", "createdAt": "2020-06-15T21:53:43Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieHFileWriter.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.fs.HoodieWrapperFileSystem;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hbase.KeyValue;\n+import org.apache.hadoop.hbase.io.hfile.CacheConfig;\n+import org.apache.hadoop.hbase.io.hfile.HFile;\n+import org.apache.hadoop.hbase.io.hfile.HFileContext;\n+import org.apache.hadoop.hbase.io.hfile.HFileContextBuilder;\n+import org.apache.hadoop.io.Writable;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * HoodieHFileWriter writes IndexedRecords into an HFile. The record's key is used as the key and the\n+ * AVRO encoded record bytes are saved as the value.\n+ *\n+ * Limitations (compared to columnar formats like Parquet or ORC):\n+ *  1. Records should be added in order of keys\n+ *  2. There are no column stats\n+ */\n+public class HoodieHFileWriter<T extends HoodieRecordPayload, R extends IndexedRecord>\n+    implements HoodieFileWriter<R> {\n+  private static AtomicLong recordIndex = new AtomicLong(1);\n+  private static final Logger LOG = LogManager.getLogger(HoodieHFileWriter.class);\n+\n+  public static final String KEY_SCHEMA = \"schema\";\n+  public static final String KEY_BLOOM_FILTER_META_BLOCK = \"bloomFilter\";\n+  public static final String KEY_BLOOM_FILTER_TYPE_CODE = \"bloomFilterTypeCode\";\n+  public static final String KEY_MIN_RECORD = \"minRecordKey\";\n+  public static final String KEY_MAX_RECORD = \"maxRecordKey\";\n+\n+  private final Path file;\n+  private HoodieHFileConfig hfileConfig;\n+  private final HoodieWrapperFileSystem fs;\n+  private final long maxFileSize;\n+  private final String instantTime;\n+  private final SparkTaskContextSupplier sparkTaskContextSupplier;\n+  private HFile.Writer writer;\n+  private String minRecordKey;\n+  private String maxRecordKey;\n+\n+  public HoodieHFileWriter(String instantTime, Path file, HoodieHFileConfig hfileConfig, Schema schema,\n+      SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+\n+    Configuration conf = registerFileSystem(file, hfileConfig.getHadoopConf());\n+    this.file = HoodieWrapperFileSystem.convertToHoodiePath(file, conf);\n+    this.fs = (HoodieWrapperFileSystem) this.file.getFileSystem(conf);\n+    this.hfileConfig = hfileConfig;\n+\n+    // We cannot accurately measure the snappy compressed output file size. We are choosing a\n+    // conservative 10%\n+    // TODO - compute this compression ratio dynamically by looking at the bytes written to the\n+    // stream and the actual file size reported by HDFS\n+    // this.maxFileSize = hfileConfig.getMaxFileSize()\n+    //    + Math.round(hfileConfig.getMaxFileSize() * hfileConfig.getCompressionRatio());\n+    this.maxFileSize = hfileConfig.getMaxFileSize();\n+    this.instantTime = instantTime;\n+    this.sparkTaskContextSupplier = sparkTaskContextSupplier;\n+\n+    HFileContext context = new HFileContextBuilder().withBlockSize(hfileConfig.getBlockSize())\n+          .withCompression(hfileConfig.getCompressionAlgorithm())\n+          .build();\n+    CacheConfig cacheConfig = new CacheConfig(conf);\n+    this.writer = HFile.getWriterFactory(conf, cacheConfig).withPath(this.fs, this.file).withFileContext(context).create();\n+\n+    writer.appendFileInfo(KEY_SCHEMA.getBytes(), schema.toString().getBytes());\n+  }\n+\n+  public static Configuration registerFileSystem(Path file, Configuration conf) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAwODEyNw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441008127", "bodyText": "Refactored and moved to HoodieFileWriter.", "author": "prashantwason", "createdAt": "2020-06-16T17:05:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ2ODI3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MTI5Nw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440471297", "bodyText": "Instead of this, lets return merge algorithm to be employed.", "author": "bvaradar", "createdAt": "2020-06-15T22:01:14Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -562,4 +566,28 @@ public void validateInsertSchema() throws HoodieInsertException {\n       throw new HoodieInsertException(\"Failed insert schema compability check.\", e);\n     }\n   }\n+\n+  public HoodieFileFormat getBaseFileFormat() {\n+    return metaClient.getTableConfig().getBaseFileFormat();\n+  }\n+\n+  public HoodieFileFormat getLogFileFormat() {\n+    return metaClient.getTableConfig().getLogFileFormat();\n+  }\n+\n+  public HoodieLogBlockType getLogDataBlockFormat() {\n+    switch (getBaseFileFormat()) {\n+      case PARQUET:\n+        return HoodieLogBlockType.AVRO_DATA_BLOCK;\n+      case HFILE:\n+        return HoodieLogBlockType.HFILE_DATA_BLOCK;\n+      default:\n+        throw new HoodieException(\"Base file format \" + getBaseFileFormat()\n+            + \" does not have associated log block format\");\n+    }\n+  }\n+\n+  public boolean requireSortedRecords() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAwODQyMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441008422", "bodyText": "See my comments above. I have created a new merge handle.", "author": "prashantwason", "createdAt": "2020-06-16T17:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MTI5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3NjA4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440476086", "bodyText": "This is one another shuffle which will cause performance overhead. We dont need universal order. We only need to order records that are getting written to a single partition.\nwe should just do https://spark.apache.org/docs/1.2.0/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html#repartitionAndSortWithinPartitions(org.apache.spark.Partitioner) when partitioning (See BaseCommitExecutor.java)", "author": "bvaradar", "createdAt": "2020-06-15T22:13:09Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/WriteHelper.java", "diffHunk": "@@ -52,6 +52,10 @@\n       }\n       Duration indexLookupDuration = Duration.between(lookupBegin, Instant.now());\n \n+      if (table.requireSortedRecords()) {\n+        taggedRecords = taggedRecords.sortBy(r -> r.getRecordKey(), true, taggedRecords.getNumPartitions());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3MzE1Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441073153", "bodyText": "Done. Much cleaner now.", "author": "prashantwason", "createdAt": "2020-06-16T18:54:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3NjA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3ODc4Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440478782", "bodyText": "We might eventually need such support when supporting spark datasource support on Hoodie HFile table. It would be better to move this logic to a Utils class to reuse.", "author": "bvaradar", "createdAt": "2020-06-15T22:20:18Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieClientTestUtils.java", "diffHunk": "@@ -217,6 +239,51 @@ public static SparkConf getSparkConfForTest(String appName) {\n     }\n   }\n \n+  public static Dataset<Row> readHFile(JavaSparkContext jsc, SQLContext sqlContext, String[] paths) {\n+    // TODO: this should be ported to use HoodieStorageReader\n+    List<byte[]> valuesAsList = new LinkedList<>();\n+\n+    FileSystem fs = FSUtils.getFs(paths[0], jsc.hadoopConfiguration());\n+    CacheConfig cacheConfig = new CacheConfig(fs.getConf());\n+    Schema schema = null;\n+    for (String path : paths) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MDI2Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440480262", "bodyText": "Average Size ?", "author": "bvaradar", "createdAt": "2020-06-15T22:24:14Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * DataBlock contains a list of records serialized using formats compatible with the base file format.\n+ * For each base file format there is a corresponding DataBlock format.\n+ *\n+ * The Datablock contains:\n+ *   1. Data Block version\n+ *   2. Total number of records in the block\n+ *   3. Size of a record", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3NTAzMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441075030", "bodyText": "Corrected. The size of each record is saved before the record bytes are written. But this is specific to the HoodieAvroDataBlock.", "author": "prashantwason", "createdAt": "2020-06-16T18:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MDI2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MTA4MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440481081", "bodyText": "Can you confirm there is no change to order and kinds of fields stored here. I want to make sure if we can read log files written by 0.5.x using this change.", "author": "bvaradar", "createdAt": "2020-06-15T22:26:37Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieDataBlock.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log.block;\n+\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+\n+import javax.annotation.Nonnull;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * DataBlock contains a list of records serialized using formats compatible with the base file format.\n+ * For each base file format there is a corresponding DataBlock format.\n+ *\n+ * The Datablock contains:", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3NTQ2NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441075465", "bodyText": "Yes, there is no change. The original code is HoodieAvroDataBlock. I have created a new base class HoodieDataBlock which is abstract.", "author": "prashantwason", "createdAt": "2020-06-16T18:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4MTA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NzY4MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r440487681", "bodyText": "Can we add tons of tests (expand on something like TestHoodieParquetInputFormat) for HFIle. Also, I suspect when you put things end to  end like run a proper hive/presto query, you will run into  package version mismatches which is what I had to deal with when doing metadata bootstrap.", "author": "bvaradar", "createdAt": "2020-06-15T22:45:26Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieHFileInputFormat.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileInputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.hadoop.utils.HoodieHiveUtils;\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * HoodieInputFormat for HUDI datasets which store data in HFile base file format.\n+ */\n+@UseFileSplitsFromInputFormat\n+public class HoodieHFileInputFormat extends FileInputFormat<NullWritable, ArrayWritable> implements Configurable {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA3NTkzMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441075932", "bodyText": "Added an input format test TestHoodieHFileInputFormat.\nI am still looking into adding end to end integration tests for my change.", "author": "prashantwason", "createdAt": "2020-06-16T18:59:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NzY4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTkzNjYwMw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r441936603", "bodyText": "Is this a real requirement in the first pass to have this be integrated into every query engine?\nI feel this can be a follow on.. in any case, if we add these tests, we have to also cleanup and make existing tests generic enough. We can\u2019t just duplicate the existing tests . All in all this will increase the scope", "author": "vinothchandar", "createdAt": "2020-06-18T02:42:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NzY4MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b2a4c2e54b579e6c58ea9320f8b85625b577084e", "url": "https://github.com/apache/hudi/commit/b2a4c2e54b579e6c58ea9320f8b85625b577084e", "message": "[HUDI-684] Fixed merge conflict due to upstream changes.\n\nAdded extra unit tests for HFile Input format.", "committedDate": "2020-06-17T06:50:47Z", "type": "forcePushed"}, {"oid": "75667356003fe23b5fc4df8b37bcfd8aa256c61e", "url": "https://github.com/apache/hudi/commit/75667356003fe23b5fc4df8b37bcfd8aa256c61e", "message": "[HUDI-684] Fixed integration tests.", "committedDate": "2020-06-18T21:13:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTQ0Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261442", "bodyText": "might as well expose getBaseFileExtension() in HoodieTable.", "author": "nsivabalan", "createdAt": "2020-06-21T22:11:47Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -115,7 +115,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       oldFilePath = new Path(config.getBasePath() + \"/\" + partitionPath + \"/\" + latestValidFilePath);\n       String relativePath = new Path((partitionPath.isEmpty() ? \"\" : partitionPath + \"/\")\n-          + FSUtils.makeDataFileName(instantTime, writeToken, fileId)).toString();\n+          + FSUtils.makeDataFileName(instantTime, writeToken, fileId,\n+              hoodieTable.getBaseFileFormat().getFileExtension())).toString();", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443261658", "bodyText": "minor. Do you think we should name this getNewStorageReader(). Just to be cautious to avoid some caller using this more like a getter for Reader. This does not return a singleton, but creates a new reader everytime.", "author": "nsivabalan", "createdAt": "2020-06-21T22:14:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getStorageReader() throws IOException {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0NTM5MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443845391", "bodyText": "Yes, getNewStorageReader() would be more clear.", "author": "prashantwason", "createdAt": "2020-06-22T21:48:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MTY1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443262086", "bodyText": "similar to HoodieReadHandle, how about we expose this writer instantiation in HoodieWriteHandle.\nHoodieFileWriter<IndexedRecord> getFileWriter(String instantTime, Path newFilePath){\n     return HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n}", "author": "nsivabalan", "createdAt": "2020-06-21T22:20:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieMergeHandle.java", "diffHunk": "@@ -132,7 +133,8 @@ private void init(String fileId, String partitionPath, HoodieBaseFile dataFileTo\n \n       // Create the writer for writing the new version file\n       storageWriter =\n-          HoodieStorageWriterFactory.getStorageWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);\n+          HoodieFileWriterFactory.getFileWriter(instantTime, newFilePath, hoodieTable, config, writerSchema, sparkTaskContextSupplier);", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NDAzMg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443264032", "bodyText": "minor: similar to my suggestion in reader, try to see if we need to name this as getNewFileWriter()", "author": "nsivabalan", "createdAt": "2020-06-21T22:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0ODMzNQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443848335", "bodyText": "Sounds good.", "author": "prashantwason", "createdAt": "2020-06-22T21:56:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2MjA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443265526", "bodyText": "not sure if my way of thinking is right. correct me if I am wrong. we are making our file formats flexible where in either of base file or log file can be of any format. I do understand for index purposes both base file and log file will be HFile. but wondering if tagging this config to table is the right think to do or should we have two configs, one for base file and one for log file. What in case we want to experiment w/ Hfile format for log files and parquet as base files since we might get indexing capability for log files too.", "author": "nsivabalan", "createdAt": "2020-06-21T23:09:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/CommitActionExecutor.java", "diffHunk": "@@ -115,7 +115,11 @@ public CommitActionExecutor(JavaSparkContext jsc,\n   }\n \n   protected HoodieMergeHandle getUpdateHandle(String partitionPath, String fileId, Iterator<HoodieRecord<T>> recordItr) {\n-    return new HoodieMergeHandle<>(config, instantTime, (HoodieTable<T>)table, recordItr, partitionPath, fileId, sparkTaskContextSupplier);\n+    if (table.requireSortedRecords()) {", "originalCommit": "47e9f43b0054a8b16842fca22a2809c6a8ce5384", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0OTYwMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r443849600", "bodyText": "Your understanding it correct.\nI will take care of this in a separate PR with HFile implementation.", "author": "prashantwason", "createdAt": "2020-06-22T21:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2NTUyNg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "url": "https://github.com/apache/hudi/commit/f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\nNotable changes:\n1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n4. HiveSyncTool accepts the base file format as a CLI parameter\n5. HoodieDeltaStreamer accepts the base file format as a CLI parameter", "committedDate": "2020-06-22T23:33:11Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NDUxOA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444444518", "bodyText": "Minor : Rename getFileReader => createFileReader ?", "author": "bvaradar", "createdAt": "2020-06-23T19:05:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieReadHandle.java", "diffHunk": "@@ -56,4 +61,9 @@ protected HoodieBaseFile getLatestDataFile() {\n     return hoodieTable.getBaseFileOnlyView()\n         .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n   }\n+\n+  protected HoodieFileReader getNewFileReader() throws IOException {\n+    return HoodieFileReaderFactory.getFileReader(hoodieTable.getHadoopConf(),", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NDY0NA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444654644", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-06-24T05:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NDUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NTU3Mg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444445572", "bodyText": "Similar suggestion on rename getFileWriter => createFileWriter", "author": "bvaradar", "createdAt": "2020-06-23T19:06:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/HoodieWriteHandle.java", "diffHunk": "@@ -180,4 +183,10 @@ protected int getStageId() {\n   protected long getAttemptId() {\n     return sparkTaskContextSupplier.getAttemptIdSupplier().get();\n   }\n+\n+  protected HoodieFileWriter getNewFileWriter(String instantTime, Path path, HoodieTable<T> hoodieTable,\n+                                              HoodieWriteConfig config, Schema schema,\n+                                              SparkTaskContextSupplier sparkTaskContextSupplier) throws IOException {\n+    return HoodieFileWriterFactory.getFileWriter(instantTime, path, hoodieTable, config, schema, sparkTaskContextSupplier);", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NDkwMA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444654900", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-06-24T05:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NTU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzQwOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444447409", "bodyText": "Can we move this to FSUtils class ?", "author": "bvaradar", "createdAt": "2020-06-23T19:10:14Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieFileWriter.java", "diffHunk": "@@ -33,4 +37,12 @@\n   void close() throws IOException;\n \n   void writeAvro(String key, R oldRecord) throws IOException;\n+\n+  static Configuration registerFileSystem(Path file, Configuration conf) {", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NTc2NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444655765", "bodyText": "Moved", "author": "prashantwason", "createdAt": "2020-06-24T05:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ0NzQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ1OTI4Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444459286", "bodyText": "thanks for cleaning this up", "author": "bvaradar", "createdAt": "2020-06-23T19:33:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/storage/HoodieParquetWriter.java", "diffHunk": "@@ -51,7 +49,6 @@\n   private final long maxFileSize;\n   private final HoodieAvroWriteSupport writeSupport;\n   private final String instantTime;\n-  private final Schema schema;", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444460940", "bodyText": "Is this done some where else now ?", "author": "bvaradar", "createdAt": "2020-06-23T19:36:19Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -150,11 +148,13 @@ public HoodieWriteMetadata compact(JavaSparkContext jsc, String compactionInstan\n       throw new HoodieUpsertException(\n           \"Error in finding the old file path at commit \" + instantTime + \" for fileId: \" + fileId);\n     } else {\n-      AvroReadSupport.setAvroReadSchema(getHadoopConf(), upsertHandle.getWriterSchema());", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2NTg5OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444465898", "bodyText": "nvm, found it in getRecordIterator", "author": "bvaradar", "createdAt": "2020-06-23T19:46:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2MDk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDIzOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444474239", "bodyText": "thanks for cleaning this up.", "author": "bvaradar", "createdAt": "2020-06-23T20:02:01Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -356,20 +356,7 @@ public MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompac\n    * @return\n    */\n   public MessageType readSchemaFromLogFile(Path path) throws IOException {\n-    FileSystem fs = metaClient.getRawFs();\n-    Reader reader = HoodieLogFormat.newReader(fs, new HoodieLogFile(path), null);\n-    HoodieAvroDataBlock lastBlock = null;\n-    while (reader.hasNext()) {\n-      HoodieLogBlock block = reader.next();\n-      if (block instanceof HoodieAvroDataBlock) {\n-        lastBlock = (HoodieAvroDataBlock) block;\n-      }\n-    }\n-    reader.close();\n-    if (lastBlock != null) {\n-      return new AvroSchemaConverter().convert(lastBlock.getSchema());\n-    }\n-    return null;\n+    return readSchemaFromLogFile(metaClient.getRawFs(), path);", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444486839", "bodyText": "Can we keep this in hudi-client ?", "author": "bvaradar", "createdAt": "2020-06-23T20:27:11Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetReaderIterator.java", "diffHunk": "@@ -16,7 +16,7 @@\n  * limitations under the License.\n  */\n \n-package org.apache.hudi.client.utils;\n+package org.apache.hudi.common.util;", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NjYwOA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444656608", "bodyText": "This will be hard as HoodieParquetReader (hudi-common) uses ParquetReaderIterator. And hudi-common cannot add dependency to hudi-client.", "author": "prashantwason", "createdAt": "2020-06-24T05:42:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5MTc1OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444691759", "bodyText": "Got it. hudi-hadoop-mr also needs HoodieFileReader abstraction for reading schema from file and this is reason why all these classes needs to be in hudi-common", "author": "bvaradar", "createdAt": "2020-06-24T07:15:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NjgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NzYyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444487626", "bodyText": "HFile => Avro", "author": "bvaradar", "createdAt": "2020-06-23T20:28:45Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/functional/TestHoodieAvroLogFormat.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.functional;\n+\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock.HoodieLogBlockType;\n+\n+/**\n+ * Tests HFile log format {@link HoodieHFileLogFormat}.", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444498080", "bodyText": "Can we avoid this cleanup as this is not directly related to this PR and I am also making changes on these same methods.", "author": "bvaradar", "createdAt": "2020-06-23T20:49:09Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -80,58 +77,6 @@ protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline tim\n     return timeline;\n   }\n \n-  /**", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1Nzk2Ng==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444657966", "bodyText": "This is simply moving some code into the helper class HoodieRealtimeInputFormatUtils. This has no logic change so hopefully it should not conflict with your changes.\nStill, if you want me to revert these changes, do let me know.", "author": "prashantwason", "createdAt": "2020-06-24T05:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4NzA5OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444687099", "bodyText": "I agree but knowing where I made changes to the same methods and consolidated in a different place due to other requirements :) it would make merging easier if I this is not done here. Hope, this is ok.", "author": "bvaradar", "createdAt": "2020-06-24T07:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2MDMwOQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445760309", "bodyText": "Ok. Reverted these changed.", "author": "prashantwason", "createdAt": "2020-06-25T18:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ5ODA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444504285", "bodyText": "The output format (MapredParquetOutputFormat  in case of parquet) would have to change depending on storage type here. right ? We need to parameterize that as well", "author": "bvaradar", "createdAt": "2020-06-23T20:57:50Z", "path": "hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -146,21 +146,22 @@ private void syncSchema(String tableName, boolean tableExists, boolean useRealTi\n     // Check and sync schema\n     if (!tableExists) {\n       LOG.info(\"Hive table \" + tableName + \" is not found. Creating it\");\n-      if (!useRealTimeInputFormat) {\n-        String inputFormatClassName = cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.HoodieInputFormat.class.getName()\n-            : HoodieParquetInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n-      } else {\n-        // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n-        // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n-        // /ql/exec/DDLTask.java#L3488\n-        String inputFormatClassName =\n-            cfg.usePreApacheInputFormat ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n-                : HoodieParquetRealtimeInputFormat.class.getName();\n-        hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),\n-            ParquetHiveSerDe.class.getName());\n+      HoodieFileFormat baseFileFormat = HoodieFileFormat.valueOf(cfg.baseFileFormat.toUpperCase());\n+      String inputFormatClassName = HoodieInputFormatUtils.getInputFormatClassName(baseFileFormat, useRealTimeInputFormat,\n+          new Configuration());\n+\n+      if (baseFileFormat.equals(HoodieFileFormat.PARQUET) && cfg.usePreApacheInputFormat) {\n+        // Parquet input format had an InputFormat class visible under the old naming scheme.\n+        inputFormatClassName = useRealTimeInputFormat\n+            ? com.uber.hoodie.hadoop.realtime.HoodieRealtimeInputFormat.class.getName()\n+            : com.uber.hoodie.hadoop.HoodieInputFormat.class.getName();\n       }\n+\n+      // Custom serde will not work with ALTER TABLE REPLACE COLUMNS\n+      // https://github.com/apache/hive/blob/release-1.1.0/ql/src/java/org/apache/hadoop/hive\n+      // /ql/exec/DDLTask.java#L3488\n+      hoodieHiveClient.createTable(tableName, schema, inputFormatClassName, MapredParquetOutputFormat.class.getName(),", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1ODc2MQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444658761", "bodyText": "HUDI does not implement any custom outputformat yet. So this will be a big task.\nIf my understanding is correct, the OutputFormat will be used when data is written into a HUDI dataset from Hive itself. Do we even support this case?", "author": "prashantwason", "createdAt": "2020-06-24T05:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4NDcyNg==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444684726", "bodyText": "I did not mean to implement output formats. For Parquet, the outputformat is MapredParquetOutputFormat but this will surely be different for  HFile. right ?\nWhat I meant was we need to keep a mapping (that identifies an outputformat.) Yes, we do not use the output format but registering output format as \"MapredParquetOutputFormat\" for non-parquet storage types is  misleading.\nWe can just keep an enum that identifies the output format for each storage type and use it here. Thats all is needed IMO.", "author": "bvaradar", "createdAt": "2020-06-24T07:00:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2NDQ2OQ==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445764469", "bodyText": "Done. I have creates the following functions which currently only support PARQUET but can be extended in future.\nHoodieInputFormatUtils.getOutputFormatClassName()\nHoodieInputFormatUtils.getSerDeClassName()", "author": "prashantwason", "createdAt": "2020-06-25T18:43:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNDI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNjA4OA==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r444506088", "bodyText": "Similar functionality needs to be done for Spark SQL Writer. See HoodieSparkSqlWriter.scala", "author": "bvaradar", "createdAt": "2020-06-23T21:01:18Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -177,6 +177,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--table-type\"}, description = \"Type of table. COPY_ON_WRITE (or) MERGE_ON_READ\", required = true)\n     public String tableType;\n \n+    @Parameter(names = {\"--base-file-format\"}, description = \"File format for the base files. PARQUET (or) HFILE\", required = false)", "originalCommit": "f75e6ed3c0128aa6fc47f6531a498023e8d5c60d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTc2NTY5Mw==", "url": "https://github.com/apache/hudi/pull/1687#discussion_r445765693", "bodyText": "Done. The changes are in\nhudi-spark/src/main/java/org/apache/hudi/DataSourceUtils.java\nhudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala\nhudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "author": "prashantwason", "createdAt": "2020-06-25T18:44:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUwNjA4OA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "commit"}, {"oid": "6ca70668690c33ddb5390b58c2f736210022538b", "url": "https://github.com/apache/hudi/commit/6ca70668690c33ddb5390b58c2f736210022538b", "message": "[HUDI-684] Introduced abstraction for writing and reading different types of base file formats.\n\n    Notable changes:\n    1. HoodieFileWriter and HoodieFileReader abstractions for writer/reader side of a base file format\n    2. HoodieDataBlock abstraction for creation specific data blocks for base file formats. (e.g. Parquet has HoodieAvroDataBlock)\n    3. All hardocded references to Parquet / Parquet based classes have been abstracted to call methods which accept a base file format\n    4. HiveSyncTool accepts the base file format as a CLI parameter\n    5. HoodieDeltaStreamer accepts the base file format as a CLI parameter\n    6. HoodieSparkSqlWriter accepts the base file format as a parameter", "committedDate": "2020-06-25T18:53:36Z", "type": "forcePushed"}]}