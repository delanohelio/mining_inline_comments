{"pr_number": 1306, "pr_title": "[HUDI-598] Update quick start page", "pr_createdAt": "2020-02-04T18:14:59Z", "pr_url": "https://github.com/apache/hudi/pull/1306", "timeline": [{"oid": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e", "url": "https://github.com/apache/hudi/commit/3890d0f5fc45eb1b90c430f97c7d12d935d4282e", "message": "[HUDI-598] Update quick start page", "committedDate": "2020-02-04T18:06:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NTU5Mg==", "url": "https://github.com/apache/hudi/pull/1306#discussion_r374955592", "bodyText": "hudi_ro_table -> hudi_trips_snapshot here as well.", "author": "bhasudha", "createdAt": "2020-02-04T22:19:37Z", "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -176,28 +176,28 @@ Delete records for the HoodieKeys passed in.\n \n ```scala\n // fetch total records count\n-spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n // fetch two records to be deleted\n-val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table\").limit(2)\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n \n // issue deletes\n val deletes = dataGen.generateDeletes(ds.collectAsList())\n val df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\n-df.write.format(\"org.apache.hudi\").\n-options(getQuickstartWriteConfigs).\n-option(OPERATION_OPT_KEY,\"delete\").\n-option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n-option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n-option(TABLE_NAME, tableName).\n-mode(Append).\n-save(basePath);\n+df.write.format(\"hudi\").\n+  options(getQuickstartWriteConfigs).\n+  option(OPERATION_OPT_KEY,\"delete\").\n+  option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(TABLE_NAME, tableName).\n+  mode(Append).\n+  save(basePath)\n \n // run the same read query as above.\n val roAfterDeleteViewDF = spark.\n-    read.\n-    format(\"org.apache.hudi\").\n-    load(basePath + \"/*/*/*/*\")\n+  read.\n+  format(\"hudi\").\n+  load(basePath + \"/*/*/*/*\")\n roAfterDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n // fetch should return (total - 2) records\n spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()", "originalCommit": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NjA5OA==", "url": "https://github.com/apache/hudi/pull/1306#discussion_r374956098", "bodyText": "consider changing hudi_ro_table -> hudi_trips_snapshot here as well for consistency ?", "author": "bhasudha", "createdAt": "2020-02-04T22:20:45Z", "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -176,28 +176,28 @@ Delete records for the HoodieKeys passed in.\n \n ```scala\n // fetch total records count\n-spark.sql(\"select uuid, partitionPath from hudi_ro_table\").count()\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n // fetch two records to be deleted\n-val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table\").limit(2)\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n \n // issue deletes\n val deletes = dataGen.generateDeletes(ds.collectAsList())\n val df = spark.read.json(spark.sparkContext.parallelize(deletes, 2));\n-df.write.format(\"org.apache.hudi\").\n-options(getQuickstartWriteConfigs).\n-option(OPERATION_OPT_KEY,\"delete\").\n-option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n-option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n-option(TABLE_NAME, tableName).\n-mode(Append).\n-save(basePath);\n+df.write.format(\"hudi\").\n+  options(getQuickstartWriteConfigs).\n+  option(OPERATION_OPT_KEY,\"delete\").\n+  option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+  option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(TABLE_NAME, tableName).\n+  mode(Append).\n+  save(basePath)\n \n // run the same read query as above.\n val roAfterDeleteViewDF = spark.\n-    read.\n-    format(\"org.apache.hudi\").\n-    load(basePath + \"/*/*/*/*\")\n+  read.\n+  format(\"hudi\").\n+  load(basePath + \"/*/*/*/*\")\n roAfterDeleteViewDF.registerTempTable(\"hudi_ro_table\")", "originalCommit": "3890d0f5fc45eb1b90c430f97c7d12d935d4282e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTA2NDgwOQ==", "url": "https://github.com/apache/hudi/pull/1306#discussion_r375064809", "bodyText": "Done.", "author": "lamberken", "createdAt": "2020-02-05T05:25:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NjA5OA=="}], "type": "inlineReview"}, {"oid": "b71f1c737f4bfcfb271976d600cdbf4917031898", "url": "https://github.com/apache/hudi/commit/b71f1c737f4bfcfb271976d600cdbf4917031898", "message": "Unify table name", "committedDate": "2020-02-05T05:25:11Z", "type": "commit"}]}