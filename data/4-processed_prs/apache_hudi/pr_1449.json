{"pr_number": 1449, "pr_title": "[HUDI-698]Add unit test for CleansCommand", "pr_createdAt": "2020-03-26T06:41:49Z", "pr_url": "https://github.com/apache/hudi/pull/1449", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyODY4Ng==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r399828686", "bodyText": "use standard java for this - instead of Parquet.Strings.join()", "author": "smarthi", "createdAt": "2020-03-29T17:34:39Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.parquet.Strings;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Iterator;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    String localPropsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n+    propsFilePath = \"/tmp/clean.properties\";\n+    initDFS();\n+    jsc.hadoopConfiguration().addResource(dfs.getConf());\n+    HoodieCLI.conf = dfs.getConf();\n+\n+    dfs.mkdir(new Path(\"/tmp\"), FsPermission.getDefault());\n+    dfs.copyFromLocalFile(new Path(localPropsFilePath), new Path(\"/tmp\"));\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+\n+    SparkMain.clean(jsc, HoodieCLI.basePath, \"local\", propsFilePath, \"2G\", new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+    TableHeader header =\n+        new TableHeader().addTableHeaderField(\"CleanTime\").addTableHeaderField(\"EarliestCommandRetained\")\n+            .addTableHeaderField(\"Total Files Deleted\").addTableHeaderField(\"Total Time Taken\");\n+    List<Comparable[]> rows = new ArrayList<>();\n+\n+    // EarliestCommandRetained should be 102, since hoodie.cleaner.commits.retained=2\n+    // Total Time Taken should be -1, since hoodie.metrics.on is false by default\n+    rows.add(new Comparable[]{clean.getTimestamp(), \"102\", \"0\", \"-1\"});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for cleans run.\n+   */\n+  @Test\n+  public void testRunClean() throws IOException, InterruptedException, URISyntaxException {\n+    // First, there should none of clean instant.\n+    assertEquals(0, metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    // Create partition metadata\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+    dfs.create(new Path(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE));\n+\n+    SparkEnvCommand.env.put(\"SPARK_MASTER\", \"local\");\n+\n+    List<String> configs = new ArrayList();\n+    Iterator<Map.Entry<String, String>> iterator = dfs.getConf().iterator();\n+    while (iterator.hasNext()) {\n+      Map.Entry<String, String> e = iterator.next();\n+      configs.add(e.getKey() + \"=\" + e.getValue());\n+    }\n+    String hadoopConf = Strings.join(configs, \" \");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3NDMzMg==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r405274332", "bodyText": "Please make sure we do not use STDOUT in the test case.", "author": "yanghua", "createdAt": "2020-04-08T05:55:39Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -56,8 +58,16 @@ public void init() throws IOException {\n \n     String tableName = \"test_table\";\n     tablePath = basePath + File.separator + tableName;\n-    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n-\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+    if (propsFilePath == null) {\n+      System.out.println(\"-------------------\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3ODM5MQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r405278391", "bodyText": "Please make sure we do not use STDOUT in the test case.\n\nYes, just for check something. There are some different between local and CI environment. Will remove them late.", "author": "hddong", "createdAt": "2020-04-08T06:06:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3NDMzMg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODIyNg==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278226", "bodyText": "getResource may be nullable. Here, we may need to check null firstly.", "author": "yanghua", "createdAt": "2020-04-13T01:19:15Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzU1NTM1Nw==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407555357", "bodyText": "getResource may be nullable. Here, we may need to check null firstly.\n\nHad add a check in Test. But, IMO, the check is non-essential, test will failed if getResource is null.", "author": "hddong", "createdAt": "2020-04-13T15:59:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODIyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNDUyNw==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407834527", "bodyText": "Yes, if it's null, the test would fail. However, it's a good habit to check it.", "author": "yanghua", "createdAt": "2020-04-14T02:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODIyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODQzOQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278439", "bodyText": "HoodieTableType.COPY_ON_WRITE.name is better?", "author": "yanghua", "createdAt": "2020-04-13T01:20:55Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODc3OQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278779", "bodyText": "We may need to check if the value is present here?", "author": "yanghua", "createdAt": "2020-04-13T01:23:27Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODkxOA==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278918", "bodyText": "No need to box this value, could be -1L.", "author": "yanghua", "createdAt": "2020-04-13T01:24:18Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+    TableHeader header =\n+        new TableHeader().addTableHeaderField(\"CleanTime\").addTableHeaderField(\"EarliestCommandRetained\")\n+            .addTableHeaderField(\"Total Files Deleted\").addTableHeaderField(\"Total Time Taken\");\n+    List<Comparable[]> rows = new ArrayList<>();\n+\n+    // EarliestCommandRetained should be 102, since hoodie.cleaner.commits.retained=2\n+    // Total Time Taken need read from metadata\n+    rows.add(new Comparable[]{clean.getTimestamp(), \"102\", \"0\", getLatestCleanTimeTakenInMillis().toString()});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for show partitions of a clean instant.\n+   */\n+  @Test\n+  public void testShowCleanPartitions() throws IOException {\n+    // First, run clean with two partition\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath, new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().get();\n+\n+    CommandResult cr = getShell().executeCommand(\"clean showpartitions --clean \" + clean.getTimestamp());\n+    assertTrue(cr.isSuccess());\n+\n+    TableHeader header = new TableHeader().addTableHeaderField(\"Partition Path\").addTableHeaderField(\"Cleaning policy\")\n+        .addTableHeaderField(\"Total Files Successfully Deleted\").addTableHeaderField(\"Total Failed Deletions\");\n+\n+    // There should be two partition path\n+    List<Comparable[]> rows = new ArrayList<>();\n+    rows.add(new Comparable[]{HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH,\n+        HoodieCleaningPolicy.KEEP_LATEST_COMMITS, \"0\", \"0\"});\n+    rows.add(new Comparable[]{HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        HoodieCleaningPolicy.KEEP_LATEST_COMMITS, \"0\", \"0\"});\n+\n+    String expected = HoodiePrintHelper.print(header, new HashMap<>(), \"\", false, -1, false, rows);\n+    assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Get time taken of latest instant.\n+   */\n+  private Long getLatestCleanTimeTakenInMillis() throws IOException {\n+    HoodieActiveTimeline activeTimeline = HoodieCLI.getTableMetaClient().getActiveTimeline();\n+    HoodieTimeline timeline = activeTimeline.getCleanerTimeline().filterCompletedInstants();\n+    HoodieInstant clean = timeline.getReverseOrderedInstants().findFirst().orElse(null);\n+    if (clean != null) {\n+      HoodieCleanMetadata cleanMetadata =\n+          TimelineMetadataUtils.deserializeHoodieCleanMetadata(timeline.getInstantDetails(clean).get());\n+      return cleanMetadata.getTimeTakenInMillis();\n+    }\n+    return new Long(-1);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3ODk4Ng==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407278986", "bodyText": "ditto", "author": "yanghua", "createdAt": "2020-04-13T01:24:51Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCleansCommand.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ITTestCleansCommand extends AbstractShellIntegrationTest {\n+  private String tablePath;\n+  private String propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\").getPath();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName,\n+        \"COPY_ON_WRITE\", \"\", 1, \"org.apache.hudi.common.model.HoodieAvroPayload\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "message": "Add test for cleanCommand", "committedDate": "2020-04-13T15:48:10Z", "type": "commit"}, {"oid": "5d301c5b96520bb929ccee6657df512a0edb530f", "url": "https://github.com/apache/hudi/commit/5d301c5b96520bb929ccee6657df512a0edb530f", "message": "Add test for cleanCommand", "committedDate": "2020-04-13T15:48:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407837053", "bodyText": "Are you sure we must introduce this phase in hudi-cli module?", "author": "yanghua", "createdAt": "2020-04-14T02:59:14Z", "path": "hudi-cli/pom.xml", "diffHunk": "@@ -122,6 +122,31 @@\n           <includeTestSourceDirectory>false</includeTestSourceDirectory>\n         </configuration>\n       </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-failsafe-plugin</artifactId>\n+        <version>2.22.0</version>\n+        <configuration>\n+          <includes>\n+            <include>**/ITT*.java</include>\n+          </includes>\n+        </configuration>\n+        <executions>\n+          <execution>\n+            <phase>integration-test</phase>", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk1NDEwNQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407954105", "bodyText": "Are you sure we must introduce this phase in hudi-cli module?\n\nThis line is non-essential, it can indicate that the binding is integration-test. Maybe let it here is also ok. if need remove, I will delete it.", "author": "hddong", "createdAt": "2020-04-14T08:23:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5MjE1OA==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407992158", "bodyText": "I am thinking if we want to bind with integration-test phase. Why not move to the integration test module?", "author": "yanghua", "createdAt": "2020-04-14T09:23:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5OTY4MA==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407999680", "bodyText": "@yanghua I've thought about it before, this test dependency on spring-shell and  integration-test module can not use getShell().executeCommand(\"....\") to access commands in hudi-cli.", "author": "hddong", "createdAt": "2020-04-14T09:35:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODAxMTg5Mw==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r408011893", "bodyText": "Fine. Let's try to figure out a better way.", "author": "yanghua", "createdAt": "2020-04-14T09:54:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzNzA1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODgwMQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407838801", "bodyText": "add a blank before the number 1?", "author": "yanghua", "createdAt": "2020-04-14T03:05:41Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestCleansCommand.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+public class ITTestCleansCommand extends AbstractShellIntegrationTest {\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = this.getClass().getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+  }\n+\n+  /**\n+   * Test case for cleans run.\n+   */\n+  @Test\n+  public void testRunClean() throws IOException {\n+    // First, there should none of clean instant.\n+    assertEquals(0, metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // Create partition metadata\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_SECOND_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans run --sparkMaster local --propsFilePath \" + propsFilePath.toString());\n+    assertTrue(cr.isSuccess());\n+\n+    // After run clean, there should have 1 clean instant\n+    assertEquals(\"Loaded 1 clean and the count should match\",1,", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODk2MA==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407838960", "bodyText": "add an empty line before this line?", "author": "yanghua", "createdAt": "2020-04-14T03:06:08Z", "path": "hudi-cli/src/test/resources/clean.properties", "diffHunk": "@@ -0,0 +1,19 @@\n+###\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#      http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+###\n+hoodie.cleaner.incremental.mode=true", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk0ODE0Mg==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407948142", "bodyText": "add an empty line before this line?\n\nOther properties files has no empty line, maybe use consistent format is better.", "author": "hddong", "createdAt": "2020-04-14T08:13:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODk2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzk5MjI5MA==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407992290", "bodyText": "OK", "author": "yanghua", "createdAt": "2020-04-14T09:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzODk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzOTIxMw==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407839213", "bodyText": "Can we extract the spark and Hadoop version number into a variable for upgrading purposes?", "author": "yanghua", "createdAt": "2020-04-14T03:07:08Z", "path": ".travis.yml", "diffHunk": "@@ -39,3 +39,9 @@ script:\n   - scripts/run_travis_tests.sh $TEST_SUITE\n after_success:\n   - bash <(curl -s https://codecov.io/bash)\n+before_script:\n+  - echo \"=====[ Download spark]=====\"\n+  - wget http://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz -O /tmp/spark-2.4.4.tgz", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgzOTQzMg==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407839432", "bodyText": "Download spark -> Downloading Apache Spark?", "author": "yanghua", "createdAt": "2020-04-14T03:08:00Z", "path": ".travis.yml", "diffHunk": "@@ -39,3 +39,9 @@ script:\n   - scripts/run_travis_tests.sh $TEST_SUITE\n after_success:\n   - bash <(curl -s https://codecov.io/bash)\n+before_script:\n+  - echo \"=====[ Download spark]=====\"", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg0MDI3NQ==", "url": "https://github.com/apache/hudi/pull/1449#discussion_r407840275", "bodyText": "It can be assertNotNull(...)", "author": "yanghua", "createdAt": "2020-04-14T03:11:20Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestCleansCommand.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.TableHeader;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.model.HoodieCleaningPolicy;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Test Cases for {@link CleansCommand}.\n+ */\n+public class TestCleansCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+  private URL propsFilePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    propsFilePath = TestCleansCommand.class.getClassLoader().getResource(\"clean.properties\");\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+    // Create four commits\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      // Inflight Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionAuxiliaryMetadata(tablePath,\n+          new HoodieInstant(HoodieInstant.State.INFLIGHT, HoodieTimeline.COMPACTION_ACTION, timestamp), conf);\n+      HoodieTestCommitMetadataGenerator.createCommitFileWithMetadata(tablePath, timestamp, conf);\n+    }\n+\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // reload the timeline and get all the commits before archive\n+    metaClient.getActiveTimeline().reload();\n+  }\n+\n+  /**\n+   * Test case for show all cleans.\n+   */\n+  @Test\n+  public void testShowCleans() throws Exception {\n+    // Check properties file exists.\n+    assertNotNull(\"Not found properties file\", propsFilePath);\n+\n+    // First, run clean\n+    new File(tablePath + File.separator + HoodieTestCommitMetadataGenerator.DEFAULT_FIRST_PARTITION_PATH\n+        + File.separator + HoodiePartitionMetadata.HOODIE_PARTITION_METAFILE).createNewFile();\n+    SparkMain.clean(jsc, HoodieCLI.basePath, propsFilePath.getPath(), new ArrayList<>());\n+    assertEquals(\"Loaded 1 clean and the count should match\", 1,\n+        metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"cleans show\");\n+    assertTrue(cr.isSuccess());\n+\n+    HoodieInstant clean = metaClient.getActiveTimeline().reload().getCleanerTimeline().getInstants().findFirst().orElse(null);\n+    assertTrue(clean != null);", "originalCommit": "5d301c5b96520bb929ccee6657df512a0edb530f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b14b63b14de667ecc0ff88c5d149682da15fab6c", "url": "https://github.com/apache/hudi/commit/b14b63b14de667ecc0ff88c5d149682da15fab6c", "message": "add version for download", "committedDate": "2020-04-14T08:09:36Z", "type": "commit"}]}