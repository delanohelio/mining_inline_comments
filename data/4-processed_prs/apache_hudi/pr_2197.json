{"pr_number": 2197, "pr_title": "[HUDI-1351] Improvements to the hudi test suite for scalability and repeated testing.", "pr_createdAt": "2020-10-22T06:20:11Z", "pr_url": "https://github.com/apache/hudi/pull/2197", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNDc2OA==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r511704768", "bodyText": "can you explicitly add scope \"private\" ?", "author": "n3nash", "createdAt": "2020-10-26T03:38:08Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/configuration/DFSDeltaConfig.java", "diffHunk": "@@ -36,15 +36,22 @@\n   private final Long maxFileSize;\n   // The current batch id\n   private Integer batchId;\n+  // Paralleism to use when generating input data\n+  int inputParallelism;", "originalCommit": "27149aad495ec3b0667e48c01e9be0e7afd3ad1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk4ODE2Ng==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r512988166", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-10-27T19:51:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNDc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNDgzOQ==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r511704839", "bodyText": "rename to inputGeneratorParallelism ?", "author": "n3nash", "createdAt": "2020-10-26T03:38:30Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/configuration/DFSDeltaConfig.java", "diffHunk": "@@ -36,15 +36,22 @@\n   private final Long maxFileSize;\n   // The current batch id\n   private Integer batchId;\n+  // Paralleism to use when generating input data\n+  int inputParallelism;", "originalCommit": "27149aad495ec3b0667e48c01e9be0e7afd3ad1c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjMzMQ==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r511706331", "bodyText": "What is the purpose behind this change ?", "author": "n3nash", "createdAt": "2020-10-26T03:46:46Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java", "diffHunk": "@@ -58,15 +63,15 @@\n \n   private static Logger log = LoggerFactory.getLogger(DeltaGenerator.class);\n \n-  private DeltaConfig deltaOutputConfig;\n+  private DFSDeltaConfig deltaOutputConfig;", "originalCommit": "27149aad495ec3b0667e48c01e9be0e7afd3ad1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk4OTUyNw==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r512989527", "bodyText": "DFSDeltaConfig extends DeltaConfig\nThe two settings I have added (getInputParallelism and shouldDeleteOldInputData) are in DFSDeltaConfig.", "author": "prashantwason", "createdAt": "2020-10-27T19:53:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjQyNw==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r511706427", "bodyText": "This may not work in case the last batches were rolled back. Can you take a look at RollbackNode and see what will be the implication ?", "author": "n3nash", "createdAt": "2020-10-26T03:47:24Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java", "diffHunk": "@@ -77,6 +82,16 @@ public DeltaGenerator(DeltaConfig deltaOutputConfig, JavaSparkContext jsc, Spark\n   }\n \n   public JavaRDD<DeltaWriteStats> writeRecords(JavaRDD<GenericRecord> records) {\n+    if (deltaOutputConfig.shouldDeleteOldInputData() && batchId > 1) {\n+      Path oldInputDir = new Path(deltaOutputConfig.getDeltaBasePath(), Integer.toString(batchId - 1));", "originalCommit": "27149aad495ec3b0667e48c01e9be0e7afd3ad1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk5MTU0Nw==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r512991547", "bodyText": "RollbackNode will rollback the last commit. This should not interfere will these input directories.\nThe shouldDeleteOldInputData() setting only affects the data generated in the \"input\" directory (a separate directory) which is not part of the HUDI dataset under test. For each Node in the yaml, a sub-directory in the input directory (identified by batchId) is created. Within this new sub-directory, the data to be ingested as part of the Node is written as avro files.\nWe are deleting older input sub-directories. The default is to not delete anything.", "author": "prashantwason", "createdAt": "2020-10-27T19:57:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjQyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjUzNg==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r511706536", "bodyText": "Can you please explain the startPartition with an example ? What happens when a spark stage is retried ? Take a look at how spark stage retries mess up the partition numbers to understand more..", "author": "n3nash", "createdAt": "2020-10-26T03:48:04Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/DeltaGenerator.java", "diffHunk": "@@ -95,11 +110,19 @@ public DeltaGenerator(DeltaConfig deltaOutputConfig, JavaSparkContext jsc, Spark\n     int numPartitions = operation.getNumInsertPartitions();\n     long recordsPerPartition = operation.getNumRecordsInsert() / numPartitions;\n     int minPayloadSize = operation.getRecordSize();\n-    JavaRDD<GenericRecord> inputBatch = jsc.parallelize(Collections.EMPTY_LIST)\n-        .repartition(numPartitions).mapPartitions(p -> {\n+    int startPartition = operation.getStartPartition();", "originalCommit": "27149aad495ec3b0667e48c01e9be0e7afd3ad1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA1OTg0Mw==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r513059843", "bodyText": "Suppose you insert 5 partitions. Then the following 5 new LazyRecordGeneratorIterator will be created:\nnew LazyRecordGeneratorIterator(..., 0)\nnew LazyRecordGeneratorIterator(..., 1)\nnew LazyRecordGeneratorIterator(..., 2)\nnew LazyRecordGeneratorIterator(..., 3)\nnew LazyRecordGeneratorIterator(..., 4)\nWithin the LazyRecordGeneratorIterator code, the integer for partition index (0, 1, .. above) are converted into partition timstamp (as date offset from 1970/01/01). So the first LazyRecordGeneratorIterator will be generating records from 1970/01/01, the second LazyRecordGeneratorIterator will generate records for 1970/01/02 ... and so on.\nWith this schema, the record generation always starts at offset 0. But what if you want to generate for only a specific partition? Or add new partition? This is where the start_offset comes into play.\nnew LazyRecordGeneratorIterator(..., 0 + start_offset)\nnew LazyRecordGeneratorIterator(..., 1 + start_offset)\nnew LazyRecordGeneratorIterator(..., 2 + start_offset)\nnew LazyRecordGeneratorIterator(..., 3 + start_offset)\nnew LazyRecordGeneratorIterator(..., 4 + start_offset)\nBy using a start_offset you can alter where the inserts will take place. Also new partitions can be created.\nSpark retries can alter the partition numbers here. For that, we can use a pre-formatted List with partitions here.", "author": "prashantwason", "createdAt": "2020-10-27T22:02:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjUzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzUzNTE5MA==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r513535190", "bodyText": "Okay, that makes sense @prashantwason. Spark retries are pretty common, lets handle that use-case", "author": "n3nash", "createdAt": "2020-10-28T15:24:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjUzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY5NDk1OQ==", "url": "https://github.com/apache/hudi/pull/2197#discussion_r513694959", "bodyText": "Already done. Please see the update.", "author": "prashantwason", "createdAt": "2020-10-28T19:07:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcwNjUzNg=="}], "type": "inlineReview"}, {"oid": "bc0559bc49682e514ec2a5e3817437ec17230712", "url": "https://github.com/apache/hudi/commit/bc0559bc49682e514ec2a5e3817437ec17230712", "message": "[HUDI-1351] Improvements to the hudi test suite for scalability and repeated testing.\n\n1. Added the --clean-input and --clean-output parameters to clean the input and output directories before starting the job\n2. Added the --delete-old-input parameter to deleted older batches for data already ingested. This helps keep number of redundant files low.\n3. Added the --input-parallelism parameter to restrict the parallelism when generating input data. This helps keeping the number of generated input files low.\n4. Added an option start_offset to Dag Nodes. Without ability to specify start offsets, data is generated into existing partitions. With start offset, DAG can control on which partition, the data is to be written.\n5. Fixed generation of records for correct number of partitions\n  - In the existing implementation, the partition is chosen as a random long. This does not guarantee exact number of requested partitions to be created.\n6. Changed variable blacklistedFields to be a Set as that is faster than List for membership checks.\n7. Fixed integer division for Math.ceil. If two integers are divided, the result is not double unless one of the integer is casted to double.", "committedDate": "2020-10-27T22:13:29Z", "type": "commit"}, {"oid": "bc0559bc49682e514ec2a5e3817437ec17230712", "url": "https://github.com/apache/hudi/commit/bc0559bc49682e514ec2a5e3817437ec17230712", "message": "[HUDI-1351] Improvements to the hudi test suite for scalability and repeated testing.\n\n1. Added the --clean-input and --clean-output parameters to clean the input and output directories before starting the job\n2. Added the --delete-old-input parameter to deleted older batches for data already ingested. This helps keep number of redundant files low.\n3. Added the --input-parallelism parameter to restrict the parallelism when generating input data. This helps keeping the number of generated input files low.\n4. Added an option start_offset to Dag Nodes. Without ability to specify start offsets, data is generated into existing partitions. With start offset, DAG can control on which partition, the data is to be written.\n5. Fixed generation of records for correct number of partitions\n  - In the existing implementation, the partition is chosen as a random long. This does not guarantee exact number of requested partitions to be created.\n6. Changed variable blacklistedFields to be a Set as that is faster than List for membership checks.\n7. Fixed integer division for Math.ceil. If two integers are divided, the result is not double unless one of the integer is casted to double.", "committedDate": "2020-10-27T22:13:29Z", "type": "forcePushed"}]}