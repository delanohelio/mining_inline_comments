{"pr_number": 1288, "pr_title": "[HUDI-117] Close file handle before throwing an exception due to append\u2026", "pr_createdAt": "2020-01-29T00:16:00Z", "pr_url": "https://github.com/apache/hudi/pull/1288", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDI2MA==", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190260", "bodyText": "Instead of Spark driver, lets just say when \"spark retries..\"", "author": "n3nash", "createdAt": "2020-01-29T05:06:18Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "diffHunk": "@@ -256,7 +280,22 @@ private void handleAppendExceptionOrRecoverLease(Path path, RemoteException e)\n         throw new HoodieException(e);\n       }\n     } else {\n-      throw new HoodieIOException(\"Failed to open an append stream \", e);\n+      // When fs.append() has failed and an exception is thrown, by closing the output stream\n+      // we shall force hdfs to release the lease on the log file. When Spark driver retries this task (with", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDQ0Mw==", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190443", "bodyText": "Can you add a comment here why we are throwing an exception here", "author": "n3nash", "createdAt": "2020-01-29T05:07:33Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "diffHunk": "@@ -256,7 +280,22 @@ private void handleAppendExceptionOrRecoverLease(Path path, RemoteException e)\n         throw new HoodieException(e);\n       }\n     } else {\n-      throw new HoodieIOException(\"Failed to open an append stream \", e);\n+      // When fs.append() has failed and an exception is thrown, by closing the output stream\n+      // we shall force hdfs to release the lease on the log file. When Spark driver retries this task (with\n+      // new attemptId, say taskId.1) it will be able to acquire lease on the log file (as output stream was\n+      // closed properly by taskId.0).\n+      //\n+      // If close() call were to fail throwing an exception, our best bet is to rollover to a new log file.\n+      try {\n+        close();\n+        // output stream has been successfully closed and lease on the log file has been released.\n+        throw new HoodieIOException(\"Failed to append to the output stream \", e);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDUzNg==", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190536", "bodyText": "nit : s/bloc/block", "author": "n3nash", "createdAt": "2020-01-29T05:08:01Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java", "diffHunk": "@@ -1113,6 +1113,99 @@ public void testAvroLogRecordReaderWithMixedInsertsCorruptsAndRollback()\n     assertEquals(\"We would read 0 records\", 0, scanner.getTotalLogRecords());\n   }\n \n+  /*\n+   * During a spark stage failure, when the stage is retried, tasks that are part of the previous attempt\n+   * of the stage would continue to run.  As a result two different tasks could be performing the same operation.\n+   * When trying to update the log file, only one of the tasks would succeed (one holding lease on the log file).\n+   *\n+   * In order to make progress in this scenario, second task attempting to update the log file would rollover to\n+   * a new version of the log file.  As a result, we might end up with two log files with same set of data records\n+   * present in both of them.\n+   *\n+   * Following uint tests mimic this scenario to ensure that the reader can handle merging multiple log files with\n+   * duplicate data.\n+   *\n+   */\n+  private void testAvroLogRecordReaderMergingMultipleLogFiles(int numRecordsInLog1, int numRecordsInLog2)\n+      throws IOException, URISyntaxException, InterruptedException {\n+    try {\n+      // Write one Data bloc with same InstantTime (written in same batch)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "87bc99de766a8e077a14e9a5930d215bef67a90f", "url": "https://github.com/apache/hudi/commit/87bc99de766a8e077a14e9a5930d215bef67a90f", "message": "HUDI-117 Close file handle before throwing an exception due to append failure.\nAdd test cases to handle/verify stage failure scenarios.", "committedDate": "2020-01-29T19:03:45Z", "type": "commit"}, {"oid": "87bc99de766a8e077a14e9a5930d215bef67a90f", "url": "https://github.com/apache/hudi/commit/87bc99de766a8e077a14e9a5930d215bef67a90f", "message": "HUDI-117 Close file handle before throwing an exception due to append failure.\nAdd test cases to handle/verify stage failure scenarios.", "committedDate": "2020-01-29T19:03:45Z", "type": "forcePushed"}]}