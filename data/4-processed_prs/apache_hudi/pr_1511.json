{"pr_number": 1511, "pr_title": "[HUDI-789]Adjust logic of upsert in HDFSParquetImporter", "pr_createdAt": "2020-04-13T07:15:12Z", "pr_url": "https://github.com/apache/hudi/pull/1511", "timeline": [{"oid": "a064dfcf8752dd39533d31ef397b938db031d9da", "url": "https://github.com/apache/hudi/commit/a064dfcf8752dd39533d31ef397b938db031d9da", "message": "Adjust logic of upsert in HDFSParquetImporter", "committedDate": "2020-04-13T07:12:15Z", "type": "commit"}, {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "url": "https://github.com/apache/hudi/commit/e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "message": "fix", "committedDate": "2020-04-13T09:53:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTYyOQ==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r407815629", "bodyText": "use boolean flag ?", "author": "hmatu", "createdAt": "2020-04-14T01:40:42Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java", "diffHunk": "@@ -100,6 +100,10 @@ public static void main(String[] args) {\n \n   }\n \n+  private boolean isUpsert() {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409389425", "bodyText": "Does this method use to init records for inserting? IMO, we should distinguish it with upsert.", "author": "yanghua", "createdAt": "2020-04-16T08:50:31Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDAwNTA2OA==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r410005068", "bodyText": "Does this method use to init records for inserting? IMO, we should distinguish it with upsert.\n\nYes, it is for inserting only, upsert has it's own method.\nhttps://github.com/apache/incubator-hudi/blob/e1a47ff32f900d9c723dc907784210ce756915f9/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java#L284-L286", "author": "hddong", "createdAt": "2020-04-17T05:48:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NDE0Ng==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411054146", "bodyText": "I mean for matching purpose, can we rename it to createInsertRecords?", "author": "yanghua", "createdAt": "2020-04-20T02:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5MzIwNg==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409393206", "bodyText": "Here exists redundant boxing issues. It would be better to use Double.parseDouble(xxx).", "author": "yanghua", "createdAt": "2020-04-16T08:56:15Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NDY3NA==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409394674", "bodyText": "Can we use try-with-resource here?", "author": "yanghua", "createdAt": "2020-04-16T08:58:19Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NTY2OA==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409395668", "bodyText": "I would suggest one field one line for so many arguments.", "author": "yanghua", "createdAt": "2020-04-16T08:59:50Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjE1Ng==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396156", "bodyText": "ditto, try-with-resource?", "author": "yanghua", "createdAt": "2020-04-16T09:00:36Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjgzOQ==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396839", "bodyText": "ParseException would never be thrown.", "author": "yanghua", "createdAt": "2020-04-16T09:01:42Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5OTA2MA==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409399060", "bodyText": "As a good habit, when we override the equals method, it would also be better to override the hashCode even though we did use it here.", "author": "yanghua", "createdAt": "2020-04-16T09:05:10Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +403,44 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {", "originalCommit": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e1a47ff32f900d9c723dc907784210ce756915f9", "url": "https://github.com/apache/hudi/commit/e1a47ff32f900d9c723dc907784210ce756915f9", "message": "Address", "committedDate": "2020-04-17T05:41:26Z", "type": "commit"}, {"oid": "6c68f1bfb76fb1144e6ec88678dd7eead4a60376", "url": "https://github.com/apache/hudi/commit/6c68f1bfb76fb1144e6ec88678dd7eead4a60376", "message": "remove use HashCodeBuilder", "committedDate": "2020-04-17T07:04:52Z", "type": "commit"}, {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "url": "https://github.com/apache/hudi/commit/56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "message": "Reset", "committedDate": "2020-04-17T09:10:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MjYxNQ==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411052615", "bodyText": "It's a bit long. Here, I would suggest using Objects.hash(...).", "author": "yanghua", "createdAt": "2020-04-20T02:24:05Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      HoodieModel other = (HoodieModel) o;\n+      return timestamp == other.timestamp && rowKey.equals(other.rowKey) && rider.equals(other.rider)\n+          && driver.equals(other.driver) && beginLat == other.beginLat && beginLon == other.beginLon\n+          && endLat == other.endLat && endLon == other.endLon;\n+    }\n+\n+    @Override\n+    public int hashCode() {", "originalCommit": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzUxMQ==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411053511", "bodyText": "Can we mark this class to be a static class and rename it to HoodieTripModel?", "author": "yanghua", "createdAt": "2020-04-20T02:27:55Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {", "originalCommit": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTI1ODAwNw==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411258007", "bodyText": "Can we mark this class to be a static class and rename it to HoodieTripModel?\n\nYes, and change it public, testHDFSParquetImportCommand maybe reuse.", "author": "hddong", "createdAt": "2020-04-20T10:11:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzUxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NTM0OA==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411055348", "bodyText": "This method throws IOException, so the .close method should be wrapped into a finally or try-with-resource block. The same issue exists in createRecords method.", "author": "yanghua", "createdAt": "2020-04-20T02:34:51Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -171,6 +277,30 @@ private void createRecords(Path srcFolder) throws ParseException, IOException {\n       writer.write(record);\n     }\n     writer.close();\n+    return records;\n+  }\n+\n+  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n+    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n+    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n+    List<GenericRecord> records = new ArrayList<GenericRecord>();\n+    // 10 for update\n+    for (long recordNum = 0; recordNum < 11; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    // 4 for insert\n+    for (long recordNum = 96; recordNum < 100; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n+        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build();\n+    for (GenericRecord record : records) {\n+      writer.write(record);", "originalCommit": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA2NjExMg==", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411066112", "bodyText": "rename to testImportWithInsert?", "author": "yanghua", "createdAt": "2020-04-20T03:17:00Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -150,14 +166,104 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n       for (Entry<String, Long> e : recordCounts.entrySet()) {\n         assertEquals(\"missing records\", 24, e.getValue().longValue());\n       }\n-    } finally {\n-      if (jsc != null) {\n-        jsc.stop();\n-      }\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {", "originalCommit": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f7a8a063fd6eebe8eac0c47a602edb9168fda439", "url": "https://github.com/apache/hudi/commit/f7a8a063fd6eebe8eac0c47a602edb9168fda439", "message": "address comments", "committedDate": "2020-04-20T10:14:42Z", "type": "commit"}, {"oid": "9381b9b92458c986b60058be2f0cc0c0ef9e5ce7", "url": "https://github.com/apache/hudi/commit/9381b9b92458c986b60058be2f0cc0c0ef9e5ce7", "message": "reset", "committedDate": "2020-04-21T01:21:55Z", "type": "commit"}]}