{"pr_number": 1177, "pr_title": "[HUDI-463] Redo hudi-utilities log statements using SLF4J", "pr_createdAt": "2020-01-04T02:00:22Z", "pr_url": "https://github.com/apache/hudi/pull/1177", "timeline": [{"oid": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "url": "https://github.com/apache/hudi/commit/96411f1cc371c162ae1b44cd7f01825cb99619cc", "message": "[HUDI-463] Redo hudi-utilities log statements using SLF4J", "committedDate": "2020-01-04T01:55:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NDUwNQ==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363294505", "bodyText": "err -> error?", "author": "leesf", "createdAt": "2020-01-06T13:34:41Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java", "diffHunk": "@@ -114,7 +114,7 @@ public int dataImport(JavaSparkContext jsc, int retry) {\n         ret = dataImport(jsc);\n       } while (ret != 0 && retry-- > 0);\n     } catch (Throwable t) {\n-      LOG.error(t);\n+      LOG.error(\"The dataImport err:\", t);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NTM4Mw==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363295383", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-01-06T13:37:09Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieCompactor.java", "diffHunk": "@@ -110,7 +110,7 @@ public int compact(JavaSparkContext jsc, int retry) {\n         }\n       } while (ret != 0 && retry-- > 0);\n     } catch (Throwable t) {\n-      LOG.error(t);\n+      LOG.error(\"The compact err:\", t);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NjA3MQ==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363296071", "bodyText": "add blank before , and coiuld start a new line?", "author": "leesf", "createdAt": "2020-01-06T13:39:09Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -300,8 +300,7 @@ private void refreshTimeline() throws IOException {\n     }\n \n     if (Objects.equals(checkpointStr, resumeCheckpointStr.orElse(null))) {\n-      LOG.info(\"No new data, source checkpoint has not changed. Nothing to commit. Old checkpoint=(\"\n-          + resumeCheckpointStr + \"). New Checkpoint=(\" + checkpointStr + \")\");\n+      LOG.info(\"No new data, source checkpoint has not changed. Nothing to commit. Old checkpoint=({}). New Checkpoint=({})\",resumeCheckpointStr,checkpointStr);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NzE5OQ==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363297199", "bodyText": "could be in one line?", "author": "leesf", "createdAt": "2020-01-06T13:42:15Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -385,7 +385,8 @@ public DeltaSync getDeltaSync() {\n         boolean error = false;\n         if (cfg.isAsyncCompactionEnabled()) {\n           // set Scheduler Pool.\n-          LOG.info(\"Setting Spark Pool name for delta-sync to \" + SchedulerConfGenerator.DELTASYNC_POOL_NAME);\n+          LOG.info(\"Setting Spark Pool name for delta-sync to {}\",\n+              SchedulerConfGenerator.DELTASYNC_POOL_NAME);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NzI0OA==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363297248", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-01-06T13:42:26Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -394,15 +395,16 @@ public DeltaSync getDeltaSync() {\n               long start = System.currentTimeMillis();\n               Option<String> scheduledCompactionInstant = deltaSync.syncOnce();\n               if (scheduledCompactionInstant.isPresent()) {\n-                LOG.info(\"Enqueuing new pending compaction instant (\" + scheduledCompactionInstant + \")\");\n+                LOG.info(\"Enqueuing new pending compaction instant ({})\",\n+                    scheduledCompactionInstant);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzI5NzU3NQ==", "url": "https://github.com/apache/hudi/pull/1177#discussion_r363297575", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-01-06T13:43:11Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/perf/TimelineServerPerf.java", "diffHunk": "@@ -73,7 +73,8 @@ public TimelineServerPerf(Config cfg) throws IOException {\n   private void setHostAddrFromSparkConf(SparkConf sparkConf) {\n     String hostAddr = sparkConf.get(\"spark.driver.host\", null);\n     if (hostAddr != null) {\n-      LOG.info(\"Overriding hostIp to (\" + hostAddr + \") found in spark-conf. It was \" + this.hostAddr);\n+      LOG.info(\n+          \"Overriding hostIp to ({}) found in spark-conf. It was {}\", hostAddr, this.hostAddr);", "originalCommit": "96411f1cc371c162ae1b44cd7f01825cb99619cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e7654871da0df107cba1d4a3aa5db52c0cded62b", "url": "https://github.com/apache/hudi/commit/e7654871da0df107cba1d4a3aa5db52c0cded62b", "message": "[HUDI-463] Redo hudi-utilities log statements using SLF4J", "committedDate": "2020-01-06T14:15:49Z", "type": "commit"}]}