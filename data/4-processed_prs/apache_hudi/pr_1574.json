{"pr_number": 1574, "pr_title": "[HUDI-701]Add unit test for HDFSParquetImportCommand", "pr_createdAt": "2020-04-30T01:49:50Z", "pr_url": "https://github.com/apache/hudi/pull/1574", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3MjU0MQ==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420772541", "bodyText": "use Files.exists(xx)?", "author": "yanghua", "createdAt": "2020-05-06T13:03:39Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestHDFSParquetImportCommand.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.utilities.HDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter;\n+import org.apache.hudi.utilities.TestHDFSParquetImporter.HoodieTripModel;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.text.ParseException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.jupiter.api.Assertions.assertAll;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link org.apache.hudi.cli.commands.HDFSParquetImportCommand}.\n+ */\n+public class ITTestHDFSParquetImportCommand extends AbstractShellIntegrationTest {\n+\n+  private Path sourcePath;\n+  private Path targetPath;\n+  private String tableName;\n+  private String schemaFile;\n+  private String tablePath;\n+\n+  private List<GenericRecord> insertData;\n+  private TestHDFSParquetImporter importer;\n+\n+  @BeforeEach\n+  public void init() throws IOException, ParseException {\n+    tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+    sourcePath = new Path(basePath, \"source\");\n+    targetPath = new Path(tablePath);\n+    schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+    // create schema file\n+    try (FSDataOutputStream schemaFileOS = fs.create(new Path(schemaFile))) {\n+      schemaFileOS.write(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA.getBytes());\n+    }\n+\n+    importer = new TestHDFSParquetImporter();\n+    insertData = importer.createInsertRecords(sourcePath);\n+  }\n+\n+  /**\n+   * Test case for 'hdfsparquetimport' with insert.\n+   */\n+  @Test\n+  public void testConvertWithInsert() throws IOException {\n+    String command = String.format(\"hdfsparquetimport --srcPath %s --targetPath %s --tableName %s \"\n+        + \"--tableType %s --rowKeyField %s\" + \" --partitionPathField %s --parallelism %s \"\n+        + \"--schemaFilePath %s --format %s --sparkMemory %s --retry %s --sparkMaster %s\",\n+        sourcePath.toString(), targetPath.toString(), tableName, HoodieTableType.COPY_ON_WRITE.name(),\n+        \"_row_key\", \"timestamp\", \"1\", schemaFile, \"parquet\", \"2G\", \"1\", \"local\");\n+    CommandResult cr = getShell().executeCommand(command);\n+\n+    assertAll(\"Command run success\",\n+        () -> assertTrue(cr.isSuccess()),\n+        () -> assertEquals(\"Table imported to hoodie format\", cr.getResult().toString()));\n+\n+    // Check hudi table exist\n+    String metaPath = targetPath + File.separator + HoodieTableMetaClient.METAFOLDER_NAME;\n+    assertTrue(new File(metaPath).exists(), \"Hoodie table not exist.\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420779385", "bodyText": "Please note that I am not talking about you here. But these array indexes containing numbers are very ugly and unreadable. We should think of a way to improve it.\nWe should parse all parameters and it is best to do:\n\nDefine appropriate variables to store each parameter to improve the readability of the code;\nRefactor it, yes, the parse of the parameters should be order-independent;\n\nWDYT? @hddong @vinothchandar", "author": "yanghua", "createdAt": "2020-05-06T13:13:58Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -82,17 +82,17 @@ public static void main(String[] args) throws Exception {\n         break;\n       case IMPORT:\n       case UPSERT:\n-        assert (args.length >= 12);\n+        assert (args.length >= 13);\n         String propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[11])) {\n-          propsFilePath = args[11];\n+        if (!StringUtils.isNullOrEmpty(args[12])) {\n+          propsFilePath = args[12];\n         }\n         List<String> configs = new ArrayList<>();\n-        if (args.length > 12) {\n-          configs.addAll(Arrays.asList(args).subList(12, args.length));\n+        if (args.length > 13) {\n+          configs.addAll(Arrays.asList(args).subList(13, args.length));\n         }\n-        returnCode = dataLoad(jsc, command, args[1], args[2], args[3], args[4], args[5], args[6],\n-            Integer.parseInt(args[7]), args[8], args[9], Integer.parseInt(args[10]), propsFilePath, configs);\n+        returnCode = dataLoad(jsc, command, args[3], args[4], args[5], args[6], args[7], args[8],", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM2NTg3MA==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421365870", "bodyText": "Please note that I am not talking about you here. But these array indexes containing numbers are very ugly and unreadable. We should think of a way to improve it.\nWe should parse all parameters and it is best to do:\n\nDefine appropriate variables to store each parameter to improve the readability of the code;\nRefactor it, yes, the parse of the parameters should be order-independent;\n\nWDYT? @hddong @vinothchandar\n\nAgree, and have a exist PR(#1174) by @pratyakshsharma", "author": "hddong", "createdAt": "2020-05-07T09:25:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTM5Mzg3OQ==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421393879", "bodyText": "Thanks for reminding me. It seems that PR is inactive?", "author": "yanghua", "createdAt": "2020-05-07T10:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTgwMw==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r421735803", "bodyText": "@yanghua Its been there for some time. Development was done, I was stuck at one of the test cases last when I worked on it. Will take a look at it soon :)", "author": "pratyakshsharma", "createdAt": "2020-05-07T19:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc3OTM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MDczMA==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420780730", "bodyText": "\"Spark Master \" -> \"Spark Master\" (remove the right empty backspace)", "author": "yanghua", "createdAt": "2020-05-06T13:15:57Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -57,6 +56,7 @@ public String convert(\n       @CliOption(key = \"schemaFilePath\", mandatory = true,\n           help = \"Path for Avro schema file\") final String schemaFilePath,\n       @CliOption(key = \"format\", mandatory = true, help = \"Format for the input data\") final String format,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4MjE3OA==", "url": "https://github.com/apache/hudi/pull/1574#discussion_r420782178", "bodyText": "As a refactor suggestion, it would be better to define a data structure to store the cli args to avoid change the signature of the method frequently.", "author": "yanghua", "createdAt": "2020-05-06T13:18:09Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HDFSParquetImportCommand.java", "diffHunk": "@@ -78,8 +76,8 @@ public String convert(\n       cmd = SparkCommand.UPSERT.toString();\n     }\n \n-    sparkLauncher.addAppArgs(cmd, srcPath, targetPath, tableName, tableType, rowKeyField, partitionPathField,\n-        parallelism, schemaFilePath, sparkMemory, retry, propsFilePath);\n+    sparkLauncher.addAppArgs(cmd, master, sparkMemory, srcPath, targetPath, tableName, tableType, rowKeyField,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "url": "https://github.com/apache/hudi/commit/526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "message": "Add test for HDFSParquetImportCommand", "committedDate": "2020-05-14T09:55:46Z", "type": "commit"}, {"oid": "526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "url": "https://github.com/apache/hudi/commit/526cbc6e88cf2f4e9645a3d71828662355bb3a6b", "message": "Add test for HDFSParquetImportCommand", "committedDate": "2020-05-14T09:55:46Z", "type": "forcePushed"}, {"oid": "d825a373f1e07f331e085513ca6f6f918f42f3dc", "url": "https://github.com/apache/hudi/commit/d825a373f1e07f331e085513ca6f6f918f42f3dc", "message": "fix", "committedDate": "2020-05-14T10:23:36Z", "type": "commit"}]}