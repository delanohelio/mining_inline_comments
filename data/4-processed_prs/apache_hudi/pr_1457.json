{"pr_number": 1457, "pr_title": "[HUDI-741] Added checks to validate Hoodie's schema evolution.", "pr_createdAt": "2020-03-27T18:44:27Z", "pr_url": "https://github.com/apache/hudi/pull/1457", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MTk3MA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400281970", "bodyText": "using the Handle classes at this level, breaks the layering.. Please refactor into a common helper.", "author": "vinothchandar", "createdAt": "2020-03-30T15:25:52Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzEwNw==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527107", "bodyText": "Moved to HoodieAvroUtils", "author": "prashantwason", "createdAt": "2020-03-30T22:15:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MTk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MjYyMQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400282621", "bodyText": "this whole block can be pulled into a private method like validateSchema() and reused? without being inlined into upsertRecordsInternal()", "author": "vinothchandar", "createdAt": "2020-03-30T15:26:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzE2OA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527168", "bodyText": "done.", "author": "prashantwason", "createdAt": "2020-03-30T22:15:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4MjYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4Mzk4Mw==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400283983", "bodyText": "can this and the previous line be combined into one\nSchema tableSchema = schemaUtil.getLatestSchema()\n\nI don't think any other code cares about savedParquetSchema", "author": "vinothchandar", "createdAt": "2020-03-30T15:28:28Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzI0OQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527249", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-03-30T22:15:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4Mzk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NDQ2Ng==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400284466", "bodyText": "lets have this as a single ERROR statement?", "author": "vinothchandar", "createdAt": "2020-03-30T15:29:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzMxNg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527316", "bodyText": "The same msg is also used to raise the exception a few lines below. So I did not duplicate the string and created a local variable.", "author": "prashantwason", "createdAt": "2020-03-30T22:15:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NDQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTIyNg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285226", "bodyText": "can we handle this in code nicely above before try, instead of the catch block as an error case?", "author": "vinothchandar", "createdAt": "2020-03-30T15:30:04Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);\n+        }\n+      } catch (Exception e) {\n+        // If this is the first insert into the table then schema will not be present\n+        if (hoodieTable.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().countInstants() > 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzM1OQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527359", "bodyText": "Reading the schema may fail due to IO / timeout errors too which will raise their exceptions.\nHaving this code here:\n\nremoves code duplication (as we need to throw HoodieUpsertException here)\nHandles the rare case gracefully - when reading schema fails due to IO errors but there was no schema.", "author": "prashantwason", "createdAt": "2020-03-30T22:16:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTIyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTg4MA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400285880", "bodyText": "so this is recaught below? IMO this method should just throw exception from one place..", "author": "vinothchandar", "createdAt": "2020-03-30T15:30:57Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -457,6 +461,37 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n   private JavaRDD<WriteStatus> upsertRecordsInternal(JavaRDD<HoodieRecord<T>> preppedRecords, String commitTime,\n       HoodieTable<T> hoodieTable, final boolean isUpsert) {\n \n+    if (getConfig().getSchemaCheck()) {\n+      // Ensure that the current writerSchema is compatible with the latest schema of this\n+      // dataset.\n+      // When inserting/updating data, we read records using the schema saved in the\n+      // data/log files and convert them to the GenericRecords with writerSchema.\n+      // Hence, we need to ensure that this conversion can take place without errors.\n+      try {\n+        SchemaUtil schemaUtil = new SchemaUtil(hoodieTable.getMetaClient());\n+        MessageType savedParquetSchema = schemaUtil.getDataSchema();\n+        Schema savedSchema = schemaUtil.convertParquetSchemaToAvro(savedParquetSchema);\n+        Schema writerSchema = HoodieWriteHandle.createHoodieWriteSchema(config.getSchema());\n+        if (! schemaUtil.isSchemaCompatible(savedSchema, writerSchema)) {\n+          String msg = \"WriterSchema is not compatible with the schema present in the Table\";\n+          LOG.error(msg);\n+          LOG.warn(\"WriterSchema: \" + writerSchema);\n+          LOG.warn(\"Table latest schema: \" + savedSchema);\n+          throw new HoodieUpsertException(msg);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzM5MQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527391", "bodyText": "Refactored.", "author": "prashantwason", "createdAt": "2020-03-30T22:16:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NTg4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NjQwOQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400286409", "bodyText": "rename to hoodie.avro.schema.validate ?", "author": "vinothchandar", "createdAt": "2020-03-30T15:31:42Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -52,6 +52,8 @@\n   private static final String TIMELINE_LAYOUT_VERSION = \"hoodie.timeline.layout.version\";\n   private static final String BASE_PATH_PROP = \"hoodie.base.path\";\n   private static final String AVRO_SCHEMA = \"hoodie.avro.schema\";\n+  private static final String SCHEMA_CHECK = \"hoodie.schema.check\";", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzQyOA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527428", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-03-30T22:16:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI4NjQwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400290759", "bodyText": "this needs to be attributed in LICENSE and NOTICE ..  Can we document all the changes? I am wondering if we can just wrap or extend the org.apache.avro.SchemaCompatibility class instead of copying in full..\ncc @bvaradar as well..", "author": "vinothchandar", "createdAt": "2020-03-30T15:37:26Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzQ4MQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527481", "bodyText": "The functions that are changed in org.apache.avro.SchemaCompatibility are all private.\nI can reduce the size of this file by referring to classes from org.apache.avro.SchemaCompatibility but I think starting with a copy makes it easier to see a diff.", "author": "prashantwason", "createdAt": "2020-03-30T22:16:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNDgwMg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r409104802", "bodyText": "Not required any longer as I have removed the copied file. Please see the latest diff.", "author": "prashantwason", "createdAt": "2020-04-15T20:10:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MDc1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MTU5NA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400291594", "bodyText": "lets move this to org.apache.hudi.common.avro", "author": "vinothchandar", "createdAt": "2020-03-30T15:38:29Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzUzMg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527532", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-03-30T22:16:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5MTU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400292884", "bodyText": "let's rename to TableSchemaReader and move to org.apache.hudi.common.table ? (I am trying to move us out of the habit of piling up more util classes)", "author": "vinothchandar", "createdAt": "2020-03-30T15:40:10Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/SchemaUtil.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Utilities to read Schema from data files and log files.\n+ */\n+public class SchemaUtil {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyNTMyMg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400325322", "bodyText": "Edit: TableSchemaResolver is a better name.. ?  since it does more complex than things than just reading\nand btw let's add some unit tests to both these classes?", "author": "vinothchandar", "createdAt": "2020-03-30T16:25:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDUyNzYwMA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r400527600", "bodyText": "Done. Unit test TBD.", "author": "prashantwason", "createdAt": "2020-03-30T22:16:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNDk1Ng==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r409104956", "bodyText": "Unit tests implemented.", "author": "prashantwason", "createdAt": "2020-04-15T20:11:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDI5Mjg4NA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDgxMg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964812", "bodyText": "nit : devolved/evolved", "author": "n3nash", "createdAt": "2020-04-01T23:17:58Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3MzY2MA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406973660", "bodyText": "I mean devolved here as insert with a devolved schema (with missing field) should not be allowed.", "author": "prashantwason", "createdAt": "2020-04-10T22:49:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDg2Mw==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401964863", "bodyText": "same", "author": "n3nash", "createdAt": "2020-04-01T23:18:09Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3Mzc0Nw==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406973747", "bodyText": "I mean devolved here as updates with a devolved schema (with missing field) should not be allowed.", "author": "prashantwason", "createdAt": "2020-04-10T22:50:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NDg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NTY5OA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401965698", "bodyText": "If you mean to say \"older schema\" or \"shorter schema\" or \"retracted schema\" - not sure if devolved is a work so I'm not sure are you evolving or retracting", "author": "n3nash", "createdAt": "2020-04-01T23:20:44Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestTableSchemaEvolution.java", "diffHunk": "@@ -0,0 +1,410 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client;\n+\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieInsertException;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.apache.hudi.common.HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA;\n+import static org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion.VERSION_1;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestTableSchemaEvolution extends TestHoodieClientBase {\n+  private final String initCommitTime = \"000\";\n+  private HoodieTableType tableType = HoodieTableType.COPY_ON_WRITE;\n+  private HoodieTestDataGenerator dataGenDevolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+  private HoodieTestDataGenerator dataGenEvolved = new HoodieTestDataGenerator(TRIP_EXAMPLE_SCHEMA_EVOLVED);\n+\n+  // TRIP_EXAMPLE_SCHEMA with a new_field added\n+  public static final String TRIP_EXAMPLE_SCHEMA_EVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\" + \"{\\\"name\\\": \\\"driver\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"new_field\\\", \\\"type\\\": [\\\"null\\\", \\\"string\\\"], \\\"default\\\": null},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+  // TRIP_EXAMPLE_SCHEMA with driver field removed\n+  public static final String TRIP_EXAMPLE_SCHEMA_DEVOLVED = \"{\\\"type\\\": \\\"record\\\",\" + \"\\\"name\\\": \\\"triprec\\\",\" + \"\\\"fields\\\": [ \"\n+      + \"{\\\"name\\\": \\\"timestamp\\\",\\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"_row_key\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"rider\\\", \\\"type\\\": \\\"string\\\"},\"\n+      + \"{\\\"name\\\": \\\"begin_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"begin_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"end_lat\\\", \\\"type\\\": \\\"double\\\"},\" + \"{\\\"name\\\": \\\"end_lon\\\", \\\"type\\\": \\\"double\\\"},\"\n+      + \"{\\\"name\\\": \\\"fare\\\",\\\"type\\\": {\\\"type\\\":\\\"record\\\", \\\"name\\\":\\\"fare\\\",\\\"fields\\\": [\"\n+      + \"{\\\"name\\\": \\\"amount\\\",\\\"type\\\": \\\"double\\\"},{\\\"name\\\": \\\"currency\\\", \\\"type\\\": \\\"string\\\"}]}},\"\n+      + \"{\\\"name\\\": \\\"_hoodie_is_deleted\\\", \\\"type\\\": \\\"boolean\\\", \\\"default\\\": false} ]}\";\n+\n+  @Before\n+  public void setUp() throws Exception {\n+    initResources();\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    cleanupSparkContexts();\n+  }\n+\n+  @Test\n+  public void testMORTable() throws Exception {\n+    tableType = HoodieTableType.MERGE_ON_READ;\n+    initMetaClient();\n+\n+    // Create the table\n+    HoodieTableMetaClient.initTableType(metaClient.getHadoopConf(), metaClient.getBasePath(),\n+        HoodieTableType.MERGE_ON_READ, metaClient.getTableConfig().getTableName(),\n+        metaClient.getArchivePath(), metaClient.getTableConfig().getPayloadClass(), VERSION_1);\n+\n+    HoodieWriteConfig hoodieWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA);\n+    HoodieWriteClient client = getHoodieWriteClient(hoodieWriteConfig, false);\n+\n+    // Initial inserts with TRIP_EXAMPLE_SCHEMA\n+    int numRecords = 10;\n+    insertFirstBatch(hoodieWriteConfig, client, \"001\", initCommitTime,\n+                     numRecords, HoodieWriteClient::insert, false, false, numRecords);\n+    checkLatestDeltaCommit(\"001\");\n+\n+    // Compact once so we can incrementally read later\n+    assertTrue(client.scheduleCompactionAtInstant(\"002\", Option.empty()));\n+    client.compact(\"002\");\n+\n+    // Updates with same schema is allowed\n+    final int numUpdateRecords = 5;\n+    updateBatch(hoodieWriteConfig, client, \"003\", \"002\", Option.empty(),\n+                initCommitTime, numUpdateRecords, HoodieWriteClient::upsert, false, false, 0, 0, 0);\n+    checkLatestDeltaCommit(\"003\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Delete with same schema is allowed\n+    final int numDeleteRecords = 2;\n+    numRecords -= numDeleteRecords;\n+    deleteBatch(hoodieWriteConfig, client, \"004\", \"003\", initCommitTime, numDeleteRecords,\n+                HoodieWriteClient::delete, false, false, 0, 0);\n+    checkLatestDeltaCommit(\"004\");\n+    checkReadRecords(\"000\", numRecords);\n+\n+    // Insert with devolved schema is not allowed\n+    HoodieWriteConfig hoodieDevolvedWriteConfig = getWriteConfig(TRIP_EXAMPLE_SCHEMA_DEVOLVED);\n+    client = getHoodieWriteClient(hoodieDevolvedWriteConfig, false);\n+    final List<HoodieRecord> failedRecords = dataGenDevolved.generateInserts(\"004\", numRecords);\n+    try {\n+      // We cannot use insertBatch directly here because we want to insert records\n+      // with a devolved schema and insertBatch inserts records using the TRIP_EXMPLE_SCHEMA.\n+      writeBatch(client, \"005\", \"004\", Option.empty(), \"003\", numRecords,\n+          (String s, Integer a) -> failedRecords, HoodieWriteClient::insert, false, 0, 0, 0);\n+      fail(\"Insert with devolved scheme should fail\");\n+    } catch (HoodieInsertException ex) {\n+      // no new commit\n+      checkLatestDeltaCommit(\"004\");\n+      checkReadRecords(\"000\", numRecords);\n+      client.rollback(\"005\");\n+    }\n+\n+    // Update with devolved schema is also not allowed", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3NDA1OQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406974059", "bodyText": "I had the same confusion with what word to use here. Devolved plays well with evolved and seems to be a valid word.", "author": "prashantwason", "createdAt": "2020-04-10T22:51:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NTY5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NjgwMA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r401966800", "bodyText": "@prashantwason Can you please mark the lines/methods you have changed with may be MOD for future references ? Also, can you add 1-2 lines as to why that change is needed", "author": "n3nash", "createdAt": "2020-04-01T23:24:11Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/avro/SchemaCompatibility.java", "diffHunk": "@@ -0,0 +1,566 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.avro;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import org.apache.avro.AvroRuntimeException;\n+import org.apache.avro.Schema;\n+import org.apache.avro.Schema.Field;\n+import org.apache.avro.Schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * NOTE: This code is copied from org.apache.avro.SchemaCompatibility and changed for HUDI use case.\n+ *\n+ * HUDI requires a Schema to be specified in HoodieWriteConfig and is used by the HoodieWriteClient to\n+ * create the records. The schema is also saved in the data files (parquet format) and log files (avro format).\n+ * Since a schema is required each time new data is ingested into a HUDI dataset, schema can be evolved over time.\n+ *\n+ * HUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by\n+ * checking that the data written using the old schema can be read using the new schema.\n+ *\n+ * New Schema is compatible only if:\n+ * 1. There is no change in schema\n+ * 2. A field has been added and it has a default value specified\n+ *\n+ * New Schema is incompatible if:\n+ * 1. A field has been deleted\n+ * 2. A field has been renamed (treated as delete + add)\n+ * 3. A field's type has changed to be incompatible with the older type\n+ */\n+public class SchemaCompatibility {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3NjUxMA==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406976510", "bodyText": "Done. I have added the limitation with org.apache.avro.SchemaCompatibility in the file as well as in the HUDI-741", "author": "prashantwason", "createdAt": "2020-04-10T23:03:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2NjgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2OTE3Ng==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405969176", "bodyText": "there is a pending PR to move this code into the new table.action package.. FYI.. this may need to move there as well", "author": "vinothchandar", "createdAt": "2020-04-09T05:47:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -487,6 +492,58 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, instantTime);\n   }\n \n+  /**\n+   * Ensure that the current writerSchema is compatible with the latest schema of this dataset.\n+   *\n+   * When inserting/updating data, we read records using the schema saved in the data/log files\n+   * and convert them to the GenericRecords with writerSchema. Hence, we need to ensure that\n+   * this conversion can take place without errors.\n+   *\n+   * @param hoodieTable The Hoodie Table\n+   * @param isUpsert If this is a check during upserts\n+   * @throws HoodieUpsertException If schema check fails during upserts\n+   * @throws HoodieInsertException If schema check fails during inserts\n+   */\n+  private void validateSchema(HoodieTable<T> hoodieTable, final boolean isUpsert)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk3OTYwMQ==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r405979601", "bodyText": "Since we log schema now in commit metadata, can't we just try to read it first. If it is not there, then we can try reading from parquet file.", "author": "bvaradar", "createdAt": "2020-04-09T06:21:14Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table;\n+\n+import java.io.IOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.avro.SchemaCompatibility;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+/**\n+ * Helper class to read schema from data files and log files and to convert it between different formats.\n+ */\n+public class TableSchemaResolver {\n+\n+  private static final Logger LOG = LogManager.getLogger(TableSchemaResolver.class);\n+  private HoodieTableMetaClient metaClient;\n+\n+  public TableSchemaResolver(HoodieTableMetaClient metaClient) {\n+    this.metaClient = metaClient;\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, read from any file written in the latest\n+   * commit. We will assume that the schema has not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   * @throws Exception\n+   */\n+  public MessageType getDataSchema() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk3ODgwMg==", "url": "https://github.com/apache/hudi/pull/1457#discussion_r406978802", "bodyText": "This simplifies the code. I have updated to use the schema from commit metadata.\nFor MOR tables, the compaction operation also leads to a commit (.commit extension) which saves HoodieCommitMetadata but without the SCHEMA. I guess this is a miss and not as per design. I have fixed this as I test both types of tables.", "author": "prashantwason", "createdAt": "2020-04-10T23:12:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk3OTYwMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "4f1c1842659f9526e73fc122c77d0752f11a8edf", "url": "https://github.com/apache/hudi/commit/4f1c1842659f9526e73fc122c77d0752f11a8edf", "message": "[HUDI-741] Added checks to validate Hoodie's schema evolution.\n\nHUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by checking that the data written using the old schema can be read using the new schema.\n\nCode changes:\n\n1. Added a new config in HoodieWriteConfig to enable schema validation check (disabled by default)\n2. Moved code that reads schema from base/log files into hudi-common from hudi-hive-sync\n3. Added writerSchema to the extraMetadata of compaction commits in MOR table. This is same as that for commits on COW table.\n\nTesting changes:\n\n4. Extended TestHoodieClientBase to add insertBatch API which allows inserting a new batch of unique records into a HUDI table\n5. Added a unit test to verify schema evolution for both COW and MOR tables.\n6. Added unit tests for schema compatiblity checks.", "committedDate": "2020-04-15T21:39:38Z", "type": "commit"}, {"oid": "4f1c1842659f9526e73fc122c77d0752f11a8edf", "url": "https://github.com/apache/hudi/commit/4f1c1842659f9526e73fc122c77d0752f11a8edf", "message": "[HUDI-741] Added checks to validate Hoodie's schema evolution.\n\nHUDI specific validation of schema evolution should ensure that a newer schema can be used for the dataset by checking that the data written using the old schema can be read using the new schema.\n\nCode changes:\n\n1. Added a new config in HoodieWriteConfig to enable schema validation check (disabled by default)\n2. Moved code that reads schema from base/log files into hudi-common from hudi-hive-sync\n3. Added writerSchema to the extraMetadata of compaction commits in MOR table. This is same as that for commits on COW table.\n\nTesting changes:\n\n4. Extended TestHoodieClientBase to add insertBatch API which allows inserting a new batch of unique records into a HUDI table\n5. Added a unit test to verify schema evolution for both COW and MOR tables.\n6. Added unit tests for schema compatiblity checks.", "committedDate": "2020-04-15T21:39:38Z", "type": "forcePushed"}]}