{"pr_number": 1418, "pr_title": "[HUDI-678] Make config package spark free", "pr_createdAt": "2020-03-19T09:21:57Z", "pr_url": "https://github.com/apache/hudi/pull/1418", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b43f27d93f8befdc1212cc3a6ed357da30866aad", "url": "https://github.com/apache/hudi/commit/b43f27d93f8befdc1212cc3a6ed357da30866aad", "message": "[HUDI-678] Make config package spark free", "committedDate": "2020-03-19T11:59:28Z", "type": "commit"}, {"oid": "b43f27d93f8befdc1212cc3a6ed357da30866aad", "url": "https://github.com/apache/hudi/commit/b43f27d93f8befdc1212cc3a6ed357da30866aad", "message": "[HUDI-678] Make config package spark free", "committedDate": "2020-03-19T11:59:28Z", "type": "forcePushed"}, {"oid": "5de4068333756f776c78653545d253d0a1b9b92f", "url": "https://github.com/apache/hudi/commit/5de4068333756f776c78653545d253d0a1b9b92f", "message": "[hotfix] remove useless code", "committedDate": "2020-03-20T07:21:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQ2OTgxNQ==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r395469815", "bodyText": "Could only use SparkConfig to place all these methods if only consider spark free, not multi-engine", "author": "leesf", "createdAt": "2020-03-20T07:24:53Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/config/AbstractConfig.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.config;\n+\n+import java.util.Properties;\n+\n+/**\n+ * Abstract config for multi-engine.\n+ */\n+public abstract class AbstractConfig<T> {\n+  protected Properties props;\n+\n+  public AbstractConfig(Properties props) {\n+    this.props = props;\n+  }\n+\n+  public abstract long getMaxMemoryAllowedForMerge(String maxMemoryFraction);\n+\n+  public abstract T getWriteStatusStorageLevel();\n+\n+  public abstract T getBloomIndexInputStorageLevel();\n+}", "originalCommit": "5de4068333756f776c78653545d253d0a1b9b92f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE0OTUzNg==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r396149536", "bodyText": "I feel we can just make it Spark free for now.. Multi-engine can be dealt with separately..?", "author": "vinothchandar", "createdAt": "2020-03-22T22:36:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQ2OTgxNQ=="}], "type": "inlineReview"}, {"oid": "20438f8880e55440474e0ce0c2a486753845165b", "url": "https://github.com/apache/hudi/commit/20438f8880e55440474e0ce0c2a486753845165b", "message": "address comments", "committedDate": "2020-03-23T05:54:19Z", "type": "commit"}, {"oid": "8c65f1cec6ef42df3b05ddb727dd1a96070589be", "url": "https://github.com/apache/hudi/commit/8c65f1cec6ef42df3b05ddb727dd1a96070589be", "message": "remove empty line", "committedDate": "2020-03-23T05:56:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNTcwOA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398215708", "bodyText": "single line? (also the usages above)", "author": "vinothchandar", "createdAt": "2020-03-25T22:43:57Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java", "diffHunk": "@@ -69,7 +70,8 @@ public HoodieBloomIndex(HoodieWriteConfig config) {\n \n     // Step 0: cache the input record RDD\n     if (config.getBloomIndexUseCaching()) {\n-      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+      StorageLevel storageLevel = ConfigUtils.getBloomIndexInputStorageLevel(config.getProps());", "originalCommit": "8c65f1cec6ef42df3b05ddb727dd1a96070589be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNjIxMA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398216210", "bodyText": "rename to SparkConfigUtils? to make it explicit?", "author": "vinothchandar", "createdAt": "2020-03-25T22:45:14Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/utils/ConfigUtils.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.utils;\n+\n+import org.apache.hudi.config.HoodieIndexConfig;\n+\n+import org.apache.spark.SparkEnv;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.util.Utils;\n+\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieMemoryConfig.DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n+import static org.apache.hudi.config.HoodieMemoryConfig.DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n+import static org.apache.hudi.config.HoodieWriteConfig.WRITE_STATUS_STORAGE_LEVEL;\n+\n+/**\n+ * Config utils.\n+ */\n+public class ConfigUtils {", "originalCommit": "8c65f1cec6ef42df3b05ddb727dd1a96070589be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNjQzOA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398216438", "bodyText": "remove if unused?", "author": "vinothchandar", "createdAt": "2020-03-25T22:45:48Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -565,6 +556,7 @@ public FileSystemViewStorageConfig getClientSpecifiedViewStorageConfig() {\n     private boolean isMemoryConfigSet = false;\n     private boolean isViewConfigSet = false;\n     private boolean isConsistencyGuardSet = false;\n+    private boolean isEngineConfigSet = false;", "originalCommit": "8c65f1cec6ef42df3b05ddb727dd1a96070589be", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNzI2OA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398217268", "bodyText": "Okay.. if we call a Spark specific class here, then this does not achieve the purpose right..\ni.e you cannot move ConfigUtils to hudi-spark and keep config in hudi-writer-common without making hudi-writer-common depend on ConfigUtils?\nWe should change the caller of getMaxMemoryAllowedForMerge to use ConfigUtils.getXX()` just like how you did for storage level?", "author": "vinothchandar", "createdAt": "2020-03-25T22:47:59Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,40 +112,8 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n     private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n+      return ConfigUtils.getMaxMemoryAllowedForMerge(props, maxMemoryFraction);", "originalCommit": "8c65f1cec6ef42df3b05ddb727dd1a96070589be", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNjkxMA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398316910", "bodyText": "@leesf  I think this is still not addressed.. ?", "author": "vinothchandar", "createdAt": "2020-03-26T04:52:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIxNzI2OA=="}], "type": "inlineReview"}, {"oid": "baed026d9f46420f9575741912ef8d6f84054a2d", "url": "https://github.com/apache/hudi/commit/baed026d9f46420f9575741912ef8d6f84054a2d", "message": "address comments", "committedDate": "2020-03-26T00:11:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398317120", "bodyText": "if the classes in config call the SparkConfigUtils, then we cannot claim its spark free right..\ncc @yanghua as well", "author": "vinothchandar", "createdAt": "2020-03-26T04:53:31Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieMemoryConfig.java", "diffHunk": "@@ -113,52 +112,20 @@ public Builder withWriteStatusFailureFraction(double failureFraction) {\n       return this;\n     }\n \n-    /**\n-     * Dynamic calculation of max memory to use for for spillable map. user.available.memory = spark.executor.memory *\n-     * (1 - spark.memory.fraction) spillable.available.memory = user.available.memory * hoodie.memory.fraction. Anytime\n-     * the spark.executor.memory or the spark.memory.fraction is changed, the memory used for spillable map changes\n-     * accordingly\n-     */\n-    private long getMaxMemoryAllowedForMerge(String maxMemoryFraction) {\n-      final String SPARK_EXECUTOR_MEMORY_PROP = \"spark.executor.memory\";\n-      final String SPARK_EXECUTOR_MEMORY_FRACTION_PROP = \"spark.memory.fraction\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/memory/UnifiedMemoryManager.scala#L231} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION = \"0.6\";\n-      // This is hard-coded in spark code {@link\n-      // https://github.com/apache/spark/blob/576c43fb4226e4efa12189b41c3bc862019862c6/core/src/main/scala/org/apache/\n-      // spark/SparkContext.scala#L471} so have to re-define this here\n-      final String DEFAULT_SPARK_EXECUTOR_MEMORY_MB = \"1024\"; // in MB\n-\n-      if (SparkEnv.get() != null) {\n-        // 1 GB is the default conf used by Spark, look at SparkContext.scala\n-        long executorMemoryInBytes = Utils.memoryStringToMb(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_MB)) * 1024 * 1024L;\n-        // 0.6 is the default value used by Spark,\n-        // look at {@link\n-        // https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L507}\n-        double memoryFraction = Double.parseDouble(\n-            SparkEnv.get().conf().get(SPARK_EXECUTOR_MEMORY_FRACTION_PROP, DEFAULT_SPARK_EXECUTOR_MEMORY_FRACTION));\n-        double maxMemoryFractionForMerge = Double.parseDouble(maxMemoryFraction);\n-        double userAvailableMemory = executorMemoryInBytes * (1 - memoryFraction);\n-        long maxMemoryForMerge = (long) Math.floor(userAvailableMemory * maxMemoryFractionForMerge);\n-        return Math.max(DEFAULT_MIN_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES, maxMemoryForMerge);\n-      } else {\n-        return DEFAULT_MAX_MEMORY_FOR_SPILLABLE_MAP_IN_BYTES;\n-      }\n-    }\n-\n     public HoodieMemoryConfig build() {\n       HoodieMemoryConfig config = new HoodieMemoryConfig(props);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP),\n           MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_COMPACTION);\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FRACTION_FOR_MERGE_PROP),\n           MAX_MEMORY_FRACTION_FOR_MERGE_PROP, DEFAULT_MAX_MEMORY_FRACTION_FOR_MERGE);\n+      long maxMemoryAllowedForMerge =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n       setDefaultOnCondition(props, !props.containsKey(MAX_MEMORY_FOR_MERGE_PROP), MAX_MEMORY_FOR_MERGE_PROP,\n-          String.valueOf(getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_MERGE_PROP))));\n+          String.valueOf(maxMemoryAllowedForMerge));\n+      long maxMemoryAllowedForCompaction =\n+          SparkConfigUtils.getMaxMemoryAllowedForMerge(props.getProperty(MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));", "originalCommit": "baed026d9f46420f9575741912ef8d6f84054a2d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODM0NTcwOQ==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398345709", "bodyText": "Yes, you are right. Maybe we have to extract these two lines into an extra method.", "author": "yanghua", "createdAt": "2020-03-26T06:40:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQ2NzkzNA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398467934", "bodyText": "if the classes in config call the SparkConfigUtils, then we cannot claim its spark free right..\ncc @yanghua as well\n\nGet the point, updated the PR to make it totally free.", "author": "leesf", "createdAt": "2020-03-26T10:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODMxNzEyMA=="}], "type": "inlineReview"}, {"oid": "52ad0ebaab4c41927975e5ff82997ab9d1515777", "url": "https://github.com/apache/hudi/commit/52ad0ebaab4c41927975e5ff82997ab9d1515777", "message": "address comments", "committedDate": "2020-03-26T10:29:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODQ2NzE5MA==", "url": "https://github.com/apache/hudi/pull/1418#discussion_r398467190", "bodyText": "unused methods, remove them.", "author": "leesf", "createdAt": "2020-03-26T10:31:49Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -503,22 +494,6 @@ public int getJmxPort() {\n   /**\n    * memory configs.\n    */\n-  public Double getMaxMemoryFractionPerPartitionMerge() {\n-    return Double.valueOf(props.getProperty(HoodieMemoryConfig.MAX_MEMORY_FRACTION_FOR_MERGE_PROP));\n-  }\n-\n-  public Double getMaxMemoryFractionPerCompaction() {\n-    return Double.valueOf(props.getProperty(HoodieMemoryConfig.MAX_MEMORY_FRACTION_FOR_COMPACTION_PROP));\n-  }", "originalCommit": "52ad0ebaab4c41927975e5ff82997ab9d1515777", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}