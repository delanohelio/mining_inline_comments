{"pr_number": 1996, "pr_title": "[BLOG] Async Compaction and Efficient Migration of large Parquet tables", "pr_createdAt": "2020-08-20T10:21:40Z", "pr_url": "https://github.com/apache/hudi/pull/1996", "timeline": [{"oid": "729c0514a1dcc8771491eeb26dacb210b958a375", "url": "https://github.com/apache/hudi/commit/729c0514a1dcc8771491eeb26dacb210b958a375", "message": "[BLOG] Efficient Migration of large Parquet tables", "committedDate": "2020-08-22T01:19:31Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTA4OTY4Mg==", "url": "https://github.com/apache/hudi/pull/1996#discussion_r475089682", "bodyText": "DeltaStreamer", "author": "leesf", "createdAt": "2020-08-22T13:19:18Z", "path": "docs/_posts/2020-08-21-async-compaction-deployment-model.md", "diffHunk": "@@ -0,0 +1,99 @@\n+---\n+title: \"Async Compaction Deployment Models\"\n+excerpt: \"Mechanisms for executing compaction jobs in Hudi asynchronously\"\n+author: vbalaji\n+category: blog\n+---\n+\n+We will look at different deployment models for executing compactions asynchronously.\n+\n+# Compaction\n+\n+For Merge-On-Read table, data is stored using a combination of columnar (e.g parquet) + row based (e.g avro) file formats. \n+Updates are logged to delta files & later compacted to produce new versions of columnar files synchronously or \n+asynchronously. One of th main motivations behind Merge-On-Read is to reduce data latency when ingesting records.\n+Hence, it makes sense to run compaction asynchronously without blocking ingestion.\n+\n+\n+# Async Compaction\n+\n+Async Compaction is performed in 2 steps:\n+\n+1. ***Compaction Scheduling***: This is done by the ingestion job. In this step, Hudi scans the partitions and selects **file \n+slices** to be compacted. A compaction plan is finally written to Hudi timeline.\n+1. ***Compaction Execution***: A separate process reads the compaction plan and performs compaction of file slices.\n+\n+  \n+# Deployment Models\n+\n+There are few ways by which we can execute compactions asynchronously. \n+\n+## Spark Structured Streaming\n+\n+With 0.6.0, we now have support for running async compactions in Spark \n+Structured Streaming jobs. Compactions are scheduled and executed asynchronously inside the \n+streaming job.  Async Compactions are enabled by default for structured streaming jobs\n+on Merge-On-Read table.\n+\n+Here is an example snippet in java\n+\n+```properties\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.spark.sql.streaming.OutputMode;\n+import org.apache.spark.sql.streaming.ProcessingTime;\n+\n+\n+ DataStreamWriter<Row> writer = streamingInput.writeStream().format(\"org.apache.hudi\")\n+        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), operationType)\n+        .option(DataSourceWriteOptions.TABLE_TYPE_OPT_KEY(), tableType)\n+        .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\")\n+        .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\")\n+        .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\")\n+        .option(HoodieCompactionConfig.INLINE_COMPACT_NUM_DELTA_COMMITS_PROP, \"10\")\n+        .option(DataSourceWriteOptions.ASYNC_COMPACT_ENABLE_OPT_KEY(), \"true\")\n+        .option(HoodieWriteConfig.TABLE_NAME, tableName).option(\"checkpointLocation\", checkpointLocation)\n+        .outputMode(OutputMode.Append());\n+ writer.trigger(new ProcessingTime(30000)).start(tablePath);\n+```\n+\n+## DeltaStreaminer Continuous Mode", "originalCommit": "3a100f3acecfd55e342b297a0755df52e710083e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTI4NTA5OA==", "url": "https://github.com/apache/hudi/pull/1996#discussion_r475285098", "bodyText": "Fixed. Thanks,", "author": "bvaradar", "createdAt": "2020-08-24T00:03:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTA4OTY4Mg=="}], "type": "inlineReview"}, {"oid": "fdc1eeb1c77442d1f8ffc4351e134979c2346ca8", "url": "https://github.com/apache/hudi/commit/fdc1eeb1c77442d1f8ffc4351e134979c2346ca8", "message": "[BLOG] Async Compaction Deployment Models", "committedDate": "2020-08-24T00:02:47Z", "type": "forcePushed"}, {"oid": "8fadc62d88c9ea33f4c71c8bc2c0b86b70ac2163", "url": "https://github.com/apache/hudi/commit/8fadc62d88c9ea33f4c71c8bc2c0b86b70ac2163", "message": "[BLOG] Efficient Migration of large Parquet tables", "committedDate": "2020-09-01T19:23:31Z", "type": "commit"}, {"oid": "c693f173da922faf2491899a968023585d2d258c", "url": "https://github.com/apache/hudi/commit/c693f173da922faf2491899a968023585d2d258c", "message": "[BLOG] Async Compaction Deployment Models", "committedDate": "2020-09-01T19:23:33Z", "type": "commit"}, {"oid": "c693f173da922faf2491899a968023585d2d258c", "url": "https://github.com/apache/hudi/commit/c693f173da922faf2491899a968023585d2d258c", "message": "[BLOG] Async Compaction Deployment Models", "committedDate": "2020-09-01T19:23:33Z", "type": "forcePushed"}]}