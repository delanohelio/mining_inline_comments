{"pr_number": 1452, "pr_title": "[HUDI-740]Fix can not specify the sparkMaster and code clean for SparkUtil", "pr_createdAt": "2020-03-27T03:25:07Z", "pr_url": "https://github.com/apache/hudi/pull/1452", "timeline": [{"oid": "d0be469c7325f1772181eb835af1bff52c3b9393", "url": "https://github.com/apache/hudi/commit/d0be469c7325f1772181eb835af1bff52c3b9393", "message": "Fix specify the sparkMaster of cleans run command", "committedDate": "2020-03-27T03:22:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399232900", "bodyText": "Why do you want to implement it specifically for CLEAN command? Any specific reasons? I could not understand the purpose of this PR basically. :)", "author": "pratyakshsharma", "createdAt": "2020-03-27T12:36:10Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -62,7 +63,9 @@ public static void main(String[] args) throws Exception {\n \n     SparkCommand cmd = SparkCommand.valueOf(command);\n \n-    JavaSparkContext jsc = SparkUtil.initJavaSparkConf(\"hoodie-cli-\" + command);\n+    JavaSparkContext jsc = cmd == SparkCommand.CLEAN", "originalCommit": "d0be469c7325f1772181eb835af1bff52c3b9393", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTYwOTQyMA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399609420", "bodyText": "@pratyakshsharma only CLEAN command can  specify the sparkMaster now. Otherwise, sparkMaster not contained in args of other command.", "author": "hddong", "createdAt": "2020-03-28T02:43:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTYzNDQ5Nw==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399634497", "bodyText": "I have the same concern with @pratyakshsharma , can we make other commands also support specifying the spark master?", "author": "yanghua", "createdAt": "2020-03-28T08:04:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY3MDY4OQ==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399670689", "bodyText": "@yanghua yep, it is better to add sparkMaster  for other commands. Is it need in this PR? I think I can create a new jira to do this, and then we can init jsc by sparkMaster for all commands.", "author": "hddong", "createdAt": "2020-03-28T14:51:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY3OTAzNg==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399679036", "bodyText": "@hddong I can see in SparkMain.java class that sparkMaster and sparkMemory are both present in args of other command as well. For example -\n\nhttps://github.com/apache/incubator-hudi/blob/04449f33feb300b99750c52ec37f2561aa644456/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java#L216\nhttps://github.com/apache/incubator-hudi/blob/04449f33feb300b99750c52ec37f2561aa644456/hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java#L234\n\nAm I missing something here?\nAlso all these positional arguments will be changed to proper config objects as per this PR (#1174). You might want to take a look at this one.", "author": "pratyakshsharma", "createdAt": "2020-03-28T16:10:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTczMzQzNQ==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399733435", "bodyText": "@prashantwason my mistake, thanks for your point out.", "author": "hddong", "createdAt": "2020-03-29T02:05:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTc3ODk2OQ==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399778969", "bodyText": "Let us address other commands also in this PR then :) or maybe you can wait for the other PR I mentioned before doing these changes. I would leave this decision on you :)", "author": "pratyakshsharma", "createdAt": "2020-03-29T10:43:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4ODc1NA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r399888754", "bodyText": "@pratyakshsharma yes, it's need, i'm doing. Your PR was Left behind for too long :)", "author": "hddong", "createdAt": "2020-03-30T01:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4NDE0Mg==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r405384142", "bodyText": "sounds good.", "author": "pratyakshsharma", "createdAt": "2020-04-08T09:24:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIzMjkwMA=="}], "type": "inlineReview"}, {"oid": "bafcb956dbaf8801c0497f6ccb29259700d2b530", "url": "https://github.com/apache/hudi/commit/bafcb956dbaf8801c0497f6ccb29259700d2b530", "message": "Add compact", "committedDate": "2020-03-30T04:08:12Z", "type": "commit"}, {"oid": "0647035d3b9b7efbac5155b71603074e54587a76", "url": "https://github.com/apache/hudi/commit/0647035d3b9b7efbac5155b71603074e54587a76", "message": "Merge branch 'master' into fix-clean-run-master", "committedDate": "2020-03-30T04:10:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU0OTkxNA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404549914", "bodyText": "Let's use List<SparkCommand>?", "author": "yanghua", "createdAt": "2020-04-07T05:48:27Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -118,53 +121,55 @@ public static void main(String[] args) throws Exception {\n         break;\n       case COMPACT_VALIDATE:\n         assert (args.length == 7);\n-        doCompactValidate(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6]);\n+        doCompactValidate(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]));\n         returnCode = 0;\n         break;\n       case COMPACT_REPAIR:\n         assert (args.length == 8);\n-        doCompactRepair(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactRepair(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_FILE:\n         assert (args.length == 9);\n-        doCompactUnscheduleFile(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnscheduleFile(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_PLAN:\n         assert (args.length == 9);\n-        doCompactUnschedule(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnschedule(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case CLEAN:\n         assert (args.length >= 5);\n         propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[3])) {\n-          propsFilePath = args[3];\n+        if (!StringUtils.isNullOrEmpty(args[4])) {\n+          propsFilePath = args[4];\n         }\n         configs = new ArrayList<>();\n         if (args.length > 5) {\n           configs.addAll(Arrays.asList(args).subList(5, args.length));\n         }\n-        clean(jsc, args[1], args[2], propsFilePath, args[4], configs);\n+        clean(jsc, args[3], propsFilePath, configs);\n         break;\n       default:\n         break;\n     }\n     System.exit(returnCode);\n   }\n \n-  private static void clean(JavaSparkContext jsc, String basePath, String sparkMaster, String propsFilePath,\n-                            String sparkMemory, List<String> configs) throws Exception {\n+  private static boolean sparkMasterContained(SparkCommand command) {\n+    List masterContained = Arrays.asList(SparkCommand.COMPACT_VALIDATE, SparkCommand.COMPACT_REPAIR,", "originalCommit": "0647035d3b9b7efbac5155b71603074e54587a76", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDE2MQ==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404550161", "bodyText": "The indent of arg is wrong?", "author": "yanghua", "createdAt": "2020-04-07T05:49:10Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -118,53 +121,55 @@ public static void main(String[] args) throws Exception {\n         break;\n       case COMPACT_VALIDATE:\n         assert (args.length == 7);\n-        doCompactValidate(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6]);\n+        doCompactValidate(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]));\n         returnCode = 0;\n         break;\n       case COMPACT_REPAIR:\n         assert (args.length == 8);\n-        doCompactRepair(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactRepair(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_FILE:\n         assert (args.length == 9);\n-        doCompactUnscheduleFile(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnscheduleFile(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case COMPACT_UNSCHEDULE_PLAN:\n         assert (args.length == 9);\n-        doCompactUnschedule(jsc, args[1], args[2], args[3], Integer.parseInt(args[4]), args[5], args[6],\n+        doCompactUnschedule(jsc, args[3], args[4], args[5], Integer.parseInt(args[6]),\n             Boolean.parseBoolean(args[7]), Boolean.parseBoolean(args[8]));\n         returnCode = 0;\n         break;\n       case CLEAN:\n         assert (args.length >= 5);\n         propsFilePath = null;\n-        if (!StringUtils.isNullOrEmpty(args[3])) {\n-          propsFilePath = args[3];\n+        if (!StringUtils.isNullOrEmpty(args[4])) {\n+          propsFilePath = args[4];\n         }\n         configs = new ArrayList<>();\n         if (args.length > 5) {\n           configs.addAll(Arrays.asList(args).subList(5, args.length));\n         }\n-        clean(jsc, args[1], args[2], propsFilePath, args[4], configs);\n+        clean(jsc, args[3], propsFilePath, configs);\n         break;\n       default:\n         break;\n     }\n     System.exit(returnCode);\n   }\n \n-  private static void clean(JavaSparkContext jsc, String basePath, String sparkMaster, String propsFilePath,\n-                            String sparkMemory, List<String> configs) throws Exception {\n+  private static boolean sparkMasterContained(SparkCommand command) {\n+    List masterContained = Arrays.asList(SparkCommand.COMPACT_VALIDATE, SparkCommand.COMPACT_REPAIR,\n+        SparkCommand.COMPACT_UNSCHEDULE_PLAN, SparkCommand.COMPACT_UNSCHEDULE_FILE, SparkCommand.CLEAN);\n+    return masterContained.contains(command);\n+  }\n+\n+  private static void clean(JavaSparkContext jsc, String basePath, String propsFilePath,\n+                            List<String> configs) {", "originalCommit": "0647035d3b9b7efbac5155b71603074e54587a76", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404550898", "bodyText": "Can we extract the string literals in this method to be constant fields?", "author": "yanghua", "createdAt": "2020-04-07T05:51:29Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {\n     SparkConf sparkConf = new SparkConf().setAppName(name);\n \n-    String defMasterFromEnv = sparkConf.getenv(\"SPARK_MASTER\");\n+    String defMasterFromEnv = master.orElse(sparkConf.getenv(\"SPARK_MASTER\"));", "originalCommit": "0647035d3b9b7efbac5155b71603074e54587a76", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY4NTg5Mg==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404685892", "bodyText": "Can we extract the string literals in this method to be constant fields?\n\n@yanghua There is constant fields DEFAULT_SPARK_MASTER for master.", "author": "hddong", "createdAt": "2020-04-07T09:57:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MDYyMA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404690620", "bodyText": "Actually, I mean, we should avoid using string literals e.g. SPARK_MASTER , spark.hadoop.mapred.output.compress and so on. WDYT?", "author": "yanghua", "createdAt": "2020-04-07T10:05:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDkwMDY3MA==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404900670", "bodyText": "@yanghua yes, necessary, how about add a file HoodieCliSparkConfig.", "author": "hddong", "createdAt": "2020-04-07T15:27:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTI3MjI1Nw==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r405272257", "bodyText": "+1 to introduce HoodieCliSparkConfig", "author": "yanghua", "createdAt": "2020-04-08T05:49:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MDg5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDU1MTgwNQ==", "url": "https://github.com/apache/hudi/pull/1452#discussion_r404551805", "bodyText": "indent issue.", "author": "yanghua", "createdAt": "2020-04-07T05:54:13Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/utils/SparkUtil.java", "diffHunk": "@@ -61,9 +62,14 @@ public static SparkLauncher initLauncher(String propertiesFile) throws URISyntax\n   }\n \n   public static JavaSparkContext initJavaSparkConf(String name) {\n+    return initJavaSparkConf(name, Option.empty(), Option.empty());\n+  }\n+\n+  public static JavaSparkContext initJavaSparkConf(String name, Option<String> master,\n+                                                   Option<String> executorMemory) {", "originalCommit": "0647035d3b9b7efbac5155b71603074e54587a76", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "97f2504301e32ab0c0bd18639bbf7a35c6331d6d", "url": "https://github.com/apache/hudi/commit/97f2504301e32ab0c0bd18639bbf7a35c6331d6d", "message": "update", "committedDate": "2020-04-07T09:55:19Z", "type": "commit"}, {"oid": "8309ffd409ee18f18596dce69dc8e47a6b7521e8", "url": "https://github.com/apache/hudi/commit/8309ffd409ee18f18596dce69dc8e47a6b7521e8", "message": "add config", "committedDate": "2020-04-07T15:23:42Z", "type": "commit"}]}