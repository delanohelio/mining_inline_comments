{"pr_number": 1602, "pr_title": "[HUDI-494] fix incorrect record size estimation", "pr_createdAt": "2020-05-08T01:43:28Z", "pr_url": "https://github.com/apache/hudi/pull/1602", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY2MzgxNQ==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r424663815", "bodyText": "This method seems only being used in a unit test and has an identical copy on upsertPartitioner. So I removed this.", "author": "garyli1019", "createdAt": "2020-05-13T18:59:25Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -299,33 +297,4 @@ public String toString() {\n       return sb.toString();\n     }\n   }\n-\n-  /**\n-   * Obtains the average record size based on records written during previous commits. Used for estimating how many\n-   * records pack into one file.\n-   */\n-  protected static long averageBytesPerRecord(HoodieTimeline commitTimeline, int defaultRecordSizeEstimate) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1OTI2NQ==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r428959265", "bodyText": "since you are at it, can we fix the naming. can you add suffix the units for size. whether its bits or bytes.", "author": "nsivabalan", "createdAt": "2020-05-21T22:58:59Z", "path": "hudi-client/src/test/java/org/apache/hudi/common/HoodieTestDataGenerator.java", "diffHunk": "@@ -70,7 +70,9 @@\n public class HoodieTestDataGenerator {\n \n   // based on examination of sample file, the schema produces the following per record size\n-  public static final int SIZE_PER_RECORD = 50 * 1024;\n+  public static final int SIZE_PER_RECORD = (int) (1.2 * 1024);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1OTMwNw==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r428959307", "bodyText": "same for BLOOM_FILTER_SIZE", "author": "nsivabalan", "createdAt": "2020-05-21T22:59:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1OTI2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk3MTg0MA==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r428971840", "bodyText": "thanks for reviewing. comments addressed.", "author": "garyli1019", "createdAt": "2020-05-21T23:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODk1OTI2NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMDA1Ng==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435630056", "bodyText": "Why did the size shrink that much? Was this wrong?", "author": "vinothchandar", "createdAt": "2020-06-05T00:37:18Z", "path": "hudi-client/src/test/java/org/apache/hudi/testutils/HoodieTestDataGenerator.java", "diffHunk": "@@ -70,7 +70,9 @@\n public class HoodieTestDataGenerator {\n \n   // based on examination of sample file, the schema produces the following per record size\n-  public static final int SIZE_PER_RECORD = 50 * 1024;\n+  public static final int BYTES_PER_RECORD = (int) (1.2 * 1024);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NjQwMA==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435646400", "bodyText": "yes, this was wrong before.", "author": "garyli1019", "createdAt": "2020-06-05T01:46:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMDA1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTA0Mw==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435631043", "bodyText": "Can we keep a single variable in HoodieTestDataGenerator that sums this up? This code need not be aware of how this is computed really", "author": "vinothchandar", "createdAt": "2020-06-05T00:41:33Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -1041,10 +1041,12 @@ private HoodieWriteConfig getSmallInsertWriteConfig(int insertSplitSize, boolean\n     HoodieWriteConfig.Builder builder = getConfigBuilder(useNullSchema ? NULL_SCHEMA : TRIP_EXAMPLE_SCHEMA);\n     return builder\n         .withCompactionConfig(\n-            HoodieCompactionConfig.newBuilder().compactionSmallFileSize(HoodieTestDataGenerator.SIZE_PER_RECORD * 15)\n-                .insertSplitSize(insertSplitSize).build()) // tolerate upto 15 records\n+            HoodieCompactionConfig.newBuilder()\n+                .compactionSmallFileSize(HoodieTestDataGenerator.BYTES_PER_RECORD * 150 + HoodieTestDataGenerator.BLOOM_FILTER_BYTES)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY1MTg4MQ==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435651881", "bodyText": "added a method to calculate this value", "author": "garyli1019", "createdAt": "2020-06-05T02:07:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTUwMQ==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435631501", "bodyText": "So the main change here is that the code will keep looking for a commit that wrote at least smallFileLimit bytes do we get as better value? Wonder if this will have any side effects. For eg if there are a bunch of small commutes like you are referring to, then it means we may fallback to the de fault vale lot more?\nWhat do think about introducing another config here that controls this threshold as a fraction of parquetSmallFileLimit? I feel we can destroy it to 0.5, as opposed to 1.0 that you have now..", "author": "vinothchandar", "createdAt": "2020-06-05T00:43:25Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -301,7 +301,7 @@ protected static long averageBytesPerRecord(HoodieTimeline commitTimeline, int d\n               .fromBytes(commitTimeline.getInstantDetails(instant).get(), HoodieCommitMetadata.class);\n           long totalBytesWritten = commitMetadata.fetchTotalBytesWritten();\n           long totalRecordsWritten = commitMetadata.fetchTotalRecordsWritten();\n-          if (totalBytesWritten > 0 && totalRecordsWritten > 0) {\n+          if (totalBytesWritten > hoodieWriteConfig.getParquetSmallFileLimit() && totalRecordsWritten > 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTY0NzE5OA==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435647198", "bodyText": "This bug would happen when a small commit made in a new partition. If we make a small commit to an existing partition, it will very likely be merged to the existing file so totalBytesWritten is should be a normal size. If there is at least one file in the timeline larger than a small file then it should be not using the default value.\nI feel like adding a new config here would increase the complexity and could be difficult for the user to understand. Should we do that for this edge case?", "author": "garyli1019", "createdAt": "2020-06-05T01:49:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTUwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTk0Njg1Nw==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r435946857", "bodyText": "I understand the issues that your are solving. It\u2019s real and we are on the same page there\n\nIf there is at least one file in the timeline larger than a small file then it should be not using the default value\n\nActually condition is more like - at least one commit with total writes greater than small file. Let\u2019s understand how this may play out with an example.\nLet\u2019s say the table is empty and we start doing small commits with 5 records each.\nCommit 1 : no candidates for small files. inserts will get grouped into  (100k default) record buckets and written to file1_c1\nCommit 2 : prev file is a small file; inserts turned to updates to file1_c2, but since last commit wrote only 5 records, it will use default value\n...\nKeeps happening until file1_cn when eventually rewritten to greater than small file.\nCommit n+1: will use derived record size instead of default value. But there are no more small files. A new file group file 2_cn+1 is created\nCommit n+2: we will keep using commit n\u2019s derived value as long as it\u2019s on the active timeline..\nA corner case here is : if commit n is archived before we have another commit that got file2 expand beyond small file size, e we will fall back to default value again - after We read all the commits in the active timeline.. This is why I suggest we have a config record.size.estimation.threshold to control what fraction of small parquet file size we can use for estimation..\nMakes sense? (Alternatively we could look into somehow computing the bloom filter overhead based on configs and subtract that from totalBytesWritten.. it\u2019s probably better? )", "author": "vinothchandar", "createdAt": "2020-06-05T14:08:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTUwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA4OTQ0OQ==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r436089449", "bodyText": "Got your point. Making a small commit is already an edge case for me so I didn't think of continuously making small commits. But agree we can use a new config to have the flexibility.\nWe have discussed subtracting the bloom filter size before, but prefer to not doing it, IIUC. Major reasons:\n\n\nEven we deduct the size of the bloom filter, there will be other metadata and the totalWriteBytes is still not representing the total record size. When the situation we discussed above happens, it is possible that the small files will still be produced.\nThis will increase the complexity when we handle other indexing like HbaseIndexing.", "author": "garyli1019", "createdAt": "2020-06-05T18:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYzMTUwMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYzNjE1MA==", "url": "https://github.com/apache/hudi/pull/1602#discussion_r436636150", "bodyText": "rename:  RECORD_SIZE_ESTIMATION_THRESHOLD_PROP", "author": "vinothchandar", "createdAt": "2020-06-08T11:48:18Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -54,6 +54,12 @@\n   public static final String PARQUET_SMALL_FILE_LIMIT_BYTES = \"hoodie.parquet.small.file.limit\";\n   // By default, treat any file <= 100MB as a small file.\n   public static final String DEFAULT_PARQUET_SMALL_FILE_LIMIT_BYTES = String.valueOf(104857600);\n+  // Hudi will use the previous commit to calculate the estimated record size by totalBytesWritten/totalRecordsWritten.\n+  // If the previous commit is too small to make an accurate estimation, Hudi will search commits in the reverse order,\n+  // until find a commit has totalBytesWritten larger than (PARQUET_SMALL_FILE_LIMIT_BYTES * RECORD_SIZE_ESTIMATION_THRESHOLD)\n+  public static final String RECORD_SIZE_ESTIMATION_THRESHOLD = \"hoodie.record.size.estimation.threshold\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9099c0ea05720c431ae16544155783d6d39d834b", "url": "https://github.com/apache/hudi/commit/9099c0ea05720c431ae16544155783d6d39d834b", "message": "HUDI-494 fix incorrect record size estimation", "committedDate": "2020-06-08T16:04:41Z", "type": "commit"}, {"oid": "9099c0ea05720c431ae16544155783d6d39d834b", "url": "https://github.com/apache/hudi/commit/9099c0ea05720c431ae16544155783d6d39d834b", "message": "HUDI-494 fix incorrect record size estimation", "committedDate": "2020-06-08T16:04:41Z", "type": "forcePushed"}]}