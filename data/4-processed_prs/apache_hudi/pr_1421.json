{"pr_number": 1421, "pr_title": "[HUDI-724] Parallelize getSmallFiles for partitions", "pr_createdAt": "2020-03-20T01:40:19Z", "pr_url": "https://github.com/apache/hudi/pull/1421", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDU2Mg==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410562", "bodyText": "I think its a reasonable thing to parallelize this.. Listing of cleaning etc has been parallelized like this before. Should be safe to do.\nAlso can we pull this block into a method? getSmallFiles(partitionPaths)", "author": "vinothchandar", "createdAt": "2020-03-20T02:07:03Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMzI5Ng==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395433296", "bodyText": "good point, will move to method like getSmallFilesForPartitions(partitionPaths, jsc)", "author": "ffcchi", "createdAt": "2020-03-20T04:19:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDY0OQ==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410649", "bodyText": "Can we change that code so that it always lists in parallel.. i.e no need for the fallback in this if block...", "author": "vinothchandar", "createdAt": "2020-03-20T02:07:38Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs\n+        List<String> partitionPathsList = new ArrayList<>(partitionPaths);\n+        JavaRDD<String> partitionPathRdds = jsc.parallelize(partitionPathsList, partitionPathsList.size());\n+        List<Tuple2<String, List<SmallFile>>> partitionSmallFileTuples =\n+                partitionPathRdds.map(it -> new Tuple2<String, List<SmallFile>>(it, getSmallFiles(it))).collect();\n+\n+        for (Tuple2<String, List<SmallFile>> tuple : partitionSmallFileTuples) {\n+          partitionSmallFilesMap.put(tuple._1, tuple._2);\n+        }\n+      }\n+\n       for (String partitionPath : partitionPaths) {\n         WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n         if (pStat.getNumInserts() > 0) {\n \n-          List<SmallFile> smallFiles = getSmallFiles(partitionPath);\n+          List<SmallFile> smallFiles;\n+          if (partitionSmallFilesMap.isEmpty()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMzEwNA==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395433104", "bodyText": "sounds good. will do", "author": "ffcchi", "createdAt": "2020-03-20T04:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDY0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDk5Mw==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395410993", "bodyText": "guess this is not needed anymore since we aggregate it into a map already.. (never liked this line. so lgtm)", "author": "vinothchandar", "createdAt": "2020-03-20T02:09:33Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieMergeOnReadTable.java", "diffHunk": "@@ -374,16 +374,12 @@ public void finalizeWrite(JavaSparkContext jsc, String instantTs, List<HoodieWri\n             sf.location = new HoodieRecordLocation(FSUtils.getCommitTime(filename), FSUtils.getFileId(filename));\n             sf.sizeBytes = getTotalFileSize(smallFileSlice);\n             smallFileLocations.add(sf);\n-            // Update the global small files list\n-            smallFiles.add(sf);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQzMjk3Mw==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395432973", "bodyText": "the UpsertPartitioner actually holds a global list small files for all partitions. since the work will be distributed to Task nodes, the method can't  the small file to global list anymore. So I moved the logic of updating that global list out side of the parallelism section.", "author": "ffcchi", "createdAt": "2020-03-20T04:16:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTQxMDk5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395545539", "bodyText": "Do we really need all this passing around of jsc object ? We can just directly pass it from within this function right, as its inherited.", "author": "umehrot2", "createdAt": "2020-03-20T10:21:29Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/HoodieWriteClient.java", "diffHunk": "@@ -486,11 +486,11 @@ private void saveWorkloadProfileMetadataToInflight(WorkloadProfile profile, Hood\n     return updateIndexAndCommitIfNeeded(writeStatusRDD, hoodieTable, commitTime);\n   }\n \n-  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile) {\n+  private Partitioner getPartitioner(HoodieTable table, boolean isUpsert, WorkloadProfile profile, JavaSparkContext jsc) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY2NDIxMQ==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395664211", "bodyText": "nice catch, the HoodieWriterClient do have jsc already.  will update", "author": "ffcchi", "createdAt": "2020-03-20T14:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTY5NDE3OA==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395694178", "bodyText": "+1", "author": "bvaradar", "createdAt": "2020-03-20T14:59:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU0NTUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU1MzcwMw==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395553703", "bodyText": "nit: probably remove this comment", "author": "umehrot2", "createdAt": "2020-03-20T10:38:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU2Mjc3OQ==", "url": "https://github.com/apache/hudi/pull/1421#discussion_r395562779", "bodyText": "You may want to refactor this to something like:\npartitionSmallFilesMap = partitionPathRdds.mapToPair((PairFunction<String, String, List<SmallFile>>) \n  partitionPath -> new Tuple2<>(partitionPath, getSmallFiles(partitionPath))).collectAsMap();", "author": "umehrot2", "createdAt": "2020-03-20T10:57:30Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -602,18 +602,39 @@ private int addUpdateBucket(String fileIdHint) {\n       return bucket;\n     }\n \n-    private void assignInserts(WorkloadProfile profile) {\n+    private void assignInserts(WorkloadProfile profile, JavaSparkContext jsc) {\n       // for new inserts, compute buckets depending on how many records we have for each partition\n       Set<String> partitionPaths = profile.getPartitionPaths();\n       long averageRecordSize =\n           averageBytesPerRecord(metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n               config.getCopyOnWriteRecordSizeEstimate());\n       LOG.info(\"AvgRecordSize => \" + averageRecordSize);\n+\n+      HashMap<String, List<SmallFile>> partitionSmallFilesMap = new HashMap<>();\n+      if (jsc != null && partitionPaths.size() > 1) {\n+        //Parellelize the GetSmallFile Operation by using RDDs\n+        List<String> partitionPathsList = new ArrayList<>(partitionPaths);\n+        JavaRDD<String> partitionPathRdds = jsc.parallelize(partitionPathsList, partitionPathsList.size());\n+        List<Tuple2<String, List<SmallFile>>> partitionSmallFileTuples =\n+                partitionPathRdds.map(it -> new Tuple2<String, List<SmallFile>>(it, getSmallFiles(it))).collect();\n+\n+        for (Tuple2<String, List<SmallFile>> tuple : partitionSmallFileTuples) {\n+          partitionSmallFilesMap.put(tuple._1, tuple._2);\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "201b60ce6083583ab53029070058df98e830104d", "url": "https://github.com/apache/hudi/commit/201b60ce6083583ab53029070058df98e830104d", "message": "[HUDI-724] Parallelize getSmallFiles for partitions", "committedDate": "2020-03-20T23:57:41Z", "type": "commit"}, {"oid": "201b60ce6083583ab53029070058df98e830104d", "url": "https://github.com/apache/hudi/commit/201b60ce6083583ab53029070058df98e830104d", "message": "[HUDI-724] Parallelize getSmallFiles for partitions", "committedDate": "2020-03-20T23:57:41Z", "type": "forcePushed"}]}