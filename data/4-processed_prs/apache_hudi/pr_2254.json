{"pr_number": 2254, "pr_title": "[HUDI-1350] Support Partition level delete API in HUDI", "pr_createdAt": "2020-11-14T14:19:45Z", "pr_url": "https://github.com/apache/hudi/pull/2254", "timeline": [{"oid": "a278a54af1df45e7a723cc1b5ed25953a4663c0e", "url": "https://github.com/apache/hudi/commit/a278a54af1df45e7a723cc1b5ed25953a4663c0e", "message": "[HUDI-1350] Support Partition level delete API in HUDI", "committedDate": "2020-11-14T14:23:39Z", "type": "commit"}, {"oid": "a278a54af1df45e7a723cc1b5ed25953a4663c0e", "url": "https://github.com/apache/hudi/commit/a278a54af1df45e7a723cc1b5ed25953a4663c0e", "message": "[HUDI-1350] Support Partition level delete API in HUDI", "committedDate": "2020-11-14T14:23:39Z", "type": "forcePushed"}, {"oid": "9a5371dfa45c31aad012fdf229977f54d0cb68ab", "url": "https://github.com/apache/hudi/commit/9a5371dfa45c31aad012fdf229977f54d0cb68ab", "message": "Merge branch 'master' into  HUDI-1350", "committedDate": "2020-11-17T05:04:03Z", "type": "forcePushed"}, {"oid": "92c114ade6b1bab838030e900793ff1c2cc02de6", "url": "https://github.com/apache/hudi/commit/92c114ade6b1bab838030e900793ff1c2cc02de6", "message": "Merge  branch 'master' into  HUDI-1350", "committedDate": "2020-11-18T02:14:53Z", "type": "forcePushed"}, {"oid": "533af553618371427e85de214b96669c8d249bab", "url": "https://github.com/apache/hudi/commit/533af553618371427e85de214b96669c8d249bab", "message": "Merge  branch 'master' into  HUDI-1350", "committedDate": "2020-11-18T14:53:35Z", "type": "commit"}, {"oid": "533af553618371427e85de214b96669c8d249bab", "url": "https://github.com/apache/hudi/commit/533af553618371427e85de214b96669c8d249bab", "message": "Merge  branch 'master' into  HUDI-1350", "committedDate": "2020-11-18T14:53:35Z", "type": "forcePushed"}, {"oid": "3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "url": "https://github.com/apache/hudi/commit/3af20d869ffeb3216d14b41a5cd59ae1c0f87d1c", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350", "committedDate": "2020-11-20T00:30:48Z", "type": "forcePushed"}, {"oid": "a279a39a37f48d261834705e58be220bdbcc804c", "url": "https://github.com/apache/hudi/commit/a279a39a37f48d261834705e58be220bdbcc804c", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350", "committedDate": "2020-11-20T01:37:21Z", "type": "commit"}, {"oid": "a279a39a37f48d261834705e58be220bdbcc804c", "url": "https://github.com/apache/hudi/commit/a279a39a37f48d261834705e58be220bdbcc804c", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1350", "committedDate": "2020-11-20T01:37:21Z", "type": "forcePushed"}, {"oid": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "url": "https://github.com/apache/hudi/commit/f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1350", "committedDate": "2020-12-09T15:20:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2ODAxOQ==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r540568019", "bodyText": "can you parallelize this? It is very similar to what you did for 'insert_overwrite_table'. Try to reuse the code from there if possible", "author": "satishkotha", "createdAt": "2020-12-10T23:09:02Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  protected String getCommitActionType() {\n+    return HoodieTimeline.REPLACE_COMMIT_ACTION;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());\n+    Map<String, List<String>> partitionToReplaceFileIds = partitions.stream()", "originalCommit": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQzOTcyMA==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544439720", "bodyText": "okay, deletePartition and insertoverwritetable implement insertoverwritecommitexecutor. And reuse the code", "author": "lw309637554", "createdAt": "2020-12-16T16:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2ODAxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2OTg3Ng==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542769876", "bodyText": "Do you think using JavaRDD for partitions makes more sense (given all other APIs are using RDD)?", "author": "satishkotha", "createdAt": "2020-12-14T20:51:02Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -157,6 +157,15 @@ private synchronized FileSystemViewManager getViewManager() {\n    */\n   public abstract HoodieWriteMetadata<O> delete(HoodieEngineContext context, String instantTime, K keys);\n \n+  /**\n+   * Deletes all data of partitions.\n+   * @param context    HoodieEngineContext\n+   * @param instantTime Instant Time for the action\n+   * @param partitions   {@link List} of partition to be deleted\n+   * @return HoodieWriteMetadata\n+   */\n+  public abstract HoodieWriteMetadata deletePartitions(HoodieEngineContext context, String instantTime, List<String> partitions);", "originalCommit": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQzNjUyOQ==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544436529", "bodyText": "thanks, i thinks \"List\" will be ok. Because flink and spark will implement HoodieTable, use javaRDD not suitable to flink.", "author": "lw309637554", "createdAt": "2020-12-16T16:23:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc2OTg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NTM0Mg==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r542775342", "bodyText": "could you try to simplify this test to avoid code duplication. Also, see if its possible to add similar test for MOR tables.", "author": "satishkotha", "createdAt": "2020-12-14T20:56:08Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,110 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, batch1RecordsCount, DEFAULT_FIRST_PARTITION_PATH);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batch1Buckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";", "originalCommit": "f9ecf2a9e5d2f081043e5b4cbb43bb222d56be8c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDQ0MjY0OA==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r544442648", "bodyText": "thanks , have abstract two method insertPartitionRecordsWithCommit\u3001deletePartitionWithCommit. And reuse it.\nNow deletePartitionAction implement insertOverwriteAction , I think copy on write can cover it.  Add MOR is not very necessary.", "author": "lw309637554", "createdAt": "2020-12-16T16:31:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mjc3NTM0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTM2NDc0MA==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r545364740", "bodyText": "you may want to assert that other partitions still have base files.", "author": "satishkotha", "createdAt": "2020-12-17T19:57:01Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -999,6 +999,103 @@ private void verifyInsertOverwritePartitionHandling(int batch1RecordsCount, int\n     verifyRecordsWritten(commitTime2, inserts2, statuses);\n   }\n \n+  /**\n+   * Test scenario of writing fewer file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithFewerRecordsFirstPartition() throws Exception {\n+    verifyDeletePartitionsHandling(1000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing similar number file groups in partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingWithSimilarNumberOfRecords() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 3000, 3000);\n+  }\n+\n+  /**\n+   * Test scenario of writing more file groups for first partition than second an third partition.\n+   */\n+  @Test\n+  public void verifyDeletePartitionsHandlingHandlingWithFewerRecordsSecondThirdPartition() throws Exception {\n+    verifyDeletePartitionsHandling(3000, 1000, 1000);\n+  }\n+\n+  private Set<String> insertPartitionRecordsWithCommit(SparkRDDWriteClient client, int recordsCount, String commitTime1, String partitionPath) {\n+    client.startCommitWithTime(commitTime1);\n+    List<HoodieRecord> inserts1 = dataGen.generateInsertsForPartition(commitTime1, recordsCount, partitionPath);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 2);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime1).collect();\n+    assertNoWriteErrors(statuses);\n+    Set<String> batchBuckets = statuses.stream().map(s -> s.getFileId()).collect(Collectors.toSet());\n+    verifyRecordsWritten(commitTime1, inserts1, statuses);\n+    return batchBuckets;\n+  }\n+\n+  private Set<String> deletePartitionWithCommit(SparkRDDWriteClient client, String commitTime, List<String> deletePartitionPath) {\n+    client.startCommitWithTime(commitTime, HoodieTimeline.REPLACE_COMMIT_ACTION);\n+    HoodieWriteResult writeResult = client.deletePartitions(deletePartitionPath, commitTime);\n+    Set<String> deletePartitionReplaceFileIds =\n+        writeResult.getPartitionToReplaceFileIds().entrySet()\n+            .stream().flatMap(entry -> entry.getValue().stream()).collect(Collectors.toSet());\n+    return deletePartitionReplaceFileIds;\n+  }\n+\n+  /**\n+   *  1) Do write1 (upsert) with 'batch1RecordsCount' number of records for first partition.\n+   *  2) Do write2 (upsert) with 'batch2RecordsCount' number of records for second partition.\n+   *  3) Do write3 (upsert) with 'batch3RecordsCount' number of records for third partition.\n+   *  4) delete first partition and check result.\n+   *  5) delete second and third partition and check result.\n+   *\n+   */\n+  private void verifyDeletePartitionsHandling(int batch1RecordsCount, int batch2RecordsCount, int batch3RecordsCount) throws Exception {\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(2000, false);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    dataGen = new HoodieTestDataGenerator();\n+\n+    // Do Inserts for DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime1 = \"001\";\n+    Set<String> batch1Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch1RecordsCount, commitTime1, DEFAULT_FIRST_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_SECOND_PARTITION_PATH\n+    String commitTime2 = \"002\";\n+    Set<String> batch2Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch2RecordsCount, commitTime2, DEFAULT_SECOND_PARTITION_PATH);\n+\n+    // Do Inserts for DEFAULT_THIRD_PARTITION_PATH\n+    String commitTime3 = \"003\";\n+    Set<String> batch3Buckets =\n+        this.insertPartitionRecordsWithCommit(client, batch3RecordsCount, commitTime3, DEFAULT_THIRD_PARTITION_PATH);\n+\n+    // delete DEFAULT_FIRST_PARTITION_PATH\n+    String commitTime4 = \"004\";\n+    Set<String> deletePartitionReplaceFileIds1 =\n+        deletePartitionWithCommit(client, commitTime4, Arrays.asList(DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(batch1Buckets, deletePartitionReplaceFileIds1);\n+    List<HoodieBaseFile> baseFiles = HoodieClientTestUtils.getLatestBaseFiles(basePath, fs,\n+        String.format(\"%s/%s/*\", basePath, DEFAULT_FIRST_PARTITION_PATH));\n+    assertEquals(0, baseFiles.size());\n+", "originalCommit": "17fbc00fa912a74221fdd2c5b5d2f931dc9cee9d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "549eec5e1ee38ca9f03421b25698c10cd0d65dce", "url": "https://github.com/apache/hudi/commit/549eec5e1ee38ca9f03421b25698c10cd0d65dce", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction", "committedDate": "2020-12-23T15:33:04Z", "type": "commit"}, {"oid": "549eec5e1ee38ca9f03421b25698c10cd0d65dce", "url": "https://github.com/apache/hudi/commit/549eec5e1ee38ca9f03421b25698c10cd0d65dce", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction", "committedDate": "2020-12-23T15:33:04Z", "type": "forcePushed"}, {"oid": "eecd8a746d51f01eb669be6c4e9076f8b47512e9", "url": "https://github.com/apache/hudi/commit/eecd8a746d51f01eb669be6c4e9076f8b47512e9", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1350", "committedDate": "2020-12-23T16:33:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjMyNw==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072327", "bodyText": "can we please use the HoodieTimer class here", "author": "vinothchandar", "createdAt": "2020-12-27T06:29:48Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();", "originalCommit": "eecd8a746d51f01eb669be6c4e9076f8b47512e9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjQwMQ==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549072401", "bodyText": "so we are saying the files have been replaced with an empty file list?", "author": "vinothchandar", "createdAt": "2020-12-27T06:31:14Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkDeletePartitionCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.commit;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SparkDeletePartitionCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends SparkInsertOverwriteCommitActionExecutor<T> {\n+\n+  private List<String> partitions;\n+  public SparkDeletePartitionCommitActionExecutor(HoodieEngineContext context,\n+                                                  HoodieWriteConfig config, HoodieTable table,\n+                                                  String instantTime, List<String> partitions) {\n+    super(context, config, table, instantTime,null, WriteOperationType.DELETE_PARTITION);\n+    this.partitions = partitions;\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    Instant indexStartTime = Instant.now();\n+    HoodieWriteMetadata result = new HoodieWriteMetadata();\n+    result.setIndexUpdateDuration(Duration.between(indexStartTime, Instant.now()));\n+    result.setWriteStatuses(jsc.emptyRDD());", "originalCommit": "eecd8a746d51f01eb669be6c4e9076f8b47512e9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTEwMTMyMQ==", "url": "https://github.com/apache/hudi/pull/2254#discussion_r549101321", "bodyText": "yes, just as insertoverwrite action and  insertoverwritetable action , they have write records, so they set PartitionToReplaceFileIds and also set the sWriteStatuses. But DeletePartitionCommitAction with no records to write , can just setPartitionToReplaceFileIds.  I think it make sense", "author": "lw309637554", "createdAt": "2020-12-27T11:31:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA3MjQwMQ=="}], "type": "inlineReview"}, {"oid": "a7e0c8189dee91ec227d79c1a11461ae4cbe3f63", "url": "https://github.com/apache/hudi/commit/a7e0c8189dee91ec227d79c1a11461ae4cbe3f63", "message": "[HUDI-1350] Support Partition level delete API in HUDI base InsertOverwriteCommitAction", "committedDate": "2020-12-27T11:27:39Z", "type": "commit"}]}