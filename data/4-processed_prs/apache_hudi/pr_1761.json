{"pr_number": 1761, "pr_title": "[MINOR] Add documentation for using multi-column table keys and for n\u2026", "pr_createdAt": "2020-06-23T23:27:04Z", "pr_url": "https://github.com/apache/hudi/pull/1761", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446113235", "bodyText": "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY should be changed to KEYGENERATOR_CLASS_OPT_KEY? and also when specify no partitioning tables, would use NonpartitionedKeyGenerator", "author": "leesf", "createdAt": "2020-06-26T10:54:49Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4OTcwOQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446389709", "bodyText": "Yap, seems both are necessary:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            **PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n          \n          \n            \n            **RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n          \n          \n            \n            Default value: `\"uuid\"`<br>\n          \n          \n            \n            \n          \n          \n            \n            **PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using both `KEYGENERATOR_CLASS_OPT_KEY` and `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "author": "afeldman1", "createdAt": "2020-06-26T20:15:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5OTM1OA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446499358", "bodyText": "Yap, seems both are necessary:\n\nThe HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is used to sync to hive, if you are meaning sync to hive during the write, it makes sense, or it makes no sense.", "author": "leesf", "createdAt": "2020-06-27T08:03:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5NjI1Mg==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446696252", "bodyText": "I see under the \"syncing to hive\" section it states \"Both tools above support syncing of the table\u2019s latest schema to Hive metastore\" which implies that syncing table metadata to hive is not mandatory but optional. Is this actually true? It seems that using a metastore is actually mandatory...", "author": "afeldman1", "createdAt": "2020-06-28T20:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2MTg1Nw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446961857", "bodyText": "One would only write data into hudi but not syncing to hive, so the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY  is only used when syncing to hive metastore.", "author": "leesf", "createdAt": "2020-06-29T13:15:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NDg0MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447174840", "bodyText": "Will add clarification that HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is only necessary when using hive.", "author": "afeldman1", "createdAt": "2020-06-29T18:38:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMzIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115055", "bodyText": "hi, why this cannot change across writes? would you please clarify? IIRU, It would be changed across writes.", "author": "leesf", "createdAt": "2020-06-26T10:59:16Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3MjgwMQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446372801", "bodyText": "Good catch, that was meant to go in the description of the next parameter, TABLE_TYPE_OPT_KEY.\nThe text can be made more clear by using the following language instead\nInstead of \"Note: this cannot change across writes.\"\nUse \"Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark SaveMode.Append mode.\"\nAlso, I believe that the key and partition columns, cannot be changed after the table is first created as well, so this note snippet can be added to those fields as well. Is this correct?", "author": "afeldman1", "createdAt": "2020-06-26T19:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4NDA4NA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446384084", "bodyText": "Btw, there seems to be a mismatch in the data deletion documentation as well. To delete records it seems the only thing necessary is to set OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL and HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY to classOf[GlobalDeleteKeyGenerator].getCanonicalName, however https://hudi.apache.org/docs/writing_data.html#deletes specifies a hard delete procedure that is different from this one.  The procedure there is listed as setting PAYLOAD_CLASS_OPT_KEY to \"org.apache.hudi.EmptyHoodieRecordPayload\".", "author": "afeldman1", "createdAt": "2020-06-26T20:01:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5ODkzMw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446498933", "bodyText": "in fact, there are some ways to delete records in hudi, 1. we would use \"org.apache.hudi.EmptyHoodieRecordPayload\" to delete a batch of records, 2. we could directly use delete api, 3. we could we _hoodie_is_delete flag to delete record. Also the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY  has nothing to do with the writing path IIRC, it is only related to hive sync.", "author": "leesf", "createdAt": "2020-06-27T07:58:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjQ5OTE0Ng==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446499146", "bodyText": "Good catch, that was meant to go in the description of the next parameter, TABLE_TYPE_OPT_KEY.\nThe text can be made more clear by using the following language instead\nInstead of \"Note: this cannot change across writes.\"\nUse \"Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark SaveMode.Append mode.\"\nAlso, I believe that the key and partition columns, cannot be changed after the table is first created as well, so this note snippet can be added to those fields as well. Is this correct?\n\nYes, TABLE_TYPE_OPT_KEY is not meant to be changed in next write, and OPERATION_OPT_KEY would be changed during different writes, and maybe I misunderstand what you meant.", "author": "leesf", "createdAt": "2020-06-27T08:00:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5NTk0Mg==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446695942", "bodyText": "In regards to deletion: I believe method 2 that you mentioned with delete api, is the same thing as as setting  OPERATION_OPT_KEY to DELETE_OPERATION_OPT_VAL (DELETE_OPERATION_OPT_VAL has a string value of \"delete\"). In terms of method 3, I believe this flag method is only applicable when using the DeltaStreamer based on this blogpost https://hudi.apache.org/blog/delete-support-in-hudi/ , is this correct? And for method 1 of setting PAYLOAD_CLASS_OPT_KEY to \"org.apache.hudi.EmptyHoodieRecordPayload\", could you please clarify what the difference is between using method 1 and method 2? It seems the steps are the same as first a DataFrame must be prepared including the full records that need to be deleted, and then the write operation must be done using one of the 2 methods..", "author": "afeldman1", "createdAt": "2020-06-28T20:42:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2NTAxNg==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446965016", "bodyText": "the flag method is not only applicable when using DeltaStreamer, but also applicable when using HoodieWriteClient/Spark DataSource, in this way, the ingested batch record would consist of both new insert records and delete records, that means you would insert and delete records with one batch, but when setting to EmptyHoodieRecordPayload, the ingested batch record is considered to be all delete records.", "author": "leesf", "createdAt": "2020-06-29T13:20:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NDE3MQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447174171", "bodyText": "Understood. Will clarify the Deletes section with this.", "author": "afeldman1", "createdAt": "2020-06-29T18:37:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTc0MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446115740", "bodyText": "TimestampBasedKeyGenerator and GlobalDeleteKeyGenerator are also available.", "author": "leesf", "createdAt": "2020-06-26T11:01:03Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM4MTc1MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446381750", "bodyText": "Good call, also it seems that default is now actually classOf[SlashEncodedDayPartitionValueExtractor].getCanonicalName, which is also available.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n          \n          \n            \n            `classOf[SlashEncodedDayPartitionValueExtractor].getCanonicalName` (default), `classOf[MultiPartKeysValueExtractor].getCanonicalName`, `classOf[TimestampBasedKeyGenerator].getCanonicalName`, `classOf[NonPartitionedExtractor].getCanonicalName`, `classOf[GlobalDeleteKeyGenerator].getCanonicalName` (to be used when OPERATION_OPT_KEY is set to DELETE_OPERATION_OPT_VAL)", "author": "afeldman1", "createdAt": "2020-06-26T19:56:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTgxMQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119811", "bodyText": "would be changed to format(\"hudi\")", "author": "leesf", "createdAt": "2020-06-26T11:10:57Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM2NTg0Mw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446365843", "bodyText": "This seems to be specified as \"org.apache.hudi\" by all the existing documentation that I can find and when I run the code as well. See example of current usage here: https://hudi.apache.org/docs/writing_data.html#datasource-writer", "author": "afeldman1", "createdAt": "2020-06-26T19:17:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTgxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTg0Mw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446119843", "bodyText": "would be changed to format(\"hudi\")", "author": "leesf", "createdAt": "2020-06-26T11:11:03Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>\n+Available values:<br>\n+`classOf[MultiPartKeysValueExtractor].getCanonicalName` (default), `classOf[NonPartitionedExtractor].getCanonicalName`\n+\n+\n+Example:\n+Upsert a DataFrame, specifying the necessary field names for `recordKey => _row_key`, `partitionPath => partition`, and `precombineKey => timestamp`\n \n ```java\n inputDF.write()\n        .format(\"org.apache.hudi\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3MzQ5Nw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446373497", "bodyText": "marking resolved as duplicate", "author": "afeldman1", "createdAt": "2020-06-26T19:35:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExOTg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMDE1NQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446120155", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-06-26T11:11:45Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM3NDAyNA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446374024", "bodyText": "Same reply as well,\n\nThis seems to be specified as \"org.apache.hudi\" by all the existing documentation that I can find and when I run the code as well. See example of current usage here: https://hudi.apache.org/docs/writing_data.html#datasource-writer", "author": "afeldman1", "createdAt": "2020-06-26T19:36:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMDE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446121818", "bodyText": "when partition path is ${tablePath}/a/b/c, the load would be changed to tablePath + \"/*/*/*/*\" accordingly.", "author": "leesf", "createdAt": "2020-06-26T11:15:32Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -136,6 +136,16 @@ The Spark Datasource API is a popular way of authoring Spark ETL pipelines. Hudi\n datasources work (e.g: `spark.read.parquet`). Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11-<hudi version>.jar` to classpath of drivers \n and executors. Alternatively, hudi-spark-bundle can also fetched via the `--packages` options (e.g: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.3`).\n \n+### Snapshot query {#spark-snap-query}\n+This method can be used to retrieve the data table at the present point in time.\n+\n+```scala\n+val hudiIncQueryDF = spark\n+     .read()\n+     .format(\"org.apache.hudi\")\n+     .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY(), DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL())\n+     .load(tablePath + \"/*\") //Include \"/*\" at the end of the path if the table is partitioned", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjY5OTU1Ng==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446699556", "bodyText": "You are correct, updating. Checking the actual partitioned directories seems like something that we should add into the hudi framework itself.", "author": "afeldman1", "createdAt": "2020-06-28T21:20:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjcwMzczNg==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446703736", "bodyText": "Well, it would be one fewer, no? For each partition level so tablePath + \"/*/*/*\"", "author": "afeldman1", "createdAt": "2020-06-28T22:03:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2NTkzOQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446965939", "bodyText": "no, you would refer to http://hudi.apache.org/docs/quick-start-guide.html#query-data", "author": "leesf", "createdAt": "2020-06-29T13:21:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwOTU2NQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448009565", "bodyText": "This is a separate point from documentation, but ideally wouldn't it be better for Hudi to figure out which sub-directories need to be read for the partitions, instead of expecting to be passed the tree level of the partitions?", "author": "afeldman1", "createdAt": "2020-06-30T22:14:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyMTgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM5MDk5Mw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446390993", "bodyText": "Add clarity on current limitation\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`\n          \n          \n            \n            `classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`", "author": "afeldman1", "createdAt": "2020-06-26T20:18:40Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma seperated notaion, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify paritioning/no partitioning using `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified here will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use. Note: this cannot change across writes.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName`, `classOf[ComplexKeyGenerator].getName`", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446966748", "bodyText": "hi, again the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY is not needed when not syncing to hive.", "author": "leesf", "createdAt": "2020-06-29T13:22:35Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3ODA1Ng==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447178056", "bodyText": "This notes that HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY only needs to be specified if using hive. Do you have a suggestion on how to phrase it differently?", "author": "afeldman1", "createdAt": "2020-06-29T18:44:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjQ2Mg==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447592462", "bodyText": "how about\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY, and using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY when syncing to hive ?", "author": "leesf", "createdAt": "2020-06-30T10:50:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2ODQ1Nw==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447768457", "bodyText": "Building on that, how about:?\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY. If synchronizing to hive, also specify using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY.", "author": "afeldman1", "createdAt": "2020-06-30T15:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM0NTI0MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448345240", "bodyText": "Building on that, how about:?\nSpecify partitioning/no partitioning using KEYGENERATOR_CLASS_OPT_KEY. If synchronizing to hive, also specify using HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY.\n\nresonable.", "author": "leesf", "createdAt": "2020-07-01T12:59:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Njc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Nzk5MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r446967990", "bodyText": "same to this section.", "author": "leesf", "createdAt": "2020-06-29T13:24:19Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -176,15 +176,49 @@ In some cases, you may want to migrate your existing table into Hudi beforehand.\n \n ## Datasource Writer\n \n-The `hudi-spark` module offers the DataSource API to write (and also read) any data frame into a Hudi table.\n-Following is how we can upsert a dataframe, while specifying the field names that need to be used\n-for `recordKey => _row_key`, `partitionPath => partition` and `precombineKey => timestamp`\n+The `hudi-spark` module offers the DataSource API to write (and read) a Spark DataFrame into a Hudi table. There are a number of options available:\n \n+**`HoodieWriteConfig`**:\n+\n+**TABLE_NAME** (Required)<br>\n+\n+\n+**`DataSourceWriteOptions`**:\n+\n+**RECORDKEY_FIELD_OPT_KEY** (Required): Primary key field(s). Nested fields can be specified using the dot notation eg: `a.b.c`. When using multiple columns as primary key use comma separated notation, eg: `\"col1,col2,col3,etc\"`. Single or multiple columns as primary key specified by `KEYGENERATOR_CLASS_OPT_KEY` property.<br>\n+Default value: `\"uuid\"`<br>\n+\n+**PARTITIONPATH_FIELD_OPT_KEY** (Required): Columns to be used for partitioning the table. To prevent partitioning, provide empty string as value eg: `\"\"`. Specify partitioning/no partitioning using `KEYGENERATOR_CLASS_OPT_KEY` and if using hive `HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY`<br>\n+Default value: `\"partitionpath\"`<br>\n+\n+**PRECOMBINE_FIELD_OPT_KEY** (Required): When two records have the same key value, the record with the largest value from the field specified will be choosen.<br>\n+Default value: `\"ts\"`<br>\n+\n+**OPERATION_OPT_KEY**: The [write operations](#write-operations) to use.<br>\n+Available values:<br>\n+`UPSERT_OPERATION_OPT_VAL` (default), `BULK_INSERT_OPERATION_OPT_VAL`, `INSERT_OPERATION_OPT_VAL`, `DELETE_OPERATION_OPT_VAL`\n+\n+**TABLE_TYPE_OPT_KEY**: The [type of table](/docs/concepts.html#table-types) to write to. Note: After the initial creation of a table, this value must stay consistent when writing to (updating) the table using the Spark `SaveMode.Append` mode.<br>\n+Available values:<br>\n+[`COW_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#copy-on-write-table) (default), [`MOR_TABLE_TYPE_OPT_VAL`](/docs/concepts.html#merge-on-read-table)\n+\n+**KEYGENERATOR_CLASS_OPT_KEY**: Key generator class, that will extract the key out of incoming record. If single column key use `SimpleKeyGenerator`. For multiple column keys use `ComplexKeyGenerator`. Note: A custom key generator class can be written/provided here as well. Primary key columns should be provided via `RECORDKEY_FIELD_OPT_KEY` option.<br>\n+Available values:<br>\n+`classOf[SimpleKeyGenerator].getName` (default), `classOf[NonpartitionedKeyGenerator].getName` (Non-partitioned tables can currently only have a single key column, [HUDI-1053](https://issues.apache.org/jira/browse/HUDI-1053)), `classOf[ComplexKeyGenerator].getName`\n+\n+\n+**HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY**: Specify if the table should or should not be partitioned.<br>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzE3NTk4NQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447175985", "bodyText": "Also adding clarity that this is only necessary when using hive.", "author": "afeldman1", "createdAt": "2020-06-29T18:40:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk2Nzk5MA=="}], "type": "inlineReview"}, {"oid": "6768a13c9c1b0cce5caed5e2966c1b48d695ed63", "url": "https://github.com/apache/hudi/commit/6768a13c9c1b0cce5caed5e2966c1b48d695ed63", "message": "[MINOR] Add documentation for using multi-column table keys and for not partitioning tables", "committedDate": "2020-06-29T23:28:12Z", "type": "commit"}, {"oid": "39cdafb67bd58bc1ca79d52a43fd00ade4262558", "url": "https://github.com/apache/hudi/commit/39cdafb67bd58bc1ca79d52a43fd00ade4262558", "message": "Changes based on PR comments", "committedDate": "2020-06-29T23:28:12Z", "type": "commit"}, {"oid": "39cdafb67bd58bc1ca79d52a43fd00ade4262558", "url": "https://github.com/apache/hudi/commit/39cdafb67bd58bc1ca79d52a43fd00ade4262558", "message": "Changes based on PR comments", "committedDate": "2020-06-29T23:28:12Z", "type": "forcePushed"}, {"oid": "22b71470713c2e48f1b697f2682ea7d0b2e376cd", "url": "https://github.com/apache/hudi/commit/22b71470713c2e48f1b697f2682ea7d0b2e376cd", "message": "Fix snapshot query wildcard asterisks, clarify configs only required by hive, clarify deletion, and clean-up for consistency", "committedDate": "2020-06-30T04:09:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447591291", "bodyText": "hi, would we keep the partitionpath as it is?", "author": "leesf", "createdAt": "2020-06-30T10:48:47Z", "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -117,7 +117,7 @@ df.write.format(\"hudi\").\n   options(getQuickstartWriteConfigs).\n   option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n   option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n-  option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+  option(PARTITIONPATH_FIELD_OPT_KEY, \"partition_path\").", "originalCommit": "22b71470713c2e48f1b697f2682ea7d0b2e376cd", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0MTU4MA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447741580", "bodyText": "Well in some places it said \"partitionpath\" and in some places it said \"partitionPath\". I do like snake case better for text names, but I'm open to any of the options as long as we're consistent.\nFor instance:\nLine 89: spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\nLine 120: option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").", "author": "afeldman1", "createdAt": "2020-06-30T14:46:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM0NTk2NA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r448345964", "bodyText": "Well in some places it said \"partitionpath\" and in some places it said \"partitionPath\". I do like snake case better for text names, but I'm open to any of the options as long as we're consistent.\nFor instance:\nLine 89: spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\nLine 120: option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n\nlet's change to partitionpath.", "author": "leesf", "createdAt": "2020-07-01T13:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzAzMA==", "url": "https://github.com/apache/hudi/pull/1761#discussion_r447593030", "bodyText": "great.", "author": "leesf", "createdAt": "2020-06-30T10:52:00Z", "path": "docs/_docs/2_2_writing_data.md", "diffHunk": "@@ -220,9 +253,16 @@ For more info refer to [Delete support in Hudi](https://cwiki.apache.org/conflue\n \n  - **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. \n  This can be simply achieved by ensuring the appropriate fields are nullable in the table schema and simply upserting the table after setting these fields to null.\n- - **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the table. This can be achieved by issuing an upsert with a custom payload implementation\n- via either DataSource or DeltaStreamer which always returns Optional.Empty as the combined value. Hudi ships with a built-in `org.apache.hudi.EmptyHoodieRecordPayload` class that does exactly this.\n  \n+ - **Hard Deletes** : A stronger form of deletion is to physically remove any trace of the record from the table. This can be achieved in 3 different ways.\n+\n+   1) Using DataSource, set `OPERATION_OPT_KEY` to `DELETE_OPERATION_OPT_VAL`. This will remove all records in the DataSet being submitted.\n+   \n+   2) Using DataSource, set `PAYLOAD_CLASS_OPT_KEY` to `\"org.apache.hudi.EmptyHoodieRecordPayload\"`. This will remove all records in the DataSet being submitted. \n+   \n+   3) Using DataSource or DeltaStreamer, add a column named `_hoodie_is_deleted` to DataSet. The value of this column must be set to `true` for all records to be deleted and either `false` or left null for any records to be upserted.\n+    ", "originalCommit": "22b71470713c2e48f1b697f2682ea7d0b2e376cd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4fa23993eee8423e948f755e015dcc48cdd44bd1", "url": "https://github.com/apache/hudi/commit/4fa23993eee8423e948f755e015dcc48cdd44bd1", "message": "Standardize  to  and clarify wording", "committedDate": "2020-07-01T16:33:43Z", "type": "commit"}]}