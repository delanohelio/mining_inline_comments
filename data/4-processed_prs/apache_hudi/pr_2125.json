{"pr_number": 2125, "pr_title": "[HUDI-1301] use spark INCREMENTAL mode query hudi dataset support sch\u2026", "pr_createdAt": "2020-09-28T02:13:31Z", "pr_url": "https://github.com/apache/hudi/pull/2125", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgwMjgxMA==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r496802810", "bodyText": "Is getTableAvroSchemaWithoutMetadataFields more proper?", "author": "leesf", "createdAt": "2020-09-29T15:16:31Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -175,20 +175,45 @@ public MessageType getTableParquetSchema() throws Exception {\n    * @throws Exception\n    */\n   public Schema getTableAvroSchemaWithoutMetadataFields() throws Exception {\n-    Option<Schema> schemaFromCommitMetadata = getTableSchemaFromCommitMetadata(false);\n+    HoodieTimeline timeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+    Option<Schema> schemaFromCommitMetadata = getTableSchemaFromCommitMetadata(timeline.lastInstant().get(), false);\n     return schemaFromCommitMetadata.isPresent() ? schemaFromCommitMetadata.get() :\n            HoodieAvroUtils.removeMetadataFields(getTableAvroSchemaFromDataFile());\n   }\n \n+  /**\n+   * Gets users data schema for a hoodie table in Avro format of the instant.\n+   *\n+   * @param instant will get the instant data schema\n+   * @return  Avro user data schema\n+   * @throws Exception\n+   */\n+  public Schema getTableAvroSchemaWithoutMetadataFieldsForInstant(HoodieInstant instant) throws Exception {", "originalCommit": "1cbd9a4e2dc0210cc02b3e31ee2bdbce060c3781", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgzMTk3NA==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r496831974", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-09-29T15:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgwMjgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgwNDUwMQ==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r496804501", "bodyText": "instant,false -> instant, false", "author": "leesf", "createdAt": "2020-09-29T15:18:53Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java", "diffHunk": "@@ -175,20 +175,45 @@ public MessageType getTableParquetSchema() throws Exception {\n    * @throws Exception\n    */\n   public Schema getTableAvroSchemaWithoutMetadataFields() throws Exception {\n-    Option<Schema> schemaFromCommitMetadata = getTableSchemaFromCommitMetadata(false);\n+    HoodieTimeline timeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+    Option<Schema> schemaFromCommitMetadata = getTableSchemaFromCommitMetadata(timeline.lastInstant().get(), false);\n     return schemaFromCommitMetadata.isPresent() ? schemaFromCommitMetadata.get() :\n            HoodieAvroUtils.removeMetadataFields(getTableAvroSchemaFromDataFile());\n   }\n \n+  /**\n+   * Gets users data schema for a hoodie table in Avro format of the instant.\n+   *\n+   * @param instant will get the instant data schema\n+   * @return  Avro user data schema\n+   * @throws Exception\n+   */\n+  public Schema getTableAvroSchemaWithoutMetadataFieldsForInstant(HoodieInstant instant) throws Exception {\n+    Option<Schema> schemaFromCommitMetadata = getTableSchemaFromCommitMetadata(instant,false);", "originalCommit": "1cbd9a4e2dc0210cc02b3e31ee2bdbce060c3781", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgzMTgwMw==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r496831803", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-09-29T15:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjgwNDUwMQ=="}], "type": "inlineReview"}, {"oid": "6a508f4da106b50c67cebdd0b700184994848079", "url": "https://github.com/apache/hudi/commit/6a508f4da106b50c67cebdd0b700184994848079", "message": "[HUDI-1301] use spark INCREMENTAL mode query hudi dataset support schema version", "committedDate": "2020-09-29T15:48:21Z", "type": "forcePushed"}, {"oid": "0202e025d210f54fd8252641fa7101a2e63e71dc", "url": "https://github.com/apache/hudi/commit/0202e025d210f54fd8252641fa7101a2e63e71dc", "message": "[HUDI-1301] use spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-09-30T02:00:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQ4MjI3Nw==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r497482277", "bodyText": "when commitsToReturn is empty (in case that user pass the end time less than smallest commit in timeline), commitsToReturn.last will throw the following exception\njava.util.NoSuchElementException\n\tat scala.collection.LinearSeqOptimized$class.last(LinearSeqOptimized.scala:148)\n\tat scala.collection.immutable.List.last(List.scala:84)\n\tat org.apache.hudi.IncrementalRelation.<init>(IncrementalRelation.scala:88)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:95)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:51)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\nI think we would use the same behavior as before that use the latest commit schema when commitsToReturn is empty, and there is no data returned since the there is no commits between start and end time . @lw309637554 WDYT?", "author": "leesf", "createdAt": "2020-09-30T12:49:37Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -82,11 +81,11 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     optParams.getOrElse(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, lastInstant.getTimestamp))\n     .getInstants.iterator().toList\n \n-  // use schema from a file produced in the latest instant\n-  val latestSchema: StructType = {\n+  // use schema from a file produced in the end instant\n+  val endInstantSchema: StructType = {\n     log.info(\"Inferring schema..\")\n     val schemaResolver = new TableSchemaResolver(metaClient)\n-    val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields\n+    val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields(commitsToReturn.last)", "originalCommit": "0202e025d210f54fd8252641fa7101a2e63e71dc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTE1MDU4Mg==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r499150582", "bodyText": "thanks for your remind, agree with you", "author": "lw309637554", "createdAt": "2020-10-03T14:04:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQ4MjI3Nw=="}], "type": "inlineReview"}, {"oid": "a8002b43c7edb4e19d3b32b87614d63cdbf81f56", "url": "https://github.com/apache/hudi/commit/a8002b43c7edb4e19d3b32b87614d63cdbf81f56", "message": "[HUDI-1301] use spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-03T14:11:12Z", "type": "forcePushed"}, {"oid": "04408df5e8351dc9411f6f7180d38400fa9dd5cb", "url": "https://github.com/apache/hudi/commit/04408df5e8351dc9411f6f7180d38400fa9dd5cb", "message": "[HUDI-1301]  use spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-03T15:10:59Z", "type": "forcePushed"}, {"oid": "a28e7a1461ca9df16ffa11b42c07082f17aa2e47", "url": "https://github.com/apache/hudi/commit/a28e7a1461ca9df16ffa11b42c07082f17aa2e47", "message": "[HUDI-1301]  use  spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-03T16:22:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDg3OQ==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r499304879", "bodyText": "I think we should introduce a new ReadOption to control this? and default to existing behavior of using the latest schema.", "author": "vinothchandar", "createdAt": "2020-10-05T00:24:39Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -82,11 +81,12 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     optParams.getOrElse(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, lastInstant.getTimestamp))\n     .getInstants.iterator().toList\n \n-  // use schema from a file produced in the latest instant\n-  val latestSchema: StructType = {\n+  // use schema from a file produced in the end instant\n+  val endInstantSchema: StructType = {", "originalCommit": "a28e7a1461ca9df16ffa11b42c07082f17aa2e47", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0NDU4Mw==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r499744583", "bodyText": "thanks, i think  add a ReadOption to control this make sense now. But as the Time Traveling feature, not only data , but also schema should use the end version.  I also want to know about Hudi\u2019s plan on time travel ?\n@vinothchandar", "author": "lw309637554", "createdAt": "2020-10-05T17:02:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTMwNDg3OQ=="}], "type": "inlineReview"}, {"oid": "8d2f66b36b1ffb0476443c131db066d1bbcbc3d6", "url": "https://github.com/apache/hudi/commit/8d2f66b36b1ffb0476443c131db066d1bbcbc3d6", "message": "[HUDI-1301]  use  spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-08T15:18:05Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODI4OA==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r502728288", "bodyText": "Changing to INCREMENTAL_READ_SCHEMA_USE_END_INSTANTTIME_OPT_KEY = \"hoodie.datasource.read.schema.use.end.instanttime\"?", "author": "leesf", "createdAt": "2020-10-10T01:29:11Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -108,6 +108,15 @@ object DataSourceReadOptions {\n     */\n   val END_INSTANTTIME_OPT_KEY = \"hoodie.datasource.read.end.instanttime\"\n \n+  /**\n+    * If use the end instant schema when incrementally fetched data to.\n+    *\n+    * Default: false (use latest instant schema)\n+    *\n+    */\n+  val INCREMENTAL_READ_SCHEMA_USE_ENDINSTANT_OPT_KEY = \"hoodie.datasource.read.schema.use_endInstant\"", "originalCommit": "8d2f66b36b1ffb0476443c131db066d1bbcbc3d6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3NDE3Mg==", "url": "https://github.com/apache/hudi/pull/2125#discussion_r502774172", "bodyText": "make sense. have resolved", "author": "lw309637554", "createdAt": "2020-10-10T10:17:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcyODI4OA=="}], "type": "inlineReview"}, {"oid": "a33b191a4b1ddc9cb2e592aa3a1af5c491b465b0", "url": "https://github.com/apache/hudi/commit/a33b191a4b1ddc9cb2e592aa3a1af5c491b465b0", "message": "[HUDI-1301]  use  spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-10T09:12:13Z", "type": "forcePushed"}, {"oid": "0b5c440655d07172a8a4c1243503cb8ca4f555f5", "url": "https://github.com/apache/hudi/commit/0b5c440655d07172a8a4c1243503cb8ca4f555f5", "message": "[HUDI-1301]  use  spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-10T11:23:04Z", "type": "commit"}, {"oid": "0b5c440655d07172a8a4c1243503cb8ca4f555f5", "url": "https://github.com/apache/hudi/commit/0b5c440655d07172a8a4c1243503cb8ca4f555f5", "message": "[HUDI-1301]  use  spark INCREMENTAL mode query hudi dataset support schema version.", "committedDate": "2020-10-10T11:23:04Z", "type": "forcePushed"}]}