{"pr_number": 1810, "pr_title": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "pr_createdAt": "2020-07-08T16:13:06Z", "pr_url": "https://github.com/apache/hudi/pull/1810", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NDU5Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451944593", "bodyText": "Can we use TableSchemaResolver to handle all hudi schema related methods?\n\n  \n    \n      hudi/hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java\n    \n    \n         Line 338\n      in\n      2603cfb\n    \n    \n    \n    \n\n        \n          \n           public MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionCommitOpt) throws Exception {", "author": "garyli1019", "createdAt": "2020-07-09T03:34:36Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);\n+    }\n+\n+    try {\n+      if (resultSet != null) {\n+        resultSet.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the resultset opened \", e);\n+    }\n+  }\n+\n+  /**\n+   * Gets the schema for a hoodie table. Depending on the type of table, try to read schema from commit metadata if\n+   * present, else fallback to reading from any file written in the latest commit. We will assume that the schema has\n+   * not changed within a single atomic write.\n+   *\n+   * @return Parquet schema for this table\n+   */\n+  public MessageType getDataSchema() {\n+    try {\n+      return new TableSchemaResolver(metaClient).getTableParquetSchema();\n+    } catch (Exception e) {\n+      throw new HoodieSyncException(\"Failed to read data schema\", e);\n+    }\n+  }\n+\n+  @SuppressWarnings(\"OptionalUsedAsFieldOrParameterType\")\n+  public List<String> getPartitionsWrittenToSince(Option<String> lastCommitTimeSynced) {\n+    if (!lastCommitTimeSynced.isPresent()) {\n+      LOG.info(\"Last commit time synced is not known, listing all partitions in \" + basePath + \",FS :\" + fs);\n+      try {\n+        return FSUtils.getAllPartitionPaths(fs, basePath, assumeDatePartitioning);\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Failed to list all partitions in \" + basePath, e);\n+      }\n+    } else {\n+      LOG.info(\"Last commit time synced is \" + lastCommitTimeSynced.get() + \", Getting commits since then\");\n+\n+      HoodieTimeline timelineToSync = activeTimeline.findInstantsAfter(lastCommitTimeSynced.get(), Integer.MAX_VALUE);\n+      return timelineToSync.getInstants().map(s -> {\n+        try {\n+          return HoodieCommitMetadata.fromBytes(activeTimeline.getInstantDetails(s).get(), HoodieCommitMetadata.class);\n+        } catch (IOException e) {\n+          throw new HoodieIOException(\"Failed to get partitions written since \" + lastCommitTimeSynced, e);\n+        }\n+      }).flatMap(s -> s.getPartitionToWriteStats().keySet().stream()).distinct().collect(Collectors.toList());\n+    }\n+  }\n+\n+  private MessageType readSchemaFromLastCompaction(Option<HoodieInstant> lastCompactionCommitOpt) throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE1MzkxOA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452153918", "bodyText": "A very good suggestion. I just use the readSchemaFromLastCompaction in old hudi-hive-sync module. I think i can replace the readSchemaFromLastCompaction to TableSchemaResolver. readSchemaFromLastCompaction()", "author": "lw309637554", "createdAt": "2020-07-09T11:37:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NDU5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDMzNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190335", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-07-11T12:29:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NDU5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTUwMA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451945500", "bodyText": "I am thinking if we can put the hudi-sync-common into hudi-common. Then we have separate modules for each query engines like hudi-sync-hive hudi-sync-dla e.t.c\nThoughts?", "author": "garyli1019", "createdAt": "2020-07-09T03:38:52Z", "path": "hudi-sync/hudi-hive-sync/pom.xml", "diffHunk": "@@ -43,6 +45,11 @@\n       <artifactId>hudi-hadoop-mr</artifactId>\n       <version>${project.version}</version>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hudi</groupId>\n+      <artifactId>hudi-sync-common</artifactId>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE1NzQwNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452157405", "bodyText": "Thanks, I thank put hudi-sync-common base class to hudi-common is make sense. but hudi-sync-hive \u3001hudi-sync-dla under hudi/     directory  will make hudi code so much moudle .\nI think another choice to  put hudi-sync-common base class to hudi-common , and hudi-sync-hive \u3001hudi-sync-dla  etc.. under hudi-sync.\nwhat about yours suggestion @vinothchandar @leesf ?", "author": "lw309637554", "createdAt": "2020-07-09T11:44:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTUwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyNDE2MA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452824160", "bodyText": "hi, I am incline to put hudi-sync-common, hudi-sync-dla and hudi-sync-hive modules under hudi-sync, it will keep the structure same as the refactored hudi-client which is consisted of hudi-client-common, hudi-client-spark, hudi-client-flink, hudi-client-java modules.\nhudi-sync\n|--hudi-sync-common\n|--hudi-sync-hive\n|--hive-sync-dla\n\n\nhudi-client\n|--hudi-client-common\n|--hudi-client-java\n|--hudi-client-spark\n|--hudi-client-flink", "author": "leesf", "createdAt": "2020-07-10T12:53:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTUwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDQ1OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190458", "bodyText": "thanks, be consistent with hudi-client will be better", "author": "lw309637554", "createdAt": "2020-07-11T12:30:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODM0NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451948344", "bodyText": "Vote for putting this into the base class because we need to ensure the schema backward compatible for all query engines.", "author": "garyli1019", "createdAt": "2020-07-09T03:52:08Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -272,6 +268,7 @@ void createTable(String tableName, MessageType storageSchema, String inputFormat\n   /**\n    * Get the table schema.\n    */\n+  //???? overwrite", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE2MDY5OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452160698", "bodyText": "agree with you. can put the abstract method getTableSchema to base syncClient.", "author": "lw309637554", "createdAt": "2020-07-09T11:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODM0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDQ3NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190474", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-07-11T12:30:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODM0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODU3NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451948574", "bodyText": "nit: alphabetical order", "author": "garyli1019", "createdAt": "2020-07-09T03:52:57Z", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/SchemaDifference.java", "diffHunk": "@@ -20,12 +20,14 @@\n \n import org.apache.parquet.schema.MessageType;\n \n-import java.util.ArrayList;\n import java.util.Collections;\n-import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n import java.util.StringJoiner;\n+import java.util.ArrayList;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE2MDgyMg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452160822", "bodyText": "okay", "author": "lw309637554", "createdAt": "2020-07-09T11:51:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODU3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDQ3Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190477", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-07-11T12:30:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0ODU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0OTEzMQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451949131", "bodyText": "nit: order", "author": "garyli1019", "createdAt": "2020-07-09T03:55:22Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -63,10 +67,11 @@\n \n import java.io.IOException;\n import java.io.Serializable;\n-import java.util.ArrayList;\n import java.util.HashMap;\n-import java.util.List;\n import java.util.Objects;\n+import java.util.Properties;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE2MDg3Ng==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452160876", "bodyText": "thanks", "author": "lw309637554", "createdAt": "2020-07-09T11:52:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0OTEzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDQ4Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190487", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-07-11T12:30:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0OTEzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r451950401", "bodyText": "Should we change --enable-hive-sync to --enable-sync above?", "author": "garyli1019", "createdAt": "2020-07-09T04:00:11Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -237,6 +237,9 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--hoodie-sync-client-tool-class\"}, description = \"Meta sync client tool, using comma to separate multi tools\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjE2NjA3Ng==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452166076", "bodyText": "This place is worth discussing. For compatible, we just add a --hoodie-sync-client-tool-class.\nBut i think use change --enable-hive-sync to   --enable-sync reasonable , or just add a new parameter --enable-sync and compatible the --enable-hive-sync parameter for old users. cc @vinothchandar @leesf", "author": "lw309637554", "createdAt": "2020-07-09T12:02:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyNzY2OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r452827668", "bodyText": "yes, --enable-sync is reasonable since hudi will not only supports sync to hive, but also other meta @garyli1019 . but as @lw309637554 pointed out, the compatible is a problem here, so the current solution is compromised solution. I am ok to rename to --enable-sync and document the break change here.", "author": "leesf", "createdAt": "2020-07-10T13:00:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzE5MDc2Ng==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453190766", "bodyText": "ok , i  can add a new --enable-sync  as the default choice , and also support --enable-hive-sync  for a duplicate parameter.  what do you think about ? @garyli1019 @leesf", "author": "lw309637554", "createdAt": "2020-07-11T12:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg1Mjk4Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r453852987", "bodyText": "sounds good to me", "author": "garyli1019", "createdAt": "2020-07-13T18:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDg1MzE4Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454853182", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-07-15T07:38:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNDM5Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464804393", "bodyText": "I think we can name this little shorter. --sync-tool-class-list ?", "author": "vinothchandar", "createdAt": "2020-08-04T05:16:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk1MDQwMQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "committedDate": "2020-07-15T04:17:54Z", "type": "commit"}, {"oid": "4c08849c4d7226118202ca4dbeaa63634b16daf3", "url": "https://github.com/apache/hudi/commit/4c08849c4d7226118202ca4dbeaa63634b16daf3", "message": "[HUDI-875] Abstract hudi-sync-common, and support hudi-hive-sync", "committedDate": "2020-07-15T04:17:54Z", "type": "forcePushed"}, {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "message": " [HUDI-875] Abstract support hudi-dla-sync", "committedDate": "2020-07-15T09:18:54Z", "type": "commit"}, {"oid": "36979503d9f4d44281136caadc51ab681e1cf056", "url": "https://github.com/apache/hudi/commit/36979503d9f4d44281136caadc51ab681e1cf056", "message": " [HUDI-875] Abstract support hudi-dla-sync", "committedDate": "2020-07-15T09:18:54Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454969135", "bodyText": "if user sync both hive and dla meta, the dla meta would not get synced.?", "author": "leesf", "createdAt": "2020-07-15T11:02:28Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS", "originalCommit": "36979503d9f4d44281136caadc51ab681e1cf056", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3NDg4OQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454974889", "bodyText": "yes, this will back compatibility", "author": "lw309637554", "createdAt": "2020-07-15T11:14:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTI0Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464789247", "bodyText": "would users sync to both?  down the line it may make sense to provide support for syncing to multiple things.\nbut even here, if we just append the HiveSync class when hiveSyncEnabled=true, we can support syncing to both Hive and dla?", "author": "vinothchandar", "createdAt": "2020-08-04T04:17:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTExODU3NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465118574", "bodyText": "yes, when user set hiveSyncEnabled and --sync-tool-classes, sync both hive and --sync-tool-classes make sense. i will fix it", "author": "lw309637554", "createdAt": "2020-08-04T15:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2OTEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3Mzk2MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r454973961", "bodyText": "since dla meta do not support alter table properties yet, it would be simpler here", "author": "leesf", "createdAt": "2020-07-15T11:12:23Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();\n+    /*if (tableExists) {\n+      lastCommitTimeSynced = hoodieDLAClient.getLastCommitTimeSynced(tableName);\n+    }*/\n+    LOG.info(\"Last commit time synced was found to be \" + lastCommitTimeSynced.orElse(\"null\"));", "originalCommit": "36979503d9f4d44281136caadc51ab681e1cf056", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAxMTAyMw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r455011023", "bodyText": "yes", "author": "lw309637554", "createdAt": "2020-07-15T12:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk3Mzk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ4MzIxMQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r455483211", "bodyText": "Is there any concern that not using this way to syncHive()?", "author": "garyli1019", "createdAt": "2020-07-16T03:10:00Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -255,6 +262,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            syncHive(basePath, fs, parameters)\n+          }\n+          case _ => {\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            val properties = new Properties();\n+            properties.putAll(parameters)\n+            properties.put(\"basePath\", basePath.toString)\n+            val syncHoodie = ReflectionUtils.loadClass(impl.trim, Array[Class[_]](classOf[Properties], classOf[FileSystem]), properties, fs).asInstanceOf[AbstractSyncTool]\n+            syncHoodie.syncHoodieTable()", "originalCommit": "36979503d9f4d44281136caadc51ab681e1cf056", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTUzMTU5Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r455531592", "bodyText": "because HoodieHiveClient  Constructor   is different like this", "author": "lw309637554", "createdAt": "2020-07-16T06:11:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ4MzIxMQ=="}], "type": "inlineReview"}, {"oid": "458d7ebfff40114bb4c8536fec25038bf89deed1", "url": "https://github.com/apache/hudi/commit/458d7ebfff40114bb4c8536fec25038bf89deed1", "message": " [HUDI-875] merge master", "committedDate": "2020-08-01T16:05:38Z", "type": "commit"}, {"oid": "fcc3a9c1444f8488164a570d506abe6ab245644c", "url": "https://github.com/apache/hudi/commit/fcc3a9c1444f8488164a570d506abe6ab245644c", "message": "Merge branch 'master' into pull/1810", "committedDate": "2020-08-04T04:04:43Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5OTMxNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464799315", "bodyText": "can this line be shared . the fs initialization?", "author": "vinothchandar", "createdAt": "2020-08-04T04:57:23Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)\n+            syncHive(basePath, fs, parameters)\n+          }\n+          case _ => {\n+            val fs = FSUtils.getFs(basePath.toString, hadoopConf)", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5OTQ5NQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464799495", "bodyText": "can we explicitly match for Hive instead of default? we may change the default for e.g and it would be an issue.", "author": "vinothchandar", "createdAt": "2020-08-04T04:58:09Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,43 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters.get(SYNC_CLIENT_TOOL_CLASS).get\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = DEFAULT_SYNC_CLIENT_TOOL_CLASS\n+    }\n+    var metaSyncSuccess = true\n+    if (metaSyncEnabled) {\n+      val impls = syncClientToolClass.split(\",\")\n+      impls.foreach(impl => {\n+        val syncSuccess = impl.trim match {\n+          case DEFAULT_SYNC_CLIENT_TOOL_CLASS => {\n+            log.info(\"Syncing to Hive Metastore (URL: \" + parameters(HIVE_URL_OPT_KEY) + \")\")", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805558", "bodyText": "can we print a warning around this, so the user knows?\nhere's my take. we can change the code so that --enable-sync and --sync-tool-class-list are the main drivers out of which we derive a Set<String> denoting all the sync tool classes. if --enable-hive-sync is specified, then we simply add the hive sync tool class to this set.. rest of the code just syncs to all sync tools in this set.\nthis way, --enable-hive-sync will be just isolated to the initial command line parsing code. We can apply the same method to datasource as well, if you don't see issues  @lw309637554 @leesf wdyt?", "author": "vinothchandar", "createdAt": "2020-08-04T05:20:06Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTEzMzA5Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465133092", "bodyText": "agree with you ,and  i will do it", "author": "lw309637554", "createdAt": "2020-08-04T15:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTcyMA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805720", "bodyText": "nit:indent", "author": "vinothchandar", "createdAt": "2020-08-04T05:20:44Z", "path": "packaging/hudi-hive-sync-bundle/pom.xml", "diffHunk": "@@ -66,7 +66,8 @@\n                 <includes>\n                   <include>org.apache.hudi:hudi-common</include>\n                   <include>org.apache.hudi:hudi-hadoop-mr</include>\n-                  <include>org.apache.hudi:hudi-hive-sync</include>\n+                  <include>org.apache.hudi:hudi-sync-common</include>\n+\t\t  <include>org.apache.hudi:hudi-hive-sync</include>", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464805989", "bodyText": "what if both hive and meta sync are off? we would still emit metrics for meta?", "author": "vinothchandar", "createdAt": "2020-08-04T05:21:33Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs, long syncNs, boolean hiveSync) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+      if (hiveSync) {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(syncNs));\n+      } else {\n+        Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"metaSyncDuration\"), getDurationInMs(syncNs));", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjU5NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806594", "bodyText": "should we derive the metric name from the sync tool class. i.e instead of metaSyncDuration, we do dlaSyncDuration?  that seems more usable and understandable", "author": "vinothchandar", "createdAt": "2020-08-04T05:24:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1NzA1Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465157057", "bodyText": "i have do it , different  sync tool class have its own metrics with name of sync class", "author": "lw309637554", "createdAt": "2020-08-04T15:57:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNTk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r464806335", "bodyText": "is there a way to do this by iterating over the configured sync tool classes? i.e only do it when sync is configured?", "author": "vinothchandar", "createdAt": "2020-08-04T05:23:01Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -442,7 +449,8 @@ private void refreshTimeline() throws IOException {\n     long overallTimeMs = overallTimerContext != null ? overallTimerContext.stop() : 0;\n \n     // Send DeltaStreamer Metrics\n-    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, hiveSyncTimeMs, true);\n+    metrics.updateDeltaStreamerMetrics(overallTimeMs, metaSyncTimeMs, false);", "originalCommit": "fcc3a9c1444f8488164a570d506abe6ab245644c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE1ODYwOA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465158608", "bodyText": "ok  , have do this in syncMeta", "author": "lw309637554", "createdAt": "2020-08-04T15:59:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgwNjMzNQ=="}], "type": "inlineReview"}, {"oid": "e35037775016004364176de7f46091ce2b8ba49e", "url": "https://github.com/apache/hudi/commit/e35037775016004364176de7f46091ce2b8ba49e", "message": "Smaller CR feedback", "committedDate": "2020-08-04T05:43:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODA4Ng==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465228086", "bodyText": "actually like META_SYNC better here. it was more meaningful. wdyt?", "author": "vinothchandar", "createdAt": "2020-08-04T17:54:43Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,44 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQzMzMxNg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465433316", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T02:21:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODcyNg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465228726", "bodyText": "if someone does hiveSyncEnabled == true && metaSyncEnabled == true && syncClientToolClass = org.apache.hudi.hive.HiveSyncTool, we will sync two times? can we just a set to hold the classes.", "author": "vinothchandar", "createdAt": "2020-08-04T17:55:57Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -261,6 +268,44 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig\n   }\n \n+  private def metaSync(parameters: Map[String, String],\n+                       basePath: Path,\n+                       hadoopConf: Configuration): Boolean = {\n+    val hiveSyncEnabled = parameters.get(HIVE_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var metaSyncEnabled = parameters.get(HUDI_SYNC_ENABLED_OPT_KEY).exists(r => r.toBoolean)\n+    var syncClientToolClass = parameters(SYNC_CLIENT_TOOL_CLASS)\n+    // for backward compatibility\n+    if (hiveSyncEnabled) {\n+      metaSyncEnabled = true\n+      syncClientToolClass = String.format(\"%s,%s\", syncClientToolClass, \"org.apache.hudi.hive.HiveSyncTool\")", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODQzNw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465418437", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-08-05T01:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0ODQyNA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465448424", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:21:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyODcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyOTQ1Nw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465229457", "bodyText": "lets do HiveSyncTool.class.getName or soemthing?", "author": "vinothchandar", "createdAt": "2020-08-04T17:57:13Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -267,9 +267,16 @@ public Operation convert(String value) throws ParameterException {\n         description = \"Should duplicate records from source be dropped/filtered out before insert/bulk-insert\")\n     public Boolean filterDupes = false;\n \n+    //will abandon in the future version, recommended use --enable-sync\n     @Parameter(names = {\"--enable-hive-sync\"}, description = \"Enable syncing to hive\")\n     public Boolean enableHiveSync = false;\n \n+    @Parameter(names = {\"--enable-sync\"}, description = \"Enable syncing meta\")\n+    public Boolean enableMetaSync = false;\n+\n+    @Parameter(names = {\"--sync-tool-classes\"}, description = \"Meta sync client tool, using comma to separate multi tools\")\n+    public String syncClientToolClass = \"org.apache.hudi.hive.HiveSyncTool\";", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQzNDExMw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465434113", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T02:24:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIyOTQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNTg2OA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465415868", "bodyText": "we would add a TODO here once DLA supports alter table properties.", "author": "leesf", "createdAt": "2020-08-05T01:16:41Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/DLASyncTool.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import com.beust.jcommander.JCommander;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.dla.util.Utils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.InvalidTableException;\n+import org.apache.hudi.hadoop.HoodieParquetInputFormat;\n+import org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.hudi.sync.common.AbstractSyncTool;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Tool to sync a hoodie table with a dla table. Either use it as a api\n+ * DLASyncTool.syncHoodieTable(DLASyncConfig) or as a command line java -cp hoodie-hive.jar DLASyncTool [args]\n+ * <p>\n+ * This utility will get the schema from the latest commit and will sync dla table schema Also this will sync the\n+ * partitions incrementally (all the partitions modified since the last commit)\n+ */\n+@SuppressWarnings(\"WeakerAccess\")\n+public class DLASyncTool extends AbstractSyncTool {\n+\n+  private static final Logger LOG = LogManager.getLogger(DLASyncTool.class);\n+  public static final String SUFFIX_SNAPSHOT_TABLE = \"_rt\";\n+  public static final String SUFFIX_READ_OPTIMIZED_TABLE = \"_ro\";\n+\n+  private final DLASyncConfig cfg;\n+  private final HoodieDLAClient hoodieDLAClient;\n+  private final String snapshotTableName;\n+  private final Option<String> roTableTableName;\n+\n+  public DLASyncTool(Properties properties, FileSystem fs) {\n+    super(properties, fs);\n+    this.hoodieDLAClient = new HoodieDLAClient(Utils.propertiesToConfig(properties), fs);\n+    this.cfg = Utils.propertiesToConfig(properties);\n+    switch (hoodieDLAClient.getTableType()) {\n+      case COPY_ON_WRITE:\n+        this.snapshotTableName = cfg.tableName;\n+        this.roTableTableName = Option.empty();\n+        break;\n+      case MERGE_ON_READ:\n+        this.snapshotTableName = cfg.tableName + SUFFIX_SNAPSHOT_TABLE;\n+        this.roTableTableName = cfg.skipROSuffix ? Option.of(cfg.tableName) :\n+            Option.of(cfg.tableName + SUFFIX_READ_OPTIMIZED_TABLE);\n+        break;\n+      default:\n+        LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+        throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+    }\n+  }\n+\n+  @Override\n+  public void syncHoodieTable() {\n+    try {\n+      switch (hoodieDLAClient.getTableType()) {\n+        case COPY_ON_WRITE:\n+          syncHoodieTable(snapshotTableName, false);\n+          break;\n+        case MERGE_ON_READ:\n+          // sync a RO table for MOR\n+          syncHoodieTable(roTableTableName.get(), false);\n+          // sync a RT table for MOR\n+          syncHoodieTable(snapshotTableName, true);\n+          break;\n+        default:\n+          LOG.error(\"Unknown table type \" + hoodieDLAClient.getTableType());\n+          throw new InvalidTableException(hoodieDLAClient.getBasePath());\n+      }\n+    } catch (RuntimeException re) {\n+      LOG.error(\"Got runtime exception when dla syncing\", re);\n+    } finally {\n+      hoodieDLAClient.close();\n+    }\n+  }\n+\n+  private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n+    LOG.info(\"Trying to sync hoodie table \" + tableName + \" with base path \" + hoodieDLAClient.getBasePath()\n+        + \" of type \" + hoodieDLAClient.getTableType());\n+    // Check if the necessary table exists\n+    boolean tableExists = hoodieDLAClient.doesTableExist(tableName);\n+    // Get the parquet schema for this table looking at the latest commit\n+    MessageType schema = hoodieDLAClient.getDataSchema();\n+    // Sync schema if needed\n+    syncSchema(tableName, tableExists, useRealtimeInputFormat, schema);\n+\n+    LOG.info(\"Schema sync complete. Syncing partitions for \" + tableName);\n+    // Get the last time we successfully synced partitions\n+    Option<String> lastCommitTimeSynced = Option.empty();", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTA2NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449064", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:24:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNTg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjIxNw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465416217", "bodyText": "please use // TODO here", "author": "leesf", "createdAt": "2020-08-05T01:18:06Z", "path": "hudi-sync/hudi-dla-sync/src/main/java/org/apache/hudi/dla/HoodieDLAClient.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.dla;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.hive.HiveSyncConfig;\n+import org.apache.hudi.hive.HoodieHiveSyncException;\n+import org.apache.hudi.hive.PartitionValueExtractor;\n+import org.apache.hudi.hive.SchemaDifference;\n+import org.apache.hudi.hive.util.HiveSchemaUtil;\n+import org.apache.hudi.sync.common.AbstractSyncHoodieClient;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+import java.sql.DriverManager;\n+import java.sql.DatabaseMetaData;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class HoodieDLAClient extends AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(HoodieDLAClient.class);\n+  private static final String HOODIE_LAST_COMMIT_TIME_SYNC = \"hoodie_last_sync\";\n+  // Make sure we have the dla JDBC driver in classpath\n+  private static final String DRIVER_NAME = \"com.mysql.jdbc.Driver\";\n+  private static final String DLA_ESCAPE_CHARACTER = \"\";\n+  private static final String TBL_PROPERTIES_STR = \"TBLPROPERTIES\";\n+\n+  static {\n+    try {\n+      Class.forName(DRIVER_NAME);\n+    } catch (ClassNotFoundException e) {\n+      throw new IllegalStateException(\"Could not find \" + DRIVER_NAME + \" in classpath. \", e);\n+    }\n+  }\n+\n+  private Connection connection;\n+  private DLASyncConfig dlaConfig;\n+  private PartitionValueExtractor partitionValueExtractor;\n+\n+  public HoodieDLAClient(DLASyncConfig syncConfig, FileSystem fs) {\n+    super(syncConfig.basePath, syncConfig.assumeDatePartitioning, fs);\n+    this.dlaConfig = syncConfig;\n+    try {\n+      this.partitionValueExtractor =\n+          (PartitionValueExtractor) Class.forName(dlaConfig.partitionValueExtractorClass).newInstance();\n+    } catch (Exception e) {\n+      throw new HoodieException(\n+          \"Failed to initialize PartitionValueExtractor class \" + dlaConfig.partitionValueExtractorClass, e);\n+    }\n+    createDLAConnection();\n+  }\n+\n+  private void createDLAConnection() {\n+    if (connection == null) {\n+      try {\n+        Class.forName(DRIVER_NAME);\n+      } catch (ClassNotFoundException e) {\n+        LOG.error(\"Unable to load DLA driver class\", e);\n+        return;\n+      }\n+      try {\n+        this.connection = DriverManager.getConnection(dlaConfig.jdbcUrl, dlaConfig.dlaUser, dlaConfig.dlaPass);\n+        LOG.info(\"Successfully established DLA connection to  \" + dlaConfig.jdbcUrl);\n+      } catch (SQLException e) {\n+        throw new HoodieException(\"Cannot create dla connection \", e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void createTable(String tableName, MessageType storageSchema, String inputFormatClass, String outputFormatClass, String serdeClass) {\n+    try {\n+      String createSQLQuery = HiveSchemaUtil.generateCreateDDL(tableName, storageSchema, toHiveSyncConfig(), inputFormatClass, outputFormatClass, serdeClass);\n+      LOG.info(\"Creating table with \" + createSQLQuery);\n+      updateDLASQL(createSQLQuery);\n+    } catch (IOException e) {\n+      throw new HoodieException(\"Failed to create table \" + tableName, e);\n+    }\n+  }\n+\n+  public Map<String, String> getTableSchema(String tableName) {\n+    if (!doesTableExist(tableName)) {\n+      throw new IllegalArgumentException(\n+          \"Failed to get schema for table \" + tableName + \" does not exist\");\n+    }\n+    Map<String, String> schema = new HashMap<>();\n+    ResultSet result = null;\n+    try {\n+      DatabaseMetaData databaseMetaData = connection.getMetaData();\n+      result = databaseMetaData.getColumns(dlaConfig.databaseName, dlaConfig.databaseName, tableName, null);\n+      while (result.next()) {\n+        String columnName = result.getString(4);\n+        String columnType = result.getString(6);\n+        if (\"DECIMAL\".equals(columnType)) {\n+          int columnSize = result.getInt(\"COLUMN_SIZE\");\n+          int decimalDigits = result.getInt(\"DECIMAL_DIGITS\");\n+          columnType += String.format(\"(%s,%s)\", columnSize, decimalDigits);\n+        }\n+        schema.put(columnName, columnType);\n+      }\n+      return schema;\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed to get table schema for \" + tableName, e);\n+    } finally {\n+      closeQuietly(result, null);\n+    }\n+  }\n+\n+  @Override\n+  public void addPartitionsToTable(String tableName, List<String> partitionsToAdd) {\n+    if (partitionsToAdd.isEmpty()) {\n+      LOG.info(\"No partitions to add for \" + tableName);\n+      return;\n+    }\n+    LOG.info(\"Adding partitions \" + partitionsToAdd.size() + \" to table \" + tableName);\n+    String sql = constructAddPartitions(tableName, partitionsToAdd);\n+    updateDLASQL(sql);\n+  }\n+\n+  public String constructAddPartitions(String tableName, List<String> partitions) {\n+    return constructDLAAddPartitions(tableName, partitions);\n+  }\n+\n+  String generateAbsolutePathStr(Path path) {\n+    String absolutePathStr = path.toString();\n+    if (path.toUri().getScheme() == null) {\n+      absolutePathStr = getDefaultFs() + absolutePathStr;\n+    }\n+    return absolutePathStr.endsWith(\"/\") ? absolutePathStr : absolutePathStr + \"/\";\n+  }\n+\n+  public List<String> constructChangePartitions(String tableName, List<String> partitions) {\n+    List<String> changePartitions = new ArrayList<>();\n+    String useDatabase = \"USE \" + DLA_ESCAPE_CHARACTER + dlaConfig.databaseName + DLA_ESCAPE_CHARACTER;\n+    changePartitions.add(useDatabase);\n+    String alterTable = \"ALTER TABLE \" + DLA_ESCAPE_CHARACTER + tableName + DLA_ESCAPE_CHARACTER;\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      String changePartition =\n+          alterTable + \" ADD IF NOT EXISTS PARTITION (\" + partitionClause + \") LOCATION '\" + fullPartitionPathStr + \"'\";\n+      changePartitions.add(changePartition);\n+    }\n+    return changePartitions;\n+  }\n+\n+  /**\n+   * Generate Hive Partition from partition values.\n+   *\n+   * @param partition Partition path\n+   * @return\n+   */\n+  public String getPartitionClause(String partition) {\n+    List<String> partitionValues = partitionValueExtractor.extractPartitionValuesInPath(partition);\n+    ValidationUtils.checkArgument(dlaConfig.partitionFields.size() == partitionValues.size(),\n+        \"Partition key parts \" + dlaConfig.partitionFields + \" does not match with partition values \" + partitionValues\n+            + \". Check partition strategy. \");\n+    List<String> partBuilder = new ArrayList<>();\n+    for (int i = 0; i < dlaConfig.partitionFields.size(); i++) {\n+      partBuilder.add(dlaConfig.partitionFields.get(i) + \"='\" + partitionValues.get(i) + \"'\");\n+    }\n+    return partBuilder.stream().collect(Collectors.joining(\",\"));\n+  }\n+\n+  private String constructDLAAddPartitions(String tableName, List<String> partitions) {\n+    StringBuilder alterSQL = new StringBuilder(\"ALTER TABLE \");\n+    alterSQL.append(DLA_ESCAPE_CHARACTER).append(dlaConfig.databaseName)\n+        .append(DLA_ESCAPE_CHARACTER).append(\".\").append(DLA_ESCAPE_CHARACTER)\n+        .append(tableName).append(DLA_ESCAPE_CHARACTER).append(\" ADD IF NOT EXISTS \");\n+    for (String partition : partitions) {\n+      String partitionClause = getPartitionClause(partition);\n+      Path partitionPath = FSUtils.getPartitionPath(dlaConfig.basePath, partition);\n+      String fullPartitionPathStr = generateAbsolutePathStr(partitionPath);\n+      alterSQL.append(\"  PARTITION (\").append(partitionClause).append(\") LOCATION '\").append(fullPartitionPathStr)\n+          .append(\"' \");\n+    }\n+    return alterSQL.toString();\n+  }\n+\n+  private void updateDLASQL(String sql) {\n+    Statement stmt = null;\n+    try {\n+      stmt = connection.createStatement();\n+      LOG.info(\"Executing SQL \" + sql);\n+      stmt.execute(sql);\n+    } catch (SQLException e) {\n+      throw new HoodieException(\"Failed in executing SQL \" + sql, e);\n+    } finally {\n+      closeQuietly(null, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public boolean doesTableExist(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+    } catch (SQLException e) {\n+      return false;\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public Option<String> getLastCommitTimeSynced(String tableName) {\n+    String sql = consutructShowCreateTableSQL(tableName);\n+    Statement stmt = null;\n+    ResultSet rs = null;\n+    try {\n+      stmt = connection.createStatement();\n+      rs = stmt.executeQuery(sql);\n+      if (rs.next()) {\n+        String table = rs.getString(2);\n+        Map<String, String> attr = new HashMap<>();\n+        int index = table.indexOf(TBL_PROPERTIES_STR);\n+        if (index != -1) {\n+          String sub = table.substring(index + TBL_PROPERTIES_STR.length());\n+          sub = sub.replaceAll(\"\\\\(\", \"\").replaceAll(\"\\\\)\", \"\").replaceAll(\"'\", \"\");\n+          String[] str = sub.split(\",\");\n+\n+          for (int i = 0; i < str.length; i++) {\n+            String key = str[i].split(\"=\")[0].trim();\n+            String value = str[i].split(\"=\")[1].trim();\n+            attr.put(key, value);\n+          }\n+        }\n+        return Option.ofNullable(attr.getOrDefault(HOODIE_LAST_COMMIT_TIME_SYNC, null));\n+      }\n+      return Option.empty();\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get the last commit time synced from the table\", e);\n+    } finally {\n+      closeQuietly(rs, stmt);\n+    }\n+  }\n+\n+  @Override\n+  public void updateLastCommitTimeSynced(String tableName) {\n+    // dla do not support update tblproperties, so do nothing.", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTIyNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449225", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:25:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNjIxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzA1Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417053", "bodyText": "please change to warn", "author": "leesf", "createdAt": "2020-08-05T01:21:08Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0OTM2Mw==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465449363", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:25:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzA1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzEzNg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417136", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-08-05T01:21:28Z", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.sync.common;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.sql.Statement;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public abstract class AbstractSyncHoodieClient {\n+  private static final Logger LOG = LogManager.getLogger(AbstractSyncHoodieClient.class);\n+  protected final HoodieTableMetaClient metaClient;\n+  protected HoodieTimeline activeTimeline;\n+  protected final HoodieTableType tableType;\n+  protected final FileSystem fs;\n+  private String basePath;\n+  private boolean assumeDatePartitioning;\n+\n+  public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning, FileSystem fs) {\n+    this.metaClient = new HoodieTableMetaClient(fs.getConf(), basePath, true);\n+    this.tableType = metaClient.getTableType();\n+    this.basePath = basePath;\n+    this.assumeDatePartitioning = assumeDatePartitioning;\n+    this.fs = fs;\n+    this.activeTimeline = metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+  }\n+\n+  public abstract void createTable(String tableName, MessageType storageSchema,\n+                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+\n+  public abstract boolean doesTableExist(String tableName);\n+\n+  public abstract Option<String> getLastCommitTimeSynced(String tableName);\n+\n+  public abstract void updateLastCommitTimeSynced(String tableName);\n+\n+  public abstract void addPartitionsToTable(String tableName, List<String> partitionsToAdd);\n+\n+  public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n+\n+  public abstract Map<String, String> getTableSchema(String tableName);\n+\n+  public HoodieTimeline getActiveTimeline() {\n+    return activeTimeline;\n+  }\n+\n+  public HoodieTableType getTableType() {\n+    return tableType;\n+  }\n+\n+  public String getBasePath() {\n+    return metaClient.getBasePath();\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  public void closeQuietly(ResultSet resultSet, Statement stmt) {\n+    try {\n+      if (stmt != null) {\n+        stmt.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the statement opened \", e);\n+    }\n+\n+    try {\n+      if (resultSet != null) {\n+        resultSet.close();\n+      }\n+    } catch (SQLException e) {\n+      LOG.error(\"Could not close the resultset opened \", e);", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1MDM1MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465450351", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:29:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzUzOQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465417539", "bodyText": "use HiveSyncTool.class.getName here?", "author": "leesf", "createdAt": "2020-08-05T01:23:05Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -475,12 +480,38 @@ private String startCommit() {\n     throw lastException;\n   }\n \n-  /**\n-   * Sync to Hive.\n-   */\n-  public void syncHiveIfNeeded() {\n+  private void syncMeta(HoodieDeltaStreamerMetrics metrics) {\n+    String syncClientToolClass = cfg.syncClientToolClass;\n+    // for backward compatibility\n     if (cfg.enableHiveSync) {\n-      syncHive();\n+      cfg.enableMetaSync = true;\n+      syncClientToolClass = String.format(\"%s,%s\", cfg.syncClientToolClass, \"org.apache.hudi.hive.HiveSyncTool\");", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1MDk3MQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465450971", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T03:31:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxNzUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODM2Ng==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465418366", "bodyText": "use HiveSyncTool.class.getName?", "author": "leesf", "createdAt": "2020-08-05T01:25:46Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -258,11 +258,14 @@ object DataSourceWriteOptions {\n     */\n   val STREAMING_IGNORE_FAILED_BATCH_OPT_KEY = \"hoodie.datasource.write.streaming.ignore.failed.batch\"\n   val DEFAULT_STREAMING_IGNORE_FAILED_BATCH_OPT_VAL = \"true\"\n+  val SYNC_CLIENT_TOOL_CLASS = \"hoodie.sync.client.tool.class\"\n+  val DEFAULT_SYNC_CLIENT_TOOL_CLASS = \"org.apache.hudi.hive.HiveSyncTool\"", "originalCommit": "3ba46533b866908feb33315fd9df058d57375cf3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1ODM5NA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465458394", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T04:01:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxODM2Ng=="}], "type": "inlineReview"}, {"oid": "fe598ff9af58355469ca08157d7369b7150bde71", "url": "https://github.com/apache/hudi/commit/fe598ff9af58355469ca08157d7369b7150bde71", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T04:29:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465468482", "bodyText": "do we need the entire class name here? Would that not make for a long metric name? :)\nMay be have a getShortName() method for the AbstractSyncTool class and return \"hive\" and \"dla\" from them?", "author": "vinothchandar", "createdAt": "2020-08-05T04:42:04Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamerMetrics.java", "diffHunk": "@@ -67,10 +77,15 @@ String getMetricsName(String action, String metric) {\n     return config == null ? null : String.format(\"%s.%s.%s\", tableName, action, metric);\n   }\n \n-  public void updateDeltaStreamerMetrics(long durationInNs, long hiveSyncNs) {\n+  public void updateDeltaStreamerMetrics(long durationInNs) {\n     if (config.isMetricsOn()) {\n       Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"duration\"), getDurationInMs(durationInNs));\n-      Metrics.registerGauge(getMetricsName(\"deltastreamer\", \"hiveSyncDuration\"), getDurationInMs(hiveSyncNs));\n+    }\n+  }\n+\n+  public void updateDeltaStreamerMetaSyncMetrics(String syncClassName, long syncNs) {", "originalCommit": "fe598ff9af58355469ca08157d7369b7150bde71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ5NTgxNQ==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465495815", "bodyText": "ok ,i will do it", "author": "lw309637554", "createdAt": "2020-08-05T06:13:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTUwMzEyNA==", "url": "https://github.com/apache/hudi/pull/1810#discussion_r465503124", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-08-05T06:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2ODQ4Mg=="}], "type": "inlineReview"}, {"oid": "015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "url": "https://github.com/apache/hudi/commit/015ac1b69fb5443e9a24fcf84b31b21a66f020f4", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T06:31:48Z", "type": "forcePushed"}, {"oid": "856ef5a9cbf45784932fde3574d3e8e8435f5c26", "url": "https://github.com/apache/hudi/commit/856ef5a9cbf45784932fde3574d3e8e8435f5c26", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T09:55:36Z", "type": "forcePushed"}, {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T13:08:33Z", "type": "commit"}, {"oid": "3808c3253e6d71c0b7b504046be0c1e9bde969d7", "url": "https://github.com/apache/hudi/commit/3808c3253e6d71c0b7b504046be0c1e9bde969d7", "message": "[HUDI-875] complete metasync metrics and sync all syncclasses", "committedDate": "2020-08-05T13:08:33Z", "type": "forcePushed"}, {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "message": "merge master", "committedDate": "2020-08-06T03:07:13Z", "type": "commit"}, {"oid": "4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "url": "https://github.com/apache/hudi/commit/4dae21dd4f23fdbe3e394886e3c94765ed2377f8", "message": "merge master", "committedDate": "2020-08-06T03:07:13Z", "type": "forcePushed"}]}