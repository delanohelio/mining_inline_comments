{"pr_number": 1624, "pr_title": "[HUDI-706]Add unit test for SavepointsCommand", "pr_createdAt": "2020-05-12T15:08:24Z", "pr_url": "https://github.com/apache/hudi/pull/1624", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4MzExNg==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r425183116", "bodyText": "any reason to return 1?", "author": "leesf", "createdAt": "2020-05-14T14:31:11Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -282,6 +293,19 @@ private static int rollback(JavaSparkContext jsc, String instantTime, String bas\n     }\n   }\n \n+  private static int createSavepoint(JavaSparkContext jsc, String commitTime, String user,\n+      String comments, String basePath) throws Exception {\n+    HoodieWriteClient client = createHoodieClient(jsc, basePath);\n+    try {\n+      client.savepoint(commitTime, user, comments);\n+      LOG.info(String.format(\"The commit \\\"%s\\\" has been savepointed.\", commitTime));\n+      return 0;\n+    } catch (HoodieSavepointException se) {\n+      LOG.info(String.format(\"Failed: Could not create savepoint \\\"%s\\\".\", commitTime));\n+      return 1;", "originalCommit": "bdc8c8e8634ae1ff7860402be7ab1adff8486af0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNjcxNQ==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r426126715", "bodyText": "any reason to return 1?\n\nIt should be -1 here :)", "author": "hddong", "createdAt": "2020-05-16T07:00:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4MzExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4MzMzMQ==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r425183331", "bodyText": "this returns -1 while upper return 1", "author": "leesf", "createdAt": "2020-05-14T14:31:27Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -294,6 +318,18 @@ private static int rollbackToSavepoint(JavaSparkContext jsc, String savepointTim\n     }\n   }\n \n+  private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, String basePath) throws Exception {\n+    HoodieWriteClient client = createHoodieClient(jsc, basePath);\n+    try {\n+      client.deleteSavepoint(savepointTime);\n+      LOG.info(String.format(\"Savepoint \\\"%s\\\" deleted.\", savepointTime));\n+      return 0;\n+    } catch (Exception e) {\n+      LOG.info(String.format(\"Failed: Could not delete savepoint \\\"%s\\\".\", savepointTime));\n+      return -1;", "originalCommit": "bdc8c8e8634ae1ff7860402be7ab1adff8486af0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4NDUwMQ==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r425184501", "bodyText": "also test MERGE_ON_READ?", "author": "leesf", "createdAt": "2020-05-14T14:32:50Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestSavepointsCommand.java", "diffHunk": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link org.apache.hudi.cli.commands.SavepointsCommand}.\n+ */\n+public class TestSavepointsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @BeforeEach\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),", "originalCommit": "bdc8c8e8634ae1ff7860402be7ab1adff8486af0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEyNjc2NA==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r426126764", "bodyText": "also test MERGE_ON_READ?\n\nSavePoint is not support of MERGE_ON_READ table now.", "author": "hddong", "createdAt": "2020-05-16T07:01:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4NDUwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNTE4NDg2OA==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r425184868", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-05-14T14:33:21Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestSavepointsCommand.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+\n+import static org.junit.jupiter.api.Assertions.assertAll;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link org.apache.hudi.cli.commands.SavepointsCommand}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestSavepointsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @BeforeEach\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),", "originalCommit": "bdc8c8e8634ae1ff7860402be7ab1adff8486af0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "url": "https://github.com/apache/hudi/commit/89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "message": "Add test for SavepointsCommand", "committedDate": "2020-05-16T07:07:02Z", "type": "commit"}, {"oid": "89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "url": "https://github.com/apache/hudi/commit/89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "message": "Add test for SavepointsCommand", "committedDate": "2020-05-16T07:07:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEzODgxMw==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r426138813", "bodyText": "I think we could change to log.warn, wdyt?", "author": "leesf", "createdAt": "2020-05-16T09:55:54Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -281,6 +292,19 @@ private static int rollback(JavaSparkContext jsc, String instantTime, String bas\n     }\n   }\n \n+  private static int createSavepoint(JavaSparkContext jsc, String commitTime, String user,\n+      String comments, String basePath) throws Exception {\n+    HoodieWriteClient client = createHoodieClient(jsc, basePath);\n+    try {\n+      client.savepoint(commitTime, user, comments);\n+      LOG.info(String.format(\"The commit \\\"%s\\\" has been savepointed.\", commitTime));\n+      return 0;\n+    } catch (HoodieSavepointException se) {\n+      LOG.info(String.format(\"Failed: Could not create savepoint \\\"%s\\\".\", commitTime));", "originalCommit": "89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjI2ODQyMw==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r426268423", "bodyText": "I think we could change to log.warn, wdyt?\n\nYes, warn is better here.", "author": "hddong", "createdAt": "2020-05-17T14:33:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEzODgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEzODgxNg==", "url": "https://github.com/apache/hudi/pull/1624#discussion_r426138816", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-05-16T09:56:01Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -293,6 +317,18 @@ private static int rollbackToSavepoint(JavaSparkContext jsc, String savepointTim\n     }\n   }\n \n+  private static int deleteSavepoint(JavaSparkContext jsc, String savepointTime, String basePath) throws Exception {\n+    HoodieWriteClient client = createHoodieClient(jsc, basePath);\n+    try {\n+      client.deleteSavepoint(savepointTime);\n+      LOG.info(String.format(\"Savepoint \\\"%s\\\" deleted.\", savepointTime));\n+      return 0;\n+    } catch (Exception e) {\n+      LOG.info(String.format(\"Failed: Could not delete savepoint \\\"%s\\\".\", savepointTime));", "originalCommit": "89d8c2b2bff6d0a51b0ff404a5c29db2319facd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "91cbc59a23027eb2bf6c1b3b6c6d195ce5871472", "url": "https://github.com/apache/hudi/commit/91cbc59a23027eb2bf6c1b3b6c6d195ce5871472", "message": "Change exception log to warn", "committedDate": "2020-05-17T14:32:54Z", "type": "commit"}, {"oid": "ee59cd04c5c210a802a0c1d848b2b0dfa89ad9df", "url": "https://github.com/apache/hudi/commit/ee59cd04c5c210a802a0c1d848b2b0dfa89ad9df", "message": "Merge branch 'master' into add-test-SavepointsCommand", "committedDate": "2020-05-19T07:05:47Z", "type": "commit"}]}