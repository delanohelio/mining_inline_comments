{"pr_number": 1476, "pr_title": "[HUDI-757] Added hudi-cli command to export metadata of Instants.", "pr_createdAt": "2020-04-01T05:39:27Z", "pr_url": "https://github.com/apache/hudi/pull/1476", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2ODI4Ng==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r401968286", "bodyText": "Can you add where does this command allow one to export this information to ?", "author": "n3nash", "createdAt": "2020-04-01T23:28:53Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CLI command to export various information from a HUDI dataset.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjUzMjEzNA==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r402532134", "bodyText": "Done.", "author": "prashantwason", "createdAt": "2020-04-02T18:41:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2ODI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2OTgwNA==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r401969804", "bodyText": "Can you use the HoodieActiveTimeline here instead to filter out all non requested/inflight. once you do that, the base path to the .hoodie is known to you and the name of the commit file can be gotten from HoodieInstant - then you can do a getFileStatus() if needed - it's cleaner this way rather than you doing indexOf and substring etc - these can break", "author": "n3nash", "createdAt": "2020-04-01T23:33:37Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CLI command to export various information from a HUDI dataset.\n+ */\n+@Component\n+public class ExportCommand implements CommandMarker {\n+\n+  @CliCommand(value = \"export instants\", help = \"Export Instants and their metadata from the Timeline\")\n+  public String showArchivedCommits(\n+      @CliOption(key = {\"limit\"}, help = \"Limit Instants\", unspecifiedDefaultValue = \"-1\") final Integer limit,\n+      @CliOption(key = {\"actions\"}, help = \"Comma seperated list of Instant actions to export\",\n+        unspecifiedDefaultValue = \"clean,commit,deltacommit,rollback,savepoint,restore\") final String filter,\n+      @CliOption(key = {\"desc\"}, help = \"Ordering\", unspecifiedDefaultValue = \"false\") final boolean descending,\n+      @CliOption(key = {\"localFolder\"}, help = \"Local Folder to export to\", mandatory = true) String localFolder)\n+      throws IOException {\n+\n+    final String basePath = HoodieCLI.getTableMetaClient().getBasePath();\n+    final Path archivePath = new Path(basePath + \"/.hoodie/.commits_.archive*\");\n+    final Path metaPath = new Path(basePath + \"/.hoodie/\");\n+    final Set<String> actionSet = new HashSet<String>(Arrays.asList(filter.split(\",\")));\n+    int numExports = limit == -1 ? Integer.MAX_VALUE : limit;\n+    int numCopied = 0;\n+\n+    if (! new File(localFolder).isDirectory()) {\n+      throw new RuntimeException(localFolder + \" is not a valid local directory\");\n+    }\n+\n+    // The non archived instants are of the format <instantTime>.<action>.<requested/inflight>. We only\n+    // want the completed ones which do not have the requested/inflight suffix.\n+    FileStatus[] statuses = FSUtils.getFs(basePath, HoodieCLI.conf).listStatus(metaPath);\n+    List<FileStatus> nonArchivedStatuses = Arrays.stream(statuses).filter(f -> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM3NjQ4Mw==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r403376483", "bodyText": "Good idea. It even makes the filtering easier.", "author": "prashantwason", "createdAt": "2020-04-03T23:03:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTk2OTgwNA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMDM3Mg==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r409110372", "bodyText": "Hi @prashantwason, we can use HoodieException here", "author": "lamberken", "createdAt": "2020-04-15T20:21:36Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CLI commands to export various information from a HUDI dataset.\n+ *\n+ * \"export instants\": Export Instants and their metadata from the Timeline to a local\n+ *                    directory specified by the parameter --localFolder\n+ *      The instants are exported in the json format.\n+ */\n+@Component\n+public class ExportCommand implements CommandMarker {\n+\n+  @CliCommand(value = \"export instants\", help = \"Export Instants and their metadata from the Timeline\")\n+  public String exportInstants(\n+      @CliOption(key = {\"limit\"}, help = \"Limit Instants\", unspecifiedDefaultValue = \"-1\") final Integer limit,\n+      @CliOption(key = {\"actions\"}, help = \"Comma seperated list of Instant actions to export\",\n+        unspecifiedDefaultValue = \"clean,commit,deltacommit,rollback,savepoint,restore\") final String filter,\n+      @CliOption(key = {\"desc\"}, help = \"Ordering\", unspecifiedDefaultValue = \"false\") final boolean descending,\n+      @CliOption(key = {\"localFolder\"}, help = \"Local Folder to export to\", mandatory = true) String localFolder)\n+      throws Exception {\n+\n+    final String basePath = HoodieCLI.getTableMetaClient().getBasePath();\n+    final Path archivePath = new Path(basePath + \"/.hoodie/.commits_.archive*\");\n+    final Set<String> actionSet = new HashSet<String>(Arrays.asList(filter.split(\",\")));\n+    int numExports = limit == -1 ? Integer.MAX_VALUE : limit;\n+    int numCopied = 0;\n+\n+    if (! new File(localFolder).isDirectory()) {\n+      throw new RuntimeException(localFolder + \" is not a valid local directory\");\n+    }\n+\n+    // The non archived instants can be listed from the Timeline.\n+    HoodieTimeline timeline = HoodieCLI.getTableMetaClient().getActiveTimeline().filterCompletedInstants()\n+        .filter(i -> actionSet.contains(i.getAction()));\n+    List<HoodieInstant> nonArchivedInstants = timeline.getInstants().collect(Collectors.toList());\n+\n+    // Archived instants are in the commit archive files\n+    FileStatus[] statuses = FSUtils.getFs(basePath, HoodieCLI.conf).globStatus(archivePath);\n+    List<FileStatus> archivedStatuses = Arrays.stream(statuses).sorted((f1, f2) -> (int)(f1.getModificationTime() - f2.getModificationTime())).collect(Collectors.toList());\n+\n+    if (descending) {\n+      Collections.reverse(nonArchivedInstants);\n+      numCopied = copyNonArchivedInstants(nonArchivedInstants, numExports, localFolder);\n+      if (numCopied < numExports) {\n+        Collections.reverse(archivedStatuses);\n+        numCopied += copyArchivedInstants(archivedStatuses, actionSet, numExports - numCopied, localFolder);\n+      }\n+    } else {\n+      numCopied = copyArchivedInstants(archivedStatuses, actionSet, numExports, localFolder);\n+      if (numCopied < numExports) {\n+        numCopied += copyNonArchivedInstants(nonArchivedInstants, numExports - numCopied, localFolder);\n+      }\n+    }\n+\n+    return \"Exported \" + numCopied + \" Instants to \" + localFolder;\n+  }\n+\n+  private int copyArchivedInstants(List<FileStatus> statuses, Set<String> actionSet, int limit, String localFolder) throws Exception {\n+    int copyCount = 0;\n+\n+    for (FileStatus fs : statuses) {\n+      // read the archived file\n+      Reader reader = HoodieLogFormat.newReader(FSUtils.getFs(HoodieCLI.getTableMetaClient().getBasePath(), HoodieCLI.conf),\n+          new HoodieLogFile(fs.getPath()), HoodieArchivedMetaEntry.getClassSchema());\n+\n+      // read the avro blocks\n+      while (reader.hasNext() && copyCount < limit) {\n+        HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();\n+        for (IndexedRecord ir : blk.getRecords()) {\n+          // Archived instants are saved as arvo encoded HoodieArchivedMetaEntry records. We need to get the\n+          // metadata record from the entry and convert it to json.\n+          HoodieArchivedMetaEntry archiveEntryRecord = (HoodieArchivedMetaEntry) SpecificData.get()\n+              .deepCopy(HoodieArchivedMetaEntry.SCHEMA$, ir);\n+\n+          final String action = archiveEntryRecord.get(\"actionType\").toString();\n+          if (!actionSet.contains(action)) {\n+            continue;\n+          }\n+\n+          GenericRecord metadata = null;\n+          switch (action) {\n+            case HoodieTimeline.CLEAN_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCleanMetadata();\n+              break;\n+            case HoodieTimeline.COMMIT_ACTION:\n+            case HoodieTimeline.DELTA_COMMIT_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCommitMetadata();\n+              break;\n+            case HoodieTimeline.ROLLBACK_ACTION:\n+              metadata = archiveEntryRecord.getHoodieRollbackMetadata();\n+              break;\n+            case HoodieTimeline.SAVEPOINT_ACTION:\n+              metadata = archiveEntryRecord.getHoodieSavePointMetadata();\n+              break;\n+            case HoodieTimeline.COMPACTION_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCompactionMetadata();\n+              break;\n+            default:\n+              throw new RuntimeException(\"Unknown type of action \" + action);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE1ODQ2Nw==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r409158467", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-04-15T21:58:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMDM3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMDU3OQ==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r409110579", "bodyText": "Ditto.", "author": "lamberken", "createdAt": "2020-04-15T20:21:58Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/ExportCommand.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieArchivedMetaEntry;\n+import org.apache.hudi.avro.model.HoodieCleanMetadata;\n+import org.apache.hudi.avro.model.HoodieRollbackMetadata;\n+import org.apache.hudi.avro.model.HoodieSavepointMetadata;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.HoodieLogFormat.Reader;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.springframework.shell.core.CommandMarker;\n+import org.springframework.shell.core.annotation.CliCommand;\n+import org.springframework.shell.core.annotation.CliOption;\n+import org.springframework.stereotype.Component;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * CLI commands to export various information from a HUDI dataset.\n+ *\n+ * \"export instants\": Export Instants and their metadata from the Timeline to a local\n+ *                    directory specified by the parameter --localFolder\n+ *      The instants are exported in the json format.\n+ */\n+@Component\n+public class ExportCommand implements CommandMarker {\n+\n+  @CliCommand(value = \"export instants\", help = \"Export Instants and their metadata from the Timeline\")\n+  public String exportInstants(\n+      @CliOption(key = {\"limit\"}, help = \"Limit Instants\", unspecifiedDefaultValue = \"-1\") final Integer limit,\n+      @CliOption(key = {\"actions\"}, help = \"Comma seperated list of Instant actions to export\",\n+        unspecifiedDefaultValue = \"clean,commit,deltacommit,rollback,savepoint,restore\") final String filter,\n+      @CliOption(key = {\"desc\"}, help = \"Ordering\", unspecifiedDefaultValue = \"false\") final boolean descending,\n+      @CliOption(key = {\"localFolder\"}, help = \"Local Folder to export to\", mandatory = true) String localFolder)\n+      throws Exception {\n+\n+    final String basePath = HoodieCLI.getTableMetaClient().getBasePath();\n+    final Path archivePath = new Path(basePath + \"/.hoodie/.commits_.archive*\");\n+    final Set<String> actionSet = new HashSet<String>(Arrays.asList(filter.split(\",\")));\n+    int numExports = limit == -1 ? Integer.MAX_VALUE : limit;\n+    int numCopied = 0;\n+\n+    if (! new File(localFolder).isDirectory()) {\n+      throw new RuntimeException(localFolder + \" is not a valid local directory\");\n+    }\n+\n+    // The non archived instants can be listed from the Timeline.\n+    HoodieTimeline timeline = HoodieCLI.getTableMetaClient().getActiveTimeline().filterCompletedInstants()\n+        .filter(i -> actionSet.contains(i.getAction()));\n+    List<HoodieInstant> nonArchivedInstants = timeline.getInstants().collect(Collectors.toList());\n+\n+    // Archived instants are in the commit archive files\n+    FileStatus[] statuses = FSUtils.getFs(basePath, HoodieCLI.conf).globStatus(archivePath);\n+    List<FileStatus> archivedStatuses = Arrays.stream(statuses).sorted((f1, f2) -> (int)(f1.getModificationTime() - f2.getModificationTime())).collect(Collectors.toList());\n+\n+    if (descending) {\n+      Collections.reverse(nonArchivedInstants);\n+      numCopied = copyNonArchivedInstants(nonArchivedInstants, numExports, localFolder);\n+      if (numCopied < numExports) {\n+        Collections.reverse(archivedStatuses);\n+        numCopied += copyArchivedInstants(archivedStatuses, actionSet, numExports - numCopied, localFolder);\n+      }\n+    } else {\n+      numCopied = copyArchivedInstants(archivedStatuses, actionSet, numExports, localFolder);\n+      if (numCopied < numExports) {\n+        numCopied += copyNonArchivedInstants(nonArchivedInstants, numExports - numCopied, localFolder);\n+      }\n+    }\n+\n+    return \"Exported \" + numCopied + \" Instants to \" + localFolder;\n+  }\n+\n+  private int copyArchivedInstants(List<FileStatus> statuses, Set<String> actionSet, int limit, String localFolder) throws Exception {\n+    int copyCount = 0;\n+\n+    for (FileStatus fs : statuses) {\n+      // read the archived file\n+      Reader reader = HoodieLogFormat.newReader(FSUtils.getFs(HoodieCLI.getTableMetaClient().getBasePath(), HoodieCLI.conf),\n+          new HoodieLogFile(fs.getPath()), HoodieArchivedMetaEntry.getClassSchema());\n+\n+      // read the avro blocks\n+      while (reader.hasNext() && copyCount < limit) {\n+        HoodieAvroDataBlock blk = (HoodieAvroDataBlock) reader.next();\n+        for (IndexedRecord ir : blk.getRecords()) {\n+          // Archived instants are saved as arvo encoded HoodieArchivedMetaEntry records. We need to get the\n+          // metadata record from the entry and convert it to json.\n+          HoodieArchivedMetaEntry archiveEntryRecord = (HoodieArchivedMetaEntry) SpecificData.get()\n+              .deepCopy(HoodieArchivedMetaEntry.SCHEMA$, ir);\n+\n+          final String action = archiveEntryRecord.get(\"actionType\").toString();\n+          if (!actionSet.contains(action)) {\n+            continue;\n+          }\n+\n+          GenericRecord metadata = null;\n+          switch (action) {\n+            case HoodieTimeline.CLEAN_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCleanMetadata();\n+              break;\n+            case HoodieTimeline.COMMIT_ACTION:\n+            case HoodieTimeline.DELTA_COMMIT_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCommitMetadata();\n+              break;\n+            case HoodieTimeline.ROLLBACK_ACTION:\n+              metadata = archiveEntryRecord.getHoodieRollbackMetadata();\n+              break;\n+            case HoodieTimeline.SAVEPOINT_ACTION:\n+              metadata = archiveEntryRecord.getHoodieSavePointMetadata();\n+              break;\n+            case HoodieTimeline.COMPACTION_ACTION:\n+              metadata = archiveEntryRecord.getHoodieCompactionMetadata();\n+              break;\n+            default:\n+              throw new RuntimeException(\"Unknown type of action \" + action);\n+          }\n+\n+          final String instantTime = archiveEntryRecord.get(\"commitTime\").toString();\n+          final String outPath = localFolder + Path.SEPARATOR + instantTime + \".\" + action;\n+          writeToFile(outPath, HoodieAvroUtils.avroToJson(metadata, true));\n+          if (++copyCount == limit) {\n+            break;\n+          }\n+        }\n+      }\n+\n+      reader.close();\n+    }\n+\n+    return copyCount;\n+  }\n+\n+  private int copyNonArchivedInstants(List<HoodieInstant> instants, int limit, String localFolder) throws Exception {\n+    int copyCount = 0;\n+\n+    if (instants.isEmpty()) {\n+      return limit;\n+    }\n+    final Logger LOG = LogManager.getLogger(ExportCommand.class);\n+\n+    final HoodieTableMetaClient metaClient = HoodieCLI.getTableMetaClient();\n+    final HoodieActiveTimeline timeline = metaClient.getActiveTimeline();\n+    for (HoodieInstant instant : instants) {\n+      String localPath = localFolder + File.separator + instant.getFileName();\n+\n+      byte[] data = null;\n+      switch (instant.getAction()) {\n+        case HoodieTimeline.CLEAN_ACTION: {\n+          HoodieCleanMetadata metadata = TimelineMetadataUtils.deserializeHoodieCleanMetadata(\n+              timeline.getInstantDetails(instant).get());\n+          data = HoodieAvroUtils.avroToJson(metadata, true);\n+          break;\n+        }\n+        case HoodieTimeline.DELTA_COMMIT_ACTION:\n+        case HoodieTimeline.COMMIT_ACTION:\n+        case HoodieTimeline.COMPACTION_ACTION: {\n+          // Already in json format\n+          data = timeline.getInstantDetails(instant).get();\n+          break;\n+        }\n+        case HoodieTimeline.ROLLBACK_ACTION: {\n+          HoodieRollbackMetadata metadata = TimelineMetadataUtils.deserializeHoodieRollbackMetadata(\n+              timeline.getInstantDetails(instant).get());\n+          data = HoodieAvroUtils.avroToJson(metadata, true);\n+          break;\n+        }\n+        case HoodieTimeline.SAVEPOINT_ACTION: {\n+          HoodieSavepointMetadata metadata = TimelineMetadataUtils.deserializeHoodieSavepointMetadata(\n+              timeline.getInstantDetails(instant).get());\n+          data = HoodieAvroUtils.avroToJson(metadata, true);\n+          break;\n+        }\n+        default: {\n+          throw new RuntimeException(\"Unknown type of action \" + instant.getAction());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTE1ODUyNw==", "url": "https://github.com/apache/hudi/pull/1476#discussion_r409158527", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-04-15T21:58:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMDU3OQ=="}], "type": "inlineReview"}, {"oid": "2081e19840a4e3d9d6e7eefe51f90d6ea3a9e558", "url": "https://github.com/apache/hudi/commit/2081e19840a4e3d9d6e7eefe51f90d6ea3a9e558", "message": "[HUDI-757] Added hudi-cli command to export metadata of Instants.\n\nExample:\nhudi:db.table-> export instants --localFolder /tmp/ --limit 5 --actions clean,rollback,commit --desc false", "committedDate": "2020-04-15T21:57:34Z", "type": "commit"}, {"oid": "2081e19840a4e3d9d6e7eefe51f90d6ea3a9e558", "url": "https://github.com/apache/hudi/commit/2081e19840a4e3d9d6e7eefe51f90d6ea3a9e558", "message": "[HUDI-757] Added hudi-cli command to export metadata of Instants.\n\nExample:\nhudi:db.table-> export instants --localFolder /tmp/ --limit 5 --actions clean,rollback,commit --desc false", "committedDate": "2020-04-15T21:57:34Z", "type": "forcePushed"}]}