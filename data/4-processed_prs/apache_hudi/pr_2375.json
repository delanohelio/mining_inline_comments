{"pr_number": 2375, "pr_title": "[HUDI-1332] Introduce FlinkHoodieBloomIndex to hudi-flink-client", "pr_createdAt": "2020-12-24T07:46:38Z", "pr_url": "https://github.com/apache/hudi/pull/2375", "timeline": [{"oid": "34558ae5b7c9644c3839b1c18222954e3b2aa3a8", "url": "https://github.com/apache/hudi/commit/34558ae5b7c9644c3839b1c18222954e3b2aa3a8", "message": "[HUDI] Add bloom index for hudi-flink-client", "committedDate": "2020-12-24T07:41:48Z", "type": "commit"}, {"oid": "90710257a1f17793bb54b81db06fdfaf9b8fc41e", "url": "https://github.com/apache/hudi/commit/90710257a1f17793bb54b81db06fdfaf9b8fc41e", "message": "bug fix", "committedDate": "2020-12-29T03:24:19Z", "type": "commit"}, {"oid": "4907100670d7eed838fa55be03e483cee3a64a47", "url": "https://github.com/apache/hudi/commit/4907100670d7eed838fa55be03e483cee3a64a47", "message": "modify for check style", "committedDate": "2020-12-29T06:18:06Z", "type": "commit"}, {"oid": "b72f0a113bfd8df93e0b90a3266b79e20d739fc7", "url": "https://github.com/apache/hudi/commit/b72f0a113bfd8df93e0b90a3266b79e20d739fc7", "message": "[hudi-1332] 1\u3001modify for check-style", "committedDate": "2020-12-29T06:41:42Z", "type": "commit"}, {"oid": "df6163de8d15be7c478057418f09123619db0543", "url": "https://github.com/apache/hudi/commit/df6163de8d15be7c478057418f09123619db0543", "message": "[hudi-1332] 1\u3001modify for check-style", "committedDate": "2020-12-29T07:34:09Z", "type": "commit"}, {"oid": "bf1c47f2a3176cae5cb7504d2a0eb5f3965cd292", "url": "https://github.com/apache/hudi/commit/bf1c47f2a3176cae5cb7504d2a0eb5f3965cd292", "message": "[hudi-1332] 1\u3001modify for check-style", "committedDate": "2020-12-29T08:54:41Z", "type": "commit"}, {"oid": "1d7e9f6d91a33e240baabe04d856770d1bbcb9f5", "url": "https://github.com/apache/hudi/commit/1d7e9f6d91a33e240baabe04d856770d1bbcb9f5", "message": "[hudi-1332] 1\u3001add unit tests", "committedDate": "2021-01-06T08:35:50Z", "type": "commit"}, {"oid": "96d0e06a5eff5edec528be55fe26e0b97f58667e", "url": "https://github.com/apache/hudi/commit/96d0e06a5eff5edec528be55fe26e0b97f58667e", "message": "[hudi-1332] 1\u3001add unit tests", "committedDate": "2021-01-06T08:53:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjY0ODM2NA==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r552648364", "bodyText": "can we move all of org.apache.hudi.testutils.FlinkHoodieClientTestHarness style to the import?", "author": "garyli1019", "createdAt": "2021-01-06T14:12:27Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/index/bloom/TestFlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.*;\n+\n+public class TestFlinkHoodieBloomIndex extends org.apache.hudi.testutils.FlinkHoodieClientTestHarness {", "originalCommit": "96d0e06a5eff5edec528be55fe26e0b97f58667e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjY0ODY4NQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r552648685", "bodyText": "ditto", "author": "garyli1019", "createdAt": "2021-01-06T14:12:55Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/index/bloom/TestFlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.*;\n+\n+public class TestFlinkHoodieBloomIndex extends org.apache.hudi.testutils.FlinkHoodieClientTestHarness {\n+\n+  private static final org.apache.avro.Schema SCHEMA = getSchemaFromResource(TestFlinkHoodieBloomIndex.class, \"/exampleSchema.avsc\", true);\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with rangePruning={0}, treeFiltering={1}, bucketizedChecking={2}\";\n+\n+  public static java.util.stream.Stream<org.junit.jupiter.params.provider.Arguments> configParams() {\n+    Object[][] data =\n+        new Object[][] {{true, true, true}, {false, true, true}, {true, true, false}, {true, false, true}};\n+    return java.util.stream.Stream.of(data).map(org.junit.jupiter.params.provider.Arguments::of);\n+  }\n+\n+  @org.junit.jupiter.api.BeforeEach\n+  public void setUp() throws Exception {\n+    initFlinkContexts();\n+    initPath();\n+    initFileSystem();\n+    // We have some records to be tagged (two different partitions)\n+    initMetaClient();\n+  }\n+\n+  @org.junit.jupiter.api.AfterEach\n+  public void tearDown() throws Exception {\n+    cleanupResources();\n+  }\n+\n+  private org.apache.hudi.config.HoodieWriteConfig makeConfig(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) {\n+    return org.apache.hudi.config.HoodieWriteConfig.newBuilder().withPath(basePath)\n+        .withIndexConfig(org.apache.hudi.config.HoodieIndexConfig.newBuilder().bloomIndexPruneByRanges(rangePruning)\n+            .bloomIndexTreebasedFilter(treeFiltering).bloomIndexBucketizedChecking(bucketizedChecking)\n+            .bloomIndexKeysPerBucket(2).build())\n+        .build();\n+  }\n+\n+  @org.junit.jupiter.params.ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @org.junit.jupiter.params.provider.MethodSource(\"configParams\")\n+  public void testLoadInvolvedFiles(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) throws Exception {\n+    org.apache.hudi.config.HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n+    FlinkHoodieBloomIndex index = new FlinkHoodieBloomIndex(config);\n+    HoodieTable hoodieTable = HoodieFlinkTable.create(config, context, metaClient);\n+    org.apache.hudi.testutils.HoodieFlinkWriteableTestTable testTable = org.apache.hudi.testutils.HoodieFlinkWriteableTestTable.of(hoodieTable, SCHEMA);\n+\n+    // Create some partitions, and put some files\n+    // \"2016/01/21\": 0 file\n+    // \"2016/04/01\": 1 file (2_0_20160401010101.parquet)\n+    // \"2015/03/12\": 3 files (1_0_20150312101010.parquet, 3_0_20150312101010.parquet, 4_0_20150312101010.parquet)\n+    testTable.withPartitionMetaFiles(\"2016/01/21\", \"2016/04/01\", \"2015/03/12\");\n+\n+    RawTripTestPayload rowChange1 =\n+        new RawTripTestPayload(\"{\\\"_row_key\\\":\\\"000\\\",\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\");\n+    HoodieRecord record1 =\n+        new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n+    RawTripTestPayload rowChange2 =\n+        new RawTripTestPayload(\"{\\\"_row_key\\\":\\\"001\\\",\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\");\n+    HoodieRecord record2 =\n+        new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2);\n+    RawTripTestPayload rowChange3 =\n+        new RawTripTestPayload(\"{\\\"_row_key\\\":\\\"002\\\",\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\");\n+    HoodieRecord record3 =\n+        new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3);\n+    RawTripTestPayload rowChange4 =\n+        new RawTripTestPayload(\"{\\\"_row_key\\\":\\\"003\\\",\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\");\n+    HoodieRecord record4 =\n+        new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);\n+\n+    java.util.List<String> partitions = asList(\"2016/01/21\", \"2016/04/01\", \"2015/03/12\");\n+    java.util.List<scala.Tuple2<String, BloomIndexFileInfo>> filesList = index.loadInvolvedFiles(partitions, context, hoodieTable);\n+    // Still 0, as no valid commit\n+    assertEquals(0, filesList.size());\n+\n+    testTable.addCommit(\"20160401010101\").withInserts(\"2016/04/01\", \"2\");\n+    testTable.addCommit(\"20150312101010\").withInserts(\"2015/03/12\", \"1\")\n+        .withInserts(\"2015/03/12\", \"3\", record1)\n+        .withInserts(\"2015/03/12\", \"4\", record2, record3, record4);\n+    metaClient.reloadActiveTimeline();\n+\n+    filesList = index.loadInvolvedFiles(partitions, context, hoodieTable);\n+    assertEquals(4, filesList.size());\n+\n+    if (rangePruning) {\n+      // these files will not have the key ranges\n+      assertNull(filesList.get(0)._2().getMaxRecordKey());\n+      assertNull(filesList.get(0)._2().getMinRecordKey());\n+      assertFalse(filesList.get(1)._2().hasKeyRanges());\n+      assertNotNull(filesList.get(2)._2().getMaxRecordKey());\n+      assertNotNull(filesList.get(2)._2().getMinRecordKey());\n+      assertTrue(filesList.get(3)._2().hasKeyRanges());\n+\n+      // no longer sorted, but should have same files.\n+\n+      java.util.List<scala.Tuple2<String, BloomIndexFileInfo>> expected =\n+          asList(new Tuple2<>(\"2016/04/01\", new BloomIndexFileInfo(\"2\")),\n+              new Tuple2<>(\"2015/03/12\", new BloomIndexFileInfo(\"1\")),\n+              new Tuple2<>(\"2015/03/12\", new BloomIndexFileInfo(\"3\", \"000\", \"000\")),\n+              new Tuple2<>(\"2015/03/12\", new BloomIndexFileInfo(\"4\", \"001\", \"003\")));\n+      assertEquals(expected, filesList);\n+    }\n+  }\n+\n+  @org.junit.jupiter.params.ParameterizedTest(name = TEST_NAME_WITH_PARAMS)\n+  @org.junit.jupiter.params.provider.MethodSource(\"configParams\")\n+  public void testRangePruning(boolean rangePruning, boolean treeFiltering, boolean bucketizedChecking) {\n+    org.apache.hudi.config.HoodieWriteConfig config = makeConfig(rangePruning, treeFiltering, bucketizedChecking);\n+    FlinkHoodieBloomIndex index = new FlinkHoodieBloomIndex(config);\n+\n+    final java.util.Map<String, java.util.List<BloomIndexFileInfo>> partitionToFileIndexInfo = new HashMap<>();\n+    partitionToFileIndexInfo.put(\"2017/10/22\",\n+        asList(new BloomIndexFileInfo(\"f1\"), new BloomIndexFileInfo(\"f2\", \"000\", \"000\"),\n+            new BloomIndexFileInfo(\"f3\", \"001\", \"003\"), new BloomIndexFileInfo(\"f4\", \"002\", \"007\"),\n+            new BloomIndexFileInfo(\"f5\", \"009\", \"010\")));\n+\n+    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n+    asList(new Tuple2<>(\"2017/10/22\", \"003\"), new Tuple2<>(\"2017/10/22\", \"002\"),\n+            new Tuple2<>(\"2017/10/22\", \"005\"), new Tuple2<>(\"2017/10/22\", \"004\"))\n+            .forEach( t -> {\n+                List<String> recordKeyList = partitionRecordKeyMap.getOrDefault(t._1, new ArrayList<>());\n+                recordKeyList.add(t._2);\n+                partitionRecordKeyMap.put(t._1, recordKeyList);\n+            });\n+\n+    List<scala.Tuple2<String, HoodieKey>> comparisonKeyList =\n+        index.explodeRecordsWithFileComparisons(partitionToFileIndexInfo, partitionRecordKeyMap);\n+\n+    assertEquals(10, comparisonKeyList.size());\n+    java.util.Map<String, java.util.List<String>> recordKeyToFileComps = comparisonKeyList.stream()\n+        .collect(java.util.stream.Collectors.groupingBy(t -> t._2.getRecordKey(), java.util.stream.Collectors.mapping(t -> t._1, java.util.stream.Collectors.toList())));\n+\n+    assertEquals(4, recordKeyToFileComps.size());\n+    assertEquals(new java.util.HashSet<>(asList(\"f1\", \"f3\", \"f4\")), new java.util.HashSet<>(recordKeyToFileComps.get(\"002\")));\n+    assertEquals(new java.util.HashSet<>(asList(\"f1\", \"f3\", \"f4\")), new java.util.HashSet<>(recordKeyToFileComps.get(\"003\")));\n+    assertEquals(new java.util.HashSet<>(asList(\"f1\", \"f4\")), new java.util.HashSet<>(recordKeyToFileComps.get(\"004\")));\n+    assertEquals(new java.util.HashSet<>(asList(\"f1\", \"f4\")), new java.util.HashSet<>(recordKeyToFileComps.get(\"005\")));\n+  }\n+\n+  @org.junit.jupiter.api.Test\n+  public void testCheckUUIDsAgainstOneFile() throws Exception {\n+    final String partition = \"2016/01/31\";\n+    // Create some records to use\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"3eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    String recordStr4 = \"{\\\"_row_key\\\":\\\"4eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":32}\";\n+    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n+    HoodieRecord record1 =\n+        new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1);\n+    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n+    HoodieRecord record2 =\n+        new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2);\n+    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n+    HoodieRecord record3 =\n+        new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3);\n+    RawTripTestPayload rowChange4 = new RawTripTestPayload(recordStr4);\n+    HoodieRecord record4 =\n+        new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);\n+\n+    // We write record1, record2 to a parquet file, but the bloom filter contains (record1,\n+    // record2, record3).\n+    org.apache.hudi.common.bloom.BloomFilter filter = org.apache.hudi.common.bloom.BloomFilterFactory.createBloomFilter(10000, 0.0000001, -1, org.apache.hudi.common.bloom.BloomFilterTypeCode.SIMPLE.name());\n+    filter.add(record3.getRecordKey());\n+    org.apache.hudi.testutils.HoodieFlinkWriteableTestTable testTable = org.apache.hudi.testutils.HoodieFlinkWriteableTestTable.of(metaClient, SCHEMA, filter);\n+    String fileId = testTable.addCommit(\"000\").getFileIdWithInserts(partition, record1, record2);\n+    String filename = testTable.getBaseFileNameById(fileId);\n+\n+    // The bloom filter contains 3 records\n+    assertTrue(filter.mightContain(record1.getRecordKey()));\n+    assertTrue(filter.mightContain(record2.getRecordKey()));\n+    assertTrue(filter.mightContain(record3.getRecordKey()));\n+    assertFalse(filter.mightContain(record4.getRecordKey()));\n+\n+    // Compare with file\n+    java.util.List<String> uuids =\n+        asList(record1.getRecordKey(), record2.getRecordKey(), record3.getRecordKey(), record4.getRecordKey());\n+\n+    org.apache.hudi.config.HoodieWriteConfig config = org.apache.hudi.config.HoodieWriteConfig.newBuilder().withPath(basePath).build();\n+    HoodieFlinkTable table = HoodieFlinkTable.create(config, context, metaClient);\n+    org.apache.hudi.io.HoodieKeyLookupHandle keyHandle = new org.apache.hudi.io.HoodieKeyLookupHandle<>(config, table, Pair.of(partition, fileId));\n+    java.util.List<String> results = keyHandle.checkCandidatesAgainstFile(hadoopConf, uuids,\n+        new org.apache.hadoop.fs.Path(java.nio.file.Paths.get(basePath, partition, filename).toString()));\n+    assertEquals(results.size(), 2);\n+    assertTrue(results.get(0).equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\")\n+        || results.get(1).equals(\"1eb5b87a-1feh-4edd-87b4-6ec96dc405a0\"));\n+    assertTrue(results.get(0).equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\")\n+        || results.get(1).equals(\"2eb5b87b-1feu-4edd-87b4-6ec96dc405a0\"));\n+    // TODO(vc): Need more coverage on actual filenames\n+    // assertTrue(results.get(0)._2().equals(filename));\n+    // assertTrue(results.get(1)._2().equals(filename));\n+  }\n+\n+  @org.junit.jupiter.params.ParameterizedTest(name = TEST_NAME_WITH_PARAMS)", "originalCommit": "96d0e06a5eff5edec528be55fe26e0b97f58667e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjY1MDI2Ng==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r552650266", "bodyText": "Can we use the existing HoodieFlinkClientTestHarness. This was merged recently", "author": "garyli1019", "createdAt": "2021-01-06T14:14:38Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/FlinkHoodieClientTestHarness.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.client.HoodieFlinkWriteClient;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.HoodieCommonTestHarness;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * The test harness for resource initialization and cleanup.\n+ */\n+public abstract class FlinkHoodieClientTestHarness extends HoodieCommonTestHarness implements java.io.Serializable {", "originalCommit": "96d0e06a5eff5edec528be55fe26e0b97f58667e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjY1MDg3OA==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r552650878", "bodyText": "ditto, please move all the full package info to import", "author": "garyli1019", "createdAt": "2021-01-06T14:15:31Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java", "diffHunk": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import static org.apache.hudi.common.testutils.FileCreateUtils.baseFileName;\n+\n+public class HoodieFlinkWriteableTestTable extends org.apache.hudi.common.testutils.HoodieTestTable {\n+  private static final org.apache.log4j.Logger LOG = org.apache.log4j.LogManager.getLogger(org.apache.hudi.testutils.HoodieFlinkWriteableTestTable.class);\n+\n+  private final org.apache.avro.Schema schema;\n+  private final org.apache.hudi.common.bloom.BloomFilter filter;", "originalCommit": "96d0e06a5eff5edec528be55fe26e0b97f58667e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "683217767c4ba267f2c644b05ab4a6017af6a57b", "url": "https://github.com/apache/hudi/commit/683217767c4ba267f2c644b05ab4a6017af6a57b", "message": "Merge branch 'master' into flink-bloom-index", "committedDate": "2021-01-07T02:25:59Z", "type": "commit"}, {"oid": "500b6c7038fee2df68d0c33a3b9ea54a986ebb15", "url": "https://github.com/apache/hudi/commit/500b6c7038fee2df68d0c33a3b9ea54a986ebb15", "message": "opt unit tests", "committedDate": "2021-01-07T03:03:19Z", "type": "commit"}, {"oid": "79562268e1061f546fb83f1b56596f111d06d3bd", "url": "https://github.com/apache/hudi/commit/79562268e1061f546fb83f1b56596f111d06d3bd", "message": "[hudi-1332] 1\u3001modify for check-style", "committedDate": "2021-01-07T09:33:08Z", "type": "commit"}, {"oid": "36b92dbd5e6d6fca020a825e3d93681fe5c93ee4", "url": "https://github.com/apache/hudi/commit/36b92dbd5e6d6fca020a825e3d93681fe5c93ee4", "message": "[hudi-1332] 1\u3001modify for check-style", "committedDate": "2021-01-07T10:02:42Z", "type": "commit"}, {"oid": "3ffbc66f667019c886ae22940ebb5a012afdb08d", "url": "https://github.com/apache/hudi/commit/3ffbc66f667019c886ae22940ebb5a012afdb08d", "message": "[hudi-1332] 1\u3001add unit tests", "committedDate": "2021-01-08T01:57:46Z", "type": "commit"}, {"oid": "6aa341884e0ca6a28ac7a81b61de9a491be9df32", "url": "https://github.com/apache/hudi/commit/6aa341884e0ca6a28ac7a81b61de9a491be9df32", "message": "Merge branch 'master' into flink-bloom-index", "committedDate": "2021-01-08T07:06:01Z", "type": "commit"}, {"oid": "095fd3de5c1ec02717b5b41aa779340b8882e589", "url": "https://github.com/apache/hudi/commit/095fd3de5c1ec02717b5b41aa779340b8882e589", "message": "[opt] sync the latest code from master", "committedDate": "2021-01-08T07:10:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3MzAyMA==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554573020", "bodyText": "This class could extend HoodieWriteableTestTable. Can we directly extend from there?", "author": "garyli1019", "createdAt": "2021-01-10T14:10:30Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.HoodieAvroWriteSupport;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.bloom.BloomFilterTypeCode;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;\n+import org.apache.hudi.common.testutils.FileCreateUtils;\n+import org.apache.hudi.common.testutils.HoodieTestTable;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.io.storage.HoodieAvroParquetConfig;\n+import org.apache.hudi.io.storage.HoodieParquetWriter;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.common.testutils.FileCreateUtils.baseFileName;\n+\n+public class HoodieFlinkWriteableTestTable extends HoodieTestTable {", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3MzIwOQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554573209", "bodyText": "Thanks for adding the tests! Maybe another TODO comment here, to merge code with Spark Bloom index tests.", "author": "garyli1019", "createdAt": "2021-01-10T14:12:15Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/index/bloom/TestFlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,464 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.bloom.BloomFilterTypeCode;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieFlinkClientTestHarness;\n+import org.apache.hudi.testutils.HoodieFlinkWriteableTestTable;\n+\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.UUID.randomUUID;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit test against FlinkHoodieBloomIndex.\n+ */\n+public class TestFlinkHoodieBloomIndex extends HoodieFlinkClientTestHarness {", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3MzU0Mw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554573543", "bodyText": "nit: we usually follow the import order in hudi -> 3rd party packages -> java -> scala ->static.", "author": "garyli1019", "createdAt": "2021-01-10T14:14:46Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import com.beust.jcommander.internal.Lists;", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3NjMyNw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554576327", "bodyText": "Duplicate codes with Spark, but it's ok for now. We need to do some refactoring work later. Can we add a TODO in the comment session as a reminder?", "author": "garyli1019", "createdAt": "2021-01-10T14:36:23Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import com.beust.jcommander.internal.Lists;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.MetadataNotFoundException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieRangeInfoHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.stream.Collectors.mapping;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class FlinkHoodieBloomIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkHoodieBloomIndex.class);\n+\n+  public FlinkHoodieBloomIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records, HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n+    // Step 1: Extract out thinner Map of (partitionPath, recordKey)\n+    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n+    records.forEach(record -> {\n+      if (partitionRecordKeyMap.containsKey(record.getPartitionPath())) {\n+        partitionRecordKeyMap.get(record.getPartitionPath()).add(record.getRecordKey());\n+      } else {\n+        List<String> recordKeys = Lists.newArrayList();\n+        recordKeys.add(record.getRecordKey());\n+        partitionRecordKeyMap.put(record.getPartitionPath(), recordKeys);\n+      }\n+    });\n+\n+    // Step 2: Lookup indexes for all the partition/recordkey pair\n+    Map<HoodieKey, HoodieRecordLocation> keyFilenamePairMap =\n+        lookupIndex(partitionRecordKeyMap, context, hoodieTable);\n+\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairMap.values().size();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    List<HoodieRecord<T>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairMap, records);\n+\n+    return taggedRecords;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private Map<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      Map<String, List<String>> partitionRecordKeyMap, final HoodieEngineContext context,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = new HashMap<>();\n+    partitionRecordKeyMap.keySet().forEach(k -> recordsPerPartition.put(k, Long.valueOf(partitionRecordKeyMap.get(k).size())));\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, BloomIndexFileInfo>> fileInfoList =\n+        loadInvolvedFiles(affectedPartitionPathList, context, hoodieTable);\n+    final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo =\n+        fileInfoList.stream().collect(groupingBy(Tuple2::_1, mapping(Tuple2::_2, toList())));\n+\n+    // Step 3: Obtain a List, for each incoming record, that already exists, with the file id,\n+    // that contains it.\n+    List<Tuple2<String, HoodieKey>> fileComparisons =\n+            explodeRecordsWithFileComparisons(partitionToFileInfo, partitionRecordKeyMap);\n+    return findMatchingFilesForRecordKeys(fileComparisons, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair List.\n+   */\n+  List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3OTUyNw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554579527", "bodyText": "I think we can wrap this up and reuse it in the HoodieBloomIndexCheckFunction from spark client, so we can move this into the hudi-client-common, but let's add a TODO comment for now.", "author": "garyli1019", "createdAt": "2021-01-10T15:00:32Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/HoodieFlinkBloomIndexCheckFunction.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.client.utils.LazyIterableIterator;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieKeyLookupHandle.KeyLookupResult;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import scala.Tuple2;\n+\n+import java.util.function.Function;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+/**\n+ * Function performing actual checking of List partition containing (fileId, hoodieKeys) against the actual files.\n+ */\n+public class HoodieFlinkBloomIndexCheckFunction\n+        implements Function<Iterator<Tuple2<String, HoodieKey>>, Iterator<List<KeyLookupResult>>> {\n+\n+  private final HoodieTable hoodieTable;\n+\n+  private final HoodieWriteConfig config;\n+\n+  public HoodieFlinkBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteConfig config) {\n+    this.hoodieTable = hoodieTable;\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public Iterator<List<KeyLookupResult>> apply(Iterator<Tuple2<String, HoodieKey>> fileParitionRecordKeyTripletItr) {", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU3OTg2OQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554579869", "bodyText": "nit: please move packages to import", "author": "garyli1019", "createdAt": "2021-01-10T15:03:28Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/index/bloom/TestFlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,464 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.bloom.BloomFilterTypeCode;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.testutils.RawTripTestPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.table.HoodieFlinkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieFlinkClientTestHarness;\n+import org.apache.hudi.testutils.HoodieFlinkWriteableTestTable;\n+\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n+\n+import scala.Tuple2;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.UUID.randomUUID;\n+import static org.apache.hudi.common.testutils.SchemaTestUtil.getSchemaFromResource;\n+import static org.junit.jupiter.api.Assertions.assertDoesNotThrow;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertFalse;\n+import static org.junit.jupiter.api.Assertions.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertNull;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Unit test against FlinkHoodieBloomIndex.\n+ */\n+public class TestFlinkHoodieBloomIndex extends HoodieFlinkClientTestHarness {\n+\n+  private static final Schema SCHEMA = getSchemaFromResource(TestFlinkHoodieBloomIndex.class, \"/exampleSchema.avsc\", true);\n+  private static final String TEST_NAME_WITH_PARAMS = \"[{index}] Test with rangePruning={0}, treeFiltering={1}, bucketizedChecking={2}\";\n+\n+  public static java.util.stream.Stream<org.junit.jupiter.params.provider.Arguments> configParams() {", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4MTMxMA==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554581310", "bodyText": "let's remove this, we don't have to call this every time.", "author": "garyli1019", "createdAt": "2021-01-10T15:15:10Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -58,6 +83,15 @@ public void setTestMethodName(TestInfo testInfo) {\n     }\n   }\n \n+  @org.junit.jupiter.api.BeforeEach", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4MTc2Nw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560081767", "bodyText": "sorry if I mislead you on this comment. I was trying to say we should remove this BeforeEach here since we don't have to run this setUp before all the test cases.", "author": "garyli1019", "createdAt": "2021-01-19T10:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4MTMxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4MTY1Mw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554581653", "bodyText": "flink engine", "author": "garyli1019", "createdAt": "2021-01-10T15:17:46Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -133,4 +189,69 @@ public synchronized void invoke(HoodieRecord value, Context context) throws Exce\n       valuesList.add(value);\n     }\n   }\n+\n+  /**\n+   * Cleanups hoodie clients.\n+   */\n+  protected void cleanupClients() throws java.io.IOException {\n+    if (metaClient != null) {\n+      metaClient = null;\n+    }\n+    if (writeClient != null) {\n+      writeClient.close();\n+      writeClient = null;\n+    }\n+    if (tableView != null) {\n+      tableView.close();\n+      tableView = null;\n+    }\n+  }\n+\n+  /**\n+   * Cleanups test data generator.\n+   *\n+   */\n+  protected void cleanupTestDataGenerator() {\n+    if (dataGen != null) {\n+      dataGen = null;\n+    }\n+  }\n+\n+  /**\n+   * Cleanups the distributed file system.\n+   *\n+   * @throws IOException\n+   */\n+  protected void cleanupDFS() throws java.io.IOException {\n+    if (hdfsTestService != null) {\n+      hdfsTestService.stop();\n+      dfsCluster.shutdown();\n+      hdfsTestService = null;\n+      dfsCluster = null;\n+      dfs = null;\n+    }\n+    // Need to closeAll to clear FileSystem.Cache, required because DFS and LocalFS used in the\n+    // same JVM\n+    FileSystem.closeAll();\n+  }\n+\n+  /**\n+   * Cleanups the executor service.\n+   */\n+  protected void cleanupExecutorService() {\n+    if (this.executorService != null) {\n+      this.executorService.shutdownNow();\n+      this.executorService = null;\n+    }\n+  }\n+\n+  /**\n+   * Cleanups Flink contexts.\n+   */\n+  protected void cleanupFlinkContexts() {\n+    if (context != null) {\n+      LOG.info(\"Closing spark engine context used in previous test-case\");", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4MTkzMw==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554581933", "bodyText": "This was overridden by initFileSystem()", "author": "garyli1019", "createdAt": "2021-01-10T15:19:55Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -66,6 +100,15 @@ protected void initFlinkMiniCluster() {\n             .build());\n   }\n \n+  /**\n+   * Initializes the Flink contexts with the given application name.\n+   *\n+   */\n+  protected void initFlinkContexts() {\n+    context = new HoodieFlinkEngineContext(supplier);\n+    hadoopConf = context.getHadoopConf().get();", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4MjA3Mg==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554582072", "bodyText": "nit: import sort", "author": "garyli1019", "createdAt": "2021-01-10T15:21:20Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -18,10 +18,16 @@\n \n package org.apache.hudi.testutils;\n \n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDU4Mjk3MQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r554582971", "bodyText": "This already exists in the base class. Please remove.", "author": "garyli1019", "createdAt": "2021-01-10T15:28:09Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkClientTestHarness.java", "diffHunk": "@@ -133,4 +189,69 @@ public synchronized void invoke(HoodieRecord value, Context context) throws Exce\n       valuesList.add(value);\n     }\n   }\n+\n+  /**\n+   * Cleanups hoodie clients.\n+   */\n+  protected void cleanupClients() throws java.io.IOException {\n+    if (metaClient != null) {\n+      metaClient = null;\n+    }\n+    if (writeClient != null) {\n+      writeClient.close();\n+      writeClient = null;\n+    }\n+    if (tableView != null) {\n+      tableView.close();\n+      tableView = null;\n+    }\n+  }\n+\n+  /**\n+   * Cleanups test data generator.\n+   *\n+   */\n+  protected void cleanupTestDataGenerator() {", "originalCommit": "095fd3de5c1ec02717b5b41aa779340b8882e589", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "84240334e5e37f2ab1d734656425edc2a3430190", "url": "https://github.com/apache/hudi/commit/84240334e5e37f2ab1d734656425edc2a3430190", "message": "[hudi-1] 1\u3001modify for check-style 2\u3001add todo comments, etc.", "committedDate": "2021-01-12T07:19:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NTA4NQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560075085", "bodyText": "please rename the ans to a readable one", "author": "wangxianghu", "createdAt": "2021-01-19T10:33:05Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.MetadataNotFoundException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieRangeInfoHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import com.beust.jcommander.internal.Lists;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.mapping;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class FlinkHoodieBloomIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkHoodieBloomIndex.class);\n+\n+  public FlinkHoodieBloomIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records, HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n+    // Step 1: Extract out thinner Map of (partitionPath, recordKey)\n+    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n+    records.forEach(record -> {\n+      if (partitionRecordKeyMap.containsKey(record.getPartitionPath())) {\n+        partitionRecordKeyMap.get(record.getPartitionPath()).add(record.getRecordKey());\n+      } else {\n+        List<String> recordKeys = Lists.newArrayList();\n+        recordKeys.add(record.getRecordKey());\n+        partitionRecordKeyMap.put(record.getPartitionPath(), recordKeys);\n+      }\n+    });\n+\n+    // Step 2: Lookup indexes for all the partition/recordkey pair\n+    Map<HoodieKey, HoodieRecordLocation> keyFilenamePairMap =\n+        lookupIndex(partitionRecordKeyMap, context, hoodieTable);\n+\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairMap.values().size();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    List<HoodieRecord<T>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairMap, records);\n+\n+    return taggedRecords;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private Map<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      Map<String, List<String>> partitionRecordKeyMap, final HoodieEngineContext context,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = new HashMap<>();\n+    partitionRecordKeyMap.keySet().forEach(k -> recordsPerPartition.put(k, Long.valueOf(partitionRecordKeyMap.get(k).size())));\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, BloomIndexFileInfo>> fileInfoList =\n+        loadInvolvedFiles(affectedPartitionPathList, context, hoodieTable);\n+    final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo =\n+        fileInfoList.stream().collect(groupingBy(Tuple2::_1, mapping(Tuple2::_2, toList())));\n+\n+    // Step 3: Obtain a List, for each incoming record, that already exists, with the file id,\n+    // that contains it.\n+    List<Tuple2<String, HoodieKey>> fileComparisons =\n+            explodeRecordsWithFileComparisons(partitionToFileInfo, partitionRecordKeyMap);\n+    return findMatchingFilesForRecordKeys(fileComparisons, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair List.\n+   */\n+  //TODO duplicate code with spark, we can optimize this method later\n+  List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,\n+                                                             final HoodieTable hoodieTable) {\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream()\n+        .map(pair -> Pair.of(pair.getKey(), pair.getValue().getFileId()))\n+        .collect(toList());\n+\n+    if (config.getBloomIndexPruneByRanges()) {\n+      // also obtain file ranges, if range pruning is enabled\n+      context.setJobStatus(this.getClass().getName(), \"Obtain key ranges for file slices (range pruning=on)\");\n+      return context.map(partitionPathFileIDList, pf -> {\n+        try {\n+          HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, pf);\n+          String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys();\n+          return new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue(), minMaxKeys[0], minMaxKeys[1]));\n+        } catch (MetadataNotFoundException me) {\n+          LOG.warn(\"Unable to find range metadata in file :\" + pf);\n+          return new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue()));\n+        }\n+      }, Math.max(partitionPathFileIDList.size(), 1));\n+    } else {\n+      return partitionPathFileIDList.stream()\n+          .map(pf -> new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue()))).collect(toList());\n+    }\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String instantTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be\n+   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files\n+   * to be compared gets cut down a lot from range pruning.\n+   * <p>\n+   * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on\n+   * recordKey ranges in the index info.\n+   */\n+  List<Tuple2<String, HoodieKey>> explodeRecordsWithFileComparisons(\n+      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n+      Map<String, List<String>> partitionRecordKeyMap) {\n+    IndexFileFilter indexFileFilter =\n+        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)\n+            : new ListBasedIndexFileFilter(partitionToFileIndexInfo);\n+\n+    List<Tuple2<String, HoodieKey>> ans = new ArrayList<>();", "originalCommit": "84240334e5e37f2ab1d734656425edc2a3430190", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NTM5Mg==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560075392", "bodyText": "please correct the doc, since we are in flink engine now", "author": "wangxianghu", "createdAt": "2021-01-19T10:33:34Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/FlinkHoodieBloomIndex.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.engine.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.MetadataNotFoundException;\n+import org.apache.hudi.index.FlinkHoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieRangeInfoHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import com.beust.jcommander.internal.Lists;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.mapping;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * Indexing mechanism based on bloom filter. Each parquet file includes its row_key bloom filter in its metadata.\n+ */\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class FlinkHoodieBloomIndex<T extends HoodieRecordPayload> extends FlinkHoodieIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(FlinkHoodieBloomIndex.class);\n+\n+  public FlinkHoodieBloomIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public List<HoodieRecord<T>> tagLocation(List<HoodieRecord<T>> records, HoodieEngineContext context,\n+                                           HoodieTable<T, List<HoodieRecord<T>>, List<HoodieKey>, List<WriteStatus>> hoodieTable) {\n+    // Step 1: Extract out thinner Map of (partitionPath, recordKey)\n+    Map<String, List<String>> partitionRecordKeyMap = new HashMap<>();\n+    records.forEach(record -> {\n+      if (partitionRecordKeyMap.containsKey(record.getPartitionPath())) {\n+        partitionRecordKeyMap.get(record.getPartitionPath()).add(record.getRecordKey());\n+      } else {\n+        List<String> recordKeys = Lists.newArrayList();\n+        recordKeys.add(record.getRecordKey());\n+        partitionRecordKeyMap.put(record.getPartitionPath(), recordKeys);\n+      }\n+    });\n+\n+    // Step 2: Lookup indexes for all the partition/recordkey pair\n+    Map<HoodieKey, HoodieRecordLocation> keyFilenamePairMap =\n+        lookupIndex(partitionRecordKeyMap, context, hoodieTable);\n+\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairMap.values().size();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 3: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    List<HoodieRecord<T>> taggedRecords = tagLocationBacktoRecords(keyFilenamePairMap, records);\n+\n+    return taggedRecords;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private Map<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      Map<String, List<String>> partitionRecordKeyMap, final HoodieEngineContext context,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = new HashMap<>();\n+    partitionRecordKeyMap.keySet().forEach(k -> recordsPerPartition.put(k, Long.valueOf(partitionRecordKeyMap.get(k).size())));\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, BloomIndexFileInfo>> fileInfoList =\n+        loadInvolvedFiles(affectedPartitionPathList, context, hoodieTable);\n+    final Map<String, List<BloomIndexFileInfo>> partitionToFileInfo =\n+        fileInfoList.stream().collect(groupingBy(Tuple2::_1, mapping(Tuple2::_2, toList())));\n+\n+    // Step 3: Obtain a List, for each incoming record, that already exists, with the file id,\n+    // that contains it.\n+    List<Tuple2<String, HoodieKey>> fileComparisons =\n+            explodeRecordsWithFileComparisons(partitionToFileInfo, partitionRecordKeyMap);\n+    return findMatchingFilesForRecordKeys(fileComparisons, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair List.\n+   */\n+  //TODO duplicate code with spark, we can optimize this method later\n+  List<Tuple2<String, BloomIndexFileInfo>> loadInvolvedFiles(List<String> partitions, final HoodieEngineContext context,\n+                                                             final HoodieTable hoodieTable) {\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = getLatestBaseFilesForAllPartitions(partitions, context, hoodieTable).stream()\n+        .map(pair -> Pair.of(pair.getKey(), pair.getValue().getFileId()))\n+        .collect(toList());\n+\n+    if (config.getBloomIndexPruneByRanges()) {\n+      // also obtain file ranges, if range pruning is enabled\n+      context.setJobStatus(this.getClass().getName(), \"Obtain key ranges for file slices (range pruning=on)\");\n+      return context.map(partitionPathFileIDList, pf -> {\n+        try {\n+          HoodieRangeInfoHandle rangeInfoHandle = new HoodieRangeInfoHandle(config, hoodieTable, pf);\n+          String[] minMaxKeys = rangeInfoHandle.getMinMaxKeys();\n+          return new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue(), minMaxKeys[0], minMaxKeys[1]));\n+        } catch (MetadataNotFoundException me) {\n+          LOG.warn(\"Unable to find range metadata in file :\" + pf);\n+          return new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue()));\n+        }\n+      }, Math.max(partitionPathFileIDList.size(), 1));\n+    } else {\n+      return partitionPathFileIDList.stream()\n+          .map(pf -> new Tuple2<>(pf.getKey(), new BloomIndexFileInfo(pf.getValue()))).collect(toList());\n+    }\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String instantTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * For each incoming record, produce N output records, 1 each for each file against which the record's key needs to be\n+   * checked. For tables, where the keys have a definite insert order (e.g: timestamp as prefix), the number of files\n+   * to be compared gets cut down a lot from range pruning.\n+   * <p>\n+   * Sub-partition to ensure the records can be looked up against files & also prune file<=>record comparisons based on\n+   * recordKey ranges in the index info.\n+   */\n+  List<Tuple2<String, HoodieKey>> explodeRecordsWithFileComparisons(\n+      final Map<String, List<BloomIndexFileInfo>> partitionToFileIndexInfo,\n+      Map<String, List<String>> partitionRecordKeyMap) {\n+    IndexFileFilter indexFileFilter =\n+        config.useBloomIndexTreebasedFilter() ? new IntervalTreeBasedIndexFileFilter(partitionToFileIndexInfo)\n+            : new ListBasedIndexFileFilter(partitionToFileIndexInfo);\n+\n+    List<Tuple2<String, HoodieKey>> ans = new ArrayList<>();\n+    partitionRecordKeyMap.keySet().forEach(partitionPath ->  {\n+      List<String> hoodieRecordKeys = partitionRecordKeyMap.get(partitionPath);\n+      hoodieRecordKeys.forEach(hoodieRecordKey -> {\n+        indexFileFilter.getMatchingFilesAndPartition(partitionPath, hoodieRecordKey).forEach(partitionFileIdPair -> {\n+          ans.add(new Tuple2<>(partitionFileIdPair.getRight(),\n+                  new HoodieKey(hoodieRecordKey, partitionPath)));\n+        });\n+      });\n+    });\n+    return ans;\n+  }\n+\n+  /**\n+   * Find out <RowKey, filename> pair. All workload grouped by file-level.\n+   * <p>\n+   * Join Map(PartitionPath, RecordKey) and Map(PartitionPath, File) & then repartition such that each List\n+   * is a file, then for each file, we do (1) load bloom filter, (2) load rowKeys, (3) Tag rowKey\n+   * <p>\n+   * Make sure the parallelism is atleast the groupby parallelism for tagging location\n+   */", "originalCommit": "84240334e5e37f2ab1d734656425edc2a3430190", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NjYzMg==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560076632", "bodyText": "is it possible to make this LazyKeyCheckIterator class an independent one, for code reuse purpose", "author": "wangxianghu", "createdAt": "2021-01-19T10:35:34Z", "path": "hudi-client/hudi-flink-client/src/main/java/org/apache/hudi/index/bloom/HoodieFlinkBloomIndexCheckFunction.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.client.utils.LazyIterableIterator;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIndexException;\n+import org.apache.hudi.io.HoodieKeyLookupHandle;\n+import org.apache.hudi.io.HoodieKeyLookupHandle.KeyLookupResult;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import java.util.function.Function;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+/**\n+ * Function performing actual checking of List partition containing (fileId, hoodieKeys) against the actual files.\n+ */\n+//TODO we can move this class into the hudi-client-common and reuse it for spark client\n+public class HoodieFlinkBloomIndexCheckFunction\n+        implements Function<Iterator<Tuple2<String, HoodieKey>>, Iterator<List<KeyLookupResult>>> {\n+\n+  private final HoodieTable hoodieTable;\n+\n+  private final HoodieWriteConfig config;\n+\n+  public HoodieFlinkBloomIndexCheckFunction(HoodieTable hoodieTable, HoodieWriteConfig config) {\n+    this.hoodieTable = hoodieTable;\n+    this.config = config;\n+  }\n+\n+  @Override\n+  public Iterator<List<KeyLookupResult>> apply(Iterator<Tuple2<String, HoodieKey>> fileParitionRecordKeyTripletItr) {\n+    return new LazyKeyCheckIterator(fileParitionRecordKeyTripletItr);\n+  }\n+\n+  @Override\n+  public <V> Function<V, Iterator<List<KeyLookupResult>>> compose(Function<? super V, ? extends Iterator<Tuple2<String, HoodieKey>>> before) {\n+    return null;\n+  }\n+\n+  @Override\n+  public <V> Function<Iterator<Tuple2<String, HoodieKey>>, V> andThen(Function<? super Iterator<List<KeyLookupResult>>, ? extends V> after) {\n+    return null;\n+  }\n+\n+  class LazyKeyCheckIterator extends LazyIterableIterator<Tuple2<String, HoodieKey>, List<KeyLookupResult>> {", "originalCommit": "84240334e5e37f2ab1d734656425edc2a3430190", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc2OTU2MQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560769561", "bodyText": "maybe we can move HoodieFlinkBloomIndexCheckFunction into the hudi-client-common later then spark can reuse it.", "author": "Nieal-Yang", "createdAt": "2021-01-20T08:41:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NjYzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMwODA1NA==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r562308054", "bodyText": "maybe we can move HoodieFlinkBloomIndexCheckFunction into the hudi-client-common later then spark can reuse it.\n\nyes, could be annother pr", "author": "wangxianghu", "createdAt": "2021-01-22T01:23:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA3NjYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4NTIxNg==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560085216", "bodyText": "can we remove the duplicate codes if it's available in the base class? There are several in this class.", "author": "garyli1019", "createdAt": "2021-01-19T10:48:54Z", "path": "hudi-client/hudi-flink-client/src/test/java/org/apache/hudi/testutils/HoodieFlinkWriteableTestTable.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.hudi.testutils;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.HoodieAvroWriteSupport;\n+import org.apache.hudi.client.FlinkTaskContextSupplier;\n+import org.apache.hudi.common.bloom.BloomFilter;\n+import org.apache.hudi.common.bloom.BloomFilterFactory;\n+import org.apache.hudi.common.bloom.BloomFilterTypeCode;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.log.HoodieLogFormat;\n+import org.apache.hudi.common.table.log.block.HoodieAvroDataBlock;\n+import org.apache.hudi.common.table.log.block.HoodieLogBlock.HeaderMetadataType;\n+import org.apache.hudi.common.testutils.FileCreateUtils;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.io.storage.HoodieAvroParquetConfig;\n+import org.apache.hudi.io.storage.HoodieParquetWriter;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.parquet.avro.AvroSchemaConverter;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.common.testutils.FileCreateUtils.baseFileName;\n+\n+public class HoodieFlinkWriteableTestTable extends HoodieWriteableTestTable {\n+  private static final Logger LOG = LogManager.getLogger(HoodieFlinkWriteableTestTable.class);\n+\n+  private HoodieFlinkWriteableTestTable(String basePath, org.apache.hadoop.fs.FileSystem fs, HoodieTableMetaClient metaClient, Schema schema, BloomFilter filter) {\n+    super(basePath, fs, metaClient, schema, filter);\n+  }\n+\n+  public static HoodieFlinkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema, BloomFilter filter) {\n+    return new HoodieFlinkWriteableTestTable(metaClient.getBasePath(), metaClient.getRawFs(), metaClient, schema, filter);\n+  }\n+\n+  public static HoodieFlinkWriteableTestTable of(HoodieTableMetaClient metaClient, Schema schema) {\n+    BloomFilter filter = BloomFilterFactory.createBloomFilter(10000, 0.0000001, -1, BloomFilterTypeCode.SIMPLE.name());\n+    return of(metaClient, schema, filter);\n+  }\n+\n+  public static HoodieFlinkWriteableTestTable of(HoodieTable hoodieTable, Schema schema) {\n+    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();\n+    return of(metaClient, schema);\n+  }\n+\n+  public static HoodieFlinkWriteableTestTable of(HoodieTable hoodieTable, Schema schema, BloomFilter filter) {\n+    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();\n+    return of(metaClient, schema, filter);\n+  }\n+\n+  @Override\n+  public HoodieFlinkWriteableTestTable addCommit(String instantTime) throws Exception {\n+    return (HoodieFlinkWriteableTestTable) super.addCommit(instantTime);\n+  }\n+\n+  @Override\n+  public HoodieFlinkWriteableTestTable forCommit(String instantTime) {\n+    return (HoodieFlinkWriteableTestTable) super.forCommit(instantTime);\n+  }\n+\n+  public String getFileIdWithInserts(String partition) throws Exception {\n+    return getFileIdWithInserts(partition, new HoodieRecord[0]);\n+  }\n+\n+  public String getFileIdWithInserts(String partition, HoodieRecord... records) throws Exception {\n+    return getFileIdWithInserts(partition, java.util.Arrays.asList(records));\n+  }\n+\n+  public String getFileIdWithInserts(String partition, List<HoodieRecord> records) throws Exception {\n+    String fileId = java.util.UUID.randomUUID().toString();\n+    withInserts(partition, fileId, records);\n+    return fileId;\n+  }\n+\n+  public HoodieFlinkWriteableTestTable withInserts(String partition, String fileId) throws Exception {\n+    return withInserts(partition, fileId, new HoodieRecord[0]);\n+  }\n+\n+  public HoodieFlinkWriteableTestTable withInserts(String partition, String fileId, HoodieRecord... records) throws Exception {\n+    return withInserts(partition, fileId, java.util.Arrays.asList(records));\n+  }\n+\n+  public HoodieFlinkWriteableTestTable withInserts(String partition, String fileId, List<HoodieRecord> records) throws Exception {", "originalCommit": "84240334e5e37f2ab1d734656425edc2a3430190", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDc5MDM5NQ==", "url": "https://github.com/apache/hudi/pull/2375#discussion_r560790395", "bodyText": "yeah", "author": "Nieal-Yang", "createdAt": "2021-01-20T08:59:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDA4NTIxNg=="}], "type": "inlineReview"}, {"oid": "4f7a683d77683236f34e3402f8453351811c3949", "url": "https://github.com/apache/hudi/commit/4f7a683d77683236f34e3402f8453351811c3949", "message": "[opt] 1\u3001opt", "committedDate": "2021-01-20T09:08:52Z", "type": "commit"}, {"oid": "1ec2b66efdc7d1713d605c787dcb959c6d84c571", "url": "https://github.com/apache/hudi/commit/1ec2b66efdc7d1713d605c787dcb959c6d84c571", "message": "[opt] 1\u3001opt doc of HoodieFlinkBloomIndexCheckFunction", "committedDate": "2021-01-21T02:05:38Z", "type": "commit"}]}