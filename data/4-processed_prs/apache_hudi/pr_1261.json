{"pr_number": 1261, "pr_title": "[HUDI-403] Adds guidelines on deployment/upgrading", "pr_createdAt": "2020-01-21T02:03:49Z", "pr_url": "https://github.com/apache/hudi/pull/1261", "timeline": [{"oid": "ade07acf7b76bb79594c7cbd59524338fd76eaf1", "url": "https://github.com/apache/hudi/commit/ade07acf7b76bb79594c7cbd59524338fd76eaf1", "message": "[HUDI-403] Adds guidelines on deployment/upgrading\n\n - Moved \"Adminsitering\" page to \"Deployment\"\n - Still need to add information on deltastreamer modes/compaction", "committedDate": "2020-01-21T02:01:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4ODQ4OA==", "url": "https://github.com/apache/hudi/pull/1261#discussion_r368788488", "bodyText": "Hi, miss . at the end of statement.", "author": "lamberken", "createdAt": "2020-01-21T02:39:12Z", "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -1,51 +1,87 @@\n ---\n-title: Administering Hudi Pipelines\n-keywords: hudi, administration, operation, devops\n-permalink: /docs/admin_guide.html\n-summary: This section offers an overview of tools available to operate an ecosystem of Hudi datasets\n+title: Deployment Guide\n+keywords: hudi, administration, operation, devops, deployment\n+permalink: /docs/deployment.html\n+summary: This section offers an overview of tools available to operate an ecosystem of Hudi\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Admins/ops can gain visibility into Hudi datasets/pipelines in the following ways\n+This section provides all the help you need to deploy and operate Hudi tables at scale. \n+Specifically, we will cover the following aspects.\n \n- - [Administering via the Admin CLI](#admin-cli)\n- - [Graphite metrics](#metrics)\n- - [Spark UI of the Hudi Application](#spark-ui)\n+ - [Deployment Model](#deploying) : How various Hudi components are deployed and managed.\n+ - [Upgrading Versions](#upgrading) : Picking up new releases of Hudi, guidelines and general best-practices\n+ - [Migrating to Hudi](#migrating) : How to migrate your existing tables to Apache Hudi.\n+ - [Interacting via CLI](#cli) : Using the CLI to perform maintenance or deeper introspection\n+ - [Monitoring](#monitoring) : Tracking metrics from your hudi tables using popular tools.\n+ - [Troubleshooting](#troubleshooting) : Uncovering, triaging and resolving issues in production.\n+ \n+## Deploying\n \n-This section provides a glimpse into each of these, with some general guidance on [troubleshooting](#troubleshooting)\n+All in all, Hudi deploys with no long running servers or additional infrastructure cost to your data lake. In fact, Hudi pioneered this model of building a transactional distributed storage layer\n+using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n+Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n-## Admin CLI\n \n-Once hudi has been built, the shell can be fired by via  `cd hudi-cli && ./hudi-cli.sh`.\n-A hudi dataset resides on DFS, in a location referred to as the **basePath** and we would need this location in order to connect to a Hudi dataset.\n-Hudi library effectively manages this dataset internally, using .hoodie subfolder to track all metadata\n+## Upgrading \n+\n+New Hudi releases are listed on the [releases page](/releases), with detailed notes which list all the changes, with highlights in each release. \n+At the end of the day, Hudi is a storage system and with that comes a lot of responsibilities, which we take seriously. \n+\n+As general guidelines, \n+\n+ - We strive to keep all changes backwards compatible (i.e new code can read old data/timeline files) and we cannot we will provide upgrade/downgrade tools via the CLI\n+ - We cannot always guarantee forward compatibility (i.e old code being able to read data/timeline files written by a greater version). This is generally the norm, since no new features can be built otherwise.\n+   However any large such changes, will be turned off by default, for smooth transition to newer release. After a few releases and once enough users deem the feature stable in production, we will flip the defaults in a subsequent release.\n+ - Always upgrade the query bundles (mr-bundle, presto-bundle, spark-bundle) first and then upgrade the writers (deltastreamer, spark jobs using datasource). This often provides the best experience and it's easy to fix \n+   any issues by rolling forward/back the writer code (which typically you might have more control over)\n+ - With large, feature rich releases we recommend migrating slowly, by first testing in staging environments and running your own tests. Upgrading Hudi is no different than upgrading any database system.\n+\n+Note that release notes can override this information with specific instructions, applicable on case-by-case basis.\n+\n+## Migrating\n+\n+Currently migrating to Hudi can be done using two approaches \n+\n+- **Convert newer partitions to Hudi** : This model is suitable for large event tables (e.g: click streams, ad impressions), which also typically receive writes for the last few days alone. You can convert the last \n+   N partitions to Hudi and proceed writing as if it were a Hudi table to begin with. The Hudi query side code is able to correctly handle both hudi and non-hudi data partitions.\n+- **Full conversion to Hudi** : This model is suitable if you are currently bulk/full loading the table few times a day (e.g database ingestion). The full conversion of Hudi is simply a one-time step (akin to 1 run of your existing job),\n+   which moves all of the data into the Hudi format and provides the ability to incrementally update for future writes.\n+\n+For more details, refer to the detailed [migration guide](/docs/migration_guide.html). In the future, we will be supporting seamless zero-copy bootstrap of existing tables with all the upsert/incremental query capabilities fully supported.\n+\n+## CLI\n+\n+Once hudi has been built, the shell can be fired by via  `cd hudi-cli && ./hudi-cli.sh`. A hudi table resides on DFS, in a location referred to as the `basePath` and \n+we would need this location in order to connect to a Hudi table. Hudi library effectively manages this table internally, using `.hoodie` subfolder to track all metadata", "originalCommit": "ade07acf7b76bb79594c7cbd59524338fd76eaf1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4OTIxMg==", "url": "https://github.com/apache/hudi/pull/1261#discussion_r368789212", "bodyText": "Hi, miss . at the end of statement.", "author": "lamberken", "createdAt": "2020-01-21T02:43:08Z", "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -1,51 +1,87 @@\n ---\n-title: Administering Hudi Pipelines\n-keywords: hudi, administration, operation, devops\n-permalink: /docs/admin_guide.html\n-summary: This section offers an overview of tools available to operate an ecosystem of Hudi datasets\n+title: Deployment Guide\n+keywords: hudi, administration, operation, devops, deployment\n+permalink: /docs/deployment.html\n+summary: This section offers an overview of tools available to operate an ecosystem of Hudi\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Admins/ops can gain visibility into Hudi datasets/pipelines in the following ways\n+This section provides all the help you need to deploy and operate Hudi tables at scale. \n+Specifically, we will cover the following aspects.\n \n- - [Administering via the Admin CLI](#admin-cli)\n- - [Graphite metrics](#metrics)\n- - [Spark UI of the Hudi Application](#spark-ui)\n+ - [Deployment Model](#deploying) : How various Hudi components are deployed and managed.\n+ - [Upgrading Versions](#upgrading) : Picking up new releases of Hudi, guidelines and general best-practices", "originalCommit": "ade07acf7b76bb79594c7cbd59524338fd76eaf1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4OTI1NQ==", "url": "https://github.com/apache/hudi/pull/1261#discussion_r368789255", "bodyText": "Hi, miss . at the end of statement.", "author": "lamberken", "createdAt": "2020-01-21T02:43:20Z", "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -1,51 +1,87 @@\n ---\n-title: Administering Hudi Pipelines\n-keywords: hudi, administration, operation, devops\n-permalink: /docs/admin_guide.html\n-summary: This section offers an overview of tools available to operate an ecosystem of Hudi datasets\n+title: Deployment Guide\n+keywords: hudi, administration, operation, devops, deployment\n+permalink: /docs/deployment.html\n+summary: This section offers an overview of tools available to operate an ecosystem of Hudi\n toc: true\n last_modified_at: 2019-12-30T15:59:57-04:00\n ---\n \n-Admins/ops can gain visibility into Hudi datasets/pipelines in the following ways\n+This section provides all the help you need to deploy and operate Hudi tables at scale. \n+Specifically, we will cover the following aspects.\n \n- - [Administering via the Admin CLI](#admin-cli)\n- - [Graphite metrics](#metrics)\n- - [Spark UI of the Hudi Application](#spark-ui)\n+ - [Deployment Model](#deploying) : How various Hudi components are deployed and managed.\n+ - [Upgrading Versions](#upgrading) : Picking up new releases of Hudi, guidelines and general best-practices", "originalCommit": "ade07acf7b76bb79594c7cbd59524338fd76eaf1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODgxMjc4MQ==", "url": "https://github.com/apache/hudi/pull/1261#discussion_r368812781", "bodyText": "THanks @lamber-ken I will be adding a followup PR to add more details around compaction and deltastreamer. I will address these comments as part of the follow-up PR.", "author": "bvaradar", "createdAt": "2020-01-21T05:04:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4OTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODgxODYxNA==", "url": "https://github.com/apache/hudi/pull/1261#discussion_r368818614", "bodyText": "THanks @lamber-ken I will be adding a followup PR to add more details around compaction and deltastreamer. I will address these comments as part of the follow-up PR.\n\nYou're welcome \ud83d\ude04", "author": "lamberken", "createdAt": "2020-01-21T05:37:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODc4OTI1NQ=="}], "type": "inlineReview"}]}