{"pr_number": 1558, "pr_title": "[HUDI-796] Add deduping logic for upserts case", "pr_createdAt": "2020-04-24T12:19:10Z", "pr_url": "https://github.com/apache/hudi/pull/1558", "timeline": [{"oid": "410b2bdbaa25a2a4e3999a506686e3299cf2a457", "url": "https://github.com/apache/hudi/commit/410b2bdbaa25a2a4e3999a506686e3299cf2a457", "message": "[HUDI-796]: added deduping logic for upserts case", "committedDate": "2020-04-24T12:16:57Z", "type": "commit"}, {"oid": "5e1e5f4ec01e5cd69211283304a0351173787136", "url": "https://github.com/apache/hudi/commit/5e1e5f4ec01e5cd69211283304a0351173787136", "message": "[HUDI-796]: small fix", "committedDate": "2020-04-24T12:21:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5OTk1Ng==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416299956", "bodyText": "Let us modify help string of dryrun, statements are inaccurate :)", "author": "hddong", "createdAt": "2020-04-28T03:29:36Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,11 +64,15 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          mandatory = true) final String sparkPropertiesPath,\n+      @CliOption(key = {\"useCommitTimeForDedupe\"}, help = \"Set it to true if duplicates have never been updated\",\n+        unspecifiedDefaultValue = \"true\") final boolean useCommitTimeForDedupe,\n+      @CliOption(key = {\"dryrun\"}, help = \"Should we actually add or just print what would be done\",", "originalCommit": "5e1e5f4ec01e5cd69211283304a0351173787136", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjI5Mw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416462293", "bodyText": "Done.", "author": "pratyakshsharma", "createdAt": "2020-04-28T09:20:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI5OTk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNDY5Ng==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416314696", "bodyText": "It's better not use break here, rows.init also can get the rows will be delete.", "author": "hddong", "createdAt": "2020-04-28T04:18:38Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -103,24 +105,51 @@ class DedupeSparkJob(basePath: String,\n     // Mark all files except the one with latest commits for deletion\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n+\n+      if (useCommitTimeForDedupe) {\n+        /*\n+        This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+         */\n+        var maxCommit = -1L\n+\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c > maxCommit)\n+            maxCommit = c\n+        })\n+        rows.foreach(r => {\n+          val c = r(3).asInstanceOf[String].toLong\n+          if (c != maxCommit) {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n           }\n-          fileToDeleteKeyMap(f).add(key)\n+        })\n+      } else {\n+        /*\n+        This corresponds to the case where duplicates have been updated at least once.\n+        Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+         */\n+        val size = rows.size - 1\n+        var i = 0\n+        val loop = new Breaks\n+        loop.breakable {", "originalCommit": "5e1e5f4ec01e5cd69211283304a0351173787136", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2OTEwNw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r416469107", "bodyText": "Right. Thank you for suggesting this, I am not hands on at scala properly. :)", "author": "pratyakshsharma", "createdAt": "2020-04-28T09:30:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNDY5Ng=="}], "type": "inlineReview"}, {"oid": "6a6af53c4f06838071ffb85303f844659cec9f6c", "url": "https://github.com/apache/hudi/commit/6a6af53c4f06838071ffb85303f844659cec9f6c", "message": "[HUDI-796]: addressed code review comments", "committedDate": "2020-04-28T09:24:05Z", "type": "commit"}, {"oid": "417f5a14346745930bbec44145ffc35050e43c45", "url": "https://github.com/apache/hudi/commit/417f5a14346745930bbec44145ffc35050e43c45", "message": "[HUDI-796]: removed unused imports", "committedDate": "2020-04-28T09:29:14Z", "type": "commit"}, {"oid": "cfb1735e62265722c9aeddd38f078b06eebc7755", "url": "https://github.com/apache/hudi/commit/cfb1735e62265722c9aeddd38f078b06eebc7755", "message": "[HUDI-796]: addressed code review comments", "committedDate": "2020-05-15T11:09:55Z", "type": "commit"}, {"oid": "f285cb85d51b078bc269ff35ef0ccf2534c8f049", "url": "https://github.com/apache/hudi/commit/f285cb85d51b078bc269ff35ef0ccf2534c8f049", "message": "[HUDI-796]: in sync with master", "committedDate": "2020-05-15T12:46:25Z", "type": "commit"}, {"oid": "e4abe3a28bd02d91c4425ee8dace6d05e9118a5f", "url": "https://github.com/apache/hudi/commit/e4abe3a28bd02d91c4425ee8dace6d05e9118a5f", "message": "[HUDI-796]: added test cases", "committedDate": "2020-05-15T17:49:05Z", "type": "commit"}, {"oid": "9fde5a159a9f7fc39e895b48bcea8426da21639a", "url": "https://github.com/apache/hudi/commit/9fde5a159a9f7fc39e895b48bcea8426da21639a", "message": "[HUDI-796]: small fix for upsert dedupe", "committedDate": "2020-05-16T10:09:29Z", "type": "commit"}, {"oid": "838382a5e52719e0e9ef190d375dc7d6c91e5248", "url": "https://github.com/apache/hudi/commit/838382a5e52719e0e9ef190d375dc7d6c91e5248", "message": "[HUDI-796]: commented a test case", "committedDate": "2020-05-16T19:33:04Z", "type": "commit"}, {"oid": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "url": "https://github.com/apache/hudi/commit/5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "message": "[HUDI-796]: fixed test case", "committedDate": "2020-05-19T13:23:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428056315", "bodyText": "Can use DeDupeType.withName(\"insertType\") instead?", "author": "hddong", "createdAt": "2020-05-20T14:25:53Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -263,13 +265,26 @@ private static int compact(JavaSparkContext jsc, String basePath, String tableNa\n   }\n \n   private static int deduplicatePartitionPath(JavaSparkContext jsc, String duplicatedPartitionPath,\n-      String repairedOutputPath, String basePath, String dryRun) {\n+      String repairedOutputPath, String basePath, boolean dryRun, String dedupeType) {\n     DedupeSparkJob job = new DedupeSparkJob(basePath, duplicatedPartitionPath, repairedOutputPath, new SQLContext(jsc),\n-        FSUtils.getFs(basePath, jsc.hadoopConfiguration()));\n-    job.fixDuplicates(Boolean.parseBoolean(dryRun));\n+        FSUtils.getFs(basePath, jsc.hadoopConfiguration()), getDedupeType(dedupeType));\n+    job.fixDuplicates(dryRun);\n     return 0;\n   }\n \n+  private static Enumeration.Value getDedupeType(String type) {\n+    switch (type) {\n+      case \"insertType\":\n+        return DeDupeType.insertType();\n+      case \"updateType\":\n+        return DeDupeType.updateType();\n+      case \"upsertType\":\n+        return DeDupeType.upsertType();\n+      default:\n+        throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n+  }\n+", "originalCommit": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3MTk5MA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429171990", "bodyText": "But what difference does it create?\nDeDupeType.insertType() and DeDupeType.withName(\"insertType\") - both return the same Value.", "author": "pratyakshsharma", "createdAt": "2020-05-22T10:36:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTI1MTEyNg==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429251126", "bodyText": "@pratyakshsharma : I mean that we can use DeDupeType.withName(\"insertType\") to convert String to Enum.  getDedupeType Function may not need here.", "author": "hddong", "createdAt": "2020-05-22T13:39:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjU2OTQ4Mg==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r446569482", "bodyText": "Taken care. :)", "author": "pratyakshsharma", "createdAt": "2020-06-27T21:33:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NjMxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTUyNQ==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428071525", "bodyText": "It's better to show the three types in help string and have a type check at first line of command.", "author": "hddong", "createdAt": "2020-05-20T14:45:26Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,7 +77,9 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Check DeDupeType.scala for valid values\",\n+          unspecifiedDefaultValue = \"insertType\") final String dedupeType)", "originalCommit": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NDExOA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429174118", "bodyText": "Done.", "author": "pratyakshsharma", "createdAt": "2020-05-22T10:41:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3MTUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3ODk1Ng==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428078956", "bodyText": "Can we make it all uppercase to keep the format uniform\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-common/src/main/java/org/apache/hudi/common/model/WriteOperationType.java#L30-L34", "author": "hddong", "createdAt": "2020-05-20T14:54:37Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DeDupeType.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli\n+\n+object DeDupeType extends Enumeration {\n+\n+  type dedupeType = Value\n+\n+  val insertType = Value(\"insertType\")\n+  val updateType = Value(\"updateType\")\n+  val upsertType = Value(\"upsertType\")", "originalCommit": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NjU2Mg==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429176562", "bodyText": "Done", "author": "pratyakshsharma", "createdAt": "2020-05-22T10:47:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA3ODk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDU2MQ==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r428080561", "bodyText": "Can we use $ to get value? like:\nhttps://github.com/apache/incubator-hudi/blob/74ecc27e920c70fa4598d8e5a696954203a5b127/hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala#L144", "author": "hddong", "createdAt": "2020-05-20T14:56:33Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.updateType =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.insertType =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.upsertType =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(\"fileToDeleteKeyMap size : \" + fileToDeleteKeyMap.size + \", map: \" + fileToDeleteKeyMap)", "originalCommit": "5dcdd4100ec792f38c1017bf29dd63f3074e8aa3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3NzUzNw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r429177537", "bodyText": "Done.", "author": "pratyakshsharma", "createdAt": "2020-05-22T10:49:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDU2MQ=="}], "type": "inlineReview"}, {"oid": "a63c700e6630c0931570453fdeff6c857265301b", "url": "https://github.com/apache/hudi/commit/a63c700e6630c0931570453fdeff6c857265301b", "message": "[HUDI-796]: addressed code review comments", "committedDate": "2020-05-22T13:39:44Z", "type": "commit"}, {"oid": "72f850be6e98f7b20c0658642fbffeafd6a98db5", "url": "https://github.com/apache/hudi/commit/72f850be6e98f7b20c0658642fbffeafd6a98db5", "message": "[HUDI-796]: small change in SparkMain", "committedDate": "2020-05-22T14:26:28Z", "type": "commit"}, {"oid": "8abfd9941b11e10de715930fee32f26829b11f75", "url": "https://github.com/apache/hudi/commit/8abfd9941b11e10de715930fee32f26829b11f75", "message": "Merge branch 'master' of https://github.com/apache/incubator-hudi into hudi-796", "committedDate": "2020-06-27T19:31:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTA4MQ==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605081", "bodyText": "Can we get the representation of the dedupeType by the DeDupeType enum?", "author": "yanghua", "createdAt": "2020-06-30T11:16:31Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447605723", "bodyText": "Can we append this parameter into the list of the parameters?", "author": "yanghua", "createdAt": "2020-06-30T11:17:45Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,16 +77,21 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(\"insert_type\") && !dedupeType.equals(\"update_type\") && !dedupeType.equals(\"upsert_type\")) {\n+      throw new IllegalArgumentException(\"Please provide valid dedupe type!\");\n+    }\n     if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n       sparkPropertiesPath =\n           Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n     }\n \n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n     sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,\n-        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(),\n+        duplicatedPartitionPath, repairedOutputPath, HoodieCLI.getTableMetaClient().getBasePath(), dedupeType,", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyMjM1Nw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r448522357", "bodyText": "I did not get this. Which list are you referring to?", "author": "pratyakshsharma", "createdAt": "2020-07-01T17:46:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUyMzE3Mw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r448523173", "bodyText": "Ok, you mean I should append it at the end of the already existing parameters? Sure will do that.", "author": "pratyakshsharma", "createdAt": "2020-07-01T17:48:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNTcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNjEwMA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447606100", "bodyText": "ditto, move to the last of the method's parameter list?", "author": "yanghua", "createdAt": "2020-06-30T11:18:26Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -75,8 +76,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[3], args[4]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 7);\n-        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);\n+        assert (args.length == 8);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], Boolean.parseBoolean(args[7]), args[6]);", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzYwNzYzMA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447607630", "bodyText": "split via a new empty line?", "author": "yanghua", "createdAt": "2020-06-30T11:21:15Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODA2Mg==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758062", "bodyText": "split via empty line", "author": "yanghua", "createdAt": "2020-06-30T15:07:01Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1ODc0OA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447758748", "bodyText": "why do you add new line here?", "author": "yanghua", "createdAt": "2020-06-30T15:07:57Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -98,34 +97,92 @@ class DedupeSparkJob(basePath: String,\n         ON h.`_hoodie_record_key` = d.dupe_key\n                       \"\"\"\n     val dupeMap = sqlContext.sql(dupeDataSql).collectAsList().groupBy(r => r.getString(0))\n-    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n+    getDedupePlan(dupeMap)\n+  }\n \n-    // Mark all files except the one with latest commits for deletion\n+  private def getDedupePlan(dupeMap: Map[String, Buffer[Row]]): HashMap[String, HashSet[String]] = {\n+    val fileToDeleteKeyMap = new HashMap[String, HashSet[String]]()\n     dupeMap.foreach(rt => {\n       val (key, rows) = rt\n-      var maxCommit = -1L\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c > maxCommit)\n-          maxCommit = c\n-      })\n-\n-      rows.foreach(r => {\n-        val c = r(3).asInstanceOf[String].toLong\n-        if (c != maxCommit) {\n-          val f = r(2).asInstanceOf[String].split(\"_\")(0)\n-          if (!fileToDeleteKeyMap.contains(f)) {\n-            fileToDeleteKeyMap(f) = HashSet[String]()\n-          }\n-          fileToDeleteKeyMap(f).add(key)\n-        }\n-      })\n+\n+      dedupeType match {\n+        case DeDupeType.UPDATE_TYPE =>\n+          /*\n+          This corresponds to the case where all duplicates have been updated at least once.\n+          Once updated, duplicates are bound to have same commit time unless forcefully modified.\n+          */\n+          rows.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+        case DeDupeType.INSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created due to INSERT and have never been updated.\n+          */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            }\n+          })\n+\n+        case DeDupeType.UPSERT_TYPE =>\n+          /*\n+          This corresponds to the case where duplicates got created as a result of inserts as well as updates,\n+          i.e few duplicate records have been updated, while others were never updated.\n+           */\n+          var maxCommit = -1L\n+\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c > maxCommit)\n+              maxCommit = c\n+          })\n+          val rowsWithMaxCommit = new ListBuffer[Row]()\n+          rows.foreach(r => {\n+            val c = r(3).asInstanceOf[String].toLong\n+            if (c != maxCommit) {\n+              val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+              if (!fileToDeleteKeyMap.contains(f)) {\n+                fileToDeleteKeyMap(f) = HashSet[String]()\n+              }\n+              fileToDeleteKeyMap(f).add(key)\n+            } else {\n+              rowsWithMaxCommit += r\n+            }\n+          })\n+\n+          rowsWithMaxCommit.toList.init.foreach(r => {\n+            val f = r(2).asInstanceOf[String].split(\"_\")(0)\n+            if (!fileToDeleteKeyMap.contains(f)) {\n+              fileToDeleteKeyMap(f) = HashSet[String]()\n+            }\n+            fileToDeleteKeyMap(f).add(key)\n+          })\n+\n+        case _ => throw new IllegalArgumentException(\"Please provide valid type for deduping!\")\n+      }\n     })\n+    LOG.debug(s\"fileToDeleteKeyMap size: ${fileToDeleteKeyMap.size}, map: $fileToDeleteKeyMap\")\n     fileToDeleteKeyMap\n   }\n \n \n+", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTgyMA==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447759820", "bodyText": "Can you explain this change?", "author": "yanghua", "createdAt": "2020-06-30T15:09:25Z", "path": "hudi-cli/src/main/scala/org/apache/hudi/cli/DedupeSparkJob.scala", "diffHunk": "@@ -152,7 +209,7 @@ class DedupeSparkJob(basePath: String,\n       val newFilePath = new Path(s\"$repairOutputPath/${fileNameToPathMap(fileName).getName}\")\n       LOG.info(\" Skipping and writing new file for : \" + fileName)\n       SparkHelpers.skipKeysAndWriteNewFile(instantTime, fs, badFilePath, newFilePath, dupeFixPlan(fileName))\n-      fs.delete(badFilePath, false)\n+      fs.delete(badFilePath, true)", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzIwNDM1Ng==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r453204356", "bodyText": "We need to set it to true because somehow xyz.parquet.bad is getting considered as a directory and not a file. Hence to be able to delete, we need to set recursive to true. @yanghua", "author": "pratyakshsharma", "createdAt": "2020-07-11T15:14:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc1OTgyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc2MjcyNg==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r447762726", "bodyText": "Can we rename the exists testDeduplicate  to testDeduplicateWithDefault or testDeduplicateWithInserts?", "author": "yanghua", "createdAt": "2020-06-30T15:13:18Z", "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -145,6 +166,60 @@ public void testDeduplicate() throws IOException {\n     assertEquals(200, result.count());\n   }\n \n+  @Test\n+  public void testDeduplicateWithUpdates() throws IOException {", "originalCommit": "8abfd9941b11e10de715930fee32f26829b11f75", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e2e34c5d4304524574322c3a0d6909dc37f398f5", "url": "https://github.com/apache/hudi/commit/e2e34c5d4304524574322c3a0d6909dc37f398f5", "message": "[HUDI-796]: code review changes", "committedDate": "2020-07-01T18:34:05Z", "type": "commit"}, {"oid": "59eb45ce254e1952a03ee8023604c13c96082862", "url": "https://github.com/apache/hudi/commit/59eb45ce254e1952a03ee8023604c13c96082862", "message": "[HUDI-796]: fixed failing test cases", "committedDate": "2020-07-11T15:12:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM4NzI0Mw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r453387243", "bodyText": "Can we change it to be:\n      if (!DeDupeType.values().contains(DeDupeType.withName(dedupeType))) {}", "author": "yanghua", "createdAt": "2020-07-13T00:59:13Z", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -77,8 +78,14 @@ public String deduplicate(\n           help = \"Spark executor memory\") final String sparkMemory,\n       @CliOption(key = {\"dryrun\"},\n           help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n-          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun,\n+      @CliOption(key = {\"dedupeType\"}, help = \"Valid values are - insert_type, update_type and upsert_type\",\n+          unspecifiedDefaultValue = \"insert_type\") final String dedupeType)\n       throws Exception {\n+    if (!dedupeType.equals(DeDupeType.INSERT_TYPE().toString()) && !dedupeType.equals(DeDupeType.UPDATE_TYPE().toString())", "originalCommit": "59eb45ce254e1952a03ee8023604c13c96082862", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTcxNTQ2Nw==", "url": "https://github.com/apache/hudi/pull/1558#discussion_r489715467", "bodyText": "Done.", "author": "pratyakshsharma", "createdAt": "2020-09-16T19:49:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzM4NzI0Mw=="}], "type": "inlineReview"}, {"oid": "aae238a473d24695d07dd4dfa13fe1d8464e3d10", "url": "https://github.com/apache/hudi/commit/aae238a473d24695d07dd4dfa13fe1d8464e3d10", "message": "[HUDI-796]: In sync with master", "committedDate": "2020-09-16T19:50:24Z", "type": "commit"}, {"oid": "092ccc8673b8bfc471acb5e2a6b95a16fece7a4f", "url": "https://github.com/apache/hudi/commit/092ccc8673b8bfc471acb5e2a6b95a16fece7a4f", "message": "[HUDI-796]: fixed error", "committedDate": "2020-09-17T07:31:49Z", "type": "commit"}]}