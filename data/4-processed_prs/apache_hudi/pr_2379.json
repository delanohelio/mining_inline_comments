{"pr_number": 2379, "pr_title": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "pr_createdAt": "2020-12-25T12:35:48Z", "pr_url": "https://github.com/apache/hudi/pull/2379", "timeline": [{"oid": "76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "url": "https://github.com/apache/hudi/commit/76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "message": "[HUDI-1481]  add  structured streaming and delta streamer clustering unit test", "committedDate": "2020-12-22T23:33:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjU3OQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906579", "bodyText": "I think we need separate methods for scheduling and executing (Similar to compaction). This is because single writer limitation - scheduling can only happen when ingestion is not running. Where as executing clustering plan can happen in parallel to ingestion.", "author": "satishkotha", "createdAt": "2020-12-25T19:54:43Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClustering clustering = new HoodieClustering(jsc, cfg);\n+    clustering.cluster(cfg.retry);\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        ret = scheduleAndCluster(jsc);", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA2MzAxMA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549063010", "bodyText": "make  sense", "author": "lw309637554", "createdAt": "2020-12-27T04:20:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjU3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA5NDg3OA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549094878", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-27T10:27:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjg3Ng==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906876", "bodyText": "are we missing few porperties like\n\nsort_columns to use for clustering\nschedule clustering strategy\nexecute clustering strategy\nclustering targetFileSize\n\nAre these expected to be in propsFile? Can you give me an example props file that you used to test?", "author": "satishkotha", "createdAt": "2020-12-25T19:58:11Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkyNzc2OQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548927769", "bodyText": "i think these conf in  --props or --hoodie-conf  would be general and clear , i will add a example", "author": "lw309637554", "createdAt": "2020-12-26T00:58:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjg3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA5NDc1Ng==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549094756", "bodyText": "generate two small filegroups in /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/dest/ dir.\nthen run clustering\ncat  /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/config/clusteringjob.properties\nhoodie.clustering.inline.max.commits=2\n\n\nschedule clustering\nbin/spark-submit \n--master local[4]  \n--class org.apache.hudi.utilities.HoodieClusteringJob  \n/Users/liwei/work-space/dla/opensource/incubator-hudi/packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.1-SNAPSHOT.jar \n--schedule  \n--base-path /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/dest \n--table-name hudi_table_with_small_filegroups2 \n--instant-time 20201227161308 \n--schema-file /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/config/schema.avsc  \n--props  /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/config/clusteringjob.properties  \n--spark-memory 1g\n\n2\u3001cluster\nbin/spark-submit \n--master local[4]  \n--class org.apache.hudi.utilities.HoodieClusteringJob  \n/Users/liwei/work-space/dla/opensource/incubator-hudi/packaging/hudi-utilities-bundle/target/hudi-utilities-bundle_2.11-0.6.1-SNAPSHOT.jar \n--base-path /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/dest \n--table-name hudi_table_with_small_filegroups2 \n--instant-time 20201227161308 \n--schema-file /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/config/schema.avsc  \n--props  /Users/liwei/work-space/spark/spark-2.4.6-bin-hadoop2.7/hudi_table_with_small_filegroups2/config/clusteringjob.properties  \n--spark-memory 1g", "author": "lw309637554", "createdAt": "2020-12-27T10:26:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjk5MQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906991", "bodyText": "were you able to run this? Can you give me an example command line on how you ran this?", "author": "satishkotha", "createdAt": "2020-12-25T19:59:15Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA5NDgzOQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549094839", "bodyText": "as above", "author": "lw309637554", "createdAt": "2020-12-27T10:26:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNzE1Mg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548907152", "bodyText": "schedule returns a boolean. We can only run clustering if return value is true. You may want to add that check\nboolean isScheduled =     client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty());\nif (isScheduled) {\n// run clustering\n} else {\n// log that there is no enough data to run clustering\n}", "author": "satishkotha", "createdAt": "2020-12-25T20:02:02Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClustering clustering = new HoodieClustering(jsc, cfg);\n+    clustering.cluster(cfg.retry);\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        ret = scheduleAndCluster(jsc);\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(t);\n+    }\n+    return ret;\n+  }\n+\n+  private int scheduleAndCluster(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = new Schema.Parser().parse(fs.open(new Path(cfg.schemaFile))).toString();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty());", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA5NDk0Mw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549094943", "bodyText": "have separate  schedule and cluster", "author": "lw309637554", "createdAt": "2020-12-27T10:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNzE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0Njk3MA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548946970", "bodyText": "revert this change?", "author": "leesf", "createdAt": "2020-12-26T05:44:45Z", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -107,22 +107,23 @@ class TestCOWDataSource extends HoodieClientTestBase {\n       .options(commonOpts)\n       .mode(SaveMode.Append)\n       .save(basePath)\n+    val commitInstantTime2 = HoodieDataSourceHelpers.latestCommit(fs, basePath)\n \n     val snapshotDF2 = spark.read.format(\"hudi\").load(basePath + \"/*/*/*/*\")\n     assertEquals(100, snapshotDF2.count())\n     assertEquals(updatedVerificationVal, snapshotDF2.filter(col(\"_row_key\") === verificationRowKey).select(verificationCol).first.getString(0))\n \n     // Upsert Operation without Hudi metadata columns\n     val records2 = recordsToStrings(dataGen.generateUpdates(\"001\", 100)).toList\n-    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2 , 2))", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NTMyNA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549055324", "bodyText": "+1 looks like a no-op, please revert this change", "author": "n3nash", "createdAt": "2020-12-27T02:31:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0Njk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0NzI5Mg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548947292", "bodyText": "change to msg?", "author": "leesf", "createdAt": "2020-12-26T05:48:14Z", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0ODE4OA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548948188", "bodyText": "would replace the below three HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH with one parameter? it is a little weird to use HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH in getLatestFileGroupsFileId.", "author": "leesf", "createdAt": "2020-12-26T06:00:55Z", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTUzMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949532", "bodyText": "format", "author": "leesf", "createdAt": "2020-12-26T06:19:36Z", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+\n+    // Second insert of data\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+\n+    val hudiOptions = getInlineClusteringOpts(isInlineClustering.toString, \"2\", 100)\n+    val f1 = initStreamingWriteFuture(inputDF1.schema, sourcePath, destPath, hudiOptions)\n+\n+    val f2 = Future {\n+      inputDF1.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process one microbatch\n+      val currNumCommits = waitTillAtleastNCommits(fs, destPath, 1, 120, 5)\n+      assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, destPath, \"000\"))\n+\n+      inputDF2.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process second microbatch\n+      waitTillAtleastNCommits(fs, destPath, currNumCommits + 1, 120, 5)\n+      assertEquals(2, HoodieDataSourceHelpers.listCommitsSince(fs, destPath, \"000\").size())\n+\n+      // check have more than one file group\n+      this.metaClient = new HoodieTableMetaClient(fs.getConf, destPath, true)\n+      assertTrue(getLatestFileGroupsFileId().size > 1)\n+\n+      // check clustering result\n+      checkClusteringResult(destPath)\n+\n+      // check data correct after clustering\n+      val hoodieROViewDF2 = spark.read.format(\"org.apache.hudi\")\n+        .load(destPath + \"/*/*/*/*\")\n+      assertEquals(200, hoodieROViewDF2.count())\n+    }\n+    Await.result(Future.sequence(Seq(f1, f2)), Duration.Inf)\n+  }\n+\n+  private def getLatestFileGroupsFileId():Array[String] = {\n+    getHoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline,\n+      HoodieTestTable.of(metaClient).listAllBaseFiles())\n+    tableView.getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n+      .toArray().map(slice => slice.asInstanceOf[FileSlice].getFileGroupId.getFileId)\n+  }\n+\n+  @throws[InterruptedException]\n+  private def waitTillHasCompletedReplaceInstant(tablePath: String,\n+                                                timeoutSecs: Int, sleepSecsAfterEachRun: Int) = {\n+    val beginTime = System.currentTimeMillis\n+    var currTime = beginTime\n+    val timeoutMsecs = timeoutSecs * 1000\n+    var success = false\n+    while ({!success && (currTime - beginTime) < timeoutMsecs}) try {\n+      this.metaClient.reloadActiveTimeline()\n+      val completeReplaceSize = this.metaClient.getActiveTimeline.getCompletedReplaceTimeline().getInstants.toArray.size\n+      println(\"completeReplaceSize:\" + completeReplaceSize)\n+      if(completeReplaceSize > 0) {", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTU2NA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949564", "bodyText": "Timed-out waiting for completing replace instant appear in \" + tablePath", "author": "leesf", "createdAt": "2020-12-26T06:20:18Z", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+\n+    // Second insert of data\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+\n+    val hudiOptions = getInlineClusteringOpts(isInlineClustering.toString, \"2\", 100)\n+    val f1 = initStreamingWriteFuture(inputDF1.schema, sourcePath, destPath, hudiOptions)\n+\n+    val f2 = Future {\n+      inputDF1.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process one microbatch\n+      val currNumCommits = waitTillAtleastNCommits(fs, destPath, 1, 120, 5)\n+      assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, destPath, \"000\"))\n+\n+      inputDF2.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process second microbatch\n+      waitTillAtleastNCommits(fs, destPath, currNumCommits + 1, 120, 5)\n+      assertEquals(2, HoodieDataSourceHelpers.listCommitsSince(fs, destPath, \"000\").size())\n+\n+      // check have more than one file group\n+      this.metaClient = new HoodieTableMetaClient(fs.getConf, destPath, true)\n+      assertTrue(getLatestFileGroupsFileId().size > 1)\n+\n+      // check clustering result\n+      checkClusteringResult(destPath)\n+\n+      // check data correct after clustering\n+      val hoodieROViewDF2 = spark.read.format(\"org.apache.hudi\")\n+        .load(destPath + \"/*/*/*/*\")\n+      assertEquals(200, hoodieROViewDF2.count())\n+    }\n+    Await.result(Future.sequence(Seq(f1, f2)), Duration.Inf)\n+  }\n+\n+  private def getLatestFileGroupsFileId():Array[String] = {\n+    getHoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline,\n+      HoodieTestTable.of(metaClient).listAllBaseFiles())\n+    tableView.getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n+      .toArray().map(slice => slice.asInstanceOf[FileSlice].getFileGroupId.getFileId)\n+  }\n+\n+  @throws[InterruptedException]\n+  private def waitTillHasCompletedReplaceInstant(tablePath: String,\n+                                                timeoutSecs: Int, sleepSecsAfterEachRun: Int) = {\n+    val beginTime = System.currentTimeMillis\n+    var currTime = beginTime\n+    val timeoutMsecs = timeoutSecs * 1000\n+    var success = false\n+    while ({!success && (currTime - beginTime) < timeoutMsecs}) try {\n+      this.metaClient.reloadActiveTimeline()\n+      val completeReplaceSize = this.metaClient.getActiveTimeline.getCompletedReplaceTimeline().getInstants.toArray.size\n+      println(\"completeReplaceSize:\" + completeReplaceSize)\n+      if(completeReplaceSize > 0) {\n+        success = true\n+      }\n+    } catch {\n+      case te: TableNotFoundException =>\n+        log.info(\"Got table not found exception. Retrying\")\n+    } finally {\n+      Thread.sleep(sleepSecsAfterEachRun * 1000)\n+      currTime = System.currentTimeMillis\n+    }\n+    if (!success) throw new IllegalStateException(\"Timed-out waiting for \"  + \" have completed replace instant appear in \" + tablePath)", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTY3Mw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949673", "bodyText": "would rename to HoodieClusterer?", "author": "leesf", "createdAt": "2020-12-26T06:22:18Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {", "originalCommit": "484f36ba2c739788c9b861b67f01508bf6503975", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzkxNA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549057914", "bodyText": "Please rename to HoodieClusteringJob", "author": "n3nash", "createdAt": "2020-12-27T03:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTY3Mw=="}], "type": "inlineReview"}, {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-27T10:49:16Z", "type": "commit"}, {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-27T10:49:16Z", "type": "forcePushed"}, {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399", "committedDate": "2020-12-28T06:52:17Z", "type": "commit"}, {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399", "committedDate": "2020-12-28T06:52:17Z", "type": "forcePushed"}, {"oid": "e9149d9d169757503c97cbcaf263a0ec5d24596d", "url": "https://github.com/apache/hudi/commit/e9149d9d169757503c97cbcaf263a0ec5d24596d", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-28T14:56:48Z", "type": "forcePushed"}, {"oid": "c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "url": "https://github.com/apache/hudi/commit/c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-29T03:40:35Z", "type": "forcePushed"}, {"oid": "44d0c65d07367065e010d84c06f0a5370d37b862", "url": "https://github.com/apache/hudi/commit/44d0c65d07367065e010d84c06f0a5370d37b862", "message": "[HUDI-1399] support  a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-29T05:07:42Z", "type": "forcePushed"}, {"oid": "e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "url": "https://github.com/apache/hudi/commit/e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "message": "[HUDI-1399]  support  a independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-29T06:12:35Z", "type": "forcePushed"}, {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "url": "https://github.com/apache/hudi/commit/370be89f339280cd96bf89a18d3aab7545d3a6fe", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-29T07:08:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNDI0Mg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549824242", "bodyText": "hmm, this is actually tricky. what if the replace_commit belongs to insert_overwrite? We have to rollback insert_overwrite related replacecommits, but not clustering related ones.\nWe could move this method out of timeline into a utils method and parse content to filter only clustering instants.", "author": "satishkotha", "createdAt": "2020-12-29T19:38:55Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -95,6 +95,13 @@ public HoodieTimeline filterPendingExcludingCompaction() {\n             && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION))), details);\n   }\n \n+  @Override\n+  public HoodieTimeline filterPendingExcludingCompactionAndClustering() {\n+    return new HoodieDefaultTimeline(instants.stream().filter(instant -> (!instant.isCompleted())\n+        && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION))\n+        && (!instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION))), details);", "originalCommit": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM4NDY3NQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r550384675", "bodyText": "thanks, make sense .", "author": "lw309637554", "createdAt": "2020-12-31T03:12:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNDI0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNTQ2Mg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549825462", "bodyText": "can we get schema from latest commit instead of requiring additional config to simplify?", "author": "satishkotha", "createdAt": "2020-12-29T19:43:53Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)", "originalCommit": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNjUwNA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549826504", "bodyText": "I think you created a ticket for this TODO? could you link the ticket here?", "author": "satishkotha", "createdAt": "2020-12-29T19:47:28Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -296,7 +296,9 @@ public void refreshTimeline() throws IOException {\n     // Retrieve the previous round checkpoints, if any\n     Option<String> resumeCheckpointStr = Option.empty();\n     if (commitTimelineOpt.isPresent()) {\n-      Option<HoodieInstant> lastCommit = commitTimelineOpt.get().lastInstant();\n+      // TODO: now not support replace action", "originalCommit": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM4NDY4Ng==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r550384686", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-31T03:12:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNjUwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM0MDExOA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r550340118", "bodyText": "@lw309637554 @satishkotha Is this supposed to be a async service that can run in the deltastreamer / independently ? If yes, we should implement this as an HoodieAsyncService. Take a look at an example async cleaner service here -> https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AsyncCleanerService.java", "author": "n3nash", "createdAt": "2020-12-30T22:06:02Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {", "originalCommit": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM4NTM0MA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r550385340", "bodyText": "@n3nash @satishkotha  Thanks, as the discussion in https://issues.apache.org/jira/browse/HUDI-1399 , my plan is :\nfirst : support a independent clustering spark job to asynchronously clustering just like HoodieCompactor. Then users can have the base async clustering ability.\nsecond: support clustering HoodieAsyncService for deltastreamer and structured streaming.\uff08this will land in https://issues.apache.org/jira/browse/HUDI-1482 and https://issues.apache.org/jira/browse/HUDI-1483\uff09\nthird: support cli", "author": "lw309637554", "createdAt": "2020-12-31T03:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM0MDExOA=="}], "type": "inlineReview"}, {"oid": "3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "url": "https://github.com/apache/hudi/commit/3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-31T03:18:44Z", "type": "forcePushed"}, {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-31T05:06:52Z", "type": "commit"}, {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering", "committedDate": "2020-12-31T05:06:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyNzgzMA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551627830", "bodyText": "i think its better to create 'instantTime' instead of using user-specified instant time for scheduling. At least, we should validate instant time specified is greater than latest commit on the table. Otherwise, there can be unexpected side-effects.", "author": "satishkotha", "createdAt": "2021-01-04T23:25:42Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);\n+    return schema.toString();\n+  }\n+\n+  private int doCluster(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    JavaRDD<WriteStatus> writeResponse =\n+        (JavaRDD<WriteStatus>) client.cluster(cfg.clusteringInstantTime, true).getWriteStatuses();\n+    return UtilHelpers.handleErrors(jsc, cfg.clusteringInstantTime, writeResponse);\n+  }\n+\n+  private int doSchedule(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    return client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty()) ? 0 : -1;", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDM4MA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984380", "bodyText": "@satishkotha  make sense , have use client.scheduleClustering() to auto generate instantTime. Also have some questions for you. Why clustering instant time should greater than latest commit?  I see BaseScheduleCompactionActionExecutor.execute() have check  that should not have earliest write inflight instant time than compaction instant time. But BaseClusteringPlanActionExecutor.execute()  do not have similarity check. Thanks.", "author": "lw309637554", "createdAt": "2021-01-05T14:58:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyNzgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMDkyMA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551630920", "bodyText": "This is probably minor. Looks like 'getTableAvroSchema' throws runtime exception if there are no completed commits. may be add a check to verify commit timeline is not empty before calling this method? This way we can throw meaningful error message: say \"Cannot run clustering without any completed commits\"", "author": "satishkotha", "createdAt": "2021-01-04T23:35:03Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDUzMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984532", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-05T14:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMDkyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMTEyNw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551631127", "bodyText": "I'd suggest make this required only for executing clustering. For scheduling, we should auto-generate instant time to make it easy to run.", "author": "satishkotha", "createdAt": "2021-01-04T23:35:42Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NDcyMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551984722", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-05T14:59:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMTEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNTMzNQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551635335", "bodyText": "I dont see any asserts related to clustering. Can we add basic validation that new files generated by clustering have same data as previous files? Let me know if I'm misreading.", "author": "satishkotha", "createdAt": "2021-01-04T23:44:12Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -682,6 +693,58 @@ public void testInlineClustering() throws Exception {\n     });\n   }\n \n+  private HoodieClusteringJob.Config buildHoodieClusteringUtilConfig(String basePath,\n+                                                                  String clusteringInstantTime, boolean runSchedule) {\n+    HoodieClusteringJob.Config config = new HoodieClusteringJob.Config();\n+    config.basePath = basePath;\n+    config.clusteringInstantTime = clusteringInstantTime;\n+    config.runSchedule = runSchedule;\n+    config.propsFilePath = dfsBasePath + \"/clusteringjob.properties\";\n+    return config;\n+  }\n+\n+  @Test\n+  public void testHoodieAsyncClusteringJob() throws Exception {\n+    String tableBasePath = dfsBasePath + \"/asyncClustering\";\n+    // Keep it higher than batch-size to test continuous mode\n+    int totalRecords = 3000;\n+\n+    // Initial bulk insert\n+    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n+    cfg.continuousMode = true;\n+    cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();\n+    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n+    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    cfg.configs.add(String.format(\"%s=true\", HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n+    deltaStreamerTestRunner(ds, cfg, (r) -> {\n+      TestHelpers.assertAtLeastNCommits(2, tableBasePath, dfs);\n+      // for not confiict with delta streamer commit, just add 3600s\n+      String clusterInstantTime = HoodieActiveTimeline.COMMIT_FORMATTER\n+          .format(new Date(System.currentTimeMillis() + 3600 * 1000));\n+      LOG.info(\"Cluster instant time \" + clusterInstantTime);\n+      HoodieClusteringJob.Config scheduleClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+          clusterInstantTime, true);\n+      HoodieClusteringJob scheduleClusteringJob = new HoodieClusteringJob(jsc, scheduleClusteringConfig);\n+      int scheduleClusteringResult = scheduleClusteringJob.cluster(scheduleClusteringConfig.retry);\n+      if (scheduleClusteringResult == 0) {\n+        LOG.info(\"Schedule clustering success, now cluster\");\n+        HoodieClusteringJob.Config clusterClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+            clusterInstantTime, false);\n+        HoodieClusteringJob clusterClusteringJob = new HoodieClusteringJob(jsc, clusterClusteringConfig);\n+        clusterClusteringJob.cluster(clusterClusteringConfig.retry);\n+        LOG.info(\"Cluster success\");\n+      } else {\n+        LOG.warn(\"Schedule clustering failed\");\n+      }\n+      HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n+      int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\n+      int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\n+      System.out.println(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\n+      return completeReplaceSize > 0;", "originalCommit": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTk4NjU2NQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551986565", "bodyText": "Because if always completeReplaceSize <= 0 the runner will throw time out exception.Now i add the assert for completeReplaceSize == 1. As the unit test mainly test async clustering schedule and cluster, just assert completeReplaceSize will be ok. For records check can cover in cluster unit test.", "author": "lw309637554", "createdAt": "2021-01-05T15:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNTMzNQ=="}], "type": "inlineReview"}, {"oid": "152ea1c63a7759168a20f1c50e6441c6246e9619", "url": "https://github.com/apache/hudi/commit/152ea1c63a7759168a20f1c50e6441c6246e9619", "message": "[HUDI-1498] Read clustering plan from requested file for inflight instant (#2389)", "committedDate": "2021-01-05T02:30:19Z", "type": "commit"}, {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe", "url": "https://github.com/apache/hudi/commit/4cc4b350d51240a866e71b16b92df13348ceeefe", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-05T14:40:28Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzMzU5OA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552133598", "bodyText": "Can you change this to a LOG message?", "author": "satishkotha", "createdAt": "2021-01-05T19:05:11Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -109,6 +111,9 @@ public static void main(String[] args) {\n     if (result == -1) {\n       LOG.error(resultMsg + \" failed\");\n     } else {\n+      if (cfg.runSchedule) {\n+        System.out.println(\"The schedule instant time is \" + cfg.clusteringInstantTime);", "originalCommit": "4cc4b350d51240a866e71b16b92df13348ceeefe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyOTYwNQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552329605", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-06T02:28:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzMzU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzNDMxMw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552134313", "bodyText": "changing config at this stage seems a little awkward. Do you think its better to return Option instantTime from this method? Or maybe just add the log line here with instantTime?", "author": "satishkotha", "createdAt": "2021-01-05T19:06:31Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -153,7 +161,12 @@ private int doSchedule(JavaSparkContext jsc) throws Exception {\n     String schemaStr = getSchemaFromLatestInstant();\n     SparkRDDWriteClient client =\n         UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n-    return client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty()) ? 0 : -1;\n+    Option<String> instantTime = client.scheduleClustering(Option.empty());\n+    if (instantTime.isPresent()) {\n+      cfg.clusteringInstantTime = instantTime.get();", "originalCommit": "4cc4b350d51240a866e71b16b92df13348ceeefe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjMyOTczNA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552329734", "bodyText": "done, also add a method for test", "author": "lw309637554", "createdAt": "2021-01-06T02:29:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzNDMxMw=="}], "type": "inlineReview"}, {"oid": "d53595e73e60a2f9cf799d8eeaba00416dd336ba", "url": "https://github.com/apache/hudi/commit/d53595e73e60a2f9cf799d8eeaba00416dd336ba", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-06T02:30:56Z", "type": "forcePushed"}, {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead", "url": "https://github.com/apache/hudi/commit/b165739d03ebe8fc393e069cfd099a8e1098dead", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-06T07:27:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTIxMw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554301213", "bodyText": "@lw309637554 Can you add this API to the HoodieTimeline ? You can name it filterPendingExcludingCompactionAndReplace for now.", "author": "n3nash", "createdAt": "2021-01-09T06:49:43Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -711,12 +711,30 @@ public void rollbackInflightCompaction(HoodieInstant inflightInstant, HoodieTabl\n     table.getActiveTimeline().revertCompactionInflightToRequested(inflightInstant);\n   }\n \n+  /**\n+   * Get inflight time line exclude compaction and clustering.\n+   * @param table\n+   * @return\n+   */\n+  private HoodieTimeline getInflightTimelineExcludeCompactionAndClustering(HoodieTable<T, I, K, O> table) {", "originalCommit": "b165739d03ebe8fc393e069cfd099a8e1098dead", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDQwMzU5Ng==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554403596", "bodyText": "@n3nash Hello, about this API in timeline or as a util have  discussed with @satishkotha . Finally decided to put it here. Because here just need to exclude clustering of replace, not insert_overwrite of replace. Also check clustering need use metaclient, but timeline do not have metaclient object.", "author": "lw309637554", "createdAt": "2021-01-09T12:00:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTY2NQ==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554301665", "bodyText": "This code about retry is repeated in HoodieCompactor as well, is there a way to reuse this using some kinds of Utils class ?", "author": "n3nash", "createdAt": "2021-01-09T06:54:57Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.jetbrains.annotations.TestOnly;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time, only need when cluster. \"\n+        + \"And schedule clustering can generate it.\", required = false)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0 || (!cfg.runSchedule && cfg.clusteringInstantTime == null)) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {", "originalCommit": "b165739d03ebe8fc393e069cfd099a8e1098dead", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDQzMDAwMg==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554430002", "bodyText": "done", "author": "lw309637554", "createdAt": "2021-01-09T13:55:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTY2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMjAyMw==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554302023", "bodyText": "Why is this specific test case in TestHoodieDeltaStreamer ? Is the intention to just test if clustering works with deltastreamer ? I'm worried this is polluting the tests in hoodie delta streamer, is this test case to be refactored after deltastreamer natively supports clustering ? If yes, can we open a ticket for the follow up..", "author": "n3nash", "createdAt": "2021-01-09T06:58:19Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -672,14 +681,72 @@ public void testInlineClustering() throws Exception {\n     cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n     cfg.configs.add(String.format(\"%s=%s\", HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\"));\n     cfg.configs.add(String.format(\"%s=%s\", HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP, \"2\"));\n-\n-    deltaStreamerTestRunner(cfg, (r) -> {\n+    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n+    deltaStreamerTestRunner(ds, cfg, (r) -> {\n       HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n       int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\n       int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\n       LOG.info(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\n       return completeReplaceSize > 0;\n     });\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n+    assertEquals(1, metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length);\n+  }\n+\n+  private HoodieClusteringJob.Config buildHoodieClusteringUtilConfig(String basePath,\n+                                                                  String clusteringInstantTime, boolean runSchedule) {\n+    HoodieClusteringJob.Config config = new HoodieClusteringJob.Config();\n+    config.basePath = basePath;\n+    config.clusteringInstantTime = clusteringInstantTime;\n+    config.runSchedule = runSchedule;\n+    config.propsFilePath = dfsBasePath + \"/clusteringjob.properties\";\n+    return config;\n+  }\n+\n+  @Test\n+  public void testHoodieAsyncClusteringJob() throws Exception {", "originalCommit": "b165739d03ebe8fc393e069cfd099a8e1098dead", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDQzMDQ5MA==", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554430490", "bodyText": "Yes, it  just test if clustering works with deltastreamer ,and find two issues. After natively clustering support in https://issues.apache.org/jira/browse/HUDI-1482 and https://issues.apache.org/jira/browse/HUDI-1483. I will refactored it in https://issues.apache.org/jira/browse/HUDI-1516", "author": "lw309637554", "createdAt": "2021-01-09T14:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMjAyMw=="}], "type": "inlineReview"}, {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-09T13:54:16Z", "type": "commit"}, {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time", "committedDate": "2021-01-09T13:54:16Z", "type": "forcePushed"}]}