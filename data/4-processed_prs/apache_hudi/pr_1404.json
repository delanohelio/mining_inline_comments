{"pr_number": 1404, "pr_title": "[HUDI-344] Improve exporter tests", "pr_createdAt": "2020-03-14T00:33:51Z", "pr_url": "https://github.com/apache/hudi/pull/1404", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUzOTE4MA==", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392539180", "bodyText": "This was copied over from hudi-spark module. it is not needed when using write client to prepare data", "author": "xushiyan", "createdAt": "2020-03-14T00:39:08Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -1,50 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.utilities;\n-\n-import org.apache.hudi.common.TestRawTripPayload;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.util.Option;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Test utils for data source tests.\n- */\n-public class DataSourceTestUtils {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUzOTM5OA==", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392539398", "bodyText": "Make sure data generated fall into 1 partition for ease of verification", "author": "xushiyan", "createdAt": "2020-03-14T00:40:27Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -18,205 +18,144 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n-import org.apache.hudi.common.HoodieCommonTestHarness;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.sql.SparkSession;\n-\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n \n-public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n-\n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n+  private static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n+  private static final int NUM_RECORDS = 100;\n+  private static final String COMMIT_TIME = \"20200101000000\";\n+  private static final String PARTITION_PATH = \"2020/01/01\";\n+  private static final String TABLE_NAME = \"testing\";\n+  private String sourcePath;\n+  private String targetPath;\n \n   @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n-\n+  public void setUp() throws Exception {\n+    initSparkContexts();\n+    initDFS();\n+    dataGen = new HoodieTestDataGenerator(new String[]{PARTITION_PATH});", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTM0MA==", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392595340", "bodyText": "Does '/_SUCCESS' auto generated by hudi HoodieSnapshotExporte? I would only see '/_SUCCESS' in HoodieSnapshotCopier.", "author": "leesf", "createdAt": "2020-03-14T15:12:39Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -18,205 +18,144 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n-import org.apache.hudi.common.HoodieCommonTestHarness;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.SparkSession;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n \n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.SparkSession;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n-import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n+import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n+public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n \n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n+  private static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n+  private static final int NUM_RECORDS = 100;\n+  private static final String COMMIT_TIME = \"20200101000000\";\n+  private static final String PARTITION_PATH = \"2020/01/01\";\n+  private static final String TABLE_NAME = \"testing\";\n+  private String sourcePath;\n+  private String targetPath;\n \n   @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n-\n+  public void setUp() throws Exception {\n+    initSparkContexts();\n+    initDFS();\n+    dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+\n+    // Initialize test data dirs\n+    sourcePath = dfsBasePath + \"/source/\";\n+    targetPath = dfsBasePath + \"/target/\";\n+    dfs.mkdirs(new Path(sourcePath));\n+    dfs.mkdirs(new Path(targetPath));\n+    HoodieTableMetaClient\n+        .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n+            HoodieAvroPayload.class.getName());\n+\n+    // Prepare data as source Hudi dataset\n+    HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n+    HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n+    hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n+    List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n+    JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n+    hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n+    hdfsWriteClient.close();\n+\n+    RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n+    while (itr.hasNext()) {\n+      LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n+    }\n   }\n \n   @After\n-  public void cleanup() {\n-    if (spark != null) {\n-      spark.stop();\n-    }\n+  public void tearDown() throws Exception {\n+    cleanupSparkContexts();\n+    cleanupDFS();\n+    cleanupTestDataGenerator();\n   }\n \n-  @Test\n-  public void testSnapshotExporter() throws IOException {\n-    // Insert Operation\n-    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n-    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n-    inputDF.write().format(\"hudi\")\n-        .options(commonOpts)\n-        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n-        .mode(SaveMode.Overwrite)\n-        .save(basePath);\n-    long sourceCount = inputDF.count();\n-\n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    long targetCount = spark.read().json(outputPath).count();\n-\n-    assertTrue(sourceCount == targetCount);\n-\n-    // Test Invalid OutputFormat\n-    cfg.outputFormat = \"foo\";\n-    int isError = hoodieSnapshotExporter.export(spark, cfg);\n-    assertTrue(isError == -1);\n+  private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n+    return HoodieWriteConfig.newBuilder()\n+        .withPath(basePath)\n+        .withEmbeddedTimelineServerEnabled(false)\n+        .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n+        .withParallelism(2, 2)\n+        .withBulkInsertParallelism(2)\n+        .forTable(TABLE_NAME)\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n+        .build();\n   }\n \n-  // for testEmptySnapshotCopy\n-  public void init() throws IOException {\n-    TemporaryFolder folder = new TemporaryFolder();\n-    folder.create();\n-    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n-    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n-    outputPath = rootPath + \"/output\";\n-\n-    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n-    fs = FSUtils.getFs(basePath, hadoopConf);\n-    HoodieTestUtils.init(hadoopConf, basePath);\n+  @Test\n+  public void testExportAsParquet() throws IOException {\n+    HoodieSnapshotExporter.Config cfg = new Config();\n+    cfg.sourceBasePath = sourcePath;\n+    cfg.targetOutputPath = targetPath;\n+    cfg.outputFormat = \"parquet\";\n+    new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+    assertEquals(NUM_RECORDS, sqlContext.read().parquet(targetPath).count());\n+    assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjYwMzU1NA==", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392603554", "bodyText": "@leesf Exporter seems has removed that logic for hudi case, though non-hudi case will do it automatically via spark. I'll ensure both create the success tag.", "author": "xushiyan", "createdAt": "2020-03-14T17:09:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTM0MA=="}], "type": "inlineReview"}, {"oid": "5dc21d6855b85683ba680f90e9409e289a8142e2", "url": "https://github.com/apache/hudi/commit/5dc21d6855b85683ba680f90e9409e289a8142e2", "message": "[HUDI-344] Improve exporter tests\n\n* Use HoodieWriteClient to prepare data for testing\n* Re-enabled tests that were commented out\n* Remove duplicated test util class", "committedDate": "2020-03-14T17:49:09Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "ca7df189c88c4c851a7077c497673e9a703678b3", "url": "https://github.com/apache/hudi/commit/ca7df189c88c4c851a7077c497673e9a703678b3", "message": "[HUDI-344] Parameterize testcases", "committedDate": "2020-03-14T21:18:37Z", "type": "commit"}, {"oid": "ca7df189c88c4c851a7077c497673e9a703678b3", "url": "https://github.com/apache/hudi/commit/ca7df189c88c4c851a7077c497673e9a703678b3", "message": "[HUDI-344] Parameterize testcases", "committedDate": "2020-03-14T21:18:37Z", "type": "forcePushed"}]}