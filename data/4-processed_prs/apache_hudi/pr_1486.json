{"pr_number": 1486, "pr_title": "[HUDI-759] Integrate checkpoint provider with delta streamer", "pr_createdAt": "2020-04-05T05:25:15Z", "pr_url": "https://github.com/apache/hudi/pull/1486", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405412784", "bodyText": "Let us change the variable name to conf?", "author": "pratyakshsharma", "createdAt": "2020-04-08T10:11:26Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -153,7 +153,7 @@\n   private transient HoodieWriteClient writeClient;\n \n   public DeltaSync(HoodieDeltaStreamer.Config cfg, SparkSession sparkSession, SchemaProvider schemaProvider,\n-                   TypedProperties props, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n+                   TypedProperties props, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcxOTE2Mw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405719163", "bodyText": "I keep hiveConf here for better readability. There are two other configuration cfg props here, so I believe hiveConf could be more clear for the user to know what this is for. WDYT?", "author": "garyli1019", "createdAt": "2020-04-08T18:13:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1NzgwNQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405957805", "bodyText": "but it can be misleading for e.g if user does not even configure hive sync right? I also prefer conf", "author": "vinothchandar", "createdAt": "2020-04-09T05:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1Mzk2OA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406553968", "bodyText": "This config was only used by hive sync, if the user don't config this it will use the default Hadoop config. I don't have a strong opinion here and ok to go with conf.", "author": "garyli1019", "createdAt": "2020-04-10T01:01:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU4OTMzNQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406589335", "bodyText": "ok...After a closer look, I'd still vote for naming this as hiveConf. There are cfg props in HoodieDeltaStreamer and DeltaSync. If we name it conf, I personally will get confused while reading the code. This hiveConf is specifically for Hive Sync, even it's not HiveConf type anymore, but hiveConf will still represent what it does? WDYT?", "author": "garyli1019", "createdAt": "2020-04-10T03:44:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMzUwNA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407233504", "bodyText": "I still think its confusing to have it as hiveConf...  esp when you are actually creating  HiveConf from this down below..", "author": "vinothchandar", "createdAt": "2020-04-12T18:10:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI0NDk0MA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407244940", "bodyText": "changed to conf", "author": "garyli1019", "createdAt": "2020-04-12T19:54:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMjc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMzIxMw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405413213", "bodyText": "lets change the variable name from hiveConf to conf?", "author": "pratyakshsharma", "createdAt": "2020-04-08T10:12:15Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -90,35 +90,33 @@\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()));\n+        jssc.hadoopConfiguration(), null);\n   }\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, TypedProperties props) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()), props);\n+        jssc.hadoopConfiguration(), props);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n-                             TypedProperties properties) throws IOException {\n-    this.cfg = cfg;\n-    this.deltaSyncService = new DeltaSyncService(cfg, jssc, fs, hiveConf, properties);\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf) throws IOException {\n+    this(cfg, jssc, fs, hiveConf, null);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1ODk4MA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405958980", "bodyText": "same here.", "author": "vinothchandar", "createdAt": "2020-04-09T05:08:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMzIxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMzMxOA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405413318", "bodyText": "ditto.", "author": "pratyakshsharma", "createdAt": "2020-04-08T10:12:27Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -90,35 +90,33 @@\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()));\n+        jssc.hadoopConfiguration(), null);\n   }\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, TypedProperties props) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()), props);\n+        jssc.hadoopConfiguration(), props);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n-                             TypedProperties properties) throws IOException {\n-    this.cfg = cfg;\n-    this.deltaSyncService = new DeltaSyncService(cfg, jssc, fs, hiveConf, properties);\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf) throws IOException {\n+    this(cfg, jssc, fs, hiveConf, null);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf) throws IOException {\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTQxMzYzOQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405413639", "bodyText": "ditto.", "author": "pratyakshsharma", "createdAt": "2020-04-08T10:12:59Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -371,7 +379,7 @@ public static void main(String[] args) throws Exception {\n      */\n     private transient DeltaSync deltaSync;\n \n-    public DeltaSyncService(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n+    public DeltaSyncService(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1ODc5Nw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405958797", "bodyText": "can we not pass null as a sentinel.. its risky business.. would an empty properties/map work?", "author": "vinothchandar", "createdAt": "2020-04-09T05:07:38Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -90,35 +90,33 @@\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()));\n+        jssc.hadoopConfiguration(), null);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1NDA2Mw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406554063", "bodyText": "totally understand", "author": "garyli1019", "createdAt": "2020-04-10T01:01:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1ODc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYxNzUwNQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406617505", "bodyText": "unfortunately, the null was served as a flag in many places in the delta streamer, because the command-line tool will produce null. If we want to change this, it might need a code refactoring.", "author": "garyli1019", "createdAt": "2020-04-10T06:09:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1ODc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3NDI1OQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407274259", "bodyText": "got it..", "author": "vinothchandar", "createdAt": "2020-04-13T00:50:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1ODc5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1OTE5Mw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405959193", "bodyText": "let's use Option instead of nulls?", "author": "vinothchandar", "createdAt": "2020-04-09T05:09:09Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -90,35 +90,33 @@\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()));\n+        jssc.hadoopConfiguration(), null);\n   }\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, TypedProperties props) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()), props);\n+        jssc.hadoopConfiguration(), props);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n-                             TypedProperties properties) throws IOException {\n-    this.cfg = cfg;\n-    this.deltaSyncService = new DeltaSyncService(cfg, jssc, fs, hiveConf, properties);\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf) throws IOException {\n+    this(cfg, jssc, fs, hiveConf, null);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf) throws IOException {\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf,\n+                             TypedProperties properties) throws IOException {\n+    if (cfg.initialCheckpointProvider != null && cfg.bootstrapFromPath != null && cfg.checkpoint == null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYxNzc4Mw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406617783", "bodyText": "same as above, it will need a code refactoring if we wanna get rid of null", "author": "garyli1019", "createdAt": "2020-04-10T06:10:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1OTE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzMyODA0NA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407328044", "bodyText": "lets file a separate issue for this? :)", "author": "vinothchandar", "createdAt": "2020-04-13T05:43:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1OTE5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc5MTQwMA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407791400", "bodyText": "yes https://issues.apache.org/jira/browse/HUDI-791", "author": "garyli1019", "createdAt": "2020-04-14T00:14:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk1OTE5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjAxNQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405966015", "bodyText": "IIUC setting cfg.checkpoint will force use of that timestamp instead of what we normally do - read from the last commit?\nShould we do this only when creating the dataset for the first time..", "author": "vinothchandar", "createdAt": "2020-04-09T05:35:45Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -90,35 +90,33 @@\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()));\n+        jssc.hadoopConfiguration(), null);\n   }\n \n   public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, TypedProperties props) throws IOException {\n     this(cfg, jssc, FSUtils.getFs(cfg.targetBasePath, jssc.hadoopConfiguration()),\n-        getDefaultHiveConf(jssc.hadoopConfiguration()), props);\n+        jssc.hadoopConfiguration(), props);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf,\n-                             TypedProperties properties) throws IOException {\n-    this.cfg = cfg;\n-    this.deltaSyncService = new DeltaSyncService(cfg, jssc, fs, hiveConf, properties);\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf) throws IOException {\n+    this(cfg, jssc, fs, hiveConf, null);\n   }\n \n-  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, HiveConf hiveConf) throws IOException {\n+  public HoodieDeltaStreamer(Config cfg, JavaSparkContext jssc, FileSystem fs, Configuration hiveConf,\n+                             TypedProperties properties) throws IOException {\n+    if (cfg.initialCheckpointProvider != null && cfg.bootstrapFromPath != null && cfg.checkpoint == null) {\n+      InitialCheckPointProvider checkPointProvider =\n+          UtilHelpers.createInitialCheckpointProvider(cfg.initialCheckpointProvider, new Path(cfg.bootstrapFromPath), fs);\n+      cfg.checkpoint = checkPointProvider.getCheckpoint();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1NTAzMA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406555030", "bodyText": "I think this depends on how we design the migration flow for the user.\nWhat I did myself is I use Spark datasource to do a bulkInsert to convert all the plain parquet files to Hudi format, then the second job I'd like to use delta streamer to read from Kafka. So this initialCheckpointProvider should be the first delta streamer job when switching sources.", "author": "garyli1019", "createdAt": "2020-04-10T01:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3NDUxMw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407274513", "bodyText": "yes.. do you think if we made it such that even if someone runs delta streamer few times after initial bootstrap, the initial checkpoint provider is used just once?  otherwise, you need to scramble to stop the delta streamer after the first run or manually run it by hand once before scheduling it using airflow or deploying in --continuous mode?", "author": "vinothchandar", "createdAt": "2020-04-13T00:53:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc5MzEzOQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407793139", "bodyText": "I believe the initial checkpoint provider should be just used once when the user wants to switch from one source to another. After that, the delta streamer should be able to get the checkpoint from the previous commit. We can improve this once the bootstrap is ready. At this point, I am not sure how to put everything together if we want one step to handling everything.", "author": "garyli1019", "createdAt": "2020-04-14T00:20:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2Nzk0OA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407867948", "bodyText": "okay.. lets revisit once we have bootstrap support.. cc @bvaradar as fyi", "author": "vinothchandar", "createdAt": "2020-04-14T05:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjAxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjM2MQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405966361", "bodyText": "testKafkaConnectCheckpointProvider?", "author": "vinothchandar", "createdAt": "2020-04-09T05:37:03Z", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -394,6 +394,26 @@ public void testProps() {\n         props.getString(\"hoodie.datasource.write.keygenerator.class\"));\n   }\n \n+  @Test\n+  public void testInitialCheckpointProvider() throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjYxNzg2NQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406617865", "bodyText": "done", "author": "garyli1019", "createdAt": "2020-04-10T06:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NjM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NzgyMA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405967820", "bodyText": "this is a path where the current dataset resides? I can see that we can eventually use this for actually bootstrapping the dataset.. but wondering if for now, the CheckpointProvider just takes a property containing the base path for reading/.computing checkpoints?\ni.e we can remove --bootstrap-from.. it almost sounds like we are actually bootstrapping the data.", "author": "vinothchandar", "createdAt": "2020-04-09T05:42:21Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -293,6 +295,12 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--checkpoint\"}, description = \"Resume Delta Streamer from this checkpoint.\")\n     public String checkpoint = null;\n \n+    @Parameter(names = {\"--bootstrap-from\"}, description = \"Initial bootstrap from this path\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1NTQ0NA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406555444", "bodyText": "yea, with the props file, we can get the path there. Don't need this field anymore", "author": "garyli1019", "createdAt": "2020-04-10T01:07:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2NzgyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2ODc4Mg==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r405968782", "bodyText": "it may be worth turning this into  abstract void init(Path, FileSystem) method? that way all subclasses are forced to implement the right one..\nAlso can we pass the props or the master list of properties (like we do for key generators) into this abstraction and let the CheckpointProvider use an explicit property like hoodie.deltastreamer.checkpointprovider.kafka.connect.path to derive the bootstrap path", "author": "vinothchandar", "createdAt": "2020-04-09T05:45:43Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/InitialCheckPointProvider.java", "diffHunk": "@@ -20,12 +20,23 @@\n \n import org.apache.hudi.exception.HoodieException;\n \n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n /**\n  * Provide the initial checkpoint for delta streamer.\n  */\n-public interface InitialCheckPointProvider {\n+public abstract class InitialCheckPointProvider {\n+  protected final Path path;\n+  protected final FileSystem fs;\n+\n+  public InitialCheckPointProvider(final Path basePath, final FileSystem fileSystem) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU1Mjc5OQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r406552799", "bodyText": "Adding props is a good idea. It will add more flexibility for other providers.", "author": "garyli1019", "createdAt": "2020-04-10T00:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTk2ODc4Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzMzM4OA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407233388", "bodyText": "I get where you are coming from for final but given we are not following this everywhere.. can we please limit this PR to the minimal changes needed for this functionality..", "author": "vinothchandar", "createdAt": "2020-04-12T18:09:37Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java", "diffHunk": "@@ -319,18 +331,18 @@ private static Boolean tableExists(Connection conn, Map<String, String> options)\n    * @return\n    * @throws Exception\n    */\n-  public static Schema getJDBCSchema(Map<String, String> options) throws Exception {\n-    Connection conn = createConnectionFactory(options);\n-    String url = options.get(JDBCOptions.JDBC_URL());\n-    String table = options.get(JDBCOptions.JDBC_TABLE_NAME());\n-    boolean tableExists = tableExists(conn,options);\n+  public static Schema getJDBCSchema(final Map<String, String> options) throws Exception {\n+    final Connection conn = createConnectionFactory(options);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3Mzc5NA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407273794", "bodyText": "are these from intellij formatting...", "author": "vinothchandar", "createdAt": "2020-04-13T00:46:32Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java", "diffHunk": "@@ -219,13 +230,13 @@ public static JavaSparkContext buildSparkContext(String appName, String sparkMas\n   /**\n    * Build Hoodie write client.\n    *\n-   * @param jsc Java Spark Context\n-   * @param basePath Base Path\n-   * @param schemaStr Schema\n+   * @param jsc         Java Spark Context", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzMyNjMxNw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407326317", "bodyText": "yes, these might cause by the different settings when I switched machines. I removed all this and final manually instead of reverting commit, so there are still a few lines changing due to the initial formatting. I can revert those comments as well if it's preferable.", "author": "garyli1019", "createdAt": "2020-04-13T05:35:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3Mzc5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzMyODE4OA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407328188", "bodyText": "Thanks for doing it.. looks okay for now.", "author": "vinothchandar", "createdAt": "2020-04-13T05:44:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3Mzc5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3MzkyMg==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407273922", "bodyText": "I think there is a setting not to format the lines untouched.. may be we can avoid all these whitespace changes that way..", "author": "vinothchandar", "createdAt": "2020-04-13T00:47:47Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/UtilHelpers.java", "diffHunk": "@@ -219,13 +230,13 @@ public static JavaSparkContext buildSparkContext(String appName, String sparkMas\n   /**\n    * Build Hoodie write client.\n    *\n-   * @param jsc Java Spark Context\n-   * @param basePath Base Path\n-   * @param schemaStr Schema\n+   * @param jsc         Java Spark Context\n+   * @param basePath    Base Path\n+   * @param schemaStr   Schema\n    * @param parallelism Parallelism\n    */\n   public static HoodieWriteClient createHoodieClient(JavaSparkContext jsc, String basePath, String schemaStr,\n-      int parallelism, Option<String> compactionStrategyClass, TypedProperties properties) {\n+                                                     int parallelism, Option<String> compactionStrategyClass, TypedProperties properties) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3NDEwOQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407274109", "bodyText": "would Configuration be a better choice here? is more general and the user can construct the filesystem from that? It will also carry other configurations picked up sparkContext say aws creds etc", "author": "vinothchandar", "createdAt": "2020-04-13T00:49:19Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/checkpointing/InitialCheckPointProvider.java", "diffHunk": "@@ -18,14 +18,38 @@\n \n package org.apache.hudi.utilities.checkpointing;\n \n+import org.apache.hudi.common.config.TypedProperties;\n import org.apache.hudi.exception.HoodieException;\n \n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n /**\n  * Provide the initial checkpoint for delta streamer.\n  */\n-public interface InitialCheckPointProvider {\n+public abstract class InitialCheckPointProvider {\n+  protected transient Path path;\n+  protected transient FileSystem fs;\n+  protected transient TypedProperties props;\n+\n+  static class Config {\n+    private static String CHECKPOINT_PROVIDER_PATH_PROP = \"hoodie.deltastreamer.checkpoint.provider.path\";\n+  }\n+\n+  public InitialCheckPointProvider(TypedProperties props) {\n+    this.props = props;\n+    this.path = new Path(props.getString(Config.CHECKPOINT_PROVIDER_PATH_PROP));\n+  }\n+\n+  /**\n+   * Initialize the class with the current filesystem.\n+   *\n+   * @param fileSystem\n+   */\n+  public abstract void init(FileSystem fileSystem);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc5MzQ1Mg==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407793452", "bodyText": "Good point. now it's not hiveConf any more :)", "author": "garyli1019", "createdAt": "2020-04-14T00:21:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3NDEwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzI3NDIzOQ==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407274239", "bodyText": ":). thank you.", "author": "vinothchandar", "createdAt": "2020-04-13T00:50:45Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -130,7 +130,7 @@\n   /**\n    * Hive Config.\n    */\n-  private transient HiveConf hiveConf;\n+  private transient Configuration conf;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODE4MA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407868180", "bodyText": "InitialCheckPointProvider did you intend to write the name of the class here?", "author": "vinothchandar", "createdAt": "2020-04-14T05:01:14Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/HoodieDeltaStreamer.java", "diffHunk": "@@ -293,6 +296,12 @@ public Operation convert(String value) throws ParameterException {\n     @Parameter(names = {\"--checkpoint\"}, description = \"Resume Delta Streamer from this checkpoint.\")\n     public String checkpoint = null;\n \n+    @Parameter(names = {\"--initial-checkpoint-provider\"}, description = \"Generate check point for delta streamer \"\n+        + \"for the first run. This field will override the checkpoint of last commit using the checkpoint field. \"\n+        + \"Use this field only when switch source, for example, from DFS source to Kafka Source. Check the class \"\n+        + \"org.apache.hudi.utilities.checkpointing for details\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg3MjMzOA==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407872338", "bodyText": "Do you mean we should add InitialCheckPointProvider here or we should remove this description?", "author": "garyli1019", "createdAt": "2020-04-14T05:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg3NTEyMw==", "url": "https://github.com/apache/hudi/pull/1486#discussion_r407875123", "bodyText": "Changed the description to match with --schemaprovider-class", "author": "garyli1019", "createdAt": "2020-04-14T05:27:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzg2ODE4MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b923a97cd98861a068673411e73a341bb0130650", "url": "https://github.com/apache/hudi/commit/b923a97cd98861a068673411e73a341bb0130650", "message": "HUDI-759 Integrate checkpoint provider with delta streamer", "committedDate": "2020-04-14T05:24:32Z", "type": "commit"}, {"oid": "b923a97cd98861a068673411e73a341bb0130650", "url": "https://github.com/apache/hudi/commit/b923a97cd98861a068673411e73a341bb0130650", "message": "HUDI-759 Integrate checkpoint provider with delta streamer", "committedDate": "2020-04-14T05:24:32Z", "type": "forcePushed"}]}