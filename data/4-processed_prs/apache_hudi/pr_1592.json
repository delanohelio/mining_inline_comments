{"pr_number": 1592, "pr_title": "[HUDI-822] decouple Hudi related logics from HoodieInputFormat", "pr_createdAt": "2020-05-04T21:02:22Z", "pr_url": "https://github.com/apache/hudi/pull/1592", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDUyOA==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432820528", "bodyText": "Does it more proper to a new Constant class to hold constants in HoodieRealtimeInputFormatUtils and HoodieRealtimeRecordReaderUtils, wdyt?", "author": "leesf", "createdAt": "2020-05-30T07:51:48Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -199,9 +110,9 @@ private static Configuration addProjectionField(Configuration conf, String field\n \n   private static void addRequiredProjectionFields(Configuration configuration) {\n     // Need this to do merge records in HoodieRealtimeRecordReader\n-    addProjectionField(configuration, HoodieRecord.RECORD_KEY_METADATA_FIELD, HOODIE_RECORD_KEY_COL_POS);\n-    addProjectionField(configuration, HoodieRecord.COMMIT_TIME_METADATA_FIELD, HOODIE_COMMIT_TIME_COL_POS);\n-    addProjectionField(configuration, HoodieRecord.PARTITION_PATH_METADATA_FIELD, HOODIE_PARTITION_PATH_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.RECORD_KEY_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_RECORD_KEY_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.COMMIT_TIME_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_COMMIT_TIME_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.PARTITION_PATH_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_PARTITION_PATH_COL_POS);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg3Njg3Ng==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432876876", "bodyText": "good idea", "author": "garyli1019", "createdAt": "2020-05-30T18:33:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDUyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDc4Mw==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432820783", "bodyText": "could we move some methods like readSchema, arrayWritableToString, generateProjectionSchema, getNameToFieldMap, avroToArrayWritable etc in AbstractRealtimeRecordReader to this class?", "author": "leesf", "createdAt": "2020-05-30T07:56:26Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+/**\n+ * Class to hold HoodieRealtimeRecordReader related props.\n+ */\n+public final class HoodieRealtimeRecordReaderUtils {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg3OTA3NA==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432879074", "bodyText": "another good point :)", "author": "garyli1019", "createdAt": "2020-05-30T19:02:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDc4Mw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5MTEwOQ==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432891109", "bodyText": "Calling this config is a bit misleading.. these are just constants", "author": "vinothchandar", "createdAt": "2020-05-30T22:09:07Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HoodieRealtimeConfig.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+/**\n+ * Class to hold props related to Hoodie RealtimeInputFormat and RealtimeRecordReader.\n+ */\n+public final class HoodieRealtimeConfig {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5MTI0Mg==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432891242", "bodyText": "These do seem like configs. So just leave these out in a separate file? And move lines 25-28 back to the input format?", "author": "vinothchandar", "createdAt": "2020-05-30T22:11:58Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HoodieRealtimeConfig.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+/**\n+ * Class to hold props related to Hoodie RealtimeInputFormat and RealtimeRecordReader.\n+ */\n+public final class HoodieRealtimeConfig {\n+  // These positions have to be deterministic across all tables\n+  public static final int HOODIE_COMMIT_TIME_COL_POS = 0;\n+  public static final int HOODIE_RECORD_KEY_COL_POS = 2;\n+  public static final int HOODIE_PARTITION_PATH_COL_POS = 3;\n+  public static final String HOODIE_READ_COLUMNS_PROP = \"hoodie.read.columns.set\";\n+\n+  // Fraction of mapper/reducer task memory used for compaction of log files\n+  public static final String COMPACTION_MEMORY_FRACTION_PROP = \"compaction.memory.fraction\";\n+  public static final String DEFAULT_COMPACTION_MEMORY_FRACTION = \"0.75\";\n+  // used to choose a trade off between IO vs Memory when performing compaction process\n+  // Depending on outputfile size and memory provided, choose true to avoid OOM for large file\n+  // size + small memory\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"compaction.lazy.block.read.enabled\";\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"true\";\n+\n+  // Property to set the max memory for dfs inputstream buffer size\n+  public static final String MAX_DFS_STREAM_BUFFER_SIZE_PROP = \"hoodie.memory.dfs.buffer.max.size\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4Nzg0Ng==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433587846", "bodyText": "rename: getCommitsForIncrementalQuery() to give more context.", "author": "vinothchandar", "createdAt": "2020-06-02T02:31:41Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODA3Mg==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588072", "bodyText": "rename : getFilteredCommitsTimeline() to provide more context as well, now that the code has been moved out of the original source file.", "author": "vinothchandar", "createdAt": "2020-06-02T02:32:29Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODMyNw==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588327", "bodyText": "rename: getMetaClientByBasePath() which is whatthis really is", "author": "vinothchandar", "createdAt": "2020-06-02T02:33:37Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODQxOQ==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588419", "bodyText": "this comment is wrong.", "author": "vinothchandar", "createdAt": "2020-06-02T02:34:03Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODUzOA==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588538", "bodyText": "rename: getTableMetaClientForBasePath()", "author": "vinothchandar", "createdAt": "2020-06-02T02:34:39Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODYyNQ==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588625", "bodyText": "optional: consistent folding style for args?", "author": "vinothchandar", "createdAt": "2020-06-02T02:35:01Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n+    int levels = HoodieHiveUtils.DEFAULT_LEVELS_TO_BASEPATH;\n+    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n+      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n+      metadata.readFromFS();\n+      levels = metadata.getPartitionDepth();\n+    }\n+    Path baseDir = HoodieHiveUtils.getNthParent(dataPath, levels);\n+    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n+    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+  }\n+\n+  /**\n+   * Filter a list of FileStatus based on commitsToCheck for incremental view.\n+   * @param job\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param fileStatuses\n+   * @param commitsToCheck\n+   * @return\n+   */\n+  public static List<FileStatus> filterIncrementalFileStatus(\n+      Job job, HoodieTableMetaClient tableMetaClient, HoodieTimeline timeline,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTA4NA==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589084", "bodyText": "rename: refreshFileStatus", "author": "vinothchandar", "createdAt": "2020-06-02T02:37:12Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n+    int levels = HoodieHiveUtils.DEFAULT_LEVELS_TO_BASEPATH;\n+    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n+      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n+      metadata.readFromFS();\n+      levels = metadata.getPartitionDepth();\n+    }\n+    Path baseDir = HoodieHiveUtils.getNthParent(dataPath, levels);\n+    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n+    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+  }\n+\n+  /**\n+   * Filter a list of FileStatus based on commitsToCheck for incremental view.\n+   * @param job\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param fileStatuses\n+   * @param commitsToCheck\n+   * @return\n+   */\n+  public static List<FileStatus> filterIncrementalFileStatus(\n+      Job job, HoodieTableMetaClient tableMetaClient, HoodieTimeline timeline,\n+      FileStatus[] fileStatuses, List<HoodieInstant> commitsToCheck) {\n+    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n+    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n+    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n+    List<FileStatus> returns = new ArrayList<>();\n+    for (HoodieBaseFile filteredFile : filteredFiles) {\n+      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n+      filteredFile = checkFileStatus(job.getConfiguration(), filteredFile);\n+      returns.add(filteredFile.getFileStatus());\n+    }\n+    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n+    return returns;\n+  }\n+\n+  /**\n+   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n+   * based on given table metadata.\n+   * @param fileStatuses\n+   * @param metaClientList\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n+      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n+    // This assumes the paths for different tables are grouped together\n+    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n+    HoodieTableMetaClient metadata = null;\n+    for (FileStatus status : fileStatuses) {\n+      Path inputPath = status.getPath();\n+      if (!inputPath.getName().endsWith(\".parquet\")) {\n+        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n+        // with \".\"\n+        continue;\n+      }\n+      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n+        for (HoodieTableMetaClient metaClient : metaClientList) {\n+          if (inputPath.toString().contains(metaClient.getBasePath())) {\n+            metadata = metaClient;\n+            if (!grouped.containsKey(metadata)) {\n+              grouped.put(metadata, new ArrayList<>());\n+            }\n+            break;\n+          }\n+        }\n+      }\n+      grouped.get(metadata).add(status);\n+    }\n+    return grouped;\n+  }\n+\n+  /**\n+   * Filters data files for a snapshot queried table.\n+   * @param job\n+   * @param metadata\n+   * @param fileStatuses\n+   * @return\n+   */\n+  public static List<FileStatus> filterFileStatusForSnapshotMode(\n+      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n+    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+    }\n+    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n+    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n+    // filter files on the latest commit found\n+    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n+    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+    List<FileStatus> returns = new ArrayList<>();\n+    for (HoodieBaseFile filteredFile : filteredFiles) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n+      }\n+      filteredFile = checkFileStatus(job, filteredFile);\n+      returns.add(filteredFile.getFileStatus());\n+    }\n+    return returns;\n+  }\n+\n+  /**\n+   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n+   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n+   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n+   * @param conf\n+   * @param dataFile\n+   * @return\n+   */\n+  private static HoodieBaseFile checkFileStatus(Configuration conf, HoodieBaseFile dataFile) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTQ2Nw==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589467", "bodyText": "rename: groupLogsByBaseFile()", "author": "vinothchandar", "createdAt": "2020-06-02T02:38:45Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.hadoop.realtime.HoodieRealtimeFileSplit;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class HoodieRealtimeInputFormatUtils extends HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieRealtimeInputFormatUtils.class);\n+\n+  public static InputSplit[] getRealtimeSplits(Configuration conf, Stream<FileSplit> fileSplits) throws IOException {\n+    Map<Path, List<FileSplit>> partitionsToParquetSplits =\n+        fileSplits.collect(Collectors.groupingBy(split -> split.getPath().getParent()));\n+    // TODO(vc): Should we handle also non-hoodie splits here?\n+    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getMetaClientPerPartition(conf, partitionsToParquetSplits.keySet());\n+\n+    // for all unique split parents, obtain all delta files based on delta commit timeline,\n+    // grouped on file id\n+    List<HoodieRealtimeFileSplit> rtSplits = new ArrayList<>();\n+    partitionsToParquetSplits.keySet().forEach(partitionPath -> {\n+      // for each partition path obtain the data & log file groupings, then map back to inputsplits\n+      HoodieTableMetaClient metaClient = partitionsToMetaClient.get(partitionPath);\n+      HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline());\n+      String relPartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), partitionPath);\n+\n+      try {\n+        // Both commit and delta-commits are included - pick the latest completed one\n+        Option<HoodieInstant> latestCompletedInstant =\n+            metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+\n+        Stream<FileSlice> latestFileSlices = latestCompletedInstant\n+            .map(instant -> fsView.getLatestMergedFileSlicesBeforeOrOn(relPartitionPath, instant.getTimestamp()))\n+            .orElse(Stream.empty());\n+\n+        // subgroup splits again by file id & match with log files.\n+        Map<String, List<FileSplit>> groupedInputSplits = partitionsToParquetSplits.get(partitionPath).stream()\n+            .collect(Collectors.groupingBy(split -> FSUtils.getFileId(split.getPath().getName())));\n+        latestFileSlices.forEach(fileSlice -> {\n+          List<FileSplit> dataFileSplits = groupedInputSplits.get(fileSlice.getFileId());\n+          dataFileSplits.forEach(split -> {\n+            try {\n+              List<String> logFilePaths = fileSlice.getLogFiles().sorted(HoodieLogFile.getLogFileComparator())\n+                  .map(logFile -> logFile.getPath().toString()).collect(Collectors.toList());\n+              // Get the maxCommit from the last delta or compaction or commit - when\n+              // bootstrapped from COW table\n+              String maxCommitTime = metaClient\n+                  .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n+                      HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n+                  .filterCompletedInstants().lastInstant().get().getTimestamp();\n+              rtSplits.add(new HoodieRealtimeFileSplit(split, metaClient.getBasePath(), logFilePaths, maxCommitTime));\n+            } catch (IOException e) {\n+              throw new HoodieIOException(\"Error creating hoodie real time split \", e);\n+            }\n+          });\n+        });\n+      } catch (Exception e) {\n+        throw new HoodieException(\"Error obtaining data file/log file grouping: \" + partitionPath, e);\n+      }\n+    });\n+    LOG.info(\"Returning a total splits of \" + rtSplits.size());\n+    return rtSplits.toArray(new InputSplit[0]);\n+  }\n+\n+  // Return parquet file with a list of log files in the same file group.\n+  public static Map<String, List<String>> getRealtimeFileGroup(Configuration conf, Stream<FileStatus> fileStatuses) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTY4MQ==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589681", "bodyText": "we should really see if we can move this to hudi-common ParquetUtils", "author": "vinothchandar", "createdAt": "2020-06-02T02:39:46Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericArray;\n+import org.apache.avro.generic.GenericFixed;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+public class HoodieRealtimeRecordReaderUtils {\n+\n+  /**\n+   * Reads the schema from the parquet file. This is different from ParquetUtils as it uses the twitter parquet to\n+   * support hive 1.1.0\n+   */\n+  public static MessageType readSchema(Configuration conf, Path parquetFilePath) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzYxMzQ0NQ==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433613445", "bodyText": "Not quite sure here. This seems the same as https://github.com/apache/hudi/blame/master/hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java#L172 except the ParquetUtils set some extra configs. Looks like both methods are not using the twitter parquet. Should we remove this and use the ParquetUtils instead? The tests were all passed though.", "author": "garyli1019", "createdAt": "2020-06-02T04:28:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTY4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk5NjQ3Mg==", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433996472", "bodyText": "yes we moved away from twitter parquet now completely.. lets leave this alone for now", "author": "vinothchandar", "createdAt": "2020-06-02T16:03:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTY4MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "a47f09a579e90890b73dff8a4262a84bd6a4d351", "url": "https://github.com/apache/hudi/commit/a47f09a579e90890b73dff8a4262a84bd6a4d351", "message": "HUDI-822 Rename some utils methods", "committedDate": "2020-06-02T15:43:50Z", "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "56d2a95959863c1ce44c493680424efd268bbe4f", "url": "https://github.com/apache/hudi/commit/56d2a95959863c1ce44c493680424efd268bbe4f", "message": "HUDI 822 decouple Hudi related logics from HoodieInputFormat", "committedDate": "2020-06-09T03:37:48Z", "type": "commit"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "url": "https://github.com/apache/hudi/commit/bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "message": "HUDI 822 rename some utils methods", "committedDate": "2020-06-09T07:03:54Z", "type": "commit"}, {"oid": "bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "url": "https://github.com/apache/hudi/commit/bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "message": "HUDI 822 rename some utils methods", "committedDate": "2020-06-09T07:03:54Z", "type": "forcePushed"}]}