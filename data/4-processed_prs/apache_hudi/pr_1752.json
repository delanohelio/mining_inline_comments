{"pr_number": 1752, "pr_title": "[HUDI-575] Support Async Compaction for spark streaming writes to hudi table", "pr_createdAt": "2020-06-20T21:52:55Z", "pr_url": "https://github.com/apache/hudi/pull/1752", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE2Mjc4MQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r443162781", "bodyText": "https://jira.apache.org/jira/browse/HUDI-1031 to add to docs", "author": "bvaradar", "createdAt": "2020-06-20T21:55:46Z", "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "88f34ab0253cf269225f7cb8de381985812e8ad8", "url": "https://github.com/apache/hudi/commit/88f34ab0253cf269225f7cb8de381985812e8ad8", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-06-28T09:14:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNTYwOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446615608", "bodyText": "scala has default values/optional parameters.. so instead of changing signature to pass empty everywhere.. we can just specify defaults here,", "author": "vinothchandar", "createdAt": "2020-06-28T07:55:08Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -48,7 +49,12 @@ private[hudi] object HoodieSparkSqlWriter {\n   def write(sqlContext: SQLContext,\n             mode: SaveMode,\n             parameters: Map[String, String],\n-            df: DataFrame): (Boolean, common.util.Option[String]) = {\n+            df: DataFrame,\n+            hoodieWriteClient: Option[HoodieWriteClient[HoodieRecordPayload[Nothing]]],", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNTkzOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446615938", "bodyText": "is it possible that nothing is actually scheduled here, since there is nothiing to compact?", "author": "vinothchandar", "createdAt": "2020-06-28T07:58:42Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -279,6 +297,15 @@ private[hudi] object HoodieSparkSqlWriter {\n         log.info(\"Commit \" + instantTime + \" failed!\")\n       }\n \n+      val asyncCompactionEnabled = isAsyncCompactionEnabled(client, parameters, jsc.hadoopConfiguration())\n+      val compactionInstant : common.util.Option[java.lang.String] =\n+      if (asyncCompactionEnabled) {\n+        client.scheduleCompaction(common.util.Option.of(new util.HashMap[String, String](mapAsJavaMap(metaMap))))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNjEyNA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446616124", "bodyText": "what if the user sets the writeClient config for inline = false and does not set async compaction datasource option? should we control at a single level..", "author": "vinothchandar", "createdAt": "2020-06-28T08:00:32Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -303,6 +334,18 @@ private[hudi] object HoodieSparkSqlWriter {\n             }\n           })\n       }\n+      (false, common.util.Option.empty())\n+    }\n+  }\n+\n+  private def isAsyncCompactionEnabled(client: HoodieWriteClient[HoodieRecordPayload[Nothing]],\n+                                       parameters: Map[String, String], configuration: Configuration) : Boolean = {\n+    log.info(s\"Config.isInlineCompaction ? ${client.getConfig.isInlineCompaction}\")\n+    if (!client.getConfig.isInlineCompaction", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYxNjQ3Nw==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446616477", "bodyText": "basic question..  if there are no writes , no compaction gets scheduled right? so async compaction is a no-op i.e it will check if there is some work to do, if not won't trigger anything?", "author": "vinothchandar", "createdAt": "2020-06-28T08:03:45Z", "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626008", "bodyText": "move comments that refer to a sub-class impl to that class itself?", "author": "vinothchandar", "createdAt": "2020-06-28T09:26:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/async/AsyncCompactService.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import org.apache.hudi.client.Compactor;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.locks.Condition;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.stream.IntStream;\n+\n+/**\n+ * Async Compactor Service that runs in separate thread. Currently, only one compactor is allowed to run at any time.\n+ */\n+public class AsyncCompactService extends AbstractAsyncService {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(AsyncCompactService.class);\n+\n+  /**\n+   * This is the job pool used by async compaction.\n+   * In case of deltastreamer, Spark job scheduling configs are automatically set.\n+   * As the configs needs to be set before spark context is initiated, it is not\n+   * automated for Structured Streaming.\n+   * https://spark.apache.org/docs/latest/job-scheduling.html\n+   */\n+  public static final String COMPACT_POOL_NAME = \"hoodiecompact\";\n+\n+  private final int maxConcurrentCompaction;\n+  private transient Compactor compactor;\n+  private transient JavaSparkContext jssc;\n+  private transient BlockingQueue<HoodieInstant> pendingCompactions = new LinkedBlockingQueue<>();\n+  private transient ReentrantLock queueLock = new ReentrantLock();\n+  private transient Condition consumed = queueLock.newCondition();\n+\n+  public AsyncCompactService(JavaSparkContext jssc, HoodieWriteClient client) {\n+    this.jssc = jssc;\n+    this.compactor = new Compactor(client, jssc);\n+    this.maxConcurrentCompaction = 1;\n+  }\n+\n+  /**\n+   * Enqueues new Pending compaction.\n+   */\n+  public void enqueuePendingCompaction(HoodieInstant instant) {\n+    pendingCompactions.add(instant);\n+  }\n+\n+  /**\n+   * Wait till outstanding pending compactions reduces to the passed in value.\n+   *\n+   * @param numPendingCompactions Maximum pending compactions allowed\n+   * @throws InterruptedException\n+   */\n+  public void waitTillPendingCompactionsReducesTo(int numPendingCompactions) throws InterruptedException {\n+    try {\n+      queueLock.lock();\n+      while (!isShutdown() && (pendingCompactions.size() > numPendingCompactions)) {\n+        consumed.await();\n+      }\n+    } finally {\n+      queueLock.unlock();\n+    }\n+  }\n+\n+  /**\n+   * Fetch Next pending compaction if available.\n+   *\n+   * @return\n+   * @throws InterruptedException\n+   */\n+  private HoodieInstant fetchNextCompactionInstant() throws InterruptedException {\n+    LOG.info(\"Compactor waiting for next instant for compaction upto 60 seconds\");\n+    HoodieInstant instant = pendingCompactions.poll(10, TimeUnit.SECONDS);\n+    if (instant != null) {\n+      try {\n+        queueLock.lock();\n+        // Signal waiting thread\n+        consumed.signal();\n+      } finally {\n+        queueLock.unlock();\n+      }\n+    }\n+    return instant;\n+  }\n+\n+  /**\n+   * Start Compaction Service.\n+   */\n+  @Override\n+  protected Pair<CompletableFuture, ExecutorService> startService() {\n+    ExecutorService executor = Executors.newFixedThreadPool(maxConcurrentCompaction,\n+        r -> new Thread(r, \"async_compact_thread\"));\n+    return Pair.of(CompletableFuture.allOf(IntStream.range(0, maxConcurrentCompaction).mapToObj(i -> CompletableFuture.supplyAsync(() -> {\n+      try {\n+        // Set Compactor Pool Name for allowing users to prioritize compaction\n+        LOG.info(\"Setting Spark Pool name for compaction to \" + COMPACT_POOL_NAME);\n+        jssc.setLocalProperty(\"spark.scheduler.pool\", COMPACT_POOL_NAME);\n+\n+        while (!isShutdownRequested()) {\n+          final HoodieInstant instant = fetchNextCompactionInstant();\n+\n+          if (null != instant) {\n+            LOG.info(\"Starting Compaction for instant \" + instant);\n+            compactor.compact(instant);\n+            LOG.info(\"Finished Compaction for instant \" + instant);\n+          }\n+\n+          if (shouldStopCompactor()) {\n+            return true;\n+          }\n+        }\n+        LOG.info(\"Compactor shutting down properly!!\");\n+      } catch (InterruptedException ie) {\n+        LOG.warn(\"Compactor executor thread got interrupted exception. Stopping\", ie);\n+      } catch (IOException e) {\n+        LOG.error(\"Compactor executor failed\", e);\n+        throw new HoodieIOException(e.getMessage(), e);\n+      }\n+      return true;\n+    }, executor)).toArray(CompletableFuture[]::new)), executor);\n+  }\n+\n+  /**\n+   * Spark Structured Streaming Sink implementation do not have mechanism to know when the stream is shutdown.", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzU2Mg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333562", "bodyText": "Done", "author": "bvaradar", "createdAt": "2020-08-04T21:12:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjAwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446626268", "bodyText": "this does not mean there is no work for compaction right?", "author": "vinothchandar", "createdAt": "2020-06-28T09:29:31Z", "path": "hudi-spark/src/main/java/org/apache/hudi/async/SparkStreamingWriterActivityDetector.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.async;\n+\n+import java.util.function.Supplier;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+/**\n+ * This class is used to detect activity of spark streaming writer. THis is used to decide if HoodieWriteClient\n+ * and async compactor needs to be closed. Spark Structured Streaming do not have explicit API on the Sink side to\n+ * determine if the stream is done. In this absence, async compactor proactively checks with the sink if it is\n+ * active. If there is no activity for sufficient period, async compactor shuts down. If the sink was indeed active,\n+ * a subsequent batch will re-trigger async compaction.\n+ */\n+public class SparkStreamingWriterActivityDetector {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LogManager.getLogger(SparkStreamingWriterActivityDetector.class);\n+\n+  private final Supplier<Long> lastStartBatchNanoTimeSupplier;\n+  private final Supplier<Long> lastEndBatchNanoTimeSupplier;\n+  private final long sinkInactivityTimeoutSecs;\n+\n+  private static final long SECS_TO_NANOS = 1000000000L;\n+\n+  public SparkStreamingWriterActivityDetector(\n+      Supplier<Long> lastStartBatchNanoTimeSupplier, Supplier<Long> lastEndBatchNanoTimeSupplier,\n+      long sinkInactivityTimeoutSecs) {\n+    this.lastStartBatchNanoTimeSupplier = lastStartBatchNanoTimeSupplier;\n+    this.lastEndBatchNanoTimeSupplier = lastEndBatchNanoTimeSupplier;\n+    this.sinkInactivityTimeoutSecs = sinkInactivityTimeoutSecs;\n+  }\n+\n+  /**\n+   * Detects if spark streaming write is still active based on time.\n+   * @return\n+   */\n+  public boolean hasRecentlyWritten() {\n+    long lastStartBatchTime = lastStartBatchNanoTimeSupplier.get();\n+    long lastEndBatchTime = lastEndBatchNanoTimeSupplier.get();\n+\n+    LOG.info(\"Checking if compactor needs to be stopped. \"\n+        + \"lastStartBatchTime=\" + lastStartBatchTime + \", lastEndBatchTime=\" + lastEndBatchTime\n+        + \", CurrTime=\" + System.nanoTime());\n+\n+    if (lastEndBatchTime - lastStartBatchTime < 0) {\n+      LOG.info(\"End Batch Time (\" + lastEndBatchTime + \") is less than Start Batch Time (\" + lastStartBatchTime + \")\"\n+          + \"Sink is running. So, no need to stop\");\n+      return true;\n+    }\n+\n+    long currTime = System.nanoTime();\n+    long elapsedTimeSecs = Double.valueOf(Math.ceil(1.0 * (currTime - lastEndBatchTime) / SECS_TO_NANOS)).longValue();\n+    if (elapsedTimeSecs > sinkInactivityTimeoutSecs) {\n+      LOG.warn(\"Streaming Sink has been idle for \" + elapsedTimeSecs + \" seconds\");", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzUxNA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333514", "bodyText": "This code is deleted.", "author": "bvaradar", "createdAt": "2020-08-04T21:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNjI2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446627164", "bodyText": "more importantly, we should also renable the test in TestDataSource", "author": "vinothchandar", "createdAt": "2020-06-28T09:37:55Z", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/ITTestBase.java", "diffHunk": "@@ -58,6 +58,7 @@\n   protected static final String PRESTO_COORDINATOR = \"/presto-coordinator-1\";\n   protected static final String HOODIE_WS_ROOT = \"/var/hoodie/ws\";\n   protected static final String HOODIE_JAVA_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_app.sh\";\n+  protected static final String HOODIE_JAVA_STREAMING_APP = HOODIE_WS_ROOT + \"/hudi-spark/run_hoodie_streaming_app.sh\";", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMTcwOA==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465331708", "bodyText": "Enabled it after adding timed retry logic to wait for commits", "author": "bvaradar", "createdAt": "2020-08-04T21:09:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzE2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzcwMg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446627702", "bodyText": "just confirming that reuse of writeClient across batches is fine..", "author": "vinothchandar", "createdAt": "2020-06-28T09:43:50Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false\n+\n   private val mode =\n     if (outputMode == OutputMode.Append()) {\n       SaveMode.Append\n     } else {\n       SaveMode.Overwrite\n     }\n \n-  override def addBatch(batchId: Long, data: DataFrame): Unit = {\n+  private var asyncCompactorService : AsyncCompactService = _\n+  private var writeClient : Option[HoodieWriteClient[HoodieRecordPayload[Nothing]]] = Option.empty\n+  private var lastStartBatchTimeNanos : lang.Long = System.nanoTime()\n+  private var lastEndBatchTimeNanos : lang.Long = System.nanoTime()\n+\n+  override def addBatch(batchId: Long, data: DataFrame): Unit = this.synchronized {\n+    if (isAsyncCompactorServiceShutdownAbnormally)  {\n+      throw new IllegalStateException(\"Async Compaction shutdown unexpectedly\")\n+    }\n+\n+    lastStartBatchTimeNanos = System.nanoTime()\n+\n     retry(retryCnt, retryIntervalMs)(\n       Try(\n         HoodieSparkSqlWriter.write(\n-          sqlContext,\n-          mode,\n-          options,\n-          data)\n+          sqlContext, mode, options, data, writeClient, Some(triggerAsyncCompactor))", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMzM3Mg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465333372", "bodyText": "Yes, this worked fine.", "author": "bvaradar", "createdAt": "2020-08-04T21:12:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyNzcwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODAyMQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628021", "bodyText": "this alone should be good enough to prevent the jvm from not hanging during exit? do we really need the laststart/lastend logic?", "author": "vinothchandar", "createdAt": "2020-06-28T09:46:45Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMjA1Mw==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465332053", "bodyText": "Yes, this and setting daemon mode was good enough", "author": "bvaradar", "createdAt": "2020-08-04T21:09:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODA4NQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628085", "bodyText": "Seems like this will happen each trigger/ not just first time?", "author": "vinothchandar", "createdAt": "2020-06-28T09:47:32Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -111,12 +143,64 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n \n   @annotation.tailrec\n   private def retry[T](n: Int, waitInMillis: Long)(fn: => Try[T]): Try[T] = {\n+    lastStartBatchTimeNanos = System.nanoTime()\n     fn match {\n-      case x: util.Success[T] => x\n+      case x: Success[T] =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        x\n       case _ if n > 1 =>\n         Thread.sleep(waitInMillis)\n+        lastEndBatchTimeNanos = System.nanoTime()\n         retry(n - 1, waitInMillis * 2)(fn)\n-      case f => f\n+      case f =>\n+        lastEndBatchTimeNanos = System.nanoTime()\n+        reset(false)\n+        f\n+    }\n+  }\n+\n+  protected def triggerAsyncCompactor(client: HoodieWriteClient[HoodieRecordPayload[Nothing]]): Unit = {\n+    log.info(\"Triggering Async compaction !!\")\n+    if (null == asyncCompactorService) {\n+      asyncCompactorService = new SparkStreamingAsyncCompactService(new JavaSparkContext(sqlContext.sparkContext),\n+        client, new SparkStreamingWriterActivityDetector(new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastStartBatchTimeNanos\n+        }, new Supplier[lang.Long] {\n+          override def get(): lang.Long = lastEndBatchTimeNanos\n+        }, 10))\n+      asyncCompactorService.start(new Function[java.lang.Boolean, java.lang.Boolean] {\n+        override def apply(errored: lang.Boolean): lang.Boolean = {\n+          log.info(s\"Async Compactor shutdown. Errored ? $errored\")\n+          isAsyncCompactorServiceShutdownAbnormally = errored\n+          reset(false)\n+          log.info(\"Done resetting write client.\")\n+          true\n+        }\n+      })\n+\n+      // Add Shutdown Hook\n+      Runtime.getRuntime.addShutdownHook(new Thread(new Runnable {\n+        override def run(): Unit = reset(true)\n+      }))\n+\n+      // First time, scan .hoodie folder and get all pending compactions\n+      val metaClient = new HoodieTableMetaClient(sqlContext.sparkContext.hadoopConfiguration,", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMjQxMg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465332412", "bodyText": "Now, only for the first time when async compactor is null.", "author": "bvaradar", "createdAt": "2020-08-04T21:10:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODA4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446628169", "bodyText": "why move to COW?", "author": "vinothchandar", "createdAt": "2020-06-28T09:48:22Z", "path": "hudi-spark/src/test/java/HoodieJavaStreamingApp.java", "diffHunk": "@@ -68,7 +74,7 @@\n   private String tableName = \"hoodie_test\";\n \n   @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n-  private String tableType = HoodieTableType.MERGE_ON_READ.name();\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMDkzMg==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465330932", "bodyText": "Reverted.", "author": "bvaradar", "createdAt": "2020-08-04T21:07:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyODE2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyOTI5NQ==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r446629295", "bodyText": "https://jaceklaskowski.gitbooks.io/spark-structured-streaming/spark-sql-streaming-StreamingQueryManager.html seems like there are some listeners we can exploit to know of a StreamingQuery?", "author": "vinothchandar", "createdAt": "2020-06-28T09:59:51Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieStreamingSink.scala", "diffHunk": "@@ -38,46 +50,65 @@ class HoodieStreamingSink(sqlContext: SQLContext,\n   private val retryIntervalMs = options(DataSourceWriteOptions.STREAMING_RETRY_INTERVAL_MS_OPT_KEY).toLong\n   private val ignoreFailedBatch = options(DataSourceWriteOptions.STREAMING_IGNORE_FAILED_BATCH_OPT_KEY).toBoolean\n \n+  private var isAsyncCompactorServiceShutdownAbnormally = false", "originalCommit": "88f34ab0253cf269225f7cb8de381985812e8ad8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMzMTExMw==", "url": "https://github.com/apache/hudi/pull/1752#discussion_r465331113", "bodyText": "Thanks. Fixed by setting up daemon mode for async compactor thread", "author": "bvaradar", "createdAt": "2020-08-04T21:07:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjYyOTI5NQ=="}], "type": "inlineReview"}, {"oid": "f88d0bf2fbdab85a8f24416b2ed64db5d0e29508", "url": "https://github.com/apache/hudi/commit/f88d0bf2fbdab85a8f24416b2ed64db5d0e29508", "message": "[HUDI-575] minor CR comments", "committedDate": "2020-08-02T16:35:05Z", "type": "forcePushed"}, {"oid": "54e2e256e072d55049284084ea8da25db5fc1aa0", "url": "https://github.com/apache/hudi/commit/54e2e256e072d55049284084ea8da25db5fc1aa0", "message": "Ensure Async Compaction for structured streaming is running in daemon mode", "committedDate": "2020-08-03T07:46:48Z", "type": "forcePushed"}, {"oid": "0a849a8fa55811f80030c6b486bcfe965183de8e", "url": "https://github.com/apache/hudi/commit/0a849a8fa55811f80030c6b486bcfe965183de8e", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-04T16:11:07Z", "type": "forcePushed"}, {"oid": "8d515f9d92482e046a8f8309d80d15b6825164a1", "url": "https://github.com/apache/hudi/commit/8d515f9d92482e046a8f8309d80d15b6825164a1", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-04T16:44:27Z", "type": "forcePushed"}, {"oid": "f3736c2996157285e466140c5775fb3801f42741", "url": "https://github.com/apache/hudi/commit/f3736c2996157285e466140c5775fb3801f42741", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T02:45:16Z", "type": "forcePushed"}, {"oid": "a6992eb75605a88c75c1e2cfdae9b8595ec7b293", "url": "https://github.com/apache/hudi/commit/a6992eb75605a88c75c1e2cfdae9b8595ec7b293", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:08:03Z", "type": "forcePushed"}, {"oid": "281b820e82bae3ed6372d1b39a054b128087d6e9", "url": "https://github.com/apache/hudi/commit/281b820e82bae3ed6372d1b39a054b128087d6e9", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:58:30Z", "type": "commit"}, {"oid": "281b820e82bae3ed6372d1b39a054b128087d6e9", "url": "https://github.com/apache/hudi/commit/281b820e82bae3ed6372d1b39a054b128087d6e9", "message": "[HUDI-575] Spark Streaming with async compaction support", "committedDate": "2020-08-05T05:58:30Z", "type": "forcePushed"}]}