{"pr_number": 2208, "pr_title": "[HUDI-1040] Make Hudi support Spark 3", "pr_createdAt": "2020-10-26T20:15:00Z", "pr_url": "https://github.com/apache/hudi/pull/2208", "timeline": [{"oid": "649fadd3b28ff464a1c525753094510577f53de8", "url": "https://github.com/apache/hudi/commit/649fadd3b28ff464a1c525753094510577f53de8", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-27T15:46:50Z", "type": "forcePushed"}, {"oid": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "url": "https://github.com/apache/hudi/commit/ed2714bb26e11b24707fcf13cebeb5e2116476c5", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-28T18:58:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczNjM0OQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r513736349", "bodyText": "let's file a tracking JIRA for this?", "author": "vinothchandar", "createdAt": "2020-10-28T20:22:49Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java", "diffHunk": "@@ -173,4 +176,17 @@ public static InternalRow getInternalRowWithError(String partitionPath) {\n         .withBulkInsertParallelism(2);\n   }\n \n+  private static InternalRow serializeRow(ExpressionEncoder encoder, Row row)\n+      throws InvocationTargetException, IllegalAccessException, NoSuchMethodException, ClassNotFoundException {\n+    // TODO remove reflection if Spark 2.x support is dropped", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczODEzNg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r513738136", "bodyText": "This was discussed here before? #1760 (comment)\nusing reflection in the fast path, will cause perf issues?", "author": "vinothchandar", "createdAt": "2020-10-28T20:26:09Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/AvroConversionUtils.scala", "diffHunk": "@@ -96,4 +98,16 @@ object AvroConversionUtils {\n     val name = HoodieAvroUtils.sanitizeName(tableName)\n     (s\"${name}_record\", s\"hoodie.${name}\")\n   }\n+\n+  private def deserializeRow(encoder: ExpressionEncoder[Row], internalRow: InternalRow): Row = {\n+    // TODO remove reflection if Spark 2.x support is dropped\n+    if (SPARK_VERSION.startsWith(\"2.\")) {", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwODI3Ng==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514608276", "bodyText": "+1 Lets have two separate implementations of the Row Deserializer for spark 2 and spark 3, as was done in https://github.com/apache/hudi/pull/1760/files", "author": "umehrot2", "createdAt": "2020-10-29T22:41:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczODEzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk3NTMzNw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516975337", "bodyText": "Done. Added a new module hudi-spark3 to avoid using reflection.", "author": "zhedoubushishi", "createdAt": "2020-11-03T21:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzczODEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NzEwNA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514587104", "bodyText": "Perhaps we can be more specific that we copied some methods ?", "author": "umehrot2", "createdAt": "2020-10-29T21:47:08Z", "path": "LICENSE", "diffHunk": "@@ -246,6 +246,8 @@ This product includes code from Apache Spark\n \n * org.apache.hudi.AvroConversionHelper copied from classes in org/apache/spark/sql/avro package\n \n+* org.apache.hudi.HoodieSparkUtils.scala copied from org.apache.spark.deploy.SparkHadoopUtil.scala", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU5NjExOA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514596118", "bodyText": "For my understanding, why is this needed ?", "author": "umehrot2", "createdAt": "2020-10-29T22:08:36Z", "path": "packaging/hudi-utilities-bundle/pom.xml", "diffHunk": "@@ -105,6 +106,7 @@\n                   <include>io.prometheus:simpleclient_common</include>\n                   <include>com.yammer.metrics:metrics-core</include>\n                   <include>org.apache.spark:spark-streaming-kafka-0-10_${scala.binary.version}</include>\n+                  <include>org.apache.spark:spark-token-provider-kafka-0-10_${scala.binary.version}</include>", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE1OTY3Mw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517159673", "bodyText": "This was needed when using Kafka + DeltaStreamer in Spark 3.\njava.lang.NoClassDefFoundError: org/apache/spark/kafka010/KafkaConfigUpdater\n    at org.apache.spark.streaming.kafka010.InternalKafkaConsumer.createConsumer(KafkaDataConsumer.scala:115)\n\nCan we validate this doesn't cause issues when building with Spark 2. I believe this exists only in Spark 3\nhttps://mvnrepository.com/artifact/org.apache.spark/spark-token-provider-kafka-0-10", "author": "rmpifer", "createdAt": "2020-11-04T08:09:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU5NjExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU2NDgzMA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517564830", "bodyText": "If <include>org.apache.spark:spark-token-provider-kafka-0-10_${scala.binary.version}</include> is not found when building with Spark 2, maven would just ignore it.", "author": "zhedoubushishi", "createdAt": "2020-11-04T18:59:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU5NjExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514603195", "bodyText": "What is the need to move the hudi datasource itself to hudi-spark2 ? I think we should leave it under hudi-spark and later if we want to have separate datasource implementations we can create separately under hudi-spark2 and hudi-spark3 modules. Thoughts ?", "author": "umehrot2", "createdAt": "2020-10-29T22:26:58Z", "path": "hudi-spark2/src/main/java/org/apache/hudi/internal/DefaultSource.java", "diffHunk": "@@ -18,7 +18,7 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.DataSourceUtilsForSpark2;", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NjY4Mg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r515396682", "bodyText": "Because hudi-spark depends on hudi-spark2. I cannot also let hudi-spark2 depends on hudi-spark tho copying files is not a clean way.", "author": "zhedoubushishi", "createdAt": "2020-10-30T21:45:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxMDE3OA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516410178", "bodyText": "I had misunderstood that you moved DefaultSource.scala which is the main datasource implementation. But seems like you have moved the internal datasource implementation used for bulk insert v2. So it seems fine to me.", "author": "umehrot2", "createdAt": "2020-11-03T03:27:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwMzE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgzNA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514609834", "bodyText": "As discussed internally regarding this in the code review, can you confirm if this is actually converting paths to point to local file system and not HDFS ? Also would be good to explain why you did this for reference in the description.", "author": "umehrot2", "createdAt": "2020-10-29T22:45:32Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/HoodieMergeOnReadTestUtils.java", "diffHunk": "@@ -85,7 +85,7 @@\n         .collect(Collectors.toList()));\n \n     return inputPaths.stream().map(path -> {\n-      setInputPath(jobConf, path);\n+      FileInputFormat.setInputPaths(jobConf, path);", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQyMDc2NA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516420764", "bodyText": "Me and discussed discussed it internally and this is not a concern anymore.", "author": "umehrot2", "createdAt": "2020-11-03T04:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYwOTgzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxNjY3Ng==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514616676", "bodyText": "Is this not possible through delta streamer ? Seems like not.", "author": "umehrot2", "createdAt": "2020-10-29T23:04:36Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -121,6 +122,9 @@ private[hudi] object HoodieSparkSqlWriter {\n       // short-circuit if bulk_insert via row is enabled.\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean) {\n+        if (SPARK_VERSION.startsWith(\"3.\")) {\n+          throw new HoodieException(\"Bulk insert via row is not compatible with Spark 3, it is only compatible with Spark 2!\")\n+        }", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5MjI5MQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r515392291", "bodyText": "Good point. Looks like currently it works with spark datasource. So it's not supported by delta streamer.", "author": "zhedoubushishi", "createdAt": "2020-10-30T21:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxNjY3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwOTM1Ng==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516409356", "bodyText": "Yeah. Anyways I think this message can changed to: Bulk insert using row writer is not supported with Spark 3. To use row writer switch to spark 2..", "author": "umehrot2", "createdAt": "2020-11-03T03:23:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxNjY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxODAxNQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r514618015", "bodyText": "I would suggest just keeping spark.version here. Override the spark.version respectively in hudi-spark2 and hudi-spark3 modules.", "author": "umehrot2", "createdAt": "2020-10-29T23:08:49Z", "path": "pom.xml", "diffHunk": "@@ -100,6 +104,7 @@\n     <prometheus.version>0.8.0</prometheus.version>\n     <http.version>4.4.1</http.version>\n     <spark.version>2.4.4</spark.version>\n+    <spark2.version>2.4.4</spark2.version>", "originalCommit": "ed2714bb26e11b24707fcf13cebeb5e2116476c5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTM5NjEyNQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r515396125", "bodyText": "Does Maven support overriding spark.version only in sub modules?", "author": "zhedoubushishi", "createdAt": "2020-10-30T21:44:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxODAxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxOTUxOQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516419519", "bodyText": "If any dependency or property is configured in both parent and child POMs with different values then the child POM value will take the priority.", "author": "umehrot2", "createdAt": "2020-11-03T04:12:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDYxODAxNQ=="}], "type": "inlineReview"}, {"oid": "eac8358868c44c1e031c00a11a4814a59a59d979", "url": "https://github.com/apache/hudi/commit/eac8358868c44c1e031c00a11a4814a59a59d979", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-10-30T21:38:41Z", "type": "forcePushed"}, {"oid": "5ad2ff9e28fbd309093ac294636a9d7e2c53280c", "url": "https://github.com/apache/hudi/commit/5ad2ff9e28fbd309093ac294636a9d7e2c53280c", "message": "resolve comments for fixing mor flaky", "committedDate": "2020-11-01T19:13:27Z", "type": "forcePushed"}, {"oid": "5d1870ac01af856e4504b7f0b5af15c9d08f5c9a", "url": "https://github.com/apache/hudi/commit/5d1870ac01af856e4504b7f0b5af15c9d08f5c9a", "message": "resolve comments for fixing mor flaky", "committedDate": "2020-11-01T19:16:29Z", "type": "forcePushed"}, {"oid": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "url": "https://github.com/apache/hudi/commit/0df7521bf7ba087be882da55bc6ce3fd3c717a78", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-01T22:53:34Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwNjg3MQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516406871", "bodyText": "It might make sense to create Spark2RowSerializer and Spark3RowSerializer similar to the implementations we have created for deserializers.", "author": "umehrot2", "createdAt": "2020-11-03T03:11:31Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/testutils/SparkDatasetTestUtils.java", "diffHunk": "@@ -173,4 +176,17 @@ public static InternalRow getInternalRowWithError(String partitionPath) {\n         .withBulkInsertParallelism(2);\n   }\n \n+  private static InternalRow serializeRow(ExpressionEncoder encoder, Row row)\n+      throws InvocationTargetException, IllegalAccessException, NoSuchMethodException, ClassNotFoundException {\n+    // TODO remove reflection if Spark 2.x support is dropped\n+    if (package$.MODULE$.SPARK_VERSION().startsWith(\"2.\")) {\n+      Method spark2method = encoder.getClass().getMethod(\"toRow\", Object.class);\n+      return (InternalRow) spark2method.invoke(encoder, row);", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjk4NTUwMQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516985501", "bodyText": "The problem here is hudi-spark2 already depends on hudi-common and hudi-client. Say if I create Spark2RowSerializer under hudi-spark2, I also need to make hudi-client depends on hudi-spark2 and as a result, it will bring a dependency loop.", "author": "zhedoubushishi", "createdAt": "2020-11-03T22:11:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwNjg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQwNzg2Ng==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516407866", "bodyText": "I think HoodieSparkUtils is a more appropriate place for this function.", "author": "umehrot2", "createdAt": "2020-11-03T03:16:32Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/AvroConversionUtils.scala", "diffHunk": "@@ -96,4 +99,13 @@ object AvroConversionUtils {\n     val name = HoodieAvroUtils.sanitizeName(tableName)\n     (s\"${name}_record\", s\"hoodie.${name}\")\n   }\n+\n+  def createDeserializer(encoder: ExpressionEncoder[Row]): SparkRowDeserializer = {", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxNDgxMg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516414812", "bodyText": "The javadoc formatting is off at various places in this class.", "author": "umehrot2", "createdAt": "2020-11-03T03:49:57Z", "path": "hudi-spark2/src/main/scala/org/apache/hudi/DataSourceOptionsForSpark2.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.model.HoodieTableType\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload\n+\n+/**\n+  * Options supported for writing hoodie tables.\n+  * TODO: This file is partially copied from org.apache.hudi.DataSourceWriteOptions.\n+  * Should be removed if Spark 2.x support is dropped.\n+  */", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxODk4Ng==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516418986", "bodyText": "Seems like its possible that scala.binary.version is 2.11 when compiling by default and it can conflict here because spark 3 only uses 2.12 ? We should probably override scala versions as well by default with the spark3 maven profile so that such scenarios do not happen.", "author": "umehrot2", "createdAt": "2020-11-03T04:10:11Z", "path": "packaging/hudi-utilities-bundle/pom.xml", "diffHunk": "@@ -105,6 +107,7 @@\n                   <include>io.prometheus:simpleclient_common</include>\n                   <include>com.yammer.metrics:metrics-core</include>\n                   <include>org.apache.spark:spark-streaming-kafka-0-10_${scala.binary.version}</include>\n+                  <include>org.apache.spark:spark-token-provider-kafka-0-10_${scala.binary.version}</include>", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU0OTk4Mg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517549982", "bodyText": "okay I'll override scala version inside hudi-spark3", "author": "zhedoubushishi", "createdAt": "2020-11-04T18:33:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxODk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxOTA1NQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r516419055", "bodyText": "override scala versions here ?", "author": "umehrot2", "createdAt": "2020-11-03T04:10:32Z", "path": "pom.xml", "diffHunk": "@@ -1318,6 +1325,23 @@\n         </plugins>\n       </build>\n     </profile>\n+\n+    <profile>\n+      <id>spark3</id>\n+      <properties>\n+        <spark.version>${spark3.version}</spark.version>", "originalCommit": "0df7521bf7ba087be882da55bc6ce3fd3c717a78", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcxODE5MQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517718191", "bodyText": "Done", "author": "zhedoubushishi", "createdAt": "2020-11-05T00:53:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQxOTA1NQ=="}], "type": "inlineReview"}, {"oid": "29cfc4783b2106a7f45b594703c3cef1b94f43f5", "url": "https://github.com/apache/hudi/commit/29cfc4783b2106a7f45b594703c3cef1b94f43f5", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-03T21:59:01Z", "type": "forcePushed"}, {"oid": "fe002982d099af08df5f5ba4768a7b6905fa61c8", "url": "https://github.com/apache/hudi/commit/fe002982d099af08df5f5ba4768a7b6905fa61c8", "message": "relocate createRdd", "committedDate": "2020-11-04T06:50:21Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjY1OQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517172659", "bodyText": "I'm not sure this is best to have copies DataSourceWriteOptions and DataSourceUtils. I see in hudi-client there is a a module hudi-spark-client. Could we refactor these files to go there or in some new hudi-spark-common", "author": "rmpifer", "createdAt": "2020-11-04T08:33:33Z", "path": "hudi-spark2/src/main/scala/org/apache/hudi/DataSourceOptionsForSpark2.scala", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.model.HoodieTableType\n+import org.apache.hudi.common.model.OverwriteWithLatestAvroPayload\n+\n+/**\n+ * Options supported for writing hoodie tables.\n+ * TODO: This file is partially copied from org.apache.hudi.DataSourceWriteOptions.\n+ * Should be removed if Spark 2.x support is dropped.\n+ */\n+object DataSourceWriteOptionsForSpark2 {", "originalCommit": "7fad200131f88bc61746f1d5816b28ec1abd37a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU1ODMzMg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517558332", "bodyText": "Yea this will avoid us hard copy DataSourceWriteOptions and DataSourceUtils. My concern is Spark Datasource related files ideally should under hudi-spark. So not sure which is the best practice here..\nMaybe create a new module like hudi-spark-common under hudi-spark?", "author": "zhedoubushishi", "createdAt": "2020-11-04T18:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjg5NDM0MA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r522894340", "bodyText": "Yeah I don't really think this is manageable, specially as new properties get added it will be a pain to make sure spark2 and spark3 code is in sync. hudi-spark-common module under hudi-spark seems like the way to me. But lets get vinoth's suggestion on this as well before you implement cc @vinothchandar", "author": "umehrot2", "createdAt": "2020-11-13T11:32:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjY1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTMwNA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r528435304", "bodyText": "yes hudi-spark-common is the right approach IMO as well.", "author": "vinothchandar", "createdAt": "2020-11-23T01:13:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3MjY1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3OTIwMw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517179203", "bodyText": "If we are moving these methods to HoodieSparkUtils we may want to also refactor other similar methods in this class so they are grouped together. i.e. createRddForDeletes and createDataFrame", "author": "rmpifer", "createdAt": "2020-11-04T08:45:11Z", "path": "hudi-client/hudi-spark-client/src/main/scala/org/apache/hudi/AvroConversionUtils.scala", "diffHunk": "@@ -24,35 +24,13 @@ import org.apache.hudi.avro.HoodieAvroUtils\n import org.apache.hudi.common.model.HoodieKey\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.avro.SchemaConverters\n-import org.apache.spark.sql.catalyst.encoders.RowEncoder\n import org.apache.spark.sql.types.StructType\n import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}\n \n import scala.collection.JavaConverters._\n \n object AvroConversionUtils {\n \n-  def createRdd(df: DataFrame, structName: String, recordNamespace: String): RDD[GenericRecord] = {\n-    val avroSchema = convertStructTypeToAvroSchema(df.schema, structName, recordNamespace)\n-    createRdd(df, avroSchema, structName, recordNamespace)\n-  }\n-\n-  def createRdd(df: DataFrame, avroSchema: Schema, structName: String, recordNamespace: String)\n-  : RDD[GenericRecord] = {\n-    // Use the Avro schema to derive the StructType which has the correct nullability information\n-    val dataType = SchemaConverters.toSqlType(avroSchema).dataType.asInstanceOf[StructType]\n-    val encoder = RowEncoder.apply(dataType).resolveAndBind()\n-    val deserializer = HoodieSparkUtils.createDeserializer(encoder)\n-    df.queryExecution.toRdd.map(row => deserializer.deserializeRow(row))\n-      .mapPartitions { records =>\n-        if (records.isEmpty) Iterator.empty\n-        else {\n-          val convertor = AvroConversionHelper.createConverterToAvro(dataType, structName, recordNamespace)\n-          records.map { x => convertor(x).asInstanceOf[GenericRecord] }\n-        }\n-      }\n-  }\n-\n   def createRddForDeletes(df: DataFrame, rowField: String, partitionField: String): RDD[HoodieKey] = {", "originalCommit": "fe002982d099af08df5f5ba4768a7b6905fa61c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzU0ODk5OA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r517548998", "bodyText": "Good point. It seems createRddForDeletes is never used in Hudi, I'll just remove it.\ncreateDataFrame is tricky because it is used in hudi-spark-client so I cannot move it to hudi-spark module.", "author": "zhedoubushishi", "createdAt": "2020-11-04T18:31:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3OTIwMw=="}], "type": "inlineReview"}, {"oid": "a6e0240279472cb981c620a9036add70e3efa05d", "url": "https://github.com/apache/hudi/commit/a6e0240279472cb981c620a9036add70e3efa05d", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-04T18:52:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDM1MTAzMg==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r524351032", "bodyText": "Hi, checking for if (!SPARK_VERSION.startsWith(\"2.\")) { would be more robust because of future relases (e.g. Spark 4)", "author": "sbernauer", "createdAt": "2020-11-16T15:26:34Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -121,6 +122,9 @@ private[hudi] object HoodieSparkSqlWriter {\n       // short-circuit if bulk_insert via row is enabled.\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean) {\n+        if (SPARK_VERSION.startsWith(\"3.\")) {", "originalCommit": "fdaf9eaa27e0eabef24c48d191f5ab9f66f38c70", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fb9514216e84f36beab82aaa4496af73e3a10693", "url": "https://github.com/apache/hudi/commit/fb9514216e84f36beab82aaa4496af73e3a10693", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-11-16T19:34:10Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNDk1MA==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r528434950", "bodyText": "is this change needed for this PR>. some context on why this was needed?", "author": "vinothchandar", "createdAt": "2020-11-23T01:10:26Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/MergeOnReadSnapshotRelation.scala", "diffHunk": "@@ -113,9 +113,6 @@ class MergeOnReadSnapshotRelation(val sqlContext: SQLContext,\n       hadoopConf = sqlContext.sparkSession.sessionState.newHadoopConf()\n     )\n \n-    // Follow the implementation of Spark internal HadoopRDD to handle the broadcast configuration.", "originalCommit": "2af55b357babdd0278f8273fc6658185c459f285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwODIyMQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r533108221", "bodyText": "SparkHadoopUtil becomes private in spark 3, so I need to see if these lines are necessary.\nThe reason this is needed in internal implementation is to guard against case where user passes a custom Configuration which doesn't contain credentials to access secure HDFS (apache/spark#2676). Since the Configuration being used here was created as part of spark context the credentials should already be loaded. So we can remove it.", "author": "zhedoubushishi", "createdAt": "2020-12-01T06:51:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNDk1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTA4Mw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r528435083", "bodyText": "would be good to have a JIRA with all these follow ups when Spark 2.x support is dropped", "author": "vinothchandar", "createdAt": "2020-11-23T01:11:29Z", "path": "hudi-spark2/src/main/java/org/apache/hudi/DataSourceUtilsForSpark2.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+\n+import java.util.Map;\n+\n+/**\n+ * Utilities used throughout the data source.\n+ * TODO: This file is partially copied from org.apache.hudi.DataSourceUtils.\n+ * Should be removed if Spark 2.x support is dropped.", "originalCommit": "2af55b357babdd0278f8273fc6658185c459f285", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4OTMxMw==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r533589313", "bodyText": "I created a new module hudi-spark-common to avoid creating xxxForSpark2 classes.\nSo the current structure of hudi-spark would be:\nhudi-spark-datasource\n|\n-----------------hudi-spark\n|\n-----------------hudi-spark-common\n|\n-----------------hudi-spark2\n|\n-----------------hudi-spark3", "author": "zhedoubushishi", "createdAt": "2020-12-01T17:24:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzNTQ0OQ==", "url": "https://github.com/apache/hudi/pull/2208#discussion_r528435449", "bodyText": "should we still pull this into a property above?", "author": "vinothchandar", "createdAt": "2020-11-23T01:14:31Z", "path": "hudi-spark3/pom.xml", "diffHunk": "@@ -0,0 +1,160 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  Licensed to the Apache Software Foundation (ASF) under one or more\n+  contributor license agreements.  See the NOTICE file distributed with\n+  this work for additional information regarding copyright ownership.\n+  The ASF licenses this file to You under the Apache License, Version 2.0\n+  (the \"License\"); you may not use this file except in compliance with\n+  the License.  You may obtain a copy of the License at\n+       http://www.apache.org/licenses/LICENSE-2.0\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License.\n+-->\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+  <parent>\n+    <artifactId>hudi</artifactId>\n+    <groupId>org.apache.hudi</groupId>\n+    <version>0.6.1-SNAPSHOT</version>\n+  </parent>\n+  <modelVersion>4.0.0</modelVersion>\n+\n+  <artifactId>hudi-spark3_2.12</artifactId>\n+  <packaging>jar</packaging>\n+\n+  <properties>\n+    <main.basedir>${project.parent.basedir}</main.basedir>\n+  </properties>\n+\n+  <build>\n+    <resources>\n+      <resource>\n+        <directory>src/main/resources</directory>\n+      </resource>\n+    </resources>\n+    <pluginManagement>\n+      <plugins>\n+        <plugin>\n+          <groupId>net.alchim31.maven</groupId>\n+          <artifactId>scala-maven-plugin</artifactId>\n+          <version>${scala-maven-plugin.version}</version>\n+          <configuration>\n+            <args>\n+              <arg>-nobootcp</arg>\n+            </args>\n+          </configuration>\n+        </plugin>\n+        <plugin>\n+          <groupId>org.apache.maven.plugins</groupId>\n+          <artifactId>maven-compiler-plugin</artifactId>\n+        </plugin>\n+      </plugins>\n+    </pluginManagement>\n+\n+    <plugins>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-dependency-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <id>copy-dependencies</id>\n+            <phase>prepare-package</phase>\n+            <goals>\n+              <goal>copy-dependencies</goal>\n+            </goals>\n+            <configuration>\n+              <outputDirectory>${project.build.directory}/lib</outputDirectory>\n+              <overWriteReleases>true</overWriteReleases>\n+              <overWriteSnapshots>true</overWriteSnapshots>\n+              <overWriteIfNewer>true</overWriteIfNewer>\n+            </configuration>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>net.alchim31.maven</groupId>\n+        <artifactId>scala-maven-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <id>scala-compile-first</id>\n+            <phase>process-resources</phase>\n+            <goals>\n+              <goal>add-source</goal>\n+              <goal>compile</goal>\n+            </goals>\n+          </execution>\n+          <execution>\n+            <id>scala-test-compile</id>\n+            <phase>process-test-resources</phase>\n+            <goals>\n+              <goal>testCompile</goal>\n+            </goals>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-compiler-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <phase>compile</phase>\n+            <goals>\n+              <goal>compile</goal>\n+            </goals>\n+          </execution>\n+        </executions>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.maven.plugins</groupId>\n+        <artifactId>maven-jar-plugin</artifactId>\n+        <executions>\n+          <execution>\n+            <goals>\n+              <goal>test-jar</goal>\n+            </goals>\n+            <phase>test-compile</phase>\n+          </execution>\n+        </executions>\n+        <configuration>\n+          <skip>false</skip>\n+        </configuration>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.apache.rat</groupId>\n+        <artifactId>apache-rat-plugin</artifactId>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.scalastyle</groupId>\n+        <artifactId>scalastyle-maven-plugin</artifactId>\n+      </plugin>\n+      <plugin>\n+        <groupId>org.jacoco</groupId>\n+        <artifactId>jacoco-maven-plugin</artifactId>\n+      </plugin>\n+    </plugins>\n+  </build>\n+\n+  <dependencies>\n+    <dependency>\n+      <groupId>org.scala-lang</groupId>\n+      <artifactId>scala-library</artifactId>\n+      <version>2.12.10</version>", "originalCommit": "2af55b357babdd0278f8273fc6658185c459f285", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dae14f129534cb1a6c886c2bca87fef8611d0e07", "url": "https://github.com/apache/hudi/commit/dae14f129534cb1a6c886c2bca87fef8611d0e07", "message": "Create hudi-spark-common module and refactor hudi-spark related modules", "committedDate": "2020-12-01T17:26:49Z", "type": "forcePushed"}, {"oid": "1698576919984f94930c624efff3518556370aac", "url": "https://github.com/apache/hudi/commit/1698576919984f94930c624efff3518556370aac", "message": "Create hudi-spark-common module and refactor hudi-spark related modules", "committedDate": "2020-12-01T17:33:01Z", "type": "forcePushed"}, {"oid": "2d55a9972fc4d6016f42d83286f5056fd2dfd3d7", "url": "https://github.com/apache/hudi/commit/2d55a9972fc4d6016f42d83286f5056fd2dfd3d7", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-01T19:52:47Z", "type": "forcePushed"}, {"oid": "286969293d58f6a305b87cb740cf41e8de593ca9", "url": "https://github.com/apache/hudi/commit/286969293d58f6a305b87cb740cf41e8de593ca9", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-03T18:36:05Z", "type": "forcePushed"}, {"oid": "2afb3437bf8e00fdc6eeaa7b488953e26095397c", "url": "https://github.com/apache/hudi/commit/2afb3437bf8e00fdc6eeaa7b488953e26095397c", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-04T05:01:54Z", "type": "forcePushed"}, {"oid": "8ff0ebc8d6d750e59598d60da63997841af0b8a2", "url": "https://github.com/apache/hudi/commit/8ff0ebc8d6d750e59598d60da63997841af0b8a2", "message": "Fix flaky MOR unit test", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "7ea7d3524a9679f60863d4630cb6673ec7845ad3", "url": "https://github.com/apache/hudi/commit/7ea7d3524a9679f60863d4630cb6673ec7845ad3", "message": "Update Spark APIs to make it be compatible with both spark2 & spark3", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "f266e1786c1260aa900695a84685de96b83f41e1", "url": "https://github.com/apache/hudi/commit/f266e1786c1260aa900695a84685de96b83f41e1", "message": "Refactor bulk insert v2 part to make Hudi be able to compile with Spark3", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "9b9b81819425170ae4ccbb74611294ec45826d0f", "url": "https://github.com/apache/hudi/commit/9b9b81819425170ae4ccbb74611294ec45826d0f", "message": "Add spark3 profile to handle fasterxml & spark version", "committedDate": "2020-12-08T22:09:42Z", "type": "commit"}, {"oid": "d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "url": "https://github.com/apache/hudi/commit/d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-08T22:09:43Z", "type": "commit"}, {"oid": "d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "url": "https://github.com/apache/hudi/commit/d51d3924bfae3b23969ce8b441bbf59a0ef71d32", "message": "Create hudi-spark-common module & refactor hudi-spark related modules", "committedDate": "2020-12-08T22:09:43Z", "type": "forcePushed"}]}