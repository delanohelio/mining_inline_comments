{"pr_number": 1396, "pr_title": "[HUDI-687] Stop incremental reader on RO table before a pending compaction", "pr_createdAt": "2020-03-11T03:08:50Z", "pr_url": "https://github.com/apache/hudi/pull/1396", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MjY3MQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392042671", "bodyText": "rename: instantTime", "author": "vinothchandar", "createdAt": "2020-03-13T05:56:20Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -144,6 +162,11 @@\n    */\n   HoodieTimeline findInstantsAfter(String commitTime, int numCommits);\n \n+  /**\n+   * Create a new Timeline with all instants before specified time.\n+   */\n+  HoodieTimeline findInstantsBefore(String time);", "originalCommit": "a023c310341f954ef3a9ee91e12853ca9376fe19", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjQzNjQ2MQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392436461", "bodyText": "done", "author": "satishkotha", "createdAt": "2020-03-13T19:50:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MjY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzA1NA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043054", "bodyText": "Should this belong in the HoodieTimeline class itself? It seems like you can easily build this using other methods, outside of HoodieTimeline?\nGiven this is an interim fix, I would like this to be as isolated as possible from the core classes..", "author": "vinothchandar", "createdAt": "2020-03-13T05:57:58Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -127,6 +127,24 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+\n+  /**\n+   * Get all instants (commits, delta commits) that produce new data, in the active timeline.\n+   */\n+  HoodieTimeline getCommitsTimeline();\n+\n+  /**\n+   * Timeline to include all instants before the first 'requested compaction'.\n+   *\n+   * For example, say timeline: commit at t0, deltacommit at t1, compaction requested at t2, deltacommit at t3,\n+   * this method would return t0, t1\n+   *\n+   * If there is no pending compaction this is equivalent to getting all instants in the timeline.\n+   *\n+   * @return\n+   */\n+  HoodieTimeline filterInstantsBeforePendingCompaction();", "originalCommit": "a023c310341f954ef3a9ee91e12853ca9376fe19", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzE1MA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043150", "bodyText": "In any case, rename to filterInstantsBeforeFirstPendingCompaction?", "author": "vinothchandar", "createdAt": "2020-03-13T05:58:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzA1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjQzNjYzMA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392436630", "bodyText": "Moved it to HoodieParquetInputFormat as suggested", "author": "satishkotha", "createdAt": "2020-03-13T19:50:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzA1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzI0MA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043240", "bodyText": "getCommitsAndCompactionTimeline wont do?", "author": "vinothchandar", "createdAt": "2020-03-13T05:58:51Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTimeline.java", "diffHunk": "@@ -127,6 +127,24 @@\n    */\n   HoodieTimeline getCommitsAndCompactionTimeline();\n \n+\n+  /**\n+   * Get all instants (commits, delta commits) that produce new data, in the active timeline.\n+   */\n+  HoodieTimeline getCommitsTimeline();", "originalCommit": "a023c310341f954ef3a9ee91e12853ca9376fe19", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjQzNjg1NQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392436855", "bodyText": "I removed it", "author": "satishkotha", "createdAt": "2020-03-13T19:51:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzI0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzQwNA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392043404", "bodyText": "lets move the code from HoodieDefaultTimeline here?", "author": "vinothchandar", "createdAt": "2020-03-13T05:59:42Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -116,6 +117,24 @@\n     return returns.toArray(new FileStatus[returns.size()]);\n   }\n \n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   */\n+  protected HoodieTimeline filterInstantsTimeline(HoodieTimeline timeline) {", "originalCommit": "a023c310341f954ef3a9ee91e12853ca9376fe19", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjQzNjkwOA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r392436908", "bodyText": "done", "author": "satishkotha", "createdAt": "2020-03-13T19:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0MzQwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4MTk0Mg==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395181942", "bodyText": "Was this working because these tests only dealt with 1 file-groups ? I can see count based assertions using records collected from this method", "author": "bvaradar", "createdAt": "2020-03-19T17:03:46Z", "path": "hudi-client/src/test/java/org/apache/hudi/common/HoodieMergeOnReadTestUtils.java", "diffHunk": "@@ -47,27 +48,35 @@\n \n   public static List<GenericRecord> getRecordsUsingInputFormat(List<String> inputPaths, String basePath) {\n     JobConf jobConf = new JobConf();\n+    return getRecordsUsingInputFormat(inputPaths, basePath, jobConf, new HoodieParquetRealtimeInputFormat());\n+  }\n+\n+  public static List<GenericRecord> getRecordsUsingInputFormat(List<String> inputPaths,\n+                                                               String basePath,\n+                                                               JobConf jobConf,\n+                                                               HoodieParquetInputFormat inputFormat) {\n     Schema schema = HoodieAvroUtils.addMetadataFields(\n         new Schema.Parser().parse(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA));\n-    HoodieParquetRealtimeInputFormat inputFormat = new HoodieParquetRealtimeInputFormat();\n     setPropsForInputFormat(inputFormat, jobConf, schema, basePath);\n     return inputPaths.stream().map(path -> {\n       setInputPath(jobConf, path);\n       List<GenericRecord> records = new ArrayList<>();\n       try {\n         List<InputSplit> splits = Arrays.asList(inputFormat.getSplits(jobConf, 1));\n-        RecordReader recordReader = inputFormat.getRecordReader(splits.get(0), jobConf, null);\n-        Void key = (Void) recordReader.createKey();\n-        ArrayWritable writable = (ArrayWritable) recordReader.createValue();\n-        while (recordReader.next(key, writable)) {\n-          GenericRecordBuilder newRecord = new GenericRecordBuilder(schema);\n-          // writable returns an array with [field1, field2, _hoodie_commit_time,\n-          // _hoodie_commit_seqno]\n-          Writable[] values = writable.get();\n-          schema.getFields().forEach(field -> {\n-            newRecord.set(field, values[2]);\n-          });\n-          records.add(newRecord.build());\n+        for (InputSplit split : splits) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4NjE5MQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395386191", "bodyText": "yes, correct.", "author": "satishkotha", "createdAt": "2020-03-20T00:10:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4MTk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NTgxNQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395185815", "bodyText": "nit: Add non-empty message for the assertion.", "author": "bvaradar", "createdAt": "2020-03-19T17:09:29Z", "path": "hudi-common/src/test/java/org/apache/hudi/common/table/string/TestHoodieActiveTimeline.java", "diffHunk": "@@ -160,6 +159,8 @@ public void testTimelineOperations() {\n         .filterCompletedInstants().findInstantsInRange(\"04\", \"11\").getInstants().map(HoodieInstant::getTimestamp));\n     HoodieTestUtils.assertStreamEquals(\"\", Stream.of(\"09\", \"11\"), timeline.getCommitTimeline().filterCompletedInstants()\n         .findInstantsAfter(\"07\", 2).getInstants().map(HoodieInstant::getTimestamp));\n+    HoodieTestUtils.assertStreamEquals(\"\", Stream.of(\"01\", \"03\", \"05\"), timeline.getCommitTimeline().filterCompletedInstants()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4NjA0Mw==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395386043", "bodyText": "fixed", "author": "satishkotha", "createdAt": "2020-03-20T00:09:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NTgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NzM5NQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395187395", "bodyText": "We can add similar test-cases for Copy-On-Write table with RO inputformat alone to make it uniform. @satishkotha : Can you look at it as part of this PR or a follow-up ?", "author": "bvaradar", "createdAt": "2020-03-19T17:11:58Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -80,6 +85,12 @@\n \n public class TestMergeOnReadTable extends HoodieClientTestHarness {\n \n+  private HoodieParquetInputFormat roInputFormat;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4NjAwNg==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r395386006", "bodyText": "Thanks for suggestion. I modified one of the existing tests on COW table. Please take a look.", "author": "satishkotha", "createdAt": "2020-03-20T00:09:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE4NzM5NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk1ODg5OQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r399958899", "bodyText": "Didn't realize you can have an overridden method with a return type which is a sub-type of original return type. Learnt something new today :)", "author": "bvaradar", "createdAt": "2020-03-30T06:43:56Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -136,6 +136,13 @@ public HoodieDefaultTimeline findInstantsAfter(String instantTime, int numCommit\n         details);\n   }\n \n+  @Override\n+  public HoodieDefaultTimeline findInstantsBefore(String instantTime) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403397635", "bodyText": "This seems like the crux of the change? and most of the other code is improving tests etc. If so, this seems like a  reasonable interim solution to me... Although we should encourage users to do incremental pull out of the RTInputFormat really ...\nThe core problem of \"data loss\" being brought in this issue, feels like a mis-expectation really :)", "author": "vinothchandar", "createdAt": "2020-04-04T00:42:01Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieParquetInputFormat.java", "diffHunk": "@@ -118,6 +119,34 @@\n     return returns.toArray(new FileStatus[returns.size()]);\n   }\n \n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   */\n+  protected HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    Option<HoodieInstant> pendingCompactionInstant = timeline.filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzNjc2OA==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403836768", "bodyText": "Yes, this is the crux of the change. My understanding is this bug caused data loss on derived ETL tables multiple times. These ETL tables are generated using incremental reads on \"RO\"views. As you suggested, that is core issue and switching to RT views is likely going to get rid of the problem.\nAlso given \"getting started\" and other demo examples  include incremental reads on RO views,  I think this new safeguard is useful to have, especially given that finding root cause  for this took a while.  I am fine with abandoning this change if we can remove incremental read examples on RO views in documentation.", "author": "satishkotha", "createdAt": "2020-04-06T05:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE2MjAxMw==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r404162013", "bodyText": "Once again this is not a bug :) .. Its not supposed to be used this way.. I warned against this in fact, in the past as well .. anyways, water under the bridge..\nI can see how this approach specifically helps the way your datasets... I am fine landing this change per se.. Let's introduce a jobConf variable to control this behavior? we can turn this off by default and you can ask the derived ETL to turn this on? (I am find making it default to on also, your call)\nTo be on the same page, thinking out loud.. This change is orthogonal to the compaction strategy right.. for e.g , the compaction may not compact all the data in log (lets say it only compacts the last N partitions), then the records from older partitions won't show in the change stream right until later... But what this fixes is avoiding incremental reader to advance ahead when it sees a compaction... (The data skipping happens because compaction happens at a much later time and incremental reader moves onto other delta commits for e.g)..\nP.S: Another things to  double check is that the records written into base file during compaction, have the _hoodie_commit_time of the compaction and not the original write.", "author": "vinothchandar", "createdAt": "2020-04-06T15:01:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NDA4Mg==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r405144082", "bodyText": "I can introduce jobConf variable. I'm also agreeing with you that RT is the right approach. I'm just suggesting that we remove incremental read examples in different documents or throw explicit error when someone tries incremental reads on RO views.  For example, docker demo shows incremental reads on RO views. So, people are likely to use this and end up with this difficult to debug problem.\nAlso, you are right about last statement. I already have tests to show that compaction timestamp is used for all updated records and not the update timestamp. Please see line 256 (last line in TestMergeOnReadTable#testIncrementalReadsWithCompaction)  that does not include updateTime. We validate that records include compactionCommitTime instead", "author": "satishkotha", "createdAt": "2020-04-07T22:12:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTgyNDA2Mg==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r405824062", "bodyText": "Good point.. I think we can fix it forward since MOR incr pull issue is being fixed currently and next release, we can move the demo docs to work off _rt table..", "author": "vinothchandar", "createdAt": "2020-04-08T21:24:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzYzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzkyMw==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403397923", "bodyText": "Let's make it clear that this an incremental on RO.. rename to testIncrementalROReadsWithCOmpaction()?", "author": "vinothchandar", "createdAt": "2020-04-04T00:43:45Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -186,6 +168,96 @@ public void testSimpleInsertAndUpdate() throws Exception {\n     }\n   }\n \n+  // test incremental read does not go past compaction instant for RO views\n+  // For RT views, incremental read can go past compaction\n+  @Test\n+  public void testIncrementalReadsWithCompaction() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzOTM3Ng==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403839376", "bodyText": "This actually includes RT views too.\nExample line 195: getRTIncrementalFiles(partitionPath); validateIncrementalFiles()", "author": "satishkotha", "createdAt": "2020-04-06T05:36:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5NzkyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODE2Ng==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403398166", "bodyText": "I am surprised by the amount of test helper code you needed to write yourself.. Should we be using code from some existing helpers.. or should these helpers move somewhere else, so they are beneficial over all?", "author": "vinothchandar", "createdAt": "2020-04-04T00:45:13Z", "path": "hudi-client/src/test/java/org/apache/hudi/table/TestMergeOnReadTable.java", "diffHunk": "@@ -1311,4 +1383,111 @@ private void assertNoWriteErrors(List<WriteStatus> statuses) {\n       assertFalse(\"Errors found in write of \" + status.getFileId(), status.hasErrors());\n     }\n   }\n+  \n+  private FileStatus[] insertAndGetFilePaths(List<HoodieRecord> records, HoodieWriteClient client,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzgzODc1NQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r403838755", "bodyText": "This helper code is refactored from existing test 'TestMergeOnReadTable#testSimpleInsertAndUpdate' . I don't know this setup is needed in other places. If you have any suggestions for new location, let me know.\nThere are also no tests so far with end-to-end testing of incremental reads on MOR tables (as far as i could tell). So I had to add some more utility test methods for that. Please suggest if there is a better location for that", "author": "satishkotha", "createdAt": "2020-04-06T05:34:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODE2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDE1OTQ5MQ==", "url": "https://github.com/apache/hudi/pull/1396#discussion_r404159491", "bodyText": "let's have it like this for now.. Thanks for the explanation..!", "author": "vinothchandar", "createdAt": "2020-04-06T14:58:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzM5ODE2Ng=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "692604937ad658192c2518e0be1e6a1cd4bae993", "url": "https://github.com/apache/hudi/commit/692604937ad658192c2518e0be1e6a1cd4bae993", "message": "[HUDI-687] Stop incremental reader on RO table when there is a pending compaction", "committedDate": "2020-04-09T21:54:15Z", "type": "commit"}, {"oid": "692604937ad658192c2518e0be1e6a1cd4bae993", "url": "https://github.com/apache/hudi/commit/692604937ad658192c2518e0be1e6a1cd4bae993", "message": "[HUDI-687] Stop incremental reader on RO table when there is a pending compaction", "committedDate": "2020-04-09T21:54:15Z", "type": "forcePushed"}]}