{"pr_number": 1289, "pr_title": "[HUDI-92] Provide reasonable names for Spark DAG stages in Hudi.", "pr_createdAt": "2020-01-29T00:37:47Z", "pr_url": "https://github.com/apache/hudi/pull/1289", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NTgzMA==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375665830", "bodyText": "Change to Execute unschedule operations", "author": "vinothchandar", "createdAt": "2020-02-06T06:54:49Z", "path": "hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java", "diffHunk": "@@ -356,6 +357,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met\n     } else {\n       LOG.info(\"The following compaction renaming operations needs to be performed to un-schedule\");\n       if (!dryRun) {\n+        jsc.setJobGroup(this.getClass().getSimpleName(), \"Execute renaming operations\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NTk3OA==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375665978", "bodyText": "change to Generate compaction unscheduling operations ?", "author": "vinothchandar", "createdAt": "2020-02-06T06:55:35Z", "path": "hudi-client/src/main/java/org/apache/hudi/CompactionAdminClient.java", "diffHunk": "@@ -398,6 +400,7 @@ private ValidationOpResult validateCompactionOperation(HoodieTableMetaClient met\n           \"Number of Compaction Operations :\" + plan.getOperations().size() + \" for instant :\" + compactionInstant);\n       List<CompactionOperation> ops = plan.getOperations().stream()\n           .map(CompactionOperation::convertFromAvroRecordInstance).collect(Collectors.toList());\n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Generate renaming operations\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzE5MA==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667190", "bodyText": "In general lets provide some context into what higher level context, the action is being performed i.e savepoints, compaction, rollbacks. etc . In that spirit, change to Collecting latest files for savepoint ?\nAlso wonder if we can include the commitTime in the detail i.e Collecting latest files for savepoint 20200205010000. This way, you can just go to past runs on spark history server and relate them to commits on hudi.. Even better, if someone is running deltastreamer in continuous mode, then they can see activity for commits over time", "author": "vinothchandar", "createdAt": "2020-02-06T07:00:33Z", "path": "hudi-client/src/main/java/org/apache/hudi/HoodieWriteClient.java", "diffHunk": "@@ -586,6 +586,7 @@ public boolean savepoint(String commitTime, String user, String comment) {\n           HoodieTimeline.compareTimestamps(commitTime, lastCommitRetained, HoodieTimeline.GREATER_OR_EQUAL),\n           \"Could not savepoint commit \" + commitTime + \" as this is beyond the lookup window \" + lastCommitRetained);\n \n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Collecting latest files in partition\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzU2OQ==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667569", "bodyText": "Obtain key ranges for file slices (range pruning=on)", "author": "vinothchandar", "createdAt": "2020-02-06T07:02:10Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieBloomIndex.java", "diffHunk": "@@ -254,6 +255,7 @@ private int determineParallelism(int inputParallelism, int totalSubPartitions) {\n \n     if (config.getBloomIndexPruneByRanges()) {\n       // also obtain file ranges, if range pruning is enabled\n+      jsc.setJobDescription(\"Obtain file ranges as range pruning is enabled\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2NzY4OA==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375667688", "bodyText": "Compacting file slices?", "author": "vinothchandar", "createdAt": "2020-02-06T07:02:42Z", "path": "hudi-client/src/main/java/org/apache/hudi/io/compact/HoodieMergeOnReadTableCompactor.java", "diffHunk": "@@ -94,6 +94,7 @@\n         .map(CompactionOperation::convertFromAvroRecordInstance).collect(toList());\n     LOG.info(\"Compactor compacting \" + operations + \" files\");\n \n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Compacting files\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2ODAwNg==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375668006", "bodyText": "I know the comments say files and you are just using that. but would be nice to stick to our terminologies as much as possible. https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture", "author": "vinothchandar", "createdAt": "2020-02-06T07:04:01Z", "path": "hudi-client/src/main/java/org/apache/hudi/table/HoodieCopyOnWriteTable.java", "diffHunk": "@@ -298,6 +298,7 @@ public HoodieCleanerPlan scheduleClean(JavaSparkContext jsc) {\n       int cleanerParallelism = Math.min(partitionsToClean.size(), config.getCleanerParallelism());\n       LOG.info(\"Using cleanerParallelism: \" + cleanerParallelism);\n \n+      jsc.setJobGroup(this.getClass().getSimpleName(), \"Generates List of files to be cleaned\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTY2ODM3Nw==", "url": "https://github.com/apache/hudi/pull/1289#discussion_r375668377", "bodyText": "can we do this in a single line?", "author": "vinothchandar", "createdAt": "2020-02-06T07:05:31Z", "path": "hudi-client/src/test/java/org/apache/hudi/HoodieClientTestHarness.java", "diffHunk": "@@ -107,11 +107,12 @@ protected void initSparkContexts(String appName) {\n   }\n \n   /**\n-   * Initializes the Spark contexts ({@link JavaSparkContext} and {@link SQLContext}) with a default name\n-   * <b>TestHoodieClient</b>.\n+   * Initializes the Spark contexts ({@link JavaSparkContext} and {@link SQLContext}) \n+   * with a default name matching the name of the class.\n    */\n   protected void initSparkContexts() {\n-    initSparkContexts(\"TestHoodieClient\");\n+    String ctxName = this.getClass().getSimpleName() + \"#\" + testName.getMethodName();\n+    initSparkContexts(ctxName);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "3d5265701e812657a229463b1ff7181820f1e9ee", "url": "https://github.com/apache/hudi/commit/3d5265701e812657a229463b1ff7181820f1e9ee", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI.", "committedDate": "2020-07-13T17:56:44Z", "type": "commit"}, {"oid": "3d5265701e812657a229463b1ff7181820f1e9ee", "url": "https://github.com/apache/hudi/commit/3d5265701e812657a229463b1ff7181820f1e9ee", "message": "[HUDI-92] Provide reasonable names for Spark DAG stages in HUDI.", "committedDate": "2020-07-13T17:56:44Z", "type": "forcePushed"}]}