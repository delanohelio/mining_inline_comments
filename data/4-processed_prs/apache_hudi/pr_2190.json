{"pr_number": 2190, "pr_title": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "pr_createdAt": "2020-10-20T15:04:27Z", "pr_url": "https://github.com/apache/hudi/pull/2190", "timeline": [{"oid": "5942ceea24442cc7964de848b2bdf74de18883f0", "url": "https://github.com/apache/hudi/commit/5942ceea24442cc7964de848b2bdf74de18883f0", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-21T14:58:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTUzMDA5NQ==", "url": "https://github.com/apache/hudi/pull/2190#discussion_r511530095", "bodyText": "I think we can structure this as a if block without need for the else? since the if above anyway returns out.", "author": "vinothchandar", "createdAt": "2020-10-25T00:39:13Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java", "diffHunk": "@@ -85,53 +85,55 @@ public boolean next(NullWritable aVoid, ArrayWritable arrayWritable) throws IOEx\n       // if the result is false, then there are no more records\n       return false;\n     } else {\n-      // TODO(VC): Right now, we assume all records in log, have a matching base record. (which\n-      // would be true until we have a way to index logs too)\n-      // return from delta records map if we have some match.\n-      String key = arrayWritable.get()[HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS].toString();\n-      if (deltaRecordMap.containsKey(key)) {\n-        // TODO(NA): Invoke preCombine here by converting arrayWritable to Avro. This is required since the\n-        // deltaRecord may not be a full record and needs values of columns from the parquet\n-        Option<GenericRecord> rec;\n-        if (usesCustomPayload) {\n-          rec = deltaRecordMap.get(key).getData().getInsertValue(getWriterSchema());\n-        } else {\n-          rec = deltaRecordMap.get(key).getData().getInsertValue(getReaderSchema());\n-        }\n-        if (!rec.isPresent()) {\n-          // If the record is not present, this is a delete record using an empty payload so skip this base record\n-          // and move to the next record\n-          return next(aVoid, arrayWritable);\n-        }\n-        GenericRecord recordToReturn = rec.get();\n-        if (usesCustomPayload) {\n-          // If using a custom payload, return only the projection fields. The readerSchema is a schema derived from\n-          // the writerSchema with only the projection fields\n-          recordToReturn = HoodieAvroUtils.rewriteRecordWithOnlyNewSchemaFields(rec.get(), getReaderSchema());\n-        }\n-        // we assume, a later safe record in the log, is newer than what we have in the map &\n-        // replace it. Since we want to return an arrayWritable which is the same length as the elements in the latest\n-        // schema, we use writerSchema to create the arrayWritable from the latest generic record\n-        ArrayWritable aWritable = (ArrayWritable) HoodieRealtimeRecordReaderUtils.avroToArrayWritable(recordToReturn, getHiveSchema());\n-        Writable[] replaceValue = aWritable.get();\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(String.format(\"key %s, base values: %s, log values: %s\", key, HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable),\n-              HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable)));\n-        }\n-        Writable[] originalValue = arrayWritable.get();\n-        try {\n-          // Sometime originalValue.length > replaceValue.length.\n-          // This can happen when hive query is looking for pseudo parquet columns like BLOCK_OFFSET_INSIDE_FILE\n-          System.arraycopy(replaceValue, 0, originalValue, 0,\n-              Math.min(originalValue.length, replaceValue.length));\n-          arrayWritable.set(originalValue);\n-        } catch (RuntimeException re) {\n-          LOG.error(\"Got exception when doing array copy\", re);\n-          LOG.error(\"Base record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable));\n-          LOG.error(\"Log record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable));\n-          String errMsg = \"Base-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(arrayWritable)\n-              + \" ,Log-record :\" + HoodieRealtimeRecordReaderUtils.arrayWritableToString(aWritable) + \" ,Error :\" + re.getMessage();\n-          throw new RuntimeException(errMsg, re);\n+      if (!deltaRecordMap.isEmpty()) {", "originalCommit": "5942ceea24442cc7964de848b2bdf74de18883f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3b1d7f9e670b2c75f32fe618100279d00d4dccab", "url": "https://github.com/apache/hudi/commit/3b1d7f9e670b2c75f32fe618100279d00d4dccab", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-29T16:31:28Z", "type": "forcePushed"}, {"oid": "887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "url": "https://github.com/apache/hudi/commit/887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-30T14:45:22Z", "type": "commit"}, {"oid": "887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "url": "https://github.com/apache/hudi/commit/887c25cdd81e4c166a16c252a9e5f1fa3bcea787", "message": "[HUDI-892] RealtimeParquetInputFormat skip adding projection columns if there are no log files", "committedDate": "2020-10-30T14:45:22Z", "type": "forcePushed"}, {"oid": "0b25273ca3cb8c67dd61fa37458cdf27e90b8d6b", "url": "https://github.com/apache/hudi/commit/0b25273ca3cb8c67dd61fa37458cdf27e90b8d6b", "message": "[HUDI-892]  for test", "committedDate": "2020-10-31T16:19:26Z", "type": "forcePushed"}, {"oid": "6555b66894ae3af625dc00657a6de52d1b6c34dd", "url": "https://github.com/apache/hudi/commit/6555b66894ae3af625dc00657a6de52d1b6c34dd", "message": "[HUDI-892]  for test", "committedDate": "2020-11-01T05:51:11Z", "type": "commit"}, {"oid": "b3a21af65c6717596ccaff03dedb15311c099f33", "url": "https://github.com/apache/hudi/commit/b3a21af65c6717596ccaff03dedb15311c099f33", "message": " [HUDI-892] fix bug generate array from split", "committedDate": "2020-11-01T05:52:03Z", "type": "forcePushed"}, {"oid": "23a52970b597dae4d198063a81e4c36cfe6e607e", "url": "https://github.com/apache/hudi/commit/23a52970b597dae4d198063a81e4c36cfe6e607e", "message": " [HUDI-892] fix bug generate array from split", "committedDate": "2020-11-01T08:47:18Z", "type": "forcePushed"}, {"oid": "b76a393c761b4b8f6e27d387caf99185ad008896", "url": "https://github.com/apache/hudi/commit/b76a393c761b4b8f6e27d387caf99185ad008896", "message": " [HUDI-892]  fix bug generate array from split", "committedDate": "2020-11-01T10:30:47Z", "type": "commit"}, {"oid": "b76a393c761b4b8f6e27d387caf99185ad008896", "url": "https://github.com/apache/hudi/commit/b76a393c761b4b8f6e27d387caf99185ad008896", "message": " [HUDI-892]  fix bug generate array from split", "committedDate": "2020-11-01T10:30:47Z", "type": "forcePushed"}, {"oid": "0902d583419fd907db649de5684cfd9ed1012b43", "url": "https://github.com/apache/hudi/commit/0902d583419fd907db649de5684cfd9ed1012b43", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T06:00:29Z", "type": "forcePushed"}, {"oid": "3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "url": "https://github.com/apache/hudi/commit/3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T10:21:10Z", "type": "commit"}, {"oid": "3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "url": "https://github.com/apache/hudi/commit/3a331d1f6feb8ea40b0a7a45082d62f9cf1265df", "message": "[HUDI-892] revert test log", "committedDate": "2020-11-02T10:21:10Z", "type": "forcePushed"}]}