{"pr_number": 2128, "pr_title": "[HUDI-1303] Some improvements for the HUDI Test Suite.", "pr_createdAt": "2020-09-28T21:46:23Z", "pr_url": "https://github.com/apache/hudi/pull/2128", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI3NjQ3MA==", "url": "https://github.com/apache/hudi/pull/2128#discussion_r499276470", "bodyText": "minor. we might as well do while (numEntriesToAdd-- > 0) {\nand remove line 208.", "author": "nsivabalan", "createdAt": "2020-10-04T18:47:24Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java", "diffHunk": "@@ -205,45 +193,36 @@ private Object typeConvert(Schema schema) {\n       case LONG:\n         return getNextConstrainedLong();\n       case STRING:\n-        return UUID.randomUUID().toString();\n+       return UUID.randomUUID().toString();\n       case ENUM:\n-        List<String> enumSymbols = localSchema.getEnumSymbols();\n-        return new GenericData.EnumSymbol(localSchema, enumSymbols.get(random.nextInt(enumSymbols.size() - 1)));\n+        List<String> enumSymbols = fieldSchema.getEnumSymbols();\n+        return new GenericData.EnumSymbol(fieldSchema, enumSymbols.get(random.nextInt(enumSymbols.size() - 1)));\n       case RECORD:\n-        return convert(localSchema);\n+        return getNewPayload(fieldSchema);\n       case ARRAY:\n-        Schema elementSchema = localSchema.getElementType();\n+        Schema.Field elementField = new Schema.Field(field.name(), fieldSchema.getElementType(), \"\", null);\n         List listRes = new ArrayList();\n-        if (isPrimitive(elementSchema) && this.shouldAddMore) {\n-          int numEntriesToAdd = numEntriesToAdd(elementSchema);\n-          while (numEntriesToAdd > 0) {\n-            listRes.add(typeConvert(elementSchema));\n-            numEntriesToAdd--;\n-          }\n-        } else {\n-          listRes.add(typeConvert(elementSchema));\n+        int numEntriesToAdd = extraEntriesMap.getOrDefault(field.name(), 1);\n+        while (numEntriesToAdd > 0) {", "originalCommit": "8c1b46a918dd8714eb2a4793765ad40285b0b35f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgyNTk5OA==", "url": "https://github.com/apache/hudi/pull/2128#discussion_r499825998", "bodyText": "Done", "author": "prashantwason", "createdAt": "2020-10-05T19:35:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI3NjQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI3Njk0MQ==", "url": "https://github.com/apache/hudi/pull/2128#discussion_r499276941", "bodyText": "what happens if there is only one complexField and numEntriesToAdd is > 10 ?", "author": "nsivabalan", "createdAt": "2020-10-04T18:52:51Z", "path": "hudi-integ-test/src/main/java/org/apache/hudi/integ/testsuite/generator/GenericRecordFullPayloadGenerator.java", "diffHunk": "@@ -333,23 +312,37 @@ private int getSize(Schema elementSchema) {\n    * @param elementSchema\n    * @return Number of entries to add\n    */\n-  private int numEntriesToAdd(Schema elementSchema) {\n-    // Find the size of the primitive data type in bytes\n-    int primitiveDataTypeSize = getSize(elementSchema);\n-    int numEntriesToAdd = numberOfBytesToAdd / primitiveDataTypeSize;\n-    // If more than 10 entries are being added for this same complex field and there are still more complex fields to\n-    // be visited in the schema, reduce the number of entries to add by a factor of 10 to allow for other complex\n-    // fields to pack some entries\n-    if (numEntriesToAdd % 10 > 0 && this.numberOfComplexFields > 1) {\n-      numEntriesToAdd = numEntriesToAdd / 10;\n-      numberOfBytesToAdd -= numEntriesToAdd * primitiveDataTypeSize;\n-      this.shouldAddMore = true;\n-    } else {\n-      this.numberOfBytesToAdd = 0;\n-      this.shouldAddMore = false;\n+  private void determineExtraEntriesRequired(int numberOfComplexFields, int numberOfBytesToAdd) {\n+    for (Schema.Field f : baseSchema.getFields()) {\n+      Schema elementSchema = f.schema();\n+      // Find the size of the primitive data type in bytes\n+      int primitiveDataTypeSize = 0;\n+      if (elementSchema.getType() == Type.ARRAY && isPrimitive(elementSchema.getElementType())) {\n+        primitiveDataTypeSize = getSize(elementSchema.getElementType());\n+      } else if (elementSchema.getType() == Type.MAP && isPrimitive(elementSchema.getValueType())) {\n+        primitiveDataTypeSize = getSize(elementSchema.getValueType());\n+      } else {\n+        continue;\n+      }\n+\n+      int numEntriesToAdd = numberOfBytesToAdd / primitiveDataTypeSize;\n+      // If more than 10 entries are being added for this same complex field and there are still more complex fields to\n+      // be visited in the schema, reduce the number of entries to add by a factor of 10 to allow for other complex\n+      // fields to pack some entries\n+      if (numEntriesToAdd > 10 && numberOfComplexFields > 1) {", "originalCommit": "8c1b46a918dd8714eb2a4793765ad40285b0b35f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgyNzkzMw==", "url": "https://github.com/apache/hudi/pull/2128#discussion_r499827933", "bodyText": "The idea here is to distribute the extra-entries (ins some way) across all complex fields.\n\nIf there is only one complex field then all extra bytes will be added to that single entry.\n\nint numEntriesToAdd = numberOfBytesToAdd / primitiveDataTypeSize;\n...\nextraEntriesMap.put(f.name(), numEntriesToAdd);\n\nIf there are more than 1 complex field, then we add atleast 10 entries to the first one, and so on....\n\nThe perfect solution is to equally divide the extra bytes across all complex fields but I have not covered that yet. (slightly complicated as each field may have different size, etc).", "author": "prashantwason", "createdAt": "2020-10-05T19:39:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTI3Njk0MQ=="}], "type": "inlineReview"}, {"oid": "bd7a276a68e1b5711aee4356f43b6c2a17cf6c1f", "url": "https://github.com/apache/hudi/commit/bd7a276a68e1b5711aee4356f43b6c2a17cf6c1f", "message": "[HUDI-1303] Some improvements for the HUDI Test Suite.\n\n1. Use the DAG Node's label from the yaml as its name instead of UUID names which are not descriptive when debugging issues from logs.\n2. Fix CleanNode constructor which is not correctly implemented\n3. When generating upsets, allows more granualar control over the number of inserts and upserts - zero or more inserts and upserts can be specified instead of always requiring both inserts and upserts.\n4. Fixed generation of records of specific size\n   - The current code was using a class variable \"shouldAddMore\" which was reset to false after the first record generation causing subsequent records to be of minimum size.\n   - In this change, we pre-calculate the extra size of the complex fields. When generating records, for complex fields we read the field size from this map.\n5. Refresh the timeline of the DeltaSync service before calling readFromSource. This ensures that only the newest generated data is read and data generated in the older Dag Nodes is ignored (as their AVRO files will have an older timestamp).\n6. Making --workload-generator-classname an optional parameter as most probably the default will be used", "committedDate": "2020-10-05T19:39:33Z", "type": "commit"}, {"oid": "bd7a276a68e1b5711aee4356f43b6c2a17cf6c1f", "url": "https://github.com/apache/hudi/commit/bd7a276a68e1b5711aee4356f43b6c2a17cf6c1f", "message": "[HUDI-1303] Some improvements for the HUDI Test Suite.\n\n1. Use the DAG Node's label from the yaml as its name instead of UUID names which are not descriptive when debugging issues from logs.\n2. Fix CleanNode constructor which is not correctly implemented\n3. When generating upsets, allows more granualar control over the number of inserts and upserts - zero or more inserts and upserts can be specified instead of always requiring both inserts and upserts.\n4. Fixed generation of records of specific size\n   - The current code was using a class variable \"shouldAddMore\" which was reset to false after the first record generation causing subsequent records to be of minimum size.\n   - In this change, we pre-calculate the extra size of the complex fields. When generating records, for complex fields we read the field size from this map.\n5. Refresh the timeline of the DeltaSync service before calling readFromSource. This ensures that only the newest generated data is read and data generated in the older Dag Nodes is ignored (as their AVRO files will have an older timestamp).\n6. Making --workload-generator-classname an optional parameter as most probably the default will be used", "committedDate": "2020-10-05T19:39:33Z", "type": "forcePushed"}]}