{"pr_number": 1654, "pr_title": "HUDI-921 Remove inlineCompactionEvery method in HoodieCompactionConfig.Builder", "pr_createdAt": "2020-05-23T09:54:33Z", "pr_url": "https://github.com/apache/hudi/pull/1654", "timeline": [{"oid": "80443e656fcf3b7c8a295d2d638f58762e904b31", "url": "https://github.com/apache/hudi/commit/80443e656fcf3b7c8a295d2d638f58762e904b31", "message": "HUDI-921 Remove inlineCompactionEvery method in HoodieCompactionConfig.Builder", "committedDate": "2020-05-23T09:49:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTYwNTQzNQ==", "url": "https://github.com/apache/hudi/pull/1654#discussion_r429605435", "bodyText": "this is wrong nonetheless.. we can remove this method..\nbut the issue you faced may not be due to this? I see LogFileSizeBasedCompactionStrategy is the default strategy, which chooses the largest log files for compaction (like I mentioned, we check if compaction has to be scheduled at the end of each delta commit), upto a certain size .. This default is very large (probably due to the data volumes at uber)//\n  public static final String DEFAULT_TARGET_IO_PER_COMPACTION_IN_MB = String.valueOf(500 * 1024);\nAnd it keeps compacting all the logs, thus..\nCould you try setting hoodie.compaction.target.io=1 (i.e allow upto just 1MB, artifically low threshold) and see if the log files are there.. We can also revisit if we can lower the limit for that in this PR.", "author": "vinothchandar", "createdAt": "2020-05-24T06:49:37Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieCompactionConfig.java", "diffHunk": "@@ -147,11 +147,6 @@ public Builder withInlineCompaction(Boolean inlineCompaction) {\n       return this;\n     }\n \n-    public Builder inlineCompactionEvery(int deltaCommits) {\n-      props.setProperty(INLINE_COMPACT_PROP, String.valueOf(deltaCommits));", "originalCommit": "80443e656fcf3b7c8a295d2d638f58762e904b31", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTYxMDYwMw==", "url": "https://github.com/apache/hudi/pull/1654#discussion_r429610603", "bodyText": "@sathyaprakashg : Thanks for catching this.\nYes, this method should not be there in the first place.\nThough, For context, this is innocuous as Hudi uses the property hoodie.compact.inline.max.delta.commits for figure out inline compaction interval. The associated set API  is withMaxNumDeltaCommitsBeforeCompaction()(https://hudi.apache.org/docs/configurations.html#withMaxNumDeltaCommitsBeforeCompaction). Can you try setting this value : hoodie.compact.inline.max.delta.commits to a higher value ?", "author": "bvaradar", "createdAt": "2020-05-24T08:05:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTYwNTQzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTc4NjMzOQ==", "url": "https://github.com/apache/hudi/pull/1654#discussion_r429786339", "bodyText": "Thanks @vinothchandar and @bvaradar for the information.\nI see that currently, default value of hoodie.compact.inline.max.delta.commits is 1 and it is making merge on read table to do inline compaction during every load. We can set custom value in specific load, but it could be better if default value is higher than 1, so that MOR table does not behave like COW table by recreating affected parquet file every time.\nI created MR for this. Please review and let me know your thoughts.\n#1664", "author": "sathyaprakashg", "createdAt": "2020-05-25T07:53:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTYwNTQzNQ=="}], "type": "inlineReview"}]}