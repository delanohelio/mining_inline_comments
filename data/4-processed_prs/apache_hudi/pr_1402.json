{"pr_number": 1402, "pr_title": "[HUDI-407] Adding Simple Index", "pr_createdAt": "2020-03-13T00:29:54Z", "pr_url": "https://github.com/apache/hudi/pull/1402", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MjYwOA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391972608", "bodyText": "crux of the change is in this method.", "author": "nsivabalan", "createdAt": "2020-03-13T00:30:41Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,\n+                                                   final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList =\n+        jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+          Option<HoodieInstant> latestCommitTime =\n+              hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+          List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+          if (latestCommitTime.isPresent()) {\n+            filteredFiles = hoodieTable.getROFileSystemView()\n+                .getLatestDataFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+                .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());\n+          }\n+          return filteredFiles.iterator();\n+        }).collect();\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaSparkContext jsc,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391972940", "bodyText": "@vinothchandar @bvaradar : As part of testing the new simple index, I have parametrized this test (TestHoodieClientOnCopyOnWriteStorage) and have added other index types as well. is that fine or do you think the increase in build/test time is not worth it.", "author": "nsivabalan", "createdAt": "2020-03-13T00:32:07Z", "path": "hudi-client/src/test/java/org/apache/hudi/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -79,9 +82,22 @@\n import static org.mockito.Mockito.when;\n \n @SuppressWarnings(\"unchecked\")\n+@RunWith(Parameterized.class)\n public class TestHoodieClientOnCopyOnWriteStorage extends TestHoodieClientBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private final IndexType indexType;\n+\n+  @Parameterized.Parameters(name = \"{index}: Test with IndexType={0}\")\n+  public static Collection<Object[]> data() {\n+    Object[][] data =\n+        new Object[][] {{IndexType.BLOOM},{IndexType.GLOBAL_BLOOM},{IndexType.SIMPLE}};", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNzEzOQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400427139", "bodyText": "how much does it increase by... doing the full suite with both indexes may be an overkill IMO", "author": "vinothchandar", "createdAt": "2020-03-30T19:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA2ODEwNg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405068106", "bodyText": "aprrox 2 mins per index type.", "author": "nsivabalan", "createdAt": "2020-04-07T19:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA3NjMzMQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405076331", "bodyText": "Could be okay then", "author": "vinothchandar", "createdAt": "2020-04-07T20:00:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MzIyNQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391973225", "bodyText": "As of now, I don't do the lazy record iterator. Thought will make that in a diff patch as I want to get this out sooner.", "author": "nsivabalan", "createdAt": "2020-03-13T00:33:26Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -103,6 +120,42 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                                      String baseInstantTime,\n+                                                                                                                      String fileId) {\n+    List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> rows = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMzM0MQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398223341", "bodyText": "I think we can fix it in this patch itself.. its a critical aspect", "author": "vinothchandar", "createdAt": "2020-03-25T23:04:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MzIyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1MDAyMg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392050022", "bodyText": "why these changes?", "author": "vinothchandar", "createdAt": "2020-03-13T06:26:45Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -77,15 +80,15 @@ protected HoodieIndex(HoodieWriteConfig config) {\n    * present).\n    */\n   public abstract JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) throws HoodieIndexException;\n+                                                       HoodieTable<T> hoodieTable) throws HoodieIndexException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1MjgwNQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405052805", "bodyText": "I am not sure. my previous patches didn't have any such refactoring changes. I just do the regular intellij refactoring.", "author": "nsivabalan", "createdAt": "2020-04-07T19:18:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1MDAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTA4OA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392055088", "bodyText": "new configs for this class?", "author": "vinothchandar", "createdAt": "2020-03-13T06:47:27Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTE0Mw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392055143", "bodyText": "same here", "author": "vinothchandar", "createdAt": "2020-03-13T06:47:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjI5MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056290", "bodyText": "We orginally did this to avoid shuffling the input data to do the join..against the bloom filter/range information.. I think in this scenario, all we are doing is a single join against all of records in the DFS partitions.. So I suggest, we make it much simpler and lower overhead, by  the following approach\n\ncache the recordRDD  alone\nFrom that extract affectedPartitions\nPrep the RDD of records in these partitions as partitionFileIdRecLocationEntryPairRDD\nJust join recordRDD with that..\n\nit may simpify a few things..", "author": "vinothchandar", "createdAt": "2020-03-13T06:52:03Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU3MDczMg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r396570732", "bodyText": "I have made this leaner @vinothchandar. You can check it out. I am yet to modularize bloom index and simple index. But feel free to take a look at the core logic in the mean time, since you are working on a diff impl, we might get some ideas.", "author": "nsivabalan", "createdAt": "2020-03-23T16:10:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjI5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTU5OQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400425599", "bodyText": "I have paused the other impl for now.. But let's get the abstractions right once and reuse.. with you there", "author": "vinothchandar", "createdAt": "2020-03-30T19:04:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjQ4Mw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056483", "bodyText": "overall you are following the same code structure as BloomIndex.. any way to share more code?", "author": "vinothchandar", "createdAt": "2020-03-13T06:52:48Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056996", "bodyText": "We need to create some helper classes to make this code + BloomIndex more readable ... for e.g\nClass PartitionRecordKey {\n    String ..;\n    String ..;\n}\n\nand use JavaRDD?  or is n't this really the HoodieKey ?", "author": "vinothchandar", "createdAt": "2020-03-13T06:54:55Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwNzA0Mw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399407043", "bodyText": "yes, but at some later stage, we do combine all recordKeys per partition and hence didn't disturb the flow much. If you think, we should look to re-use HoodieKey from get go, I can give it a shot.", "author": "nsivabalan", "createdAt": "2020-03-27T16:55:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNjUzNg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400426536", "bodyText": "yes the above can be reused by using HoodieKey itself.. Things like\nPair<String, String> for recordKey, FileID\nPar<String, String>  for fileID, partitionPath\ncan we replaced by pojos.. will help readability a lot", "author": "vinothchandar", "createdAt": "2020-03-30T19:05:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1MzEyOQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405053129", "bodyText": "I am using HoodieKeys in most places with latest commit.", "author": "nsivabalan", "createdAt": "2020-04-07T19:18:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzA2NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057064", "bodyText": "code reuse please", "author": "vinothchandar", "createdAt": "2020-03-13T06:55:11Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzQzNg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057436", "bodyText": "so this only works for COW ? or assumed inserts never go to logs? I would like to remove such restrictions from this index if possible..", "author": "vinothchandar", "createdAt": "2020-03-13T06:56:32Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,\n+                                                   final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList =\n+        jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+          Option<HoodieInstant> latestCommitTime =\n+              hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+          List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+          if (latestCommitTime.isPresent()) {\n+            filteredFiles = hoodieTable.getROFileSystemView()\n+                .getLatestDataFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+                .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());\n+          }\n+          return filteredFiles.iterator();\n+        }).collect();\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaSparkContext jsc,\n+                                                                                      List<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: Create JavaPairRDD< Tuple2<PartitionPath, RecordKey>, Optional<HoodieRecordLocation> > from input with  Optional<HoodieRecordLocation> as Empty.\n+    JavaPairRDD<Tuple2<String, String>, Optional<HoodieRecordLocation>> partitionRecordPairs =\n+        partitionRecordKeyPairRDD.mapToPair((PairFunction<Tuple2<String, String>, Tuple2<String, String>, Optional<HoodieRecordLocation>>) stringStringTuple2\n+            -> new Tuple2(stringStringTuple2, Optional.absent()));\n+\n+    // Step 2: Create JavaRDD< Tuple2 <Partition, FileId> > from partitions to be touched\n+    JavaRDD<Tuple2<String, String>> partitionFileIdTupleRDD = jsc.parallelize(partitionToFileIndexInfo);\n+\n+    // Step 3: For each partiion, fileId Tuple -> Fetch RDD of triplets ( Tuple2 <PartitionPath, recordKey>, HoodieRecordLocation )\n+    JavaRDD<Tuple2<Tuple2<String, String>, Option<HoodieRecordLocation>>> partitionFileIdLocationEntries = partitionFileIdTupleRDD.flatMap(partitionFileIdTuple -> {\n+      HoodieDataFile latestDataFile = getLatestDataFile(hoodieTable, Pair.of(partitionFileIdTuple._1, partitionFileIdTuple._2));\n+      List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> resultList = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(hoodieTable.getHadoopConf(), new Path(latestDataFile.getPath()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1MzUwMA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405053500", "bodyText": "yes, inserts never go to logs right?", "author": "nsivabalan", "createdAt": "2020-04-07T19:19:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzQzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA3NjUyMw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405076523", "bodyText": "for now.. you can assume this...", "author": "vinothchandar", "createdAt": "2020-04-07T20:00:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzQzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzYyNA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057624", "bodyText": "We should also implement a GLOBAL_SIMPLE?", "author": "vinothchandar", "createdAt": "2020-03-13T06:57:03Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -118,9 +121,10 @@ protected HoodieIndex(HoodieWriteConfig config) {\n   /**\n    * Each index type should implement it's own logic to release any resources acquired during the process.\n    */\n-  public void close() {}\n+  public void close() {\n+  }\n \n   public enum IndexType {\n-    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM\n+    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwNDgwNw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399404807", "bodyText": "Sure, will add it in a diff patch.", "author": "nsivabalan", "createdAt": "2020-03-27T16:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzYyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNDE4MQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400424181", "bodyText": "let's fix it in this itself?", "author": "vinothchandar", "createdAt": "2020-03-30T19:01:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzYyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Nzc3Ng==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057776", "bodyText": "fix copy-pastd comments everywhere?", "author": "vinothchandar", "createdAt": "2020-03-13T06:57:37Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMTQwOQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398221409", "bodyText": "call it SimpleBloom when it does not use any bloom filters is very confusing.. Can we make it just SimpleIndex?", "author": "vinothchandar", "createdAt": "2020-03-25T22:58:59Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -58,6 +58,11 @@\n   public static final String DEFAULT_BLOOM_INDEX_FILTER_TYPE = BloomFilterTypeCode.SIMPLE.name();\n   public static final String HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES = \"hoodie.bloom.index.filter.dynamic.max.entries\";\n   public static final String DEFAULT_HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES = \"100000\";\n+  public static final String SIMPLE_BLOOM_INDEX_USE_CACHING_PROP = \"hoodie.simple.bloom.index.use.caching\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjA1NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398222054", "bodyText": "let's write docs using higher level abstractions like file groups/slices, base/log file?", "author": "vinothchandar", "createdAt": "2020-03-25T23:00:44Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398222688", "bodyText": "I think this very confusing.. may be we need to push a bunch to HoodieIndex abstract class or have a new class introduced..", "author": "vinothchandar", "createdAt": "2020-03-25T23:02:28Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwNTg4Mg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399405882", "bodyText": "I did some refactoring in HoodieBloomIndex to reuse by SimpleIndex. You can check it out.", "author": "nsivabalan", "createdAt": "2020-03-27T16:53:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTEyOA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400425128", "bodyText": "Will do.. We should really create a new abstract class AbstactFileLevelIndex which can be extended by the BloomIndex and SimpleIndex classes..  HoodieIndex should be left alone, since stuff the HBaseIndex does not really work this way..", "author": "vinothchandar", "createdAt": "2020-03-30T19:03:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA2Nzc5NQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r405067795", "bodyText": "There is not a lot of common code blocks between bloom index and simple index except for one method (getTaggedRecord). So, didn't find a need to create AbstactFileLevelIndex. Open to discussions as to how we can add this and if its required in this patch.", "author": "nsivabalan", "createdAt": "2020-04-07T19:45:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwODE1NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399408154", "bodyText": "Since the incoming records are files, I am not sure how much value we get in introducing a LazyIterator. I have both options listed here(inline vs lazy iterator). Let me know your thoughts. Interested in understanding the nuances.", "author": "nsivabalan", "createdAt": "2020-03-27T16:57:04Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: for each <partition, fileId> pair, co locate records to be searched for\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = partitionToFileIndexInfo.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+    JavaPairRDD<String, List<String>> partitionToRecords = partitionRecordKeyPairRDD.groupByKey().mapToPair(entry -> new Tuple2(entry._1, Lists.newArrayList(entry._2)));\n+    JavaPairRDD<String, Tuple2<String, List<String>>> partitionToFileIdAndRecordsPairRDD = partitionFileIndexInfoPairRDD.join(partitionToRecords);\n+\n+    // Step 2: For each partition, fileId, List<recordKeys> Triplet -> Fetch RDD of triplets ( Tuple2< HoodieKey, HoodieRecordLocation >)\n+\n+    /*Option1:\n+    return partitionToFileIdAndRecordsPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, Tuple2<String, List<String>>>, HoodieKey, HoodieRecordLocation>) stringTuple2Tuple2 -> {\n+          HoodieDataFile latestDataFile = getLatestDataFile(hoodieTable, Pair.of(stringTuple2Tuple2._1, stringTuple2Tuple2._2._1));\n+          List<Pair<HoodieKey, HoodieRecordLocation>> resultList = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(hoodieTable.getHadoopConf(), new Path(latestDataFile.getPath()),\n+              latestDataFile.getCommitTime(), stringTuple2Tuple2._2._1, stringTuple2Tuple2._2._2);\n+          List<Tuple2<HoodieKey, HoodieRecordLocation>> toReturn = new ArrayList<>();\n+          resultList.forEach(rec -> toReturn.add(new Tuple2<>(rec.getLeft(), rec.getRight())));\n+          return toReturn.iterator();\n+        });*/\n+\n+    // Option2:\n+    return partitionToFileIdAndRecordsPairRDD.mapPartitions(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyOTc0MQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400429741", "bodyText": "rename the parameters?", "author": "vinothchandar", "createdAt": "2020-03-30T19:11:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMDY0OA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400430648", "bodyText": "I find these hard to read :( ...", "author": "vinothchandar", "createdAt": "2020-03-30T19:13:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: for each <partition, fileId> pair, co locate records to be searched for\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = partitionToFileIndexInfo.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+    JavaPairRDD<String, List<String>> partitionToRecords = partitionRecordKeyPairRDD.groupByKey().mapToPair(entry -> new Tuple2(entry._1, Lists.newArrayList(entry._2)));\n+    JavaPairRDD<String, Tuple2<String, List<String>>> partitionToFileIdAndRecordsPairRDD = partitionFileIndexInfoPairRDD.join(partitionToRecords);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNDg3Nw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407234877", "bodyText": "If you have more than 100 executors , then your performance will be limited by parallelism.. I think we should have simpleIndexParallelism default to 0. if its 0, we have parallelism=number of files, if its set to a non-zero value, we use that ?", "author": "vinothchandar", "createdAt": "2020-04-12T18:23:20Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc3MjQ4Nw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407772487", "bodyText": "got it.", "author": "nsivabalan", "createdAt": "2020-04-13T23:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNDg3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTA1Mg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407235052", "bodyText": "getLatestBaseFile() (standard terminology)", "author": "vinothchandar", "createdAt": "2020-04-12T18:24:28Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = fileInfoList.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching records from all files of interest\");\n+    return partitionFileIndexInfoPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, String>, HoodieKey, HoodieRecordLocation>) partitionPathFileId ->\n+            new RecordFetcher(partitionPathFileId, hoodieTable).getResultSet()\n+    );\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  private List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                 final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Fetch the latest base file for the given partition and fileId.\n+   *\n+   * @param hoodieTable           instance of {@link HoodieTable} in which the partition exists\n+   * @param partitionPathFilePair Partition path fileId pair\n+   * @return the latest data file for the given partition and fileId\n+   */\n+  private HoodieBaseFile getLatestDataFile(HoodieTable hoodieTable, Pair<String, String> partitionPathFilePair) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTI0MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407235240", "bodyText": "we can't call ParquetUtils from here.. that breaks abstraction. Lets design a new handle", "author": "vinothchandar", "createdAt": "2020-04-12T18:25:51Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = fileInfoList.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching records from all files of interest\");\n+    return partitionFileIndexInfoPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, String>, HoodieKey, HoodieRecordLocation>) partitionPathFileId ->\n+            new RecordFetcher(partitionPathFileId, hoodieTable).getResultSet()\n+    );\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  private List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                 final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Fetch the latest base file for the given partition and fileId.\n+   *\n+   * @param hoodieTable           instance of {@link HoodieTable} in which the partition exists\n+   * @param partitionPathFilePair Partition path fileId pair\n+   * @return the latest data file for the given partition and fileId\n+   */\n+  private HoodieBaseFile getLatestDataFile(HoodieTable hoodieTable, Pair<String, String> partitionPathFilePair) {\n+    return hoodieTable.getBaseFileOnlyView()\n+        .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n+  }\n+\n+  /**\n+   * Record Fetcher for a given partitionPath, fileId pair.\n+   */\n+  class RecordFetcher {\n+    private HoodieTable<T> table;\n+    private Tuple2<String, String> partitionPathFileIdPair;\n+\n+    RecordFetcher(Tuple2<String, String> partitionPathFileIdPair, HoodieTable<T> table) {\n+      this.partitionPathFileIdPair = partitionPathFileIdPair;\n+      this.table = table;\n+    }\n+\n+    Iterator<Tuple2<HoodieKey, HoodieRecordLocation>> getResultSet() throws Exception {\n+      HoodieBaseFile baseFile = getLatestDataFile(table, Pair.of(partitionPathFileIdPair._1, partitionPathFileIdPair._2));\n+      List<Pair<HoodieKey, HoodieRecordLocation>> records = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(table.getHadoopConf(), new Path(baseFile.getPath()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzc3NjA0NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407776044", "bodyText": "RecordFetcher is the actual handle. Already I have abstracted out. not sure what more do we need. Can you help me clarify @vinothchandar", "author": "nsivabalan", "createdAt": "2020-04-13T23:25:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTI0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NjgyMA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424086820", "bodyText": "what I mean by handle is a subclass of HoodieReadHandle .. it's an anti pattern to assume its a parquet base file from the index level.. Let's fix this", "author": "vinothchandar", "createdAt": "2020-05-12T23:16:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTI0MA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjQ3Mw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476473", "bodyText": "@vinothchandar : have removed other index types to see travis CI succeed. I got\n The job exceeded the maximum time limit for jobs, and has been terminated.\nin travis CI when enabled.", "author": "nsivabalan", "createdAt": "2020-04-27T02:58:57Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -80,9 +83,22 @@\n import static org.mockito.Mockito.when;\n \n @SuppressWarnings(\"unchecked\")\n+@RunWith(Parameterized.class)\n public class TestHoodieClientOnCopyOnWriteStorage extends TestHoodieClientBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private final IndexType indexType;\n+\n+  @Parameterized.Parameters(name = \"{index}: Test with IndexType={0}\")\n+  public static Collection<Object[]> data() {\n+    Object[][] data =\n+        new Object[][] {{IndexType.BLOOM}};", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjY5MQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476691", "bodyText": "Will remove this file. have integrated into TestHoodieIndex.", "author": "nsivabalan", "createdAt": "2020-04-27T02:59:38Z", "path": "hudi-client/src/test/java/org/apache/hudi/index/TestSimpleIndex.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieHBaseIndexConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.bloom.HoodieBloomIndex;\n+import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n+import org.apache.hudi.index.hbase.HBaseIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestSimpleIndex extends HoodieClientTestHarness {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjgwNA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476804", "bodyText": "Will remove this file as well. Integrated into TestHoodieIdex.", "author": "nsivabalan", "createdAt": "2020-04-27T02:59:59Z", "path": "hudi-client/src/test/java/org/apache/hudi/index/TestGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestGlobalSimpleIndex extends HoodieClientTestHarness {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzNjAwMQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416336001", "bodyText": "just hoodie.simple.index.update.partition.path to be consistent with the other config?", "author": "vinothchandar", "createdAt": "2020-04-28T05:25:59Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -92,6 +100,9 @@\n   public static final String BLOOM_INDEX_UPDATE_PARTITION_PATH = \"hoodie.bloom.index.update.partition.path\";\n   public static final String DEFAULT_BLOOM_INDEX_UPDATE_PARTITION_PATH = \"false\";\n \n+  public static final String GLOBAL_SIMPLE_INDEX_UPDATE_PARTITION_PATH = \"hoodie.global.simple.index.update.partition.path\";", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjM1NDczNA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416354734", "bodyText": "this can very well go into HoodieIndexUtils ?  There is no instance stats accessed in this method IIUC", "author": "vinothchandar", "createdAt": "2020-04-28T06:15:42Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -128,9 +133,26 @@ protected HoodieIndex(HoodieWriteConfig config) {\n   /**\n    * Each index type should implement it's own logic to release any resources acquired during the process.\n    */\n-  public void close() {}\n+  public void close() {\n+  }\n+\n+  protected HoodieRecord<T> getTaggedRecord(HoodieRecord<T> inputRecord, Option<HoodieRecordLocation> location) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjcxOTc0NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416719744", "bodyText": "@nsivabalan This is problematic.. since we don't cache (rightfully so) the input here for Global index,\nJavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n\nwill collect() once reading the input, and then later  return getTaggedRecords(incomingRecordsKeyedOnRecordKeys, existingRecordsKeyedOnRecordKeys); will also read the input again for joining..\nAlso why fetchRecordLocations() we should be loading all the partitions, not just the ones based on incomingRecord, right?\nGlobal should be much simpler.. All it needs is to create existingRecords based on listing from all partitions and then do the getTaggedRecords() join..", "author": "vinothchandar", "createdAt": "2020-04-28T15:42:15Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0NjUxNQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r417046515", "bodyText": "I didn't realize that we can't use the same fetchRecordLocations that we used for SimpleIndex since we calculate the list of distinct partitions which makes sense for non-global, but not for global.  Have fixed it.\nBtw, wanted to remind a difference in global when compared to non-global(I am sure you are aware of this, but wanted to call it out). We need to do left outer join with recordKeys and not HoodieKeys(which is what we do in non global simple index), since partition path could differ in existing records when compared to incoming keys.", "author": "nsivabalan", "createdAt": "2020-04-29T03:24:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjcxOTc0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzA0NjczMg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r417046732", "bodyText": "After my latest commit, I have overloaded fetchRecordLocations(), but even before that we have overloaded loadAllFilesForPartitions() in HoodieGlobalSimpleIndex. So, we are loading all partitions.", "author": "nsivabalan", "createdAt": "2020-04-29T03:25:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjcxOTc0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzMzNzMyNQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r417337325", "bodyText": "@vinothchandar : since we are re-using RecordFetcher(used in non-global index) which is returning entries keyed on HoodieKeys, we have to do another map call here after calling fetchRecordLocations. Do you think we can introduce another class to return entries keyed on RecordKeys?", "author": "nsivabalan", "createdAt": "2020-04-29T14:00:13Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+\n+    JavaPairRDD<String, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.values().map(HoodieRecord::getKey), jsc, hoodieTable,\n+        config.getGlobalSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, Tuple2<HoodieKey, HoodieRecordLocation>> existingRecordsKeyedOnRecordKeys = existingRecords.mapToPair(entry -> new Tuple2<>(entry._1.getRecordKey(),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAwOTg3NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424009874", "bodyText": "need to add such builder methods for all the simple index configs?", "author": "vinothchandar", "createdAt": "2020-05-12T20:21:39Z", "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -201,6 +212,11 @@ public Builder withBloomIndexUpdatePartitionPath(boolean updatePartitionPath) {\n       return this;\n     }\n \n+    public Builder withGlobalSimpleIndexUpdatePartitionPath(boolean updatePartitionPath) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMDY2NA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424010664", "bodyText": "why was this needed?", "author": "vinothchandar", "createdAt": "2020-05-12T20:23:03Z", "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -86,6 +86,9 @@\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n \n+  public TestHoodieClientOnCopyOnWriteStorage() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDM3ODU1OQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424378559", "bodyText": "I thought I will parametrize this test to run for diff indexes and later removed it. missed to remove the constructor.", "author": "nsivabalan", "createdAt": "2020-05-13T11:55:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMDY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAyMzU1NQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424023555", "bodyText": "we need to persist the input recordRDD here.. not the transformed RDD", "author": "vinothchandar", "createdAt": "2020-05-12T20:47:33Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0Mjk5Mw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424042993", "bodyText": "we can return the full BaseFile here itself and avoid an additional lookup later for Simple Index..\nBloomIndex just uses file IDs so it shuffles less.. but we don't have to be concerned for simple index per se", "author": "vinothchandar", "createdAt": "2020-05-12T21:25:37Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * Hoodie Index Utilities.\n+ */\n+public class HoodieIndexUtils {\n+\n+  /**\n+   * Fetches Pair of partition path and fileId for interested partitions.\n+   *\n+   * @param partitions  list of partitions of interest\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return the list of Pairs of partition path and fileId\n+   */\n+  public static List<Pair<String, String>> loadLatestDataFilesForAllPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                               final HoodieTable hoodieTable) {\n+    return jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+      Option<HoodieInstant> latestCommitTime =\n+          hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+      List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+      if (latestCommitTime.isPresent()) {\n+        filteredFiles = hoodieTable.getBaseFileOnlyView()\n+            .getLatestBaseFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+            .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0MzM3MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424043370", "bodyText": "this parallelize's parallelism has to be determined based on files being read..", "author": "vinothchandar", "createdAt": "2020-05-12T21:26:19Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(incomingRecords.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> HoodieIndexUtils.getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link JavaRDD} of HoodieKeys.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return Hoodiekeys mapped to partitionpath and filenames\n+   */\n+  JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocationInternal(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                   JavaSparkContext jsc, HoodieTable<T> hoodieTable,\n+                                                                                   int parallelism) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(hoodieKeys, jsc, hoodieTable, parallelism);\n+\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> recordLocations = incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+    return recordLocations.mapToPair(entry -> {\n+      if (entry._2.isPresent()) {\n+        return new Tuple2<>(entry._1, Option.of(Pair.of(entry._1.getPartitionPath(), entry._2.get().getFileId())));\n+      } else {\n+        return new Tuple2<>(entry._1, Option.empty());\n+      }\n+    });\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocationsBasedOnHoodieKey(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable,\n+                                                                                              int parallelism) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0NDc1Nw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424044757", "bodyText": "why sort by partition, when reading happens file by file?", "author": "vinothchandar", "createdAt": "2020-05-12T21:29:23Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(incomingRecords.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> HoodieIndexUtils.getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link JavaRDD} of HoodieKeys.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return Hoodiekeys mapped to partitionpath and filenames\n+   */\n+  JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocationInternal(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                   JavaSparkContext jsc, HoodieTable<T> hoodieTable,\n+                                                                                   int parallelism) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(hoodieKeys, jsc, hoodieTable, parallelism);\n+\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> recordLocations = incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+    return recordLocations.mapToPair(entry -> {\n+      if (entry._2.isPresent()) {\n+        return new Tuple2<>(entry._1, Option.of(Pair.of(entry._1.getPartitionPath(), entry._2.get().getFileId())));\n+      } else {\n+        return new Tuple2<>(entry._1, Option.empty());\n+      }\n+    });\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocationsBasedOnHoodieKey(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable,\n+                                                                                              int parallelism) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, parallelism);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDM0Mzg3MQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424343871", "bodyText": "yeah, sorting is not required actually.", "author": "nsivabalan", "createdAt": "2020-05-13T10:45:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0NDc1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NTk3Ng==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424085976", "bodyText": "can we use an empty list instead or null", "author": "vinothchandar", "createdAt": "2020-05-12T23:13:45Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +122,60 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                   String baseInstantTime,\n+                                                                                                   String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, null);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NjQ2MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424086460", "bodyText": "can we just limit this method to reading the parquet fields and nothing else.. i.e you can add the HoodieRecordLocation object outside on the caller side.", "author": "vinothchandar", "createdAt": "2020-05-12T23:15:27Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +122,60 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                   String baseInstantTime,\n+                                                                                                   String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, null);\n+  }\n+\n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                            String baseInstantTime,\n+                                                                                            String fileId, List<String> recordsToFilter) {\n+    List<Pair<HoodieKey, HoodieRecordLocation>> rows = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();\n+      Object obj = reader.read();\n+      while (obj != null) {\n+        if (obj instanceof GenericRecord) {\n+          String recordKey = ((GenericRecord) obj).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+          String partitionPath = ((GenericRecord) obj).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();\n+          if (recordsToFilter == null) {\n+            rows.add(Pair.of(new HoodieKey(recordKey, partitionPath), new HoodieRecordLocation(baseInstantTime, fileId)));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDEzODk1Ng==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424138956", "bodyText": "we are caching inputRecordRDD, but left outer join is using keyedInputRecordRDD. Thats why I had to cache differently. Can you help explain the rational? looks like we are not effectively using the cache, isn't ?", "author": "nsivabalan", "createdAt": "2020-05-13T02:31:35Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param inputRecordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      inputRecordRDD.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(record -> new Tuple2<>(record.getKey(), record));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingLocationsOnTable = fetchRecordLocationsForAffectedPartitions(keyedInputRecordRDD.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = keyedInputRecordRDD.leftOuterJoin(existingLocationsOnTable)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE1OTkxNw==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424159917", "bodyText": "so, the way Spark would execute this is to first compute the keyedInputRecordRDD from inputRecordRDD and then cache it - it has higher memory footprint due to the extra HoodieKey stored with every record (compared to inputRecordRDD)..\nSo if we cache inputRecordRDD, its cheap to compute keyedInputRecordRDD anyway as needed.. regardless, the code was unpersisting inputRecordRDD while caching keyedInputRecordRDD before.. So had to fix anyway", "author": "vinothchandar", "createdAt": "2020-05-13T04:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDEzODk1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDMyOTQwMQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424329401", "bodyText": "oh, I didn't know that keyedInputRecordRDD will be cached. then it makes sense. rgdn , unpersisting, we could have fixed it to unpersist keyedInputRecordRDD. so, that should not have been an issue.", "author": "nsivabalan", "createdAt": "2020-05-13T10:17:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDEzODk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0NDk4OQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424144989", "bodyText": "if we have had the other fetcher, we could avoid this mapToPair right. Can you help me understand why you have removed it ? I actually started with this solution and decided to create a separate RecordFetcher thinking we could avoid an extra map call.", "author": "nsivabalan", "createdAt": "2020-05-13T02:56:07Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param inputRecordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+\n+    JavaPairRDD<String, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> allRecordLocationsInTable = fetchAllRecordLocations(jsc, hoodieTable,\n+        config.getGlobalSimpleIndexParallelism());\n+    return getTaggedRecords(keyedInputRecordRDD, allRecordLocationsInTable);\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchAllRecordLocations(JavaSparkContext jsc,\n+                                                                                 HoodieTable hoodieTable,\n+                                                                                 int parallelism) {\n+    List<Pair<String, HoodieBaseFile>> latestBaseFiles = getAllBaseFilesInTable(jsc, hoodieTable);\n+    return fetchRecordLocations(jsc, hoodieTable, parallelism, latestBaseFiles);\n+  }\n+\n+  /**\n+   * Load all files for all partitions as <Partition, filename> pair RDD.\n+   */\n+  protected List<Pair<String, HoodieBaseFile>> getAllBaseFilesInTable(final JavaSparkContext jsc, final HoodieTable hoodieTable) {\n+    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();\n+    try {\n+      List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(), config.shouldAssumeDatePartitioning());\n+      // Obtain the latest data files from all the partitions.\n+      return getLatestBaseFilesForAllPartitions(allPartitionPaths, jsc, hoodieTable);\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to load all partitions\", e);\n+    }\n+  }\n+\n+  /**\n+   * Tag records with right {@link HoodieRecordLocation}.\n+   *\n+   * @param incomingRecords incoming {@link HoodieRecord}s\n+   * @param existingRecords existing records with {@link HoodieRecordLocation}s\n+   * @return {@link JavaRDD} of {@link HoodieRecord}s with tagged {@link HoodieRecordLocation}s\n+   */\n+  private JavaRDD<HoodieRecord<T>> getTaggedRecords(JavaPairRDD<String, HoodieRecord<T>> incomingRecords, JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords) {\n+    JavaPairRDD<String, Pair<String, HoodieRecordLocation>> existingRecordByRecordKey = existingRecords\n+        .mapToPair(entry -> new Tuple2<>(entry._1.getRecordKey(), Pair.of(entry._1.getPartitionPath(), entry._2)));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE1ODczMg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424158732", "bodyText": "My thinking was a separate class with duplicated code was not necessary. We can keep the fetching logic consistent and push the differences to this method.. the extra mapToPair() is very cheap since it not shuffling anything..", "author": "vinothchandar", "createdAt": "2020-05-13T03:56:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0NDk4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDMyODc4MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424328780", "bodyText": "gotcha \ud83d\udc4d", "author": "nsivabalan", "createdAt": "2020-05-13T10:16:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0NDk4OQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxODM1Mg==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424618352", "bodyText": "rename to HoodieKeyLocationFetchHandle\nand let's move this to the io module with the other read handles", "author": "vinothchandar", "createdAt": "2020-05-13T17:43:32Z", "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieKeyLocationFetcherHandle.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieReadHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.Iterator;\n+\n+import scala.Tuple2;\n+\n+/**\n+ * {@link HoodieRecordLocation} fetcher for all records for {@link HoodieBaseFile} of interest.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieKeyLocationFetcherHandle<T extends HoodieRecordPayload> extends HoodieReadHandle<T> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMTYwNA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424621604", "bodyText": "I think we should remove this class and just reuse HoodieKey, which is what we actually read out of the parquet file.. the baseInstantTime and fileId are actually passed in by the caller.. there is no need to actually do this at this level.. We can just reuse the values we pass in at the read handle calling code", "author": "vinothchandar", "createdAt": "2020-05-13T17:48:57Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +121,111 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo}\n+   */\n+  public static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                            String baseInstantTime,\n+                                                                            String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, Collections.EMPTY_LIST);\n+  }\n+\n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}. Filter for candidates if set.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo} matching candidateRecordKeys\n+   */\n+  static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                     String baseInstantTime,\n+                                                                     String fileId, List<String> candidatesToFilter) {\n+    List<ParquetRowInfo> parquetRowInfos = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();\n+      Object obj = reader.read();\n+      while (obj != null) {\n+        if (obj instanceof GenericRecord) {\n+          String recordKey = ((GenericRecord) obj).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+          String partitionPath = ((GenericRecord) obj).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();\n+          if (candidatesToFilter.isEmpty() || candidatesToFilter.contains(recordKey)) {\n+            parquetRowInfos.add(new ParquetRowInfo(recordKey, partitionPath, baseInstantTime, fileId));\n+          }\n+          obj = reader.read();\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read from Parquet file \" + filePath, e);\n+    }\n+    return parquetRowInfos;\n+  }\n+\n+  /**\n+   * Class to hold info for a row in Parquet like record key, partition path, base instance time and fileId.\n+   */\n+  public static class ParquetRowInfo {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMjE5MA==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424622190", "bodyText": "we don't intend on doing an filtering right.. i.e this is never called? if so, can we just have the method above?", "author": "vinothchandar", "createdAt": "2020-05-13T17:49:54Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +121,111 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo}\n+   */\n+  public static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                            String baseInstantTime,\n+                                                                            String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, Collections.EMPTY_LIST);\n+  }\n+\n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}. Filter for candidates if set.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo} matching candidateRecordKeys\n+   */\n+  static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                     String baseInstantTime,\n+                                                                     String fileId, List<String> candidatesToFilter) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMjQxOQ==", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424622419", "bodyText": "this test also needs to move", "author": "vinothchandar", "createdAt": "2020-05-13T17:50:15Z", "path": "hudi-client/src/test/java/org/apache/hudi/index/simple/TestHoodieKeyLocationFetcherHandle.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.common.HoodieTestDataGenerator.AVRO_SCHEMA_WITH_METADATA_FIELDS;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+/**\n+ * Tests {@link HoodieKeyLocationFetcherHandle}.\n+ */\n+public class TestHoodieKeyLocationFetcherHandle extends HoodieClientTestHarness {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "62f8b4c6193b06c7df32381c00082e2f0762a74e", "url": "https://github.com/apache/hudi/commit/62f8b4c6193b06c7df32381c00082e2f0762a74e", "message": "[HUDI-407] Adding Simple Index to Hoodie. This index finds the location by joining incoming records with records from base files.", "committedDate": "2020-05-16T12:37:34Z", "type": "commit"}, {"oid": "62f8b4c6193b06c7df32381c00082e2f0762a74e", "url": "https://github.com/apache/hudi/commit/62f8b4c6193b06c7df32381c00082e2f0762a74e", "message": "[HUDI-407] Adding Simple Index to Hoodie. This index finds the location by joining incoming records with records from base files.", "committedDate": "2020-05-16T12:37:34Z", "type": "forcePushed"}]}