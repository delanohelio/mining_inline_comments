{"pr_number": 2264, "pr_title": "[HUDI-1406] Add date partition based source input selector for DeltaStreamer", "pr_createdAt": "2020-11-19T21:31:23Z", "pr_url": "https://github.com/apache/hudi/pull/2264", "timeline": [{"oid": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "url": "https://github.com/apache/hudi/commit/12cf95c6446609db5a1838f4fa80b641b3d9a569", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data.", "committedDate": "2020-11-20T00:05:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkwNDQxMQ==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528904411", "bodyText": "Can we just use log4j?  I think that's what we use elsewhere directly", "author": "vinothchandar", "createdAt": "2020-11-23T18:15:50Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMDUwNw==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528910507", "bodyText": ".source.dfs.datepartitioned.selector.depth", "author": "vinothchandar", "createdAt": "2020-11-23T18:26:39Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMDgyNw==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528910827", "bodyText": ".source.dfs.datepartitioned.selector.lookback.days", "author": "vinothchandar", "createdAt": "2020-11-23T18:27:11Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTAyMQ==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911021", "bodyText": ".source.dfs.datepartitioned.selector.currentdate", "author": "vinothchandar", "createdAt": "2020-11-23T18:27:32Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTg4OQ==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911889", "bodyText": "are you assuming a certain format for the current date to be specified in? would be good to doc/comment that. better have it in the property name", "author": "vinothchandar", "createdAt": "2020-11-23T18:29:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTAyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTE4MA==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911180", "bodyText": ".source.dfs.datepartitioned.selector.listparallelism", "author": "vinothchandar", "createdAt": "2020-11-23T18:27:53Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxMTQ5Mw==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528911493", "bodyText": "can we place the defaults adjacent to the property. so its easier to read.", "author": "vinothchandar", "createdAt": "2020-11-23T18:28:27Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNDQ3Ng==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528914476", "bodyText": "move to previous line?", "author": "vinothchandar", "createdAt": "2020-11-23T18:33:57Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNTQ3NA==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528915474", "bodyText": "can we keep these in a single line?", "author": "vinothchandar", "createdAt": "2020-11-23T18:35:41Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {\n+    List<String> partitionPaths = new ArrayList<>();\n+    // get all partition paths before date partition level\n+    partitionPaths.add(rootPath);\n+    if (datePartitionDepth <= 0) {\n+      return partitionPaths;\n+    }\n+    SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+    for (int i = 0; i < datePartitionDepth; i++) {\n+      partitionPaths =\n+          context.flatMap(\n+              partitionPaths,\n+              path -> {\n+                Path subDir = new Path(path);\n+                FileSystem fileSystem = subDir.getFileSystem(serializedConf.get());\n+                // skip files/dirs whose names start with (_, ., etc)\n+                FileStatus[] statuses =", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNTk5MA==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528915990", "bodyText": "why collect this and then prune?  can't we prune also in parallel?", "author": "vinothchandar", "createdAt": "2020-11-23T18:36:37Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();\n+      for (FileStatus f : eligibleFiles) {\n+        if (currentBytes + f.getLen() >= sourceLimit) {\n+          // we have enough data, we are done\n+          break;\n+        }\n+\n+        maxModificationTime = f.getModificationTime();\n+        currentBytes += f.getLen();\n+        filteredFiles.add(f);\n+      }\n+\n+      // no data to read\n+      if (filteredFiles.isEmpty()) {\n+        return new ImmutablePair<>(\n+            Option.empty(), lastCheckpointStr.orElseGet(() -> String.valueOf(Long.MIN_VALUE)));\n+      }\n+\n+      // read the files out.\n+      String pathStr =\n+          filteredFiles.stream().map(f -> f.getPath().toString()).collect(Collectors.joining(\",\"));\n+\n+      return new ImmutablePair<>(Option.ofNullable(pathStr), String.valueOf(maxModificationTime));\n+    } catch (IOException ioe) {\n+      throw new HoodieIOException(\n+          \"Unable to read from source from checkpoint: \" + lastCheckpointStr, ioe);\n+    }\n+  }\n+\n+  /**\n+   * Prunes date level partitions to last few days configured by 'NUM_PREV_DAYS_TO_LIST' from\n+   * 'CURRENT_DATE'. Parallelizes listing by leveraging HoodieSparkEngineContext's methods.\n+   */\n+  public List<String> pruneDatePartitionPaths(\n+      HoodieSparkEngineContext context, FileSystem fs, String rootPath) {\n+    List<String> partitionPaths = new ArrayList<>();\n+    // get all partition paths before date partition level\n+    partitionPaths.add(rootPath);\n+    if (datePartitionDepth <= 0) {\n+      return partitionPaths;\n+    }\n+    SerializableConfiguration serializedConf = new SerializableConfiguration(fs.getConf());\n+    for (int i = 0; i < datePartitionDepth; i++) {\n+      partitionPaths =\n+          context.flatMap(\n+              partitionPaths,\n+              path -> {\n+                Path subDir = new Path(path);\n+                FileSystem fileSystem = subDir.getFileSystem(serializedConf.get());\n+                // skip files/dirs whose names start with (_, ., etc)\n+                FileStatus[] statuses =\n+                    fileSystem.listStatus(\n+                        subDir,\n+                        file ->\n+                            IGNORE_FILEPREFIX_LIST.stream()\n+                                .noneMatch(pfx -> file.getName().startsWith(pfx)));\n+                List<String> res = new ArrayList<>();\n+                for (FileStatus status : statuses) {\n+                  res.add(status.getPath().toString());\n+                }\n+                return res.stream();\n+              },\n+              partitionsListParallelism);\n+    }\n+\n+    // Prune date partitions to last few days", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNjk1OA==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528916958", "bodyText": "Once again, this sorting can happen in parallel right? and just collect it finally", "author": "vinothchandar", "createdAt": "2020-11-23T18:38:28Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkxNzk2OQ==", "url": "https://github.com/apache/hudi/pull/2264#discussion_r528917969", "bodyText": "can we do this also in spark?", "author": "vinothchandar", "createdAt": "2020-11-23T18:40:16Z", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/DatePartitionPathSelector.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities.sources.helpers;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.config.SerializableConfiguration;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ValidationUtils;\n+import org.apache.hudi.common.util.collection.ImmutablePair;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_DATE_PARTITION_DEPTH;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_NUM_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.DEFAULT_PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.NUM_PREV_DAYS_TO_LIST;\n+import static org.apache.hudi.utilities.sources.helpers.DatePartitionPathSelector.Config.PARTITIONS_LIST_PARALLELISM;\n+import static org.apache.hudi.utilities.sources.helpers.DFSPathSelector.Config.ROOT_INPUT_PATH_PROP;\n+\n+/**\n+ * Custom dfs path selector used to list just the last few days provided there is a date based\n+ * partition.\n+ *\n+ * <p>This is useful for workloads where there are multiple partition fields and only recent\n+ * partitions are affected by new writes. Especially if the data sits in S3, listing all historical\n+ * data can be time expensive and unnecessary for the above type of workload.\n+ *\n+ * <p>The date based partition is expected to be of the format '<date string>=yyyy-mm-dd' or\n+ * 'yyyy-mm-dd'. The date partition can be at any level. For ex. the partition path can be of the\n+ * form `<basepath>/<partition-field1>/<date-based-partition>/<partition-field3>/` or\n+ * `<basepath>/<<date-based-partition>/`\n+ */\n+public class DatePartitionPathSelector extends DFSPathSelector {\n+  private static volatile Logger log = LoggerFactory.getLogger(DatePartitionPathSelector.class);\n+  private final int datePartitionDepth;\n+  private final int numPrevDaysToList;\n+  private final LocalDate fromDate;\n+  private final LocalDate currentDate;\n+  private final int partitionsListParallelism;\n+\n+  /** Configs supported. */\n+  public static class Config {\n+    public static final String DATE_PARTITION_DEPTH =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.date_partition_depth\";\n+    public static final String NUM_PREV_DAYS_TO_LIST =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.num_prev_days_to_list\";\n+    public static final String CURRENT_DATE =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.current_date\";\n+    public static final String PARTITIONS_LIST_PARALLELISM =\n+        \"hoodie.deltastreamer.source.input.date_partition.selector.partitions.list.parallelism\";\n+    public static final int DEFAULT_DATE_PARTITION_DEPTH = 0; // Implies no (date) partition\n+    public static final int DEFAULT_NUM_DAYS_TO_LIST = 2;\n+    public static final int DEFAULT_PARTITIONS_LIST_PARALLELISM = 20;\n+  }\n+\n+  public DatePartitionPathSelector(TypedProperties props, Configuration hadoopConf) {\n+    super(props, hadoopConf);\n+    /*\n+     * datePartitionDepth = 0 is same as basepath and there is no partition. In which case\n+     * this path selector would be a no-op and lists all paths under the table basepath.\n+     */\n+    datePartitionDepth = props.getInteger(DATE_PARTITION_DEPTH, DEFAULT_DATE_PARTITION_DEPTH);\n+    // If not specified the current date is assumed by default.\n+    currentDate = LocalDate.parse(props.getString(Config.CURRENT_DATE, LocalDate.now().toString()));\n+    numPrevDaysToList = props.getInteger(NUM_PREV_DAYS_TO_LIST, DEFAULT_NUM_DAYS_TO_LIST);\n+    fromDate = currentDate.minusDays(numPrevDaysToList);\n+    partitionsListParallelism =\n+        props.getInteger(PARTITIONS_LIST_PARALLELISM, DEFAULT_PARTITIONS_LIST_PARALLELISM);\n+  }\n+\n+  @Override\n+  public Pair<Option<String>, String> getNextFilePathsAndMaxModificationTime(\n+      JavaSparkContext sparkContext, Option<String> lastCheckpointStr, long sourceLimit) {\n+    try {\n+      // obtain all eligible files under root folder.\n+      log.info(\n+          \"Root path => \"\n+              + props.getString(ROOT_INPUT_PATH_PROP)\n+              + \" source limit => \"\n+              + sourceLimit\n+              + \" depth of day partition => \"\n+              + datePartitionDepth\n+              + \" num prev days to list => \"\n+              + numPrevDaysToList\n+              + \" from current date => \"\n+              + currentDate);\n+      long lastCheckpointTime = lastCheckpointStr.map(Long::parseLong).orElse(Long.MIN_VALUE);\n+      HoodieSparkEngineContext context = new HoodieSparkEngineContext(sparkContext);\n+      List<String> prunedPaths =\n+          pruneDatePartitionPaths(context, fs, props.getString(ROOT_INPUT_PATH_PROP));\n+      List<FileStatus> eligibleFiles = new ArrayList<>();\n+      for (String path : prunedPaths) {\n+        eligibleFiles.addAll(listEligibleFiles(fs, new Path(path), lastCheckpointTime));\n+      }\n+      // sort them by modification time.\n+      eligibleFiles.sort(Comparator.comparingLong(FileStatus::getModificationTime));\n+      // Filter based on checkpoint & input size, if needed\n+      long currentBytes = 0;\n+      long maxModificationTime = Long.MIN_VALUE;\n+      List<FileStatus> filteredFiles = new ArrayList<>();", "originalCommit": "12cf95c6446609db5a1838f4fa80b641b3d9a569", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cfc9d32078d819b0650e9addecbbb79309bf70d5", "url": "https://github.com/apache/hudi/commit/cfc9d32078d819b0650e9addecbbb79309bf70d5", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data.", "committedDate": "2020-12-17T07:03:05Z", "type": "commit"}, {"oid": "cfc9d32078d819b0650e9addecbbb79309bf70d5", "url": "https://github.com/apache/hudi/commit/cfc9d32078d819b0650e9addecbbb79309bf70d5", "message": "[HUDI-1406] Add date partition based source input selector for Delta streamer\n\n- Adds ability to list only recent date based partitions from source data.", "committedDate": "2020-12-17T07:03:05Z", "type": "forcePushed"}]}