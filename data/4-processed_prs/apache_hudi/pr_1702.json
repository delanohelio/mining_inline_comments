{"pr_number": 1702, "pr_title": "[HUDI-426] Bootstrap datasource integration", "pr_createdAt": "2020-06-03T23:57:53Z", "pr_url": "https://github.com/apache/hudi/pull/1702", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk2MjE5Ng==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r434962196", "bodyText": "@umehrot2 I think we are on the same page \ud83d\ude04\nas long as we load the data as RDD[Row], then it's very flexible. We can union different formats together", "author": "garyli1019", "createdAt": "2020-06-04T02:49:03Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRelation.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+  * This is Spark relation that can be used for querying metadata/fully bootstrapped query hudi tables, as well as\n+  * non-bootstrapped tables. It implements PrunedFilteredScan interface in order to support column pruning and filter\n+  * push-down. For metadata bootstrapped files, if we query columns from both metadata and actual data then it will\n+  * perform a merge of both to return the result.\n+  *\n+  * Caveat: Filter push-down does not work when querying both metadata and actual data columns over metadata\n+  * bootstrapped files, because then the metadata file and data file can return different number of rows causing errors\n+  * merging.\n+  *\n+  * @param _sqlContext Spark SQL Context\n+  * @param userSchema User specified schema in the datasource query\n+  * @param globPaths Globbed paths obtained from the user provided path for querying\n+  * @param metaClient Hudi table meta client\n+  * @param optParams DataSource options passed by the user\n+  */\n+class HudiBootstrapRelation(@transient val _sqlContext: SQLContext,\n+                            val userSchema: StructType,\n+                            val globPaths: Seq[Path],\n+                            val metaClient: HoodieTableMetaClient,\n+                            val optParams: Map[String, String]) extends BaseRelation\n+  with PrunedFilteredScan with Logging {\n+\n+  val skeletonSchema: StructType = HudiSparkUtils.getHudiMetadataSchema\n+  var dataSchema: StructType = _\n+  var fullSchema: StructType = _\n+\n+  val fileIndex: HudiBootstrapFileIndex = buildFileIndex()\n+\n+  override def sqlContext: SQLContext = _sqlContext\n+\n+  override val needConversion: Boolean = false\n+\n+  override def schema: StructType = inferFullSchema()\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    logInfo(\"Starting scan..\")\n+\n+    // Compute splits\n+    val bootstrapSplits = fileIndex.files.map(hoodieBaseFile => {\n+      var skeletonFile: Option[PartitionedFile] = Option.empty\n+      var dataFile: PartitionedFile = null\n+\n+      if (hoodieBaseFile.getExternalBaseFile.isPresent) {\n+        skeletonFile = Option(PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen))\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getExternalBaseFile.get().getPath, 0,\n+          hoodieBaseFile.getExternalBaseFile.get().getFileLen)\n+      } else {\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen)\n+      }\n+      HudiBootstrapSplit(dataFile, skeletonFile)\n+    })\n+    val tableState = HudiBootstrapTableState(bootstrapSplits)\n+\n+    // Get required schemas for column pruning\n+    var requiredDataSchema = StructType(Seq())\n+    var requiredSkeletonSchema = StructType(Seq())\n+    requiredColumns.foreach(col => {\n+      var field = dataSchema.find(_.name == col)\n+      if (field.isDefined) {\n+        requiredDataSchema = requiredDataSchema.add(field.get)\n+      } else {\n+        field = skeletonSchema.find(_.name == col)\n+        requiredSkeletonSchema = requiredSkeletonSchema.add(field.get)\n+      }\n+    })\n+\n+    // Prepare readers for reading data file and skeleton files\n+    val dataReadFunction = new ParquetFileFormat()\n+        .buildReaderWithPartitionValues(\n+          sparkSession = _sqlContext.sparkSession,\n+          dataSchema = dataSchema,\n+          partitionSchema = StructType(Seq.empty),\n+          requiredSchema = requiredDataSchema,\n+          filters = if (requiredSkeletonSchema.isEmpty) filters else Seq() ,\n+          options = Map.empty,\n+          hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+        )\n+\n+    val skeletonReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = skeletonSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = requiredSkeletonSchema,\n+        filters = if (requiredDataSchema.isEmpty) filters else Seq(),\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+      )\n+\n+    val regularReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = fullSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = StructType(requiredSkeletonSchema.fields ++ requiredDataSchema.fields),\n+        filters = filters,\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf())\n+\n+    val rdd = new HudiBootstrapRDD(_sqlContext.sparkSession, dataReadFunction, skeletonReadFunction,\n+      regularReadFunction, requiredDataSchema, requiredSkeletonSchema, requiredColumns, tableState)\n+    rdd.asInstanceOf[RDD[Row]]", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439878477", "bodyText": "If we use vectorized reader this way, does it still have a huge performance boost?\nFrom my understanding, the regular reader iterator will read the whole row as UnsafeRow then do the column pruning before load it into memory. The vectorized reader will do the column pruning and loading data in one step. So theoretically vectorized reader would still be faster even we read it as InternalRow\nThe description I found from Spark code\nThis class can either return InternalRows or ColumnarBatches. With whole stage codegen enabled, this class returns ColumnarBatches which offers significant performance gains.", "author": "garyli1019", "createdAt": "2020-06-14T23:28:57Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {\n+    val skeletonArr  = skeletonRow.copy().toSeq(skeletonSchema)\n+    val dataArr = dataRow.copy().toSeq(dataSchema)\n+    // We need to return it in the order requested\n+    val mergedArr = requiredColumns.map(col => {\n+      if (skeletonSchema.fieldNames.contains(col)) {\n+        val idx = skeletonSchema.fieldIndex(col)\n+        skeletonArr(idx)\n+      } else {\n+        val idx = dataSchema.fieldIndex(col)\n+        dataArr(idx)\n+      }\n+    })\n+\n+    logDebug(\"Merged data and skeleton values => \" + mergedArr.mkString(\",\"))\n+    val mergedRow = InternalRow.fromSeq(mergedArr)\n+    mergedRow\n+  }\n+\n+  def read(partitionedFile: PartitionedFile, readFileFunction: PartitionedFile => Iterator[Any])\n+    : Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+\n+    import scala.collection.JavaConverters._\n+\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3ODcwMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447578702", "bodyText": "As per my understanding column pruning is independent of vectorized reader. vectorized reader will basically read a batch of rows into a columnar batch and that is what will happen here as well. However, the only difference is that we are not passing it as a columnar batch all the way down as a batch. However, even if I use regular parquet reader at some point it must be converting the columnar batch to rows I guess. Right now I am not fully sure whether I am able to 100% use all the benefits of vectorized reading with this method, but atleast it reads the data as a batch.", "author": "umehrot2", "createdAt": "2020-06-30T10:25:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3ODkzMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447578932", "bodyText": "Will do some more research on this.", "author": "umehrot2", "createdAt": "2020-06-30T10:25:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNjY0Nw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449936647", "bodyText": "We probably have to use rowIterator since we will need to merge on row level anyway, same for MOR table too. Agree that Spark will convert ColumnBatch to row at some point and it is very difficult to locate.", "author": "garyli1019", "createdAt": "2020-07-06T00:28:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzNDE0OA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455034148", "bodyText": "For MOR table, I have some ideas to speed things up by pre-reading the delete/rollback blocks and simply \"skip\" rows as long as OverwritewithLatestPayload is used..  If the user does specify a merge function, then its hard to get away from.. we can take this discussion in a separate forum.", "author": "vinothchandar", "createdAt": "2020-07-15T13:04:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTI1MA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439879250", "bodyText": "Are we changing the metadata columns?", "author": "garyli1019", "createdAt": "2020-06-14T23:38:24Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -71,13 +78,16 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     optParams.getOrElse(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, lastInstant.getTimestamp))\n     .getInstants.iterator().toList\n \n-  // use schema from latest metadata, if not present, read schema from the data file\n-  private val latestSchema = {\n-    val schemaUtil = new TableSchemaResolver(metaClient)\n-    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields);\n-    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  // use schema from a file produced in the latest instant\n+  val latestSchema: StructType = {\n+    log.info(\"Inferring schema..\")\n+    val schemaResolver = new TableSchemaResolver(metaClient)\n+    val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields\n+    val dataSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+    StructType(skeletonSchema.fields ++ dataSchema.fields)", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2MDAxNg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447560016", "bodyText": "Not really. The reason for doing it this way is to intentionally only read the user data schema and then append the metadata/skeleton schema to it. This avoids us to have unnecessary checks here, because if we read the whole schema then there will be differences. For regular hudi files, the schema would have both skeleton + user data schema whereas for bootstrapped files the schema would only have user data schema read from the source file. So to keep things simple I did it this way.", "author": "umehrot2", "createdAt": "2020-06-30T09:53:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTUwMA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439879500", "bodyText": "Should we avoid var here? seem like avoidable.", "author": "garyli1019", "createdAt": "2020-06-14T23:41:33Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -92,36 +102,69 @@ class IncrementalRelation(val sqlContext: SQLContext,\n   override def schema: StructType = latestSchema\n \n   override def buildScan(): RDD[Row] = {\n-    val fileIdToFullPath = mutable.HashMap[String, String]()\n+    val regularFileIdToFullPath = mutable.HashMap[String, String]()\n+    var metaBootstrapFileIdToFullPath = mutable.HashMap[String, String]()\n+\n     for (commit <- commitsToReturn) {\n       val metadata: HoodieCommitMetadata = HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit)\n         .get, classOf[HoodieCommitMetadata])\n-      fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+\n+      if (HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS == commit.getTimestamp) {\n+        metaBootstrapFileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+      } else {\n+        regularFileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+      }\n+    }\n+\n+    if (metaBootstrapFileIdToFullPath.nonEmpty) {\n+      // filer out meta bootstrap files that have had more commits since metadata bootstrap\n+      metaBootstrapFileIdToFullPath = metaBootstrapFileIdToFullPath\n+        .filterNot(fileIdFullPath => regularFileIdToFullPath.contains(fileIdFullPath._1))\n     }\n+\n     val pathGlobPattern = optParams.getOrElse(\n       DataSourceReadOptions.INCR_PATH_GLOB_OPT_KEY,\n       DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)\n-    val filteredFullPath = if(!pathGlobPattern.equals(DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)) {\n-      val globMatcher = new GlobPattern(\"*\" + pathGlobPattern)\n-      fileIdToFullPath.filter(p => globMatcher.matches(p._2))\n-    } else {\n-      fileIdToFullPath\n+    val (filteredRegularFullPaths, filteredMetaBootstrapFullPaths) = {\n+      if(!pathGlobPattern.equals(DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)) {\n+        val globMatcher = new GlobPattern(\"*\" + pathGlobPattern)\n+        (regularFileIdToFullPath.filter(p => globMatcher.matches(p._2)).values,\n+          metaBootstrapFileIdToFullPath.filter(p => globMatcher.matches(p._2)).values)\n+      } else {\n+        (regularFileIdToFullPath.values, metaBootstrapFileIdToFullPath.values)\n+      }\n     }\n     // unset the path filter, otherwise if end_instant_time is not the latest instant, path filter set for RO view\n     // will filter out all the files incorrectly.\n     sqlContext.sparkContext.hadoopConfiguration.unset(\"mapreduce.input.pathFilter.class\")\n     val sOpts = optParams.filter(p => !p._1.equalsIgnoreCase(\"path\"))\n-    if (filteredFullPath.isEmpty) {\n+    if (filteredRegularFullPaths.isEmpty && filteredMetaBootstrapFullPaths.isEmpty) {\n       sqlContext.sparkContext.emptyRDD[Row]\n     } else {\n       log.info(\"Additional Filters to be applied to incremental source are :\" + filters)\n-      filters.foldLeft(sqlContext.read.options(sOpts)\n-        .schema(latestSchema)\n-        .parquet(filteredFullPath.values.toList: _*)\n-        .filter(String.format(\"%s >= '%s'\", HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.head.getTimestamp))\n-        .filter(String.format(\"%s <= '%s'\",\n-          HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.last.getTimestamp)))((e, f) => e.filter(f))\n-        .toDF().rdd\n+\n+      var df: DataFrame = sqlContext.createDataFrame(sqlContext.sparkContext.emptyRDD[Row], latestSchema)", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2MzAxNw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447563017", "bodyText": "We do need re-assignment to df. First we get bootstrapped files data in df and then union to add regular parquet data to it.", "author": "umehrot2", "createdAt": "2020-06-30T09:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTA1MA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439881050", "bodyText": "IIUC, the skeleton file will only exist if there is a data file. Did I miss anything that there is a case with a stand-alone skeleton file?", "author": "garyli1019", "createdAt": "2020-06-14T23:57:53Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NjI0NA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447566244", "bodyText": "That understanding is correct. But here data schema being empty does not mean that there is not data file. It means that in the request schema they have not requested any of the data schema fields. So this is kind of an optimization where depending on what schema the user has requested, if the request fields all only belong to either skeleton file or data file we only read from that file and avoid the expensive merge operation.", "author": "umehrot2", "createdAt": "2020-06-30T10:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTQ1Ng==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439881456", "bodyText": "Is this the regular COW table data files?", "author": "garyli1019", "createdAt": "2020-06-15T00:01:44Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzMxNg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447567316", "bodyText": "No, this is for the bootstrap file case. It is the case where user requested schema only has user data fields. So we can basically just read from the source data file and avoid the merging.", "author": "umehrot2", "createdAt": "2020-06-30T10:05:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2Nzg1Nw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r447567857", "bodyText": "https://github.com/apache/hudi/pull/1702/files/2af69135eea3d773561cebc017b36a8794e71ca4#diff-809772c649e85ffb321055d9871e37e0R70 is the regular COW case.", "author": "umehrot2", "createdAt": "2020-06-30T10:06:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzMzIyNQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449933225", "bodyText": "I think this approach is better than extending the FileFormat. Ultimately, we can have a HudiRDD to handle all the file loading and merging(bootstrap files, parquet, orc, logs). Union will trigger shuffle and grouping files on the driver then use different FileFormat to read is not as clean as this approach.\nI will add the MOR stuff on top of this PR after this merged.", "author": "garyli1019", "createdAt": "2020-07-05T23:56:24Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyOTU4MQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455029581", "bodyText": "+1 here we wrap the FileFormat. Seems much simpler.. Thanks @umehrot2 for this.. educative!", "author": "vinothchandar", "createdAt": "2020-07-15T12:56:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzMzIyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449934912", "bodyText": "Are these additional paths on top of the path? Any example of the use cases?", "author": "garyli1019", "createdAt": "2020-07-06T00:12:41Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -54,29 +58,54 @@ class DefaultSource extends RelationProvider\n     val parameters = Map(QUERY_TYPE_OPT_KEY -> DEFAULT_QUERY_TYPE_OPT_VAL) ++ translateViewTypesToQueryTypes(optParams)\n \n     val path = parameters.get(\"path\")\n-    if (path.isEmpty) {\n-      throw new HoodieException(\"'path' must be specified.\")\n-    }\n \n     if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) {\n-      // this is just effectively RO view only, where `path` can contain a mix of\n-      // non-hoodie/hoodie path files. set the path filter up\n-      sqlContext.sparkContext.hadoopConfiguration.setClass(\n-        \"mapreduce.input.pathFilter.class\",\n-        classOf[HoodieROTablePathFilter],\n-        classOf[org.apache.hadoop.fs.PathFilter])\n-\n-      log.info(\"Constructing hoodie (as parquet) data source with options :\" + parameters)\n-      log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" +\n-        \"Please query the Hive table registered using Spark SQL.\")\n-      // simply return as a regular parquet relation\n-      DataSource.apply(\n-        sparkSession = sqlContext.sparkSession,\n-        userSpecifiedSchema = Option(schema),\n-        className = \"parquet\",\n-        options = parameters)\n-        .resolveRelation()\n+      val readPathsStr = parameters.get(DataSourceReadOptions.READ_PATHS_OPT_KEY)", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MzYwOA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465353608", "bodyText": "These additional paths are being used in the Incremental query code to make it work for bootstrapped tables. I need to pass a list of bootstrapped files to read, and that is why had to add support for reading from multiple paths. spark.read.parquet already has that kind of support and is being used in incremental relation already to read a list of files.", "author": "umehrot2", "createdAt": "2020-08-04T21:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMjMxMw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466222313", "bodyText": "the bootstrap.base.path is now in hoodie.properties. Should can we make this transparent for the user?", "author": "vinothchandar", "createdAt": "2020-08-06T08:06:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMyMDA5Ng==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466320096", "bodyText": "Well right now I added it only for our internal logic to support incremental query on bootstrapped tables.\nWould you want customers to use this otherwise as well, to be able to provide multiple read paths for querying ? Is that the ask here ?", "author": "umehrot2", "createdAt": "2020-08-06T10:33:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3MTEzOQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r444571139", "bodyText": "is it possible to keep this to hudi-spark ?", "author": "vinothchandar", "createdAt": "2020-06-23T23:58:26Z", "path": "hudi-client/pom.xml", "diffHunk": "@@ -101,6 +101,11 @@\n       <groupId>org.apache.spark</groupId>\n       <artifactId>spark-sql_${scala.binary.version}</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-avro_${scala.binary.version}</artifactId>", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MjA0Ng==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465352046", "bodyText": "I have explained the reason here #1876 Let me know your thoughts.", "author": "umehrot2", "createdAt": "2020-08-04T21:54:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3MTEzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MzIxMA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465353210", "bodyText": "That is why I had to introduce spark-avro in hudi-client. If you agree with the above suggestion, and do not want spark-avro to be added to hudi-client then I would suggest moving this class to hudi-spark.\n\nIf that class is support to work with Spark Datasource only, yes, lets move it to hudi-spark", "author": "vinothchandar", "createdAt": "2020-08-04T21:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3MTEzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM2NzYwOQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465367609", "bodyText": "This class is used in hudi-client inside BootstrapCommitActionExecutor. So we cannot move it to hudi-spark.", "author": "umehrot2", "createdAt": "2020-08-04T22:33:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3MTEzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNDAyMA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455024020", "bodyText": "some of this stuff is good to do, even in the regular path? in place of the current path filter approach?", "author": "vinothchandar", "createdAt": "2020-07-15T12:47:20Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiSparkUtils.scala", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hudi.common.model.HoodieRecord\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.{FileStatusCache, InMemoryFileIndex}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType}\n+import scala.collection.JavaConverters._\n+\n+\n+object HudiSparkUtils {\n+\n+  def getHudiMetadataSchema: StructType = {\n+    StructType(HoodieRecord.HOODIE_META_COLUMNS.asScala.map(col => {\n+        StructField(col, StringType, nullable = true)\n+    }))\n+  }\n+\n+  def checkAndGlobPathIfNecessary(paths: Seq[String], fs: FileSystem): Seq[Path] = {\n+    paths.flatMap(path => {\n+      val qualified = new Path(path).makeQualified(fs.getUri, fs.getWorkingDirectory)\n+      val globPaths = SparkHadoopUtil.get.globPathIfNecessary(fs, qualified)\n+      globPaths\n+    })\n+  }\n+\n+  def createInMemoryFileIndex(sparkSession: SparkSession, globbedPaths: Seq[Path]): InMemoryFileIndex = {", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM2Nzg5Mw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465367893", "bodyText": "The common useful utilities have been contributed as part of #1841", "author": "umehrot2", "createdAt": "2020-08-04T22:34:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNDAyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNTg0Nw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455025847", "bodyText": "IIUC this will leverage Spark's caching and filter on top of that based on our timeline", "author": "vinothchandar", "createdAt": "2020-07-15T12:50:23Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRelation.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+  * This is Spark relation that can be used for querying metadata/fully bootstrapped query hudi tables, as well as\n+  * non-bootstrapped tables. It implements PrunedFilteredScan interface in order to support column pruning and filter\n+  * push-down. For metadata bootstrapped files, if we query columns from both metadata and actual data then it will\n+  * perform a merge of both to return the result.\n+  *\n+  * Caveat: Filter push-down does not work when querying both metadata and actual data columns over metadata\n+  * bootstrapped files, because then the metadata file and data file can return different number of rows causing errors\n+  * merging.\n+  *\n+  * @param _sqlContext Spark SQL Context\n+  * @param userSchema User specified schema in the datasource query\n+  * @param globPaths Globbed paths obtained from the user provided path for querying\n+  * @param metaClient Hudi table meta client\n+  * @param optParams DataSource options passed by the user\n+  */\n+class HudiBootstrapRelation(@transient val _sqlContext: SQLContext,\n+                            val userSchema: StructType,\n+                            val globPaths: Seq[Path],\n+                            val metaClient: HoodieTableMetaClient,\n+                            val optParams: Map[String, String]) extends BaseRelation\n+  with PrunedFilteredScan with Logging {\n+\n+  val skeletonSchema: StructType = HudiSparkUtils.getHudiMetadataSchema\n+  var dataSchema: StructType = _\n+  var fullSchema: StructType = _\n+\n+  val fileIndex: HudiBootstrapFileIndex = buildFileIndex()\n+\n+  override def sqlContext: SQLContext = _sqlContext\n+\n+  override val needConversion: Boolean = false\n+\n+  override def schema: StructType = inferFullSchema()\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    logInfo(\"Starting scan..\")\n+\n+    // Compute splits\n+    val bootstrapSplits = fileIndex.files.map(hoodieBaseFile => {\n+      var skeletonFile: Option[PartitionedFile] = Option.empty\n+      var dataFile: PartitionedFile = null\n+\n+      if (hoodieBaseFile.getExternalBaseFile.isPresent) {\n+        skeletonFile = Option(PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen))\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getExternalBaseFile.get().getPath, 0,\n+          hoodieBaseFile.getExternalBaseFile.get().getFileLen)\n+      } else {\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen)\n+      }\n+      HudiBootstrapSplit(dataFile, skeletonFile)\n+    })\n+    val tableState = HudiBootstrapTableState(bootstrapSplits)\n+\n+    // Get required schemas for column pruning\n+    var requiredDataSchema = StructType(Seq())\n+    var requiredSkeletonSchema = StructType(Seq())\n+    requiredColumns.foreach(col => {\n+      var field = dataSchema.find(_.name == col)\n+      if (field.isDefined) {\n+        requiredDataSchema = requiredDataSchema.add(field.get)\n+      } else {\n+        field = skeletonSchema.find(_.name == col)\n+        requiredSkeletonSchema = requiredSkeletonSchema.add(field.get)\n+      }\n+    })\n+\n+    // Prepare readers for reading data file and skeleton files\n+    val dataReadFunction = new ParquetFileFormat()\n+        .buildReaderWithPartitionValues(\n+          sparkSession = _sqlContext.sparkSession,\n+          dataSchema = dataSchema,\n+          partitionSchema = StructType(Seq.empty),\n+          requiredSchema = requiredDataSchema,\n+          filters = if (requiredSkeletonSchema.isEmpty) filters else Seq() ,\n+          options = Map.empty,\n+          hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+        )\n+\n+    val skeletonReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = skeletonSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = requiredSkeletonSchema,\n+        filters = if (requiredDataSchema.isEmpty) filters else Seq(),\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+      )\n+\n+    val regularReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = fullSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = StructType(requiredSkeletonSchema.fields ++ requiredDataSchema.fields),\n+        filters = filters,\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf())\n+\n+    val rdd = new HudiBootstrapRDD(_sqlContext.sparkSession, dataReadFunction, skeletonReadFunction,\n+      regularReadFunction, requiredDataSchema, requiredSkeletonSchema, requiredColumns, tableState)\n+    rdd.asInstanceOf[RDD[Row]]\n+  }\n+\n+  def inferFullSchema(): StructType = {\n+    if (fullSchema == null) {\n+      logInfo(\"Inferring schema..\")\n+      val schemaResolver = new TableSchemaResolver(metaClient)\n+      val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields\n+      dataSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+      fullSchema = StructType(skeletonSchema.fields ++ dataSchema.fields)\n+    }\n+    fullSchema\n+  }\n+\n+  def buildFileIndex(): HudiBootstrapFileIndex = {\n+    logInfo(\"Building file index..\")\n+    val inMemoryFileIndex = HudiSparkUtils.createInMemoryFileIndex(_sqlContext.sparkSession, globPaths)\n+    val fileStatuses = inMemoryFileIndex.allFiles()", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1NDkzNA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r465354934", "bodyText": "This will basically leverage all of Spark's optimized listing logic as well as its file status cache. Then we use the list to build file system view and then pull the latest files.", "author": "umehrot2", "createdAt": "2020-08-04T22:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNTg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMTcyMQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455031721", "bodyText": "on avoiding the merge cost, my understanding is - its hard for this case where you need to actually merge these two values, re-order etc.", "author": "vinothchandar", "createdAt": "2020-07-15T12:59:59Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {", "originalCommit": "2af69135eea3d773561cebc017b36a8794e71ca4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "c8295f28784d0b5cd7882a8b767b15fc271421f7", "url": "https://github.com/apache/hudi/commit/c8295f28784d0b5cd7882a8b767b15fc271421f7", "message": "Bootstrap datasource integration", "committedDate": "2020-08-04T21:41:16Z", "type": "forcePushed"}, {"oid": "9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "url": "https://github.com/apache/hudi/commit/9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "message": "Bootstrap datasource integration", "committedDate": "2020-08-05T00:02:24Z", "type": "forcePushed"}, {"oid": "313385d95fa406a5adb7b198899abc14b572ee4a", "url": "https://github.com/apache/hudi/commit/313385d95fa406a5adb7b198899abc14b572ee4a", "message": "Bootstrap datasource integration", "committedDate": "2020-08-05T23:41:36Z", "type": "forcePushed"}, {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "url": "https://github.com/apache/hudi/commit/08e84812173d8126bc01c00b1c79ffaa5b80d623", "message": "Bootstrap datasource integration", "committedDate": "2020-08-06T02:30:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMzI1MQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466223251", "bodyText": "we can just check metaClient.getTableConfig() for the bootstrap base path?", "author": "vinothchandar", "createdAt": "2020-08-06T08:08:17Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -56,29 +58,56 @@ class DefaultSource extends RelationProvider\n     val parameters = Map(QUERY_TYPE_OPT_KEY -> DEFAULT_QUERY_TYPE_OPT_VAL) ++ translateViewTypesToQueryTypes(optParams)\n \n     val path = parameters.get(\"path\")\n-    if (path.isEmpty) {\n-      throw new HoodieException(\"'path' must be specified.\")\n-    }\n \n     if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) {\n-      // this is just effectively RO view only, where `path` can contain a mix of\n-      // non-hoodie/hoodie path files. set the path filter up\n-      sqlContext.sparkContext.hadoopConfiguration.setClass(\n-        \"mapreduce.input.pathFilter.class\",\n-        classOf[HoodieROTablePathFilter],\n-        classOf[org.apache.hadoop.fs.PathFilter])\n-\n-      log.info(\"Constructing hoodie (as parquet) data source with options :\" + parameters)\n-      log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" +\n-        \"Please query the Hive table registered using Spark SQL.\")\n-      // simply return as a regular parquet relation\n-      DataSource.apply(\n-        sparkSession = sqlContext.sparkSession,\n-        userSpecifiedSchema = Option(schema),\n-        className = \"parquet\",\n-        options = parameters)\n-        .resolveRelation()\n+      val readPathsStr = parameters.get(DataSourceReadOptions.READ_PATHS_OPT_KEY)\n+      if (path.isEmpty && readPathsStr.isEmpty) {\n+        throw new HoodieException(s\"'path' or '$READ_PATHS_OPT_KEY' or both must be specified.\")\n+      }\n+\n+      val readPaths = readPathsStr.map(p => p.split(\",\").toSeq).getOrElse(Seq())\n+      val allPaths = path.map(p => Seq(p)).getOrElse(Seq()) ++ readPaths\n+\n+      val fs = FSUtils.getFs(allPaths.head, sqlContext.sparkContext.hadoopConfiguration)\n+      val globPaths = HudiSparkUtils.checkAndGlobPathIfNecessary(allPaths, fs)\n+\n+      val tablePath = DataSourceUtils.getTablePath(fs, globPaths.toArray)\n+      log.info(\"Obtained hudi table path: \" + tablePath)\n+\n+      val metaClient = new HoodieTableMetaClient(fs.getConf, tablePath)\n+      val bootstrapIndex = BootstrapIndex.getBootstrapIndex(metaClient)\n+\n+      val isBootstrappedTable = bootstrapIndex.useIndex()", "originalCommit": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMxNzYxMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466317612", "bodyText": "Good point. I didn't realize that this is being stored in hoodie.properties file.\nIdeally we would want to have a kind of check here, which can tell us if any of the files is still bootstrapped or not. If all the files written during bootstrap have had some upsert to it, it means that a regular hudi file has been written corresponding to each one of them. In such a case we would be able to simply use spark's parquet datasource, for querying which would be faster.\nJust wanted to bring to your notice. However, for now I can do what you are suggesting.", "author": "umehrot2", "createdAt": "2020-08-06T10:27:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMzI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2NzQwMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466667402", "bodyText": "lets file a follow up on that? I feel the TableFileSystemView can already provide you that metadata where the file slices dont have an external base file.", "author": "vinothchandar", "createdAt": "2020-08-06T20:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMzI1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0ODU1NQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466748555", "bodyText": "Made the change, and have created a followup JIRA https://issues.apache.org/jira/browse/HUDI-1157", "author": "umehrot2", "createdAt": "2020-08-07T00:03:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMzI1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNTY4NA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466225684", "bodyText": "please name the classes Hoodie consistent with rest of the code base. I renamed few other classes that were checked in earlier, on gary's pr", "author": "vinothchandar", "createdAt": "2020-08-06T08:12:37Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,", "originalCommit": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMxMTc2OA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466311768", "bodyText": "I will change it to hoodie, but I thought in general the community was moving towards hudi as the naming convention.", "author": "umehrot2", "createdAt": "2020-08-06T10:16:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNTY4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2Nzc3Nw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466667777", "bodyText": "No. there are no other files like this. This is a large topic - configs, .hoodie , ..code .. Like to do everything in a more streamlined fashion.", "author": "vinothchandar", "createdAt": "2020-08-06T20:28:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNTY4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0ODg5Mg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466748892", "bodyText": "Makes sense to do it in a streamlined fashion. Made the change.", "author": "umehrot2", "createdAt": "2020-08-07T00:04:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNTY4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNzU1MA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466227550", "bodyText": "bootstrap is one time operation done during migration. not sure if the user wants to incrementally fetch that existing data as a single batch. Not sure what it means to incrementally query from bootstrap instant ts?  can we just avoid this whole thing by only supporting incremental query on commit timestamps after bootstrap?", "author": "vinothchandar", "createdAt": "2020-08-06T08:15:55Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -92,36 +102,69 @@ class IncrementalRelation(val sqlContext: SQLContext,\n   override def schema: StructType = latestSchema\n \n   override def buildScan(): RDD[Row] = {\n-    val fileIdToFullPath = mutable.HashMap[String, String]()\n+    val regularFileIdToFullPath = mutable.HashMap[String, String]()\n+    var metaBootstrapFileIdToFullPath = mutable.HashMap[String, String]()\n+\n     for (commit <- commitsToReturn) {\n       val metadata: HoodieCommitMetadata = HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit)\n         .get, classOf[HoodieCommitMetadata])\n-      fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+\n+      if (HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS == commit.getTimestamp) {", "originalCommit": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMxMDAyNA==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466310024", "bodyText": "I had discussed this with Balaji, and we concluded to invest in this so that there is no difference in terms of user experience w.r.t whether the table is bootstrapped or not. Right now users can query data in the first commit written through incremental query, and we wanted to support this experience with bootstrapped tables as well.\nA use-case I can think of:\nUser needs to access data until a provided end commit (from first commit till some provided end commit). This is possible to do with incremental query in regular Hudi tables. We would want to support this experience even with bootstrapped tables.", "author": "umehrot2", "createdAt": "2020-08-06T10:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNzU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjMxMDMwOQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466310309", "bodyText": "@bvaradar can chime in here as well, so we can reach a conclusion", "author": "umehrot2", "createdAt": "2020-08-06T10:13:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNzU1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2ODU4Mg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466668582", "bodyText": "Hmmm. that seems fair. although I would have probably done this after an ask came up. Since you have it already, let's just keep this.", "author": "vinothchandar", "createdAt": "2020-08-06T20:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNzU1MA=="}], "type": "inlineReview"}, {"oid": "4fcd7fee5a57f7e38089dea31f713e36892c43e1", "url": "https://github.com/apache/hudi/commit/4fcd7fee5a57f7e38089dea31f713e36892c43e1", "message": "Bootstrap datasource integration", "committedDate": "2020-08-06T23:44:12Z", "type": "forcePushed"}, {"oid": "923a67826ee904ccadc6225917bfc8974b118ef6", "url": "https://github.com/apache/hudi/commit/923a67826ee904ccadc6225917bfc8974b118ef6", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T09:00:10Z", "type": "forcePushed"}, {"oid": "7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "url": "https://github.com/apache/hudi/commit/7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T22:16:41Z", "type": "forcePushed"}, {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf", "url": "https://github.com/apache/hudi/commit/3f7ecde4438fe79d282187616557c5ca451055bf", "message": "Bootstrap datasource integration", "committedDate": "2020-08-07T22:56:08Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTMyMg==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467341322", "bodyText": "I believe add https://github.com/apache/hudi/blob/master/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala#L55 will resolve this issue.", "author": "garyli1019", "createdAt": "2020-08-08T01:00:21Z", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestDataSourceForBootstrap.scala", "diffHunk": "@@ -0,0 +1,616 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.functional\n+\n+import java.time.Instant\n+import java.util.Collections\n+\n+import collection.JavaConverters._\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider\n+import org.apache.hudi.client.TestBootstrap\n+import org.apache.hudi.client.bootstrap.selector.FullRecordBootstrapModeSelector\n+import org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions, HoodieDataSourceHelpers}\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.timeline.HoodieTimeline\n+import org.apache.hudi.config.{HoodieBootstrapConfig, HoodieCompactionConfig, HoodieWriteConfig}\n+import org.apache.hudi.keygen.SimpleKeyGenerator\n+import org.apache.spark.api.java.JavaSparkContext\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.{SaveMode, SparkSession}\n+import org.junit.jupiter.api.Assertions.assertEquals\n+import org.junit.jupiter.api.{BeforeEach, Test}\n+import org.junit.jupiter.api.io.TempDir\n+\n+class TestDataSourceForBootstrap {\n+\n+  var spark: SparkSession = _\n+  val commonOpts = Map(\n+    HoodieWriteConfig.INSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.UPSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.DELETE_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.BULKINSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.FINALIZE_WRITE_PARALLELISM -> \"4\",\n+    HoodieBootstrapConfig.BOOTSTRAP_PARALLELISM -> \"4\",\n+    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"_row_key\",\n+    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"partition\",\n+    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n+    HoodieWriteConfig.TABLE_NAME -> \"hoodie_test\"\n+  )\n+  var basePath: String = _\n+  var srcPath: String = _\n+  var fs: FileSystem = _\n+\n+  @BeforeEach def initialize(@TempDir tempDir: java.nio.file.Path) {\n+    spark = SparkSession.builder\n+      .appName(\"Hoodie Datasource test\")\n+      .master(\"local[2]\")\n+      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+      .getOrCreate\n+    basePath = tempDir.toAbsolutePath.toString + \"/base\"\n+    srcPath = tempDir.toAbsolutePath.toString + \"/src\"\n+    fs = FSUtils.getFs(basePath, spark.sparkContext.hadoopConfiguration)\n+  }\n+", "originalCommit": "3f7ecde4438fe79d282187616557c5ca451055bf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM1MzMzMw==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467353333", "bodyText": "Thanks @garyli1019 . You were right, I wasn't cleaning up the spark contexts after my test runs. Fixed it now.", "author": "umehrot2", "createdAt": "2020-08-08T03:03:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTMyMg=="}], "type": "inlineReview"}, {"oid": "caa597a30293eeb7c09d50c03a7feb3f79678495", "url": "https://github.com/apache/hudi/commit/caa597a30293eeb7c09d50c03a7feb3f79678495", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T01:59:15Z", "type": "forcePushed"}, {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T02:03:07Z", "type": "commit"}, {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "message": "Bootstrap datasource integration", "committedDate": "2020-08-08T02:03:07Z", "type": "forcePushed"}, {"oid": "ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "url": "https://github.com/apache/hudi/commit/ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "message": "Merge branch 'master' into umehrot2_hudi_rfc12_code_review", "committedDate": "2020-08-09T09:31:44Z", "type": "commit"}, {"oid": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "url": "https://github.com/apache/hudi/commit/e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "message": "Fix unit-tests", "committedDate": "2020-08-09T16:18:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxOTg0OQ==", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467619849", "bodyText": "@umehrot2 : ITTestBootstrapCommand is failing with the below exception. Adding a sanitization API to remove illegal characters from avro field names\nException in thread \"main\" org.apache.avro.SchemaParseException: Illegal character in: test-table_record\n    at org.apache.avro.Schema.validateName(Schema.java:1151)\n    at org.apache.avro.Schema.access$200(Schema.java:81)\n    at org.apache.avro.Schema$Name.<init>(Schema.java:489)\n    at org.apache.avro.Schema.createRecord(Schema.java:161)\n    at org.apache.avro.SchemaBuilder$RecordBuilder.fields(SchemaBuilder.java:1732)\n    at org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:173)\n    at org.apache.spark.sql.avro.SchemaConverters.toAvroType(SchemaConverters.scala)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSourceSchema(BootstrapSchemaProvider.java:97)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSchema(BootstrapSchemaProvider.java:66)\n    at org.apache.hudi.table.action.bootstrap.BootstrapCommitActionExecutor.listAndProcessSourcePartitions(BootstrapCommitActionExecutor.java:288)", "author": "bvaradar", "createdAt": "2020-08-09T19:29:00Z", "path": "hudi-client/src/main/java/org/apache/hudi/client/bootstrap/BootstrapSchemaProvider.java", "diffHunk": "@@ -64,14 +75,25 @@ public final Schema getBootstrapSchema(JavaSparkContext jsc, List<Pair<String, L\n    */\n   protected Schema getBootstrapSourceSchema(JavaSparkContext jsc,\n       List<Pair<String, List<HoodieFileStatus>>> partitions) {\n-    return partitions.stream().flatMap(p -> p.getValue().stream())\n-        .map(fs -> {\n-          try {\n-            Path filePath = FileStatusUtils.toPath(fs.getPath());\n-            return ParquetUtils.readAvroSchema(jsc.hadoopConfiguration(), filePath);\n-          } catch (Exception ex) {\n-            return null;\n-          }\n-        }).filter(x -> x != null).findAny().get();\n+    MessageType parquetSchema = partitions.stream().flatMap(p -> p.getValue().stream()).map(fs -> {\n+      try {\n+        Path filePath = FileStatusUtils.toPath(fs.getPath());\n+        return ParquetUtils.readSchema(jsc.hadoopConfiguration(), filePath);\n+      } catch (Exception ex) {\n+        return null;\n+      }\n+    }).filter(Objects::nonNull).findAny()\n+        .orElseThrow(() -> new HoodieException(\"Could not determine schema from the data files.\"));\n+\n+\n+    ParquetToSparkSchemaConverter converter = new ParquetToSparkSchemaConverter(\n+            Boolean.parseBoolean(SQLConf.PARQUET_BINARY_AS_STRING().defaultValueString()),\n+            Boolean.parseBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP().defaultValueString()));\n+    StructType sparkSchema = converter.convert(parquetSchema);\n+    String tableName = writeConfig.getTableName();\n+    String structName = tableName + \"_record\";", "originalCommit": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1ce422e53f6de4fbec05918ac489b29a65de5bd5", "url": "https://github.com/apache/hudi/commit/1ce422e53f6de4fbec05918ac489b29a65de5bd5", "message": "Add spark-avro dependency in hudi-cli and sanitize Avro field names", "committedDate": "2020-08-09T19:37:11Z", "type": "commit"}]}