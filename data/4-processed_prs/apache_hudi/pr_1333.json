{"pr_number": 1333, "pr_title": "[HUDI-589][DOCS] Fix querying_data page", "pr_createdAt": "2020-02-14T00:01:00Z", "pr_url": "https://github.com/apache/hudi/pull/1333", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTIwMg==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225202", "bodyText": "please point to quick start or some example for this", "author": "vinothchandar", "createdAt": "2020-02-14T02:37:03Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjAwNQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972005", "bodyText": "This is pointed in end of current paragraph already. (line 111)", "author": "bhasudha", "createdAt": "2020-02-23T07:28:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTQ3OQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225479", "bodyText": "can we remove this line Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. .. you don't have to build it yourself per se..", "author": "vinothchandar", "createdAt": "2020-02-14T02:38:22Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjIzMg==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972232", "bodyText": "sure.", "author": "bhasudha", "createdAt": "2020-02-23T07:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTYwMQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225601", "bodyText": "use --jars --packages", "author": "vinothchandar", "createdAt": "2020-02-14T02:38:54Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjIzNQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972235", "bodyText": "done.", "author": "bhasudha", "createdAt": "2020-02-23T07:32:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379225899", "bodyText": "For this section, can you take another stab.. it feels short and curt.. may be set some context on things like : Spark Datasources directly query underlying DFS data without Hive (and for this reason I think we should move SparkSQL up and place before this section)", "author": "vinothchandar", "createdAt": "2020-02-14T02:40:34Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQyODc1OQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379428759", "bodyText": "+1", "author": "leesf", "createdAt": "2020-02-14T13:27:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3Mjg1OA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972858", "bodyText": "Agreed. I moved this section below Spark SQL. I need some help here with adding additional context though.", "author": "bhasudha", "createdAt": "2020-02-23T07:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNTg5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjAzOA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226038", "bodyText": "lets also spend some time setting context and explaining how this uses Spark/Hive integration", "author": "vinothchandar", "createdAt": "2020-02-14T02:41:22Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjkyOQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972929", "bodyText": "Also need help here to add context on how Spark SQL integrates with Spark and Hive. Thanks!", "author": "bhasudha", "createdAt": "2020-02-23T07:44:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjExMA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226110", "bodyText": "own parquet reader", "author": "vinothchandar", "createdAt": "2020-02-14T02:41:42Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI0MA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972240", "bodyText": "sure.", "author": "bhasudha", "createdAt": "2020-02-23T07:32:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjIwNQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226205", "bodyText": "By default : are you talking about copy_on_write tables?", "author": "vinothchandar", "createdAt": "2020-02-14T02:42:14Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MTQxNA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382971414", "bodyText": "No. I meant Spark's way of handling things when I meant by default. I dint not refer to COPY_ON_Write table when I used 'by default'. If it causes ambiguity, we can rephrase it. let me know.", "author": "bhasudha", "createdAt": "2020-02-23T07:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjQwMw==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226403", "bodyText": "turning off convertMetastoreParquet please usethe exact and full config here within ``", "author": "vinothchandar", "createdAt": "2020-02-14T02:43:04Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -84,55 +102,53 @@ using the hive session property for incremental queries: `set hive.fetch.task.co\n would ensure Map Reduce execution is chosen for a Hive query, which combines partitions (comma\n separated) and calls InputFormat.listStatus() only once with all those partitions.\n \n-## Spark\n+## Spark datasource\n \n-Spark provides much easier deployment & management of Hudi jars and bundles into jobs/notebooks. At a high level, there are two ways to access Hudi tables in Spark.\n+Hudi COPY_ON_WRITE tables can be queried via Spark datasource similar to how standard datasources work (e.g: `spark.read.parquet`). \n+Both snapshot querying and incremental querying are supported here. Typically spark jobs require adding `--jars <path to jar>/hudi-spark-bundle_2.11:0.5.1-incubating`\n+to classpath of drivers and executors. Refer [building Hudi](https://github.com/apache/incubator-hudi#building-apache-hudi-from-source) for build instructions. \n+When using spark shell instead of --jars, --packages can also be used to fetch the hudi-spark-bundle like this: `--packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating`\n+For sample setup, refer to [Setup spark-shell in quickstart](/docs/quick-start-guide.html#setup-spark-shell).\n \n- - **Hudi DataSource** : Supports Read Optimized, Incremental Pulls similar to how standard datasources (e.g: `spark.read.parquet`) work.\n- - **Read as Hive tables** : Supports all three query types, including the snapshot queries, relying on the custom Hudi input formats again like Hive.\n- \n- In general, your spark job needs a dependency to `hudi-spark` or `hudi-spark-bundle_2.*-x.y.z.jar` needs to be on the class path of driver & executors (hint: use `--jars` argument)\n+## Spark SQL\n+Supports all query types across both Hudi table types, relying on the custom Hudi input formats again like Hive. \n+Typically notebook users and spark-shell users leverage spark sql for querying Hudi tables. Please add hudi-spark-bundle \n+as described above via --jars or --packages.\n  \n-### Read optimized query\n-\n-Pushing a path filter into sparkContext as follows allows for read optimized querying of a Hudi hive table using SparkSQL. \n-This method retains Spark built-in optimizations for reading Parquet files like vectorized reading on Hudi tables.\n-\n-```scala\n-spark.sparkContext.hadoopConfiguration.setClass(\"mapreduce.input.pathFilter.class\", classOf[org.apache.hudi.hadoop.HoodieROTablePathFilter], classOf[org.apache.hadoop.fs.PathFilter]);\n-```\n-\n-If you prefer to glob paths on DFS via the datasource, you can simply do something like below to get a Spark dataframe to work with. \n+### Snapshot query {#spark-snapshot-query}\n+By default, Spark SQL will try to use its own parquet support instead of Hive SerDe when reading from Hive metastore parquet tables. \n+However, for MERGE_ON_READ tables which has both parquet and avro data, this default setting needs to be turned off using set `spark.sql.hive.convertMetastoreParquet=false`. \n+This will force Spark to fallback to using the Hive Serde to read the data (planning/executions is still Spark). \n \n ```java\n-Dataset<Row> hoodieROViewDF = spark.read().format(\"org.apache.hudi\")\n-// pass any path glob, can include hudi & non-hudi tables\n-.load(\"/glob/path/pattern\");\n+$ spark-shell --driver-class-path /etc/hive/conf  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n+scala> sqlContext.sql(\"select count(*) from hudi_trips_mor_rt where datestr = '2016-10-02'\").show()\n ```\n- \n-### Snapshot query {#spark-snapshot-query}\n-Currently, near-real time data can only be queried as a Hive table in Spark using snapshot query mode. In order to do this, set `spark.sql.hive.convertMetastoreParquet=false`, forcing Spark to fallback \n-to using the Hive Serde to read the data (planning/executions is still Spark). \n \n-```java\n-$ spark-shell --jars hudi-spark-bundle_2.11-x.y.z-SNAPSHOT.jar --driver-class-path /etc/hive/conf  --packages org.apache.spark:spark-avro_2.11:2.4.4 --conf spark.sql.hive.convertMetastoreParquet=false --num-executors 10 --driver-memory 7g --executor-memory 2g  --master yarn-client\n+For COPY_ON_WRITE tables, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI0OQ==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972249", "bodyText": "okay sure.", "author": "bhasudha", "createdAt": "2020-02-23T07:32:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjU1Mg==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226552", "bodyText": "Remove this section ?", "author": "vinothchandar", "createdAt": "2020-02-14T02:43:50Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI1NA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972254", "bodyText": "done", "author": "bhasudha", "createdAt": "2020-02-23T07:32:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjYyNA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379226624", "bodyText": "COPY_ON_WRITE: typo", "author": "vinothchandar", "createdAt": "2020-02-14T02:44:12Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -145,8 +161,13 @@ Additionally, `HoodieReadClient` offers the following functionality using Hudi's\n | filterExists() | Filter out already existing records from the provided RDD[HoodieRecord]. Useful for de-duplication |\n | checkExists(keys) | Check if the provided keys exist in a Hudi table |\n \n+### Read optimized query\n+\n+For read optimized queries, either Hive SerDe can be used by turning off convertMetastoreParquet as described above or Spark's built in support can be leveraged. \n+If using spark's built in support, additionally a path filter needs to be pushed into sparkContext as described earlier.\n \n ## Presto\n \n-Presto is a popular query engine, providing interactive query performance. Presto currently supports only read optimized queries on Hudi tables. \n-This requires the `hudi-presto-bundle` jar to be placed into `<presto_install>/plugin/hive-hadoop2/`, across the installation.\n+Presto is a popular query engine, providing interactive query performance. Presto currently supports snapshot queries on\n+COPY_On_WRITE and read optimized queries on MERGE_ON_READ Hudi tables. This requires the `hudi-presto-bundle` jar ", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MjI1Nw==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382972257", "bodyText": "will fix", "author": "bhasudha", "createdAt": "2020-02-23T07:32:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTIyNjYyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379418568", "bodyText": "should we also mention the impala?", "author": "leesf", "createdAt": "2020-02-14T13:02:05Z", "path": "docs/_docs/2_3_querying_data.md", "diffHunk": "@@ -9,7 +9,7 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n \n Conceptually, Hudi stores data physically once on DFS, while providing 3 different ways of querying, as explained [before](/docs/concepts.html#query-types). \n Once the table is synced to the Hive metastore, it provides external Hive tables backed by Hudi's custom inputformats. Once the proper hudi\n-bundle has been provided, the table can be queried by popular query engines like Hive, Spark and Presto.\n+bundle has been provided, the table can be queried by popular query engines like Hive, Spark datasource, Spark SQL and Presto.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTY0NjQ1OA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r379646458", "bodyText": "it's not released yet though..but we should file a ticket for doc-ing that nonetheless, when a impala release does happen", "author": "vinothchandar", "createdAt": "2020-02-14T21:12:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjk3MDg5MA==", "url": "https://github.com/apache/hudi/pull/1333#discussion_r382970890", "bodyText": "+1 Created https://issues.apache.org/jira/browse/HUDI-630", "author": "bhasudha", "createdAt": "2020-02-23T07:10:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTQxODU2OA=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "url": "https://github.com/apache/hudi/commit/654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "message": "[HUDI-589][DOCS] Fix querying_data page\n\n- Added support matrix for COW and MOR tables\n- Change reference from (`views`|`pulls`) to `queries`\n- And minor restructuring", "committedDate": "2020-03-02T18:57:49Z", "type": "commit"}, {"oid": "654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "url": "https://github.com/apache/hudi/commit/654a4e2e4014d8f0ff7bdba3594122db56c3bf24", "message": "[HUDI-589][DOCS] Fix querying_data page\n\n- Added support matrix for COW and MOR tables\n- Change reference from (`views`|`pulls`) to `queries`\n- And minor restructuring", "committedDate": "2020-03-02T18:57:49Z", "type": "forcePushed"}]}