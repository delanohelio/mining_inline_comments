{"pr_number": 1848, "pr_title": "[HUDI-69] Support Spark Datasource for MOR table - RDD approach", "pr_createdAt": "2020-07-20T01:06:48Z", "pr_url": "https://github.com/apache/hudi/pull/1848", "timeline": [{"oid": "45015db2ab95da640d9ba7ddfdfb5799970cf19d", "url": "https://github.com/apache/hudi/commit/45015db2ab95da640d9ba7ddfdfb5799970cf19d", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-21T05:14:51Z", "type": "forcePushed"}, {"oid": "bdda1d6650365c616d233503e3ca8994cd0922cc", "url": "https://github.com/apache/hudi/commit/bdda1d6650365c616d233503e3ca8994cd0922cc", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-21T05:22:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457844997", "bodyText": "PrunedFilteredScan will change the behavior of ParquetRecordReader inside ParquetFileFormat even we are not using the vectorized reader. Still trying to figure out why... I will follow up with PrunedFilteredScan in a separate PR.", "author": "garyli1019", "createdAt": "2020-07-21T05:28:10Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NzM0NQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457867345", "bodyText": "can we file a JIRA for this. and is it possible to target this for 0.6.0 as well?", "author": "vinothchandar", "createdAt": "2020-07-21T06:33:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU1MTc4Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458551783", "bodyText": "https://issues.apache.org/jira/browse/HUDI-1050", "author": "garyli1019", "createdAt": "2020-07-22T05:56:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg0NDk5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2MzgxOA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457863818", "bodyText": "hows this different from the class we already have in hudi-common : SerializableConfiguration ?", "author": "vinothchandar", "createdAt": "2020-07-21T06:24:41Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HadoopSerializableConfiguration.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.Serializable;\n+\n+public class HadoopSerializableConfiguration implements Serializable {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIxMzgzMQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458213831", "bodyText": "Didn't know this. Will switch over.", "author": "garyli1019", "createdAt": "2020-07-21T16:04:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2MzgxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0MzQ2Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458543467", "bodyText": "done", "author": "garyli1019", "createdAt": "2020-07-22T05:30:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2MzgxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2Mzk5NA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457863994", "bodyText": "why remove this?", "author": "vinothchandar", "createdAt": "2020-07-21T06:25:06Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/AbstractRealtimeRecordReader.java", "diffHunk": "@@ -147,12 +146,4 @@ public Schema getWriterSchema() {\n   public Schema getHiveSchema() {\n     return hiveSchema;\n   }\n-\n-  public long getMaxCompactionMemoryInBytes() {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIxNTA5Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458215093", "bodyText": "Moved to a utils class. Need to call this method from HoodieMergeOnReadRDD", "author": "garyli1019", "createdAt": "2020-07-21T16:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2Mzk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDQzNQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457864435", "bodyText": "seems good to avoid the static import here and have the reader realize the class its coming from ?", "author": "vinothchandar", "createdAt": "2020-07-21T06:26:12Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeCompactedRecordReader.java", "diffHunk": "@@ -40,6 +40,8 @@\n import java.io.IOException;\n import java.util.Map;\n \n+import static org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes;", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NDUzOA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458544538", "bodyText": "done", "author": "garyli1019", "createdAt": "2020-07-22T05:33:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDQzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NDc3MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457864770", "bodyText": "ah ok. its just relocated", "author": "vinothchandar", "createdAt": "2020-07-21T06:26:54Z", "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -69,6 +71,17 @@ public static Schema readSchema(Configuration conf, Path filePath) {\n     }\n   }\n \n+  /**\n+   * get the max compaction memory in bytes from JobConf.\n+   */\n+  public static long getMaxCompactionMemoryInBytes(JobConf jobConf) {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457865291", "bodyText": "should we just make a new datasource level config for this. and translate. mixing raw InputFormat level configs here, feels a bit problematic in terms of long term maitenance>", "author": "vinothchandar", "createdAt": "2020-07-21T06:28:21Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -110,6 +112,10 @@ object DataSourceReadOptions {\n    */\n   val INCR_PATH_GLOB_OPT_KEY = \"hoodie.datasource.read.incr.path.glob\"\n   val DEFAULT_INCR_PATH_GLOB_OPT_VAL = \"\"\n+\n+\n+  val REALTIME_SKIP_MERGE_KEY = REALTIME_SKIP_MERGE_PROP", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIyNDA0OA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458224048", "bodyText": "Agree. Maybe something like SNAPSHOT_READ_STRATEGY? So we can control the logic for merge unmerge mergeWithBootstrap e.t.c", "author": "garyli1019", "createdAt": "2020-07-21T16:19:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODU0NDkxNQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458544915", "bodyText": "added MERGE_ON_READ_PAYLOAD_KEY and MERGE_ON_READ_ORDERING_KEY. Then we use the payload to do all the merging.", "author": "garyli1019", "createdAt": "2020-07-22T05:35:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMjU3NA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459222574", "bodyText": "payload is actually stored inside hoodie.properties . let me take a stab at how to handle this", "author": "vinothchandar", "createdAt": "2020-07-23T05:33:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NTc4Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457865783", "bodyText": "rename: getBaseFileOnlyView()", "author": "vinothchandar", "createdAt": "2020-07-21T06:29:29Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -123,4 +120,25 @@ class DefaultSource extends RelationProvider\n   }\n \n   override def shortName(): String = \"hudi\"\n+\n+  private def getReadOptimizedView(sqlContext: SQLContext,", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NjgwNg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457866806", "bodyText": "lets name the classes HoodieMergeOn... not Hudi.. to be consistent with rest of the code.", "author": "vinothchandar", "createdAt": "2020-07-21T06:31:59Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2NzU3NQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457867575", "bodyText": "why change this?", "author": "vinothchandar", "createdAt": "2020-07-21T06:33:54Z", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -67,7 +68,7 @@ class TestDataSource {\n     // Insert Operation\n     val records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100)).toList\n     val inputDF: Dataset[Row] = spark.read.json(spark.sparkContext.parallelize(records, 2))\n-    inputDF.write.format(\"hudi\")\n+    inputDF.write.format(\"org.apache.hudi\")", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2ODE4Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457868182", "bodyText": "nit: space after Logging? Logging { ?", "author": "vinothchandar", "createdAt": "2020-07-21T06:35:14Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457871133", "bodyText": "would n't this read all the fields out? should we not pass userSchema here?", "author": "vinothchandar", "createdAt": "2020-07-21T06:42:27Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMDc1Ng==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458230756", "bodyText": "This is related to PrunedFilteredScan. We need to support merging two records with different schemas if we don't read all fields here. This will be targeting for 0.6.0 release", "author": "garyli1019", "createdAt": "2020-07-21T16:29:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDEwNg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459224106", "bodyText": "Let me think about this more.", "author": "vinothchandar", "createdAt": "2020-07-23T05:39:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwOTY0OQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459709649", "bodyText": "We may want to support Column Pruning here. @garyli1019 atleast for the more straight forward parts like reading:\n\nReading base parquet files only without log files to be merged\nUnmerge reading logic\n\nIt should be possible to push the user request columns/schema down without complications. You can possibly introduce another parquetReaderFunction which passes the requested schema down and use it for the above defined cases.\nFor the case where merging is required I agree it may be more complicated. What if the user does not even request reading a column which has been updated in the log file ? Merging in such cases may not even be required.", "author": "umehrot2", "createdAt": "2020-07-23T20:29:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1ODM2MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459758360", "bodyText": "When I pushdown nothing and pass the full schema as user requested schema, with simply changing from TableScan() to PrunedFilteredScan, the behavior of the parquet reader was changed and not reading the correct schema. I need to dig deeper here.\nLet's focus on making the basic functionality work in this PR. I will figure this out with a follow-up PR.", "author": "garyli1019", "createdAt": "2020-07-23T22:15:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg2NDE4Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459864183", "bodyText": "After thinking for a while, I think we can handle it this way:\nBaseFileOnly\nuse the user-specified schema base file reader\nUnmerge\nuse the user-specified schema base file reader\nConvert log record to InternalRow then extract the correct schema before exiting the unMergeFileIterator\nOr the other way around.\nMerge\nUse the full schema base file reader.\nMerge two records in Avro.\nConvert to InternalRow then extract the correct schema or the other way around.\nSince the InternalRow need the position to extract the value and the schema could be nested. This could get complicated once nested columns got involved.", "author": "garyli1019", "createdAt": "2020-07-24T05:54:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTEzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457871612", "bodyText": "if we did PrunedFilteredScan we can also pass in teh filters? in any case, can we pass in the the options?", "author": "vinothchandar", "createdAt": "2020-07-21T06:43:37Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMjQ1MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458232450", "bodyText": "This field required Seq[Filter]. With PrunedFilteredScan we can just pass whatever Spark passed to buildScan(xxx, filter: Seq[Filter] to here. This field could be an empty Seq.", "author": "garyli1019", "createdAt": "2020-07-21T16:32:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMjk0NA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459712944", "bodyText": "We should try to support filter push-down. Again for the more straight forward scenarios which I mentioned in my other comment, it should be possible to just pass down the user filters in the reader.\nHowever, for Log file merging scenario we may have to take care of the following scenario:\n\nReading base file filtered out say Row X because of filter push-down.\nRow X had been updated in the log file and has an entry.\nWhile merging we need some way to tell that Row X should be filtered out from log file as well, otherwise we may end up still returning it in the result, because based on the merging logic I see that any remaining rows in log file which did not have corresponding key in base file are just appended and returned in the result.", "author": "umehrot2", "createdAt": "2020-07-23T20:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MDYyMA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459760620", "bodyText": "Missing the filter for log file probably ok because Spark will apply the filter again after the pushdown filter. Description https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L268", "author": "garyli1019", "createdAt": "2020-07-23T22:21:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3MTYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457873981", "bodyText": "from the last PR, it seems the biggest change is calling ParquetFileFormat(). and the wrapping as opposed to inheriting the code. Would life be lot simpler if we still defined our own FileFormat and then wrapped ParquetFileFormat and the code you have to read logs as Iterator<InternalRow> ? Just a thought.  https://github.com/apache/spark/blob/8c7d6f9733751503f80d5a1b2463904dfefd6843/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormat.scala#L105\nWhat you are doing is probably valid and makes it consistent with how @umehrot2 is also approaching it. if they are equivalent, then I am fine with it.", "author": "vinothchandar", "createdAt": "2020-07-21T06:49:22Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzNzUzNg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458237536", "bodyText": "I think we can make HoodieLogFileFormat to read log files in the future. Wrapping the FileFormat here gives us a lot of flexibility to adopt other formats like ORC. Wrapping inside the FileFormat could also possible by override buildReaderWithPartitionValues and call super.buildReaderWithPartitionValues to get the iterator. The downside would be we probably need two separate classes to handle ORC and parquet.", "author": "garyli1019", "createdAt": "2020-07-21T16:40:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDA0Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459224042", "bodyText": "I think we will end up reading two classes or code paths anwyay since base path for orc and parquet are different anyway", "author": "vinothchandar", "createdAt": "2020-07-23T05:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcwNTE2OQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459705169", "bodyText": "Possibly something like HoodieLogFileFormat might make sense to do in future, as it will clearly extract out the Log files reading logic.", "author": "umehrot2", "createdAt": "2020-07-23T20:20:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3Mzk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDI3NQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874275", "bodyText": "rename variable  using camel case?", "author": "vinothchandar", "createdAt": "2020-07-21T06:50:01Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HudiMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                    logPaths: Option[List[String]],\n+                                    latestCommit: String,\n+                                    tablePath: String,\n+                                    maxCompactionMemoryInBytes: Long,\n+                                    skipMerge: Boolean)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging{\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)\n+    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  }\n+\n+  private val skipMerge = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_SKIP_MERGE_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_SKIP_MERGE_VAL).toBoolean\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(new JobConf(conf))\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = latestSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(): RDD[Row] = {\n+    val parquetReaderFunction = new ParquetFileFormat().buildReaderWithPartitionValues(\n+      sparkSession = sqlContext.sparkSession,\n+      dataSchema = latestSchema,\n+      partitionSchema = StructType(Nil),\n+      requiredSchema = latestSchema,\n+      filters = Seq.empty,\n+      options = Map.empty,\n+      hadoopConf = sqlContext.sparkSession.sessionState.newHadoopConf()\n+    )\n+    val rdd = new HudiMergeOnReadRDD(sqlContext.sparkContext,\n+      sqlContext.sparkSession.sessionState.newHadoopConf(),\n+      parquetReaderFunction, latestSchema, fileIndex)\n+    rdd.asInstanceOf[RDD[Row]]\n+  }\n+\n+  def buildFileIndex(): List[HudiMergeOnReadFileSplit] = {\n+    val inMemoryFileIndex = HudiSparkUtils.createInMemoryFileIndex(sqlContext.sparkSession, globPaths)\n+    val fileStatuses = inMemoryFileIndex.allFiles()\n+    if (fileStatuses.isEmpty) {\n+      throw new HoodieException(\"No files found for reading in user provided path.\")\n+    }\n+\n+    val fsView = new HoodieTableFileSystemView(metaClient,\n+      metaClient.getActiveTimeline.getCommitsTimeline\n+        .filterCompletedInstants, fileStatuses.toArray)\n+    val latestFiles: List[HoodieBaseFile] = fsView.getLatestBaseFiles.iterator().asScala.toList\n+    val latestCommit = fsView.getLastInstant.get().getTimestamp\n+    val fileGroup = HoodieRealtimeInputFormatUtils.groupLogsByBaseFile(conf, latestFiles.asJava).asScala\n+    val FileSplits = fileGroup.map(kv => {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDY1NQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874655", "bodyText": "rename baseFileReadFunction", "author": "vinothchandar", "createdAt": "2020-07-21T06:50:58Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NDc1Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457874752", "bodyText": "hudi vs hoodie", "author": "vinothchandar", "createdAt": "2020-07-21T06:51:12Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NjIxOQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457876219", "bodyText": "lets not leak parquet in the naming within this class. we can keep it generic as base vs log files", "author": "vinothchandar", "createdAt": "2020-07-21T06:54:31Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3NjY2Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457876662", "bodyText": "print the mergeParquetPartition split variable in the error message?", "author": "vinothchandar", "createdAt": "2020-07-21T06:55:25Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457878885", "bodyText": "this seems to indicate that we will keep scanning the remaining entries in the log and hand them out if dataFileIterator runs out. We need to be careful about how it interplays with split generation. specifically, only works if the each base file has only 1 split..\nHudiMergeOnReadFileSplit(partitionedFile, logPaths, latestCommit,\n         metaClient.getBasePath, maxCompactionMemoryInBytes, skipMerge)\n\nhere partitionedFile has to be a single file and not an input Split. otherwise we will face an issue that the log entry belongs to a different split and the ultimate query will have duplicates.", "author": "vinothchandar", "createdAt": "2020-07-21T07:00:30Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MTg5MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458241891", "bodyText": "Yes, this HudiMergeOnReadFileSplit is based on we pack one baseFile into one partitionedFile during the buildFileIndex. I believe Spark won't split this secretly somewhere...", "author": "garyli1019", "createdAt": "2020-07-21T16:46:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyNDQ4Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459224482", "bodyText": "FileFormat could break up a file into splits I think. We can wait for @umehrot2 also to chime in here.", "author": "vinothchandar", "createdAt": "2020-07-23T05:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzMDI4Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459230287", "bodyText": "One partitioned file will go into one split if I understand this correctly https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala#L355\nThat would be difficult to handle if Spark split one parquet into two splits somewhere else.", "author": "garyli1019", "createdAt": "2020-07-23T06:03:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyOTgzNg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459729836", "bodyText": "@vinothchandar @garyli1019 Spark's FileFormat uses can split the parquet files up if there are multiple row groups in the file. In Spark's implementation one PartitionedFile is not a complete file, that is why you can see the start position and length in that.\nBut in both this implementation and in bootstrap case we are packaging complete file as a split, to avoid the complexity of dealing with partial splits.", "author": "umehrot2", "createdAt": "2020-07-23T21:09:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3ODg4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTIyNQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457879225", "bodyText": "why not containsKey. keySet() may create a copy?", "author": "vinothchandar", "createdAt": "2020-07-21T07:01:13Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MjkzMw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458242933", "bodyText": "good point", "author": "garyli1019", "createdAt": "2020-07-21T16:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTIyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTYzMg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457879632", "bodyText": "is the remove needed. this map is often spillable.. we should just make sure the remove does not incur additional I/O or soemethng", "author": "vinothchandar", "createdAt": "2020-07-21T07:02:13Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0NTY2MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458245661", "bodyText": "This is tricky. We need something like skipableIterator.\n\nThe hudiLogRecords are immutable. So I am making a copy of the keySet() to handle the remove and iterator it through to read back from hudiLogRecords.\nIt's possible that the parquet has duplicate keys and we need to merge with a single logRecord multiple times. This is the behavior of how the compaction handles duplicate keys.", "author": "garyli1019", "createdAt": "2020-07-21T16:52:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg3OTYzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MDA1Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457880057", "bodyText": "same comment here about double checking if this is actually ok", "author": "vinothchandar", "createdAt": "2020-07-21T07:03:15Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)\n+            mergeRowWithLog(curRow)\n+          } else {\n+            curRow\n+          }\n+        } else {\n+          val curKey = hudiLogRecordsIterator.next()", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MTQ5OQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r457881499", "bodyText": "this implicitly assumes OvewriteWithLatestPayload ? can we just convert the parquet row as well to Avro and then perform the merge actually calling the right API. HoodieRecordPayload#combineAndGetUpdateValue() ?  This is a correctness issue we need to resolve in the PR..\nideally, adding a test case as well to go along with this would be good", "author": "vinothchandar", "createdAt": "2020-07-21T07:06:15Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.{HadoopSerializableConfiguration, HoodieRealtimeConfig}\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HudiMergeOnReadPartition(index: Int, split: HudiMergeOnReadFileSplit) extends Partition\n+\n+class HudiMergeOnReadRDD(sc: SparkContext,\n+                         broadcastedConf: Broadcast[HadoopSerializableConfiguration],\n+                         dataReadFunction: PartitionedFile => Iterator[Any],\n+                         dataSchema: StructType,\n+                         hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hudiRealtimeFileSplits: List[HudiMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new HadoopSerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[HadoopSerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hudiRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HudiMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, dataReadFunction)\n+      case unMergeSplit if unMergeSplit.skipMerge =>\n+        unMergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case mergeSplit if !mergeSplit.skipMerge =>\n+        mergeFileIterator(mergeParquetPartition.split, dataReadFunction)\n+      case _ => throw new HoodieException(\"Unable to select an Iterator to read the Hudi MOR File Split\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hudiRealtimeFileSplits.zipWithIndex.map(file => HudiMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.config\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                  readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var hudiLogRecordsIterator = hudiLogRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        dataFileIterator.hasNext || hudiLogRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          dataFileIterator.next()\n+        } else {\n+          val curAvrokey = hudiLogRecordsIterator.next()\n+          val curAvroRecord = hudiLogRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HudiMergeOnReadFileSplit,\n+                                readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val dataFileIterator = read(split.dataFile, readFileFunction)\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val hudiLogRecords = scanLog(split, logSchema).getRecords\n+      private var parquetFinished = false\n+      private var logRecordToRead = hudiLogRecords.keySet()\n+      private var hudiLogRecordsIterator: Iterator[String] = _\n+\n+      override def hasNext: Boolean = {\n+        if (dataFileIterator.hasNext) {\n+          true\n+        } else {\n+          if (!parquetFinished) {\n+            parquetFinished = true\n+            hudiLogRecordsIterator = logRecordToRead.iterator().asScala\n+          }\n+          hudiLogRecordsIterator.hasNext\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (dataFileIterator.hasNext) {\n+          val curRow = dataFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (hudiLogRecords.keySet().contains(curKey)) {\n+            logRecordToRead.remove(curKey)\n+            mergeRowWithLog(curRow)\n+          } else {\n+            curRow\n+          }\n+        } else {\n+          val curKey = hudiLogRecordsIterator.next()\n+          getAvroRecord(curKey)\n+        }\n+      }\n+\n+      private def getAvroRecord(curKey: String): InternalRow = {\n+        val curAvroRecord = hudiLogRecords.get(curKey).getData.getInsertValue(logSchema).get()", "originalCommit": "bdda1d6650365c616d233503e3ca8994cd0922cc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0Nzg4MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r458247881", "bodyText": "Will do.", "author": "garyli1019", "createdAt": "2020-07-21T16:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg4MTQ5OQ=="}], "type": "inlineReview"}, {"oid": "a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "url": "https://github.com/apache/hudi/commit/a4c7069abaafc7faa0476d6a190a6cf55fbd2bbe", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-22T05:59:19Z", "type": "forcePushed"}, {"oid": "41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "url": "https://github.com/apache/hudi/commit/41d1d05d1baa05b89f7bdbdc4458d3e8926f8982", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-22T06:02:19Z", "type": "forcePushed"}, {"oid": "f38c76c784b9ba462bfefbf7700a762f70ca2087", "url": "https://github.com/apache/hudi/commit/f38c76c784b9ba462bfefbf7700a762f70ca2087", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-22T17:35:30Z", "type": "forcePushed"}, {"oid": "5b051e575463e11aa9b30ee62c01ef5546151114", "url": "https://github.com/apache/hudi/commit/5b051e575463e11aa9b30ee62c01ef5546151114", "message": "[HUDI-69] Support Spark Datasource for MOR table", "committedDate": "2020-07-22T17:42:53Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5NDA1Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459294057", "bodyText": "Can we try to avoid having to do this part always ? It is not required for the BaseFileOnly case, so it would be good if we can avoid it to decrease the unnecessary overhead for Read Optimized queries.", "author": "umehrot2", "createdAt": "2020-07-23T08:33:35Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -58,26 +61,20 @@ class DefaultSource extends RelationProvider\n       throw new HoodieException(\"'path' must be specified.\")\n     }\n \n+    val fs = FSUtils.getFs(path.get, sqlContext.sparkContext.hadoopConfiguration)\n+    val globPaths = HudiSparkUtils.checkAndGlobPathIfNecessary(Seq(path.get), fs)\n+    val tablePath = DataSourceUtils.getTablePath(fs, globPaths.toArray)\n+    val metaClient = new HoodieTableMetaClient(fs.getConf, tablePath)", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0MjU2OA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459742568", "bodyText": "I think I can move this into the SNAPSHOT_QUERY session.", "author": "garyli1019", "createdAt": "2020-07-23T21:37:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5NDA1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI5ODE1MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459298150", "bodyText": "Shouldn't we call it something like MergeOnReadSnapshotRelation ?", "author": "umehrot2", "createdAt": "2020-07-23T08:41:07Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      payload: String,\n+                                      orderingVal: String)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwMDY3Ng==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459300676", "bodyText": "Instead of doing this, you can just do:\nval tableSchema = schemaUtil.getTableAvroSchema", "author": "umehrot2", "createdAt": "2020-07-23T08:45:37Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/SnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.avro.HoodieAvroUtils\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, TableScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      payload: String,\n+                                      orderingVal: String)\n+\n+class SnapshotRelation (val sqlContext: SQLContext,\n+                        val optParams: Map[String, String],\n+                        val userSchema: StructType,\n+                        val globPaths: Seq[Path],\n+                        val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with TableScan with Logging {\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val latestSchema = {\n+    val schemaUtil = new TableSchemaResolver(metaClient)\n+    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields)", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDczNQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459754735", "bodyText": "good to know. thanks", "author": "garyli1019", "createdAt": "2020-07-23T22:06:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwMDY3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMxMzUzNA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459313534", "bodyText": "Instead of using prefix Hoodie for all the newly defined classes, shouldn't we be using Hudi. Isn't that where the community is headed towards ?", "author": "umehrot2", "createdAt": "2020-07-23T09:08:51Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NTU3Ng==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459745576", "bodyText": "This problem is tricky. Both sides make sense to me. It seems like impossible to completely switch from hoodie to hudi everywhere. Should we define a standard for the naming convension? Like class name -> hoodie, package name -> hudi @vinothchandar", "author": "garyli1019", "createdAt": "2020-07-23T21:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMxMzUzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc3Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459714773", "bodyText": "Just for my understanding, what is this use-case where we want to return un-merged rows ? Do we do something similar for MOR queries through input format where we want to return un-merged rows ?", "author": "umehrot2", "createdAt": "2020-07-23T20:39:37Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NzY3MQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459747671", "bodyText": "yes we have this option for Hive https://github.com/apache/hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/RealtimeUnmergedRecordReader.java\nThe performance will be better without merging. We can avoid the type conversion at least Row -> Avro -> Merge -> Avro -> Row", "author": "garyli1019", "createdAt": "2020-07-23T21:49:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxNDc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMDE1OA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459720158", "bodyText": "Does this need explicit casting ?", "author": "umehrot2", "createdAt": "2020-07-23T20:50:26Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NjQyNQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459746425", "bodyText": "I followed the HadoopRDD implementation here https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L120", "author": "garyli1019", "createdAt": "2020-07-23T21:46:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyMDE1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNDU3Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459724572", "bodyText": "@vinothchandar This seems like something we can consider using at other places in Hudi code like AvroConversionHelper to convert Avro Records to Rows, instead of maintaining the conversion code in-house.", "author": "umehrot2", "createdAt": "2020-07-23T20:59:06Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0ODY3NA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459748674", "bodyText": "agree here. Our AvroConversionHelper is handling Row, which is not an extension of InternalRow. If we don't need Row specifically, I think we can adapt to the Spark Internal serializer.", "author": "garyli1019", "createdAt": "2020-07-23T21:52:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNDU3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjAzMQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459726031", "bodyText": "Instead of looping over baseFileIterator and performing the check whether that key exists in logRecords, will it be more efficient to do it the other way round. Loop over logRecords and perform merge. In the end append all the remaining base file rows.\n\n\nAlso is this a good practice to perform the actual fetching in hasNext function, instead of next ?", "author": "umehrot2", "createdAt": "2020-07-23T21:01:55Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,227 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.config.SerializableConfiguration\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.{HoodieMergedLogRecordScanner, LogReaderUtils}\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, IndexedRecord}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.{Partition, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer, SchemaConverters}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(sc: SparkContext,\n+                           broadcastedConf: Broadcast[SerializableConfiguration],\n+                           baseFileReadFunction: PartitionedFile => Iterator[Any],\n+                           dataSchema: StructType,\n+                           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  // Broadcast the hadoop Configuration to executors.\n+  def this(sc: SparkContext,\n+           config: Configuration,\n+           dataReadFunction: PartitionedFile => Iterator[Any],\n+           dataSchema: StructType,\n+           hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit]) = {\n+    this(\n+      sc,\n+      sc.broadcast(new SerializableConfiguration(config))\n+      .asInstanceOf[Broadcast[SerializableConfiguration]],\n+      dataReadFunction,\n+      dataSchema,\n+      hoodieRealtimeFileSplits)\n+  }\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    val baseFileIterator = read(mergeParquetPartition.split.dataFile, baseFileReadFunction)\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        baseFileIterator\n+      case unMergeSplit if unMergeSplit.payload\n+        .equals(DataSourceReadOptions.DEFAULT_MERGE_ON_READ_PAYLOAD_VAL) =>\n+        unMergeFileIterator(unMergeSplit, baseFileIterator)\n+      case mergeSplit if !mergeSplit.payload.isEmpty =>\n+        mergeFileIterator(mergeSplit, baseFileIterator)\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    hoodieRealtimeFileSplits.zipWithIndex.map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig(): Configuration = {\n+    broadcastedConf.value.get()\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def unMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val logSchema = getLogAvroSchema(split)\n+      private val sparkTypes = SchemaConverters.toSqlType(logSchema).dataType.asInstanceOf[StructType]\n+      private val converter = new AvroDeserializer(logSchema, sparkTypes)\n+      private val logRecords = scanLog(split, logSchema).getRecords\n+      private val logRecordsIterator = logRecords.keySet().iterator().asScala\n+\n+      override def hasNext: Boolean = {\n+        baseFileIterator.hasNext || logRecordsIterator.hasNext\n+      }\n+\n+      override def next(): InternalRow = {\n+        if (baseFileIterator.hasNext) {\n+          baseFileIterator.next()\n+        } else {\n+          val curAvrokey = logRecordsIterator.next()\n+          val curAvroRecord = logRecords.get(curAvrokey).getData.getInsertValue(logSchema).get()\n+          converter.deserialize(curAvroRecord).asInstanceOf[InternalRow]\n+        }\n+      }\n+    }\n+\n+  private def mergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                baseFileIterator: Iterator[InternalRow]): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val avroSchema = getLogAvroSchema(split)\n+      private val sparkSchema = SchemaConverters.toSqlType(avroSchema).dataType.asInstanceOf[StructType]\n+      private val avroToRowConverter = new AvroDeserializer(avroSchema, sparkSchema)\n+      private val rowToAvroConverter = new AvroSerializer(sparkSchema, avroSchema, false)\n+      private val logRecords = scanLog(split, avroSchema).getRecords\n+      private val logRecordToRead = logRecords.keySet()\n+\n+      private var baseFileFinished = false\n+      private var logRecordsIterator: Iterator[String] = _\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {", "originalCommit": "5b051e575463e11aa9b30ee62c01ef5546151114", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc0NjIzNA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459746234", "bodyText": "Actually on further thought the first point may not be possible, since if we iterator over log records it will be difficult to find the corresponding base parquet record using record key (for merging).", "author": "umehrot2", "createdAt": "2020-07-23T21:46:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjAzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDM1Mw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r459754353", "bodyText": "The log scanner scans all the log files in one batch and handled the merging internally. The output is a hashmap we can use directly. This logRecordsIterator is just looping through the hashmap and doesn't load the row one by one like the daseFileIterator.\n\n\nThis is a little bit tricky. If the hasNext return true, but next() doesn't return a value, Spark will throw an exception. In our logic, we don't know hasNext will be true of false until we find the qualified record to read. For example, 100 records in base file and 100 delete records in the log file. We will read 0 row and hasNext should return false in the first call, but we have iterated through the whole base file already. There is a test case for this example.", "author": "garyli1019", "createdAt": "2020-07-23T22:05:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcyNjAzMQ=="}], "type": "inlineReview"}, {"oid": "eefad4beb4f15864b1ea9925b0d4f54deba776fc", "url": "https://github.com/apache/hudi/commit/eefad4beb4f15864b1ea9925b0d4f54deba776fc", "message": "[HUDI-69] Support PruneFilteredScan", "committedDate": "2020-07-26T23:22:32Z", "type": "forcePushed"}, {"oid": "d8ef9d3dd2eed57983ccb43f235806be848f11e3", "url": "https://github.com/apache/hudi/commit/d8ef9d3dd2eed57983ccb43f235806be848f11e3", "message": "[HUDI-69] Support PruneFilteredScan", "committedDate": "2020-07-27T03:24:55Z", "type": "forcePushed"}, {"oid": "4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "url": "https://github.com/apache/hudi/commit/4b05f0f7b12e083b66084b11a80a207f33bd7dcc", "message": "[HUDI-69] Support PruneFilteredScan", "committedDate": "2020-07-27T20:39:19Z", "type": "forcePushed"}, {"oid": "e6c77567658e73a3db26681ed25e54c8214ca1cd", "url": "https://github.com/apache/hudi/commit/e6c77567658e73a3db26681ed25e54c8214ca1cd", "message": "[HUDI-69] Support PruneFilteredScan", "committedDate": "2020-07-29T04:43:38Z", "type": "forcePushed"}, {"oid": "8f6c63b91d8af0430a35df843c3e6e722d1d6dc5", "url": "https://github.com/apache/hudi/commit/8f6c63b91d8af0430a35df843c3e6e722d1d6dc5", "message": "[HUDI-69] Remove unnecessary payload config", "committedDate": "2020-08-04T05:48:47Z", "type": "forcePushed"}, {"oid": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "url": "https://github.com/apache/hudi/commit/73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table", "committedDate": "2020-08-04T06:01:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyNjExMg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464826112", "bodyText": "great!", "author": "vinothchandar", "createdAt": "2020-08-04T06:25:17Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, GenericRecordBuilder}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Partition, SerializableWritable, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(@transient sc: SparkContext,\n+                           @transient config: Configuration,\n+                           fullSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           requiredSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           tableState: HoodieMergeOnReadTableState)\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  private val confBroadcast = sc.broadcast(new SerializableWritable(config))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader)\n+      case skipMergeSplit if skipMergeSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_SKIP_MERGE_OPT_VAL) =>\n+        skipMergeFileIterator(\n+          skipMergeSplit,\n+          read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader),\n+          getConfig\n+        )\n+      case payloadCombineSplit if payloadCombineSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_PAYLOAD_COMBINE_OPT_VAL) =>\n+        payloadCombineFileIterator(\n+          payloadCombineSplit,\n+          read(mergeParquetPartition.split.dataFile, fullSchemaFileReader),\n+          getConfig\n+        )\n+      case _ => throw new HoodieException(s\"Unable to select an Iterator to read the Hoodie MOR File Split for \" +\n+        s\"file path: ${mergeParquetPartition.split.dataFile.filePath}\" +\n+        s\"log paths: ${mergeParquetPartition.split.logPaths.toString}\" +\n+        s\"hoodie table path: ${mergeParquetPartition.split.tablePath}\" +\n+        s\"spark partition Index: ${mergeParquetPartition.index}\" +\n+        s\"merge type: ${mergeParquetPartition.split.mergeType}\")\n+    }\n+  }\n+\n+  override protected def getPartitions: Array[Partition] = {\n+    tableState\n+      .hoodieRealtimeFileSplits\n+      .zipWithIndex\n+      .map(file => HoodieMergeOnReadPartition(file._2, file._1)).toArray\n+  }\n+\n+  private def getConfig: Configuration = {\n+    val conf = confBroadcast.value.value\n+    HoodieMergeOnReadRDD.CONFIG_INSTANTIATION_LOCK.synchronized {\n+      new Configuration(conf)\n+    }\n+  }\n+\n+  private def read(partitionedFile: PartitionedFile,\n+                   readFileFunction: PartitionedFile => Iterator[Any]): Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala\n+    })\n+    rows\n+  }\n+\n+  private def skipMergeFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                  baseFileIterator: Iterator[InternalRow],\n+                                  config: Configuration): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val tableAvroSchema = new Schema.Parser().parse(tableState.tableAvroSchema)\n+      private val requiredAvroSchema = new Schema.Parser().parse(tableState.requiredAvroSchema)\n+      private val requiredFieldPosition =\n+        tableState.requiredStructSchema\n+          .map(f => tableAvroSchema.getField(f.name).pos()).toList\n+      private val recordBuilder = new GenericRecordBuilder(requiredAvroSchema)\n+      private val deserializer = new AvroDeserializer(requiredAvroSchema, tableState.requiredStructSchema)\n+      private val unsafeProjection = UnsafeProjection.create(tableState.requiredStructSchema)\n+      private val logRecords = HoodieMergeOnReadRDD.scanLog(split, tableAvroSchema, config).getRecords\n+      private val logRecordsKeyIterator = logRecords.keySet().iterator().asScala\n+\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {\n+          recordToLoad = baseFileIterator.next()\n+          true\n+        } else {\n+          if (logRecordsKeyIterator.hasNext) {\n+            val curAvrokey = logRecordsKeyIterator.next()\n+            val curAvroRecord = logRecords.get(curAvrokey).getData.getInsertValue(tableAvroSchema)\n+            if (!curAvroRecord.isPresent) {\n+              // delete record found, skipping\n+              this.hasNext\n+            } else {\n+              val requiredAvroRecord = AvroConversionUtils\n+                .buildAvroRecordBySchema(curAvroRecord.get(), requiredAvroSchema, requiredFieldPosition, recordBuilder)\n+              recordToLoad = unsafeProjection(deserializer.deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+              true\n+            }\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+\n+      override def next(): InternalRow = {\n+        recordToLoad\n+      }\n+    }\n+\n+  private def payloadCombineFileIterator(split: HoodieMergeOnReadFileSplit,\n+                                baseFileIterator: Iterator[InternalRow],\n+                                config: Configuration): Iterator[InternalRow] =\n+    new Iterator[InternalRow] {\n+      private val tableAvroSchema = new Schema.Parser().parse(tableState.tableAvroSchema)\n+      private val requiredAvroSchema = new Schema.Parser().parse(tableState.requiredAvroSchema)\n+      private val requiredFieldPosition =\n+        tableState.requiredStructSchema\n+          .map(f => tableAvroSchema.getField(f.name).pos()).toList\n+      private val serializer = new AvroSerializer(tableState.tableStructSchema, tableAvroSchema, false)\n+      private val requiredDeserializer = new AvroDeserializer(requiredAvroSchema, tableState.requiredStructSchema)\n+      private val recordBuilder = new GenericRecordBuilder(requiredAvroSchema)\n+      private val unsafeProjection = UnsafeProjection.create(tableState.requiredStructSchema)\n+      private val logRecords = HoodieMergeOnReadRDD.scanLog(split, tableAvroSchema, config).getRecords\n+      private val logRecordsKeyIterator = logRecords.keySet().iterator().asScala\n+      private val keyToSkip = mutable.Set.empty[String]\n+\n+      private var recordToLoad: InternalRow = _\n+\n+      @scala.annotation.tailrec\n+      override def hasNext: Boolean = {\n+        if (baseFileIterator.hasNext) {\n+          val curRow = baseFileIterator.next()\n+          val curKey = curRow.getString(HOODIE_RECORD_KEY_COL_POS)\n+          if (logRecords.containsKey(curKey)) {\n+            // duplicate key found, merging\n+            keyToSkip.add(curKey)\n+            val mergedAvroRecord = mergeRowWithLog(curRow, curKey)\n+            if (!mergedAvroRecord.isPresent) {\n+              // deleted\n+              this.hasNext\n+            } else {\n+              // load merged record as InternalRow with required schema\n+              val requiredAvroRecord = AvroConversionUtils\n+                .buildAvroRecordBySchema(\n+                  mergedAvroRecord.get(),\n+                  requiredAvroSchema,\n+                  requiredFieldPosition,\n+                  recordBuilder\n+                )\n+              recordToLoad = unsafeProjection(requiredDeserializer\n+                .deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+              true\n+            }\n+          } else {\n+            // No merge needed, load current row with required schema\n+            recordToLoad = unsafeProjection(createRowWithRequiredSchema(curRow))\n+            true\n+          }\n+        } else {\n+          if (logRecordsKeyIterator.hasNext) {\n+            val curKey = logRecordsKeyIterator.next()\n+            if (keyToSkip.contains(curKey)) {\n+              this.hasNext\n+            } else {\n+              val insertAvroRecord =\n+                logRecords.get(curKey).getData.getInsertValue(tableAvroSchema)\n+              if (!insertAvroRecord.isPresent) {\n+                // stand alone delete record, skipping\n+                this.hasNext\n+              } else {\n+                val requiredAvroRecord = AvroConversionUtils\n+                  .buildAvroRecordBySchema(\n+                    insertAvroRecord.get(),\n+                    requiredAvroSchema,\n+                    requiredFieldPosition,\n+                    recordBuilder\n+                  )\n+                recordToLoad = unsafeProjection(requiredDeserializer\n+                  .deserialize(requiredAvroRecord).asInstanceOf[InternalRow])\n+                true\n+              }\n+            }\n+          } else {\n+            false\n+          }\n+        }\n+      }\n+\n+      override def next(): InternalRow = recordToLoad\n+\n+      private def createRowWithRequiredSchema(row: InternalRow): InternalRow = {\n+        val rowToReturn = new SpecificInternalRow(tableState.requiredStructSchema)\n+        val posIterator = requiredFieldPosition.iterator\n+        var curIndex = 0\n+        tableState.requiredStructSchema.foreach(\n+          f => {\n+            val curPos = posIterator.next()\n+            val curField = row.get(curPos, f.dataType)\n+            rowToReturn.update(curIndex, curField)\n+            curIndex = curIndex + 1\n+          }\n+        )\n+        rowToReturn\n+      }\n+\n+      private def mergeRowWithLog(curRow: InternalRow, curKey: String) = {\n+        val historyAvroRecord = serializer.serialize(curRow).asInstanceOf[GenericRecord]\n+        logRecords.get(curKey).getData.combineAndGetUpdateValue(historyAvroRecord, tableAvroSchema)", "originalCommit": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgyNjM5MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464826390", "bodyText": "remove the >>> ?", "author": "vinothchandar", "createdAt": "2020-08-04T06:25:59Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/MergeOnReadSnapshotRelation.scala", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeInputFormatUtils\n+import org.apache.hudi.hadoop.utils.HoodieRealtimeRecordReaderUtils.getMaxCompactionMemoryInBytes\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hadoop.mapred.JobConf\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+case class HoodieMergeOnReadFileSplit(dataFile: PartitionedFile,\n+                                      logPaths: Option[List[String]],\n+                                      latestCommit: String,\n+                                      tablePath: String,\n+                                      maxCompactionMemoryInBytes: Long,\n+                                      mergeType: String)\n+\n+case class HoodieMergeOnReadTableState(tableStructSchema: StructType,\n+                                       requiredStructSchema: StructType,\n+                                       tableAvroSchema: String,\n+                                       requiredAvroSchema: String,\n+                                       hoodieRealtimeFileSplits: List[HoodieMergeOnReadFileSplit])\n+\n+class MergeOnReadSnapshotRelation(val sqlContext: SQLContext,\n+                                  val optParams: Map[String, String],\n+                                  val userSchema: StructType,\n+                                  val globPaths: Seq[Path],\n+                                  val metaClient: HoodieTableMetaClient)\n+  extends BaseRelation with PrunedFilteredScan with Logging {\n+\n+  private val conf = sqlContext.sparkContext.hadoopConfiguration\n+  private val jobConf = new JobConf(conf)\n+  // use schema from latest metadata, if not present, read schema from the data file\n+  private val schemaUtil = new TableSchemaResolver(metaClient)\n+  private val tableAvroSchema = schemaUtil.getTableAvroSchema\n+  private val tableStructSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableAvroSchema)\n+  private val mergeType = optParams.getOrElse(\n+    DataSourceReadOptions.REALTIME_MERGE_OPT_KEY,\n+    DataSourceReadOptions.DEFAULT_REALTIME_MERGE_OPT_VAL)\n+  private val maxCompactionMemoryInBytes = getMaxCompactionMemoryInBytes(jobConf)\n+  private val fileIndex = buildFileIndex()\n+\n+  override def schema: StructType = tableStructSchema\n+\n+  override def needConversion: Boolean = false\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    log.debug(s\">>> buildScan requiredColumns = ${requiredColumns.mkString(\",\")}\")", "originalCommit": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMTAyOQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464831029", "bodyText": "I think we can still do better here. if the payload is OverwriteWithLatest... , then all we need to do is project the keys alone. right? no need for reading the full schema per se. ?", "author": "vinothchandar", "createdAt": "2020-08-04T06:38:17Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieMergeOnReadRDD.scala", "diffHunk": "@@ -0,0 +1,274 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.hudi.hadoop.config.HoodieRealtimeConfig\n+import org.apache.hudi.hadoop.utils.HoodieInputFormatUtils.HOODIE_RECORD_KEY_COL_POS\n+\n+import org.apache.avro.Schema\n+import org.apache.avro.generic.{GenericRecord, GenericRecordBuilder}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.spark.{Partition, SerializableWritable, SparkContext, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.avro.{AvroDeserializer, AvroSerializer}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{SpecificInternalRow, UnsafeProjection, UnsafeRow}\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable\n+import scala.util.Try\n+\n+case class HoodieMergeOnReadPartition(index: Int, split: HoodieMergeOnReadFileSplit) extends Partition\n+\n+class HoodieMergeOnReadRDD(@transient sc: SparkContext,\n+                           @transient config: Configuration,\n+                           fullSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           requiredSchemaFileReader: PartitionedFile => Iterator[Any],\n+                           tableState: HoodieMergeOnReadTableState)\n+  extends RDD[InternalRow](sc, Nil) {\n+\n+  private val confBroadcast = sc.broadcast(new SerializableWritable(config))\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val mergeParquetPartition = split.asInstanceOf[HoodieMergeOnReadPartition]\n+    mergeParquetPartition.split match {\n+      case dataFileOnlySplit if dataFileOnlySplit.logPaths.isEmpty =>\n+        read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader)\n+      case skipMergeSplit if skipMergeSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_SKIP_MERGE_OPT_VAL) =>\n+        skipMergeFileIterator(\n+          skipMergeSplit,\n+          read(mergeParquetPartition.split.dataFile, requiredSchemaFileReader),\n+          getConfig\n+        )\n+      case payloadCombineSplit if payloadCombineSplit.mergeType\n+        .equals(DataSourceReadOptions.REALTIME_PAYLOAD_COMBINE_OPT_VAL) =>\n+        payloadCombineFileIterator(\n+          payloadCombineSplit,\n+          read(mergeParquetPartition.split.dataFile, fullSchemaFileReader),", "originalCommit": "73a10478e2d6aed717b9701a4cf6745ea6d27aaa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNDEwOQ==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464834109", "bodyText": "See HoodieParquetRealtimeInputFormat#addRequiredProjectionFields() for referecne", "author": "vinothchandar", "createdAt": "2020-08-04T06:45:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMTAyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg0Mjg0Nw==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r464842847", "bodyText": "Yes there is still some room for improvement. Spark schema is tricky, when it passed the required columns into the PrunedFilterScan, the order of the columns are different from the actual schema, and it will get reorder in somewhere else before return to the user. The projected InternalRow and the projected Avro record will have a different order of schema. I will look into this but not sure if I am able to get it into this release.", "author": "garyli1019", "createdAt": "2020-08-04T07:05:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMTAyOQ=="}], "type": "inlineReview"}, {"oid": "a2b1467e7961a88ca91a3657807039351b64d834", "url": "https://github.com/apache/hudi/commit/a2b1467e7961a88ca91a3657807039351b64d834", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature.", "committedDate": "2020-08-04T06:52:55Z", "type": "forcePushed"}, {"oid": "496daba54f981b082b6da8cfd642c5ec748b9b58", "url": "https://github.com/apache/hudi/commit/496daba54f981b082b6da8cfd642c5ec748b9b58", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature.", "committedDate": "2020-08-04T17:21:48Z", "type": "forcePushed"}, {"oid": "52066a3d867682a010a41c1c11b638615edef674", "url": "https://github.com/apache/hudi/commit/52066a3d867682a010a41c1c11b638615edef674", "message": "[HUDI-69] SUPPORT Spark Datasource for MOR table\n\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature.", "committedDate": "2020-08-04T20:31:04Z", "type": "forcePushed"}, {"oid": "9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "url": "https://github.com/apache/hudi/commit/9e50d46c03a50e4fbd372f1dffdf6e0e84585c79", "message": "fix flaky test", "committedDate": "2020-08-04T22:44:15Z", "type": "forcePushed"}, {"oid": "1bbb23d04a483802407f85d98c89668f19be881b", "url": "https://github.com/apache/hudi/commit/1bbb23d04a483802407f85d98c89668f19be881b", "message": "Adding debug statements for test failure", "committedDate": "2020-08-05T13:50:43Z", "type": "forcePushed"}, {"oid": "a053967b65240ee394e2a33c228fe617c203d8f6", "url": "https://github.com/apache/hudi/commit/a053967b65240ee394e2a33c228fe617c203d8f6", "message": "Adding debug statements for test failure", "committedDate": "2020-08-05T16:20:46Z", "type": "forcePushed"}, {"oid": "fc5425ddeb2ac075f230793c5ff7906786fc66f3", "url": "https://github.com/apache/hudi/commit/fc5425ddeb2ac075f230793c5ff7906786fc66f3", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T05:35:01Z", "type": "forcePushed"}, {"oid": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "url": "https://github.com/apache/hudi/commit/dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T06:02:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NjIyNA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466166224", "bodyText": "@vinothchandar Looks like some unrelated change was added during the rebase. Maybe this is related to the issue.", "author": "garyli1019", "createdAt": "2020-08-06T06:06:52Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkUtils.scala", "diffHunk": "@@ -27,9 +27,9 @@ import org.apache.spark.sql.types.{StringType, StructField, StructType}\n import scala.collection.JavaConverters._\n \n \n-object HudiSparkUtils {\n+object HoodieSparkUtils {", "originalCommit": "dcd26632c4c17f72fb6d96b1c65e8fed5a83e437", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NzcwNA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466167704", "bodyText": "nvm, this was intended based on previous comments to keep the consistency with others.", "author": "garyli1019", "createdAt": "2020-08-06T06:11:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NjIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2ODQ2MA==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466168460", "bodyText": "this was something that i should have caught in the earlier pr . I renamed it here, thinking we will be landing this quickly. oh well. lets just get the test to pass", "author": "vinothchandar", "createdAt": "2020-08-06T06:13:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE2NjIyNA=="}], "type": "inlineReview"}, {"oid": "15f95a2c560fa5bc03b97db0591d518635257e3a", "url": "https://github.com/apache/hudi/commit/15f95a2c560fa5bc03b97db0591d518635257e3a", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T06:36:05Z", "type": "forcePushed"}, {"oid": "a9533a099cacd86e79b65c483d41397ad62ff76f", "url": "https://github.com/apache/hudi/commit/a9533a099cacd86e79b65c483d41397ad62ff76f", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T07:32:54Z", "type": "forcePushed"}, {"oid": "f70258dacf59e80888f5c95c93dfbc7e5ab90164", "url": "https://github.com/apache/hudi/commit/f70258dacf59e80888f5c95c93dfbc7e5ab90164", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T07:48:32Z", "type": "forcePushed"}, {"oid": "52a4532e75ae50cffbc4c3daaddbb6827c251975", "url": "https://github.com/apache/hudi/commit/52a4532e75ae50cffbc4c3daaddbb6827c251975", "message": "Adding debug statements for test failure", "committedDate": "2020-08-06T08:30:50Z", "type": "forcePushed"}, {"oid": "4731d25e1223ca025df6a146ae8760f2f81d99d8", "url": "https://github.com/apache/hudi/commit/4731d25e1223ca025df6a146ae8760f2f81d99d8", "message": "[HUDI-69] Support Spark Datasource for MOR table\n- This PR implements Spark Datasource for MOR table in the RDD approach.\n- Implemented SnapshotRelation\n- Implemented HudiMergeOnReadRDD\n- Implemented separate Iterator to handle merge and unmerge record reader.\n- Added TestMORDataSource to verify this feature.", "committedDate": "2020-08-06T21:08:50Z", "type": "commit"}, {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f", "url": "https://github.com/apache/hudi/commit/d8beca97ef56b1fd1aac2ea9241d82d41103776f", "message": "[MINOR] fix some tests to pass CI", "committedDate": "2020-08-06T21:12:09Z", "type": "commit"}, {"oid": "d8beca97ef56b1fd1aac2ea9241d82d41103776f", "url": "https://github.com/apache/hudi/commit/d8beca97ef56b1fd1aac2ea9241d82d41103776f", "message": "[MINOR] fix some tests to pass CI", "committedDate": "2020-08-06T21:12:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyODA3Mg==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466828072", "bodyText": "@bvaradar keeping this code actually. given now, we are adding support for MOR snapshot query, if we don't unset this, then if you do a RO query and then a snapshot query, then this will filter out all except latest base files. which will be problematic.\ncc @garyli1019 does this make sense?  Let me see if I can add a test case for this", "author": "vinothchandar", "createdAt": "2020-08-07T05:26:13Z", "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -132,11 +132,15 @@ class DefaultSource extends RelationProvider\n \n     log.info(\"Constructing hoodie (as parquet) data source with options :\" + optParams)\n     // simply return as a regular parquet relation\n-    DataSource.apply(\n+    val relation =  DataSource.apply(\n       sparkSession = sqlContext.sparkSession,\n       userSpecifiedSchema = Option(schema),\n       className = \"parquet\",\n       options = optParams)\n       .resolveRelation()\n+\n+    sqlContext.sparkContext.hadoopConfiguration.unset(\"mapreduce.input.pathFilter.class\")", "originalCommit": "d8beca97ef56b1fd1aac2ea9241d82d41103776f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyOTg1Ng==", "url": "https://github.com/apache/hudi/pull/1848#discussion_r466829856", "bodyText": "yes make sense. We unset this before the incremental query. When we only have two data source query type, it's fine to unset before running incremental query, but right now we have to unset this here. Still wondering why the local build is able to pass though...", "author": "garyli1019", "createdAt": "2020-08-07T05:33:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyODA3Mg=="}], "type": "inlineReview"}, {"oid": "60cec20ba14076e94f365269d2e68cf41fabc508", "url": "https://github.com/apache/hudi/commit/60cec20ba14076e94f365269d2e68cf41fabc508", "message": "Clean up test file name, add tests for mixed query type tests\n\n - We can now revert the change made in DefaultSource", "committedDate": "2020-08-07T06:35:08Z", "type": "commit"}]}