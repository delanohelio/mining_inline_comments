{"pr_number": 2275, "pr_title": "[HUDI-1354] Block updates and replace on file groups in clustering", "pr_createdAt": "2020-11-23T16:52:14Z", "pr_url": "https://github.com/apache/hudi/pull/2275", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA2MjU2Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533062562", "bodyText": "please avoid sleep in tests. It tends to cause flakiness. If we can write the test such that it waits til a certain condition is met (or times out), that would be much more preferable.", "author": "vinothchandar", "createdAt": "2020-12-01T04:17:20Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestCopyOnWriteActionExecutor.java", "diffHunk": "@@ -456,4 +479,95 @@ public void testBulkInsertRecords(String bulkInsertMode) throws Exception {\n   public void testBulkInsertRecordsWithGlobalSort(String bulkInsertMode) throws Exception {\n     testBulkInsertRecords(bulkInsertMode);\n   }\n+\n+  protected HoodieInstant createRequestedReplaceInstant(HoodieTableMetaClient metaClient, String clusterTime, List<FileSlice>[] fileSlices) throws IOException {\n+    HoodieClusteringPlan clusteringPlan =\n+        ClusteringUtils.createClusteringPlan(CLUSTERING_STRATEGY_CLASS, STRATEGY_PARAMS, fileSlices, Collections.emptyMap());\n+\n+    HoodieInstant clusteringInstant = new HoodieInstant(HoodieInstant.State.REQUESTED, HoodieTimeline.REPLACE_COMMIT_ACTION, clusterTime);\n+    HoodieRequestedReplaceMetadata requestedReplaceMetadata = HoodieRequestedReplaceMetadata.newBuilder()\n+        .setClusteringPlan(clusteringPlan).setOperationType(WriteOperationType.CLUSTER.name()).build();\n+    metaClient.getActiveTimeline().saveToPendingReplaceCommit(clusteringInstant, TimelineMetadataUtils.serializeRequestedReplaceMetadata(requestedReplaceMetadata));\n+    return clusteringInstant;\n+  }\n+\n+  @Test\n+  public void testUpdateRejectForClustering() throws Exception {\n+    Properties properties = new Properties();\n+    // set max bytes small can easy generate multi file group\n+    properties.setProperty(HoodieStorageConfig.PARQUET_FILE_MAX_BYTES, \"1024\");\n+    HoodieWriteConfig config = makeHoodieClientConfig(properties);\n+    String firstCommitTime = makeNewCommitTime();\n+    SparkRDDWriteClient writeClient = getHoodieWriteClient(config);\n+    writeClient.startCommitWithTime(firstCommitTime);\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    String partitionPath = \"2016/01/31\";\n+\n+    // 1. insert three record with two filegroup\n+    HoodieSparkCopyOnWriteTable table = (HoodieSparkCopyOnWriteTable) HoodieSparkTable.create(config, context, metaClient);\n+    // Get some records belong to the same partition (2016/01/31)\n+    String recordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":12}\";\n+    String recordStr2 = \"{\\\"_row_key\\\":\\\"8eb5b87b-1feu-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:20:41.415Z\\\",\\\"number\\\":100}\";\n+    String recordStr3 = \"{\\\"_row_key\\\":\\\"8eb5b87c-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+\n+    List<HoodieRecord> records = new ArrayList<>();\n+    RawTripTestPayload rowChange1 = new RawTripTestPayload(recordStr1);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange1.getRowKey(), rowChange1.getPartitionPath()), rowChange1));\n+    RawTripTestPayload rowChange2 = new RawTripTestPayload(recordStr2);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange2.getRowKey(), rowChange2.getPartitionPath()), rowChange2));\n+    RawTripTestPayload rowChange3 = new RawTripTestPayload(recordStr3);\n+    records.add(new HoodieRecord(new HoodieKey(rowChange3.getRowKey(), rowChange3.getPartitionPath()), rowChange3));\n+\n+    writeClient.insert(jsc.parallelize(records, 1), firstCommitTime);\n+    List<String> firstInsertFileGroupIds = table.getFileSystemView().getAllFileGroups(partitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    List<List<FileSlice>> firstInsertFileSlicesList = table.getFileSystemView().getAllFileGroups(partitionPath)\n+        .map(fileGroup -> fileGroup.getAllFileSlices().collect(Collectors.toList())).collect(Collectors.toList());\n+    assertEquals(2, firstInsertFileGroupIds.size());\n+    List<FileSlice>[] fileSlices = (List<FileSlice>[])firstInsertFileSlicesList.toArray(new List[firstInsertFileSlicesList.size()]);\n+\n+    // 2. generate clustering plan the filegroups\n+    String clusterTime1 = \"1\";\n+    createRequestedReplaceInstant(this.metaClient, clusterTime1, fileSlices);\n+\n+    // 3. insert one record with no updating reject exception\n+    String insertRecordStr4 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc40pp0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n+    RawTripTestPayload rowChange4 = new RawTripTestPayload(insertRecordStr4);\n+    HoodieRecord insertedRecord1 =\n+        new HoodieRecord(new HoodieKey(rowChange4.getRowKey(), rowChange4.getPartitionPath()), rowChange4);\n+    List<HoodieRecord> insertRecords = Arrays.asList(insertedRecord1);\n+    Thread.sleep(1000);\n+    String secondCommitTime = makeNewCommitTime();\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    writeClient.startCommitWithTime(secondCommitTime);\n+    List<WriteStatus> statuses = writeClient.upsert(jsc.parallelize(insertRecords), secondCommitTime).collect();\n+    assertEquals(1, statuses.size());\n+\n+    // 4. insert one record and update one record\n+    String insertRecordStr5 = \"{\\\"_row_key\\\":\\\"8eb5b87d-1fej-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":51}\";\n+    // We update the 1st record & add a new record\n+    String updateRecordStr1 = \"{\\\"_row_key\\\":\\\"8eb5b87a-1feh-4edd-87b4-6ec96dc405a0\\\",\"\n+        + \"\\\"time\\\":\\\"2016-01-31T03:16:41.415Z\\\",\\\"number\\\":15}\";\n+    RawTripTestPayload updateRowChanges1 = new RawTripTestPayload(updateRecordStr1);\n+    HoodieRecord updatedRecord1 = new HoodieRecord(\n+        new HoodieKey(updateRowChanges1.getRowKey(), updateRowChanges1.getPartitionPath()), updateRowChanges1);\n+    RawTripTestPayload rowChange5 = new RawTripTestPayload(insertRecordStr5);\n+    HoodieRecord insertedRecord2 =\n+        new HoodieRecord(new HoodieKey(rowChange5.getRowKey(), rowChange5.getPartitionPath()), rowChange5);\n+    List<HoodieRecord> updatedRecords = Arrays.asList(updatedRecord1, insertedRecord2);\n+    Thread.sleep(1000);", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM3ODMwMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r539378301", "bodyText": "okay", "author": "lw309637554", "createdAt": "2020-12-09T15:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzA2MjU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNDI5Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533104292", "bodyText": "I think we need to think more about the algorithm here.  For example,\n\nf1, f2, f3 are file groups in partition.\nAssume there is pending clustering on all file groups f1, f2, f3.\nf3 is a small file. So we buildProfile would assign inserts to f3.\napplying update strategy will throw error because f3 is included.\n\nInstead, we may want to change buildProfile to exclude file groups that are in pending clustering. So, new file f4 would be created in step#3 and ingestion can continue. This way inserts can continue without errors.", "author": "satishkotha", "createdAt": "2020-12-01T06:41:23Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -103,6 +104,9 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     if (isWorkloadProfileNeeded()) {\n       profile = new WorkloadProfile(buildProfile(inputRecordsRDD));\n       LOG.info(\"Workload profile :\" + profile);\n+      // apply clustering update strategy.", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTY3NTkwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r541675903", "bodyText": "this is a good case , for generate new file f4 , can exclude small files in clustering", "author": "lw309637554", "createdAt": "2020-12-12T17:16:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNDI5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjEzNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106135", "bodyText": "what happens if workload profile is not needed? Is there a better place to do this validation? Can we do this after tagLocation is done? If any of the records have location to files in pending clustering, we can throw error.", "author": "satishkotha", "createdAt": "2020-12-01T06:45:39Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -103,6 +104,9 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     if (isWorkloadProfileNeeded()) {", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTY3NzIwMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r541677201", "bodyText": "it is make sense. But now BaseSparkCommitActionExecutor.getUpsertPartitioner depend on profile, we can be consistent with getUpsertPartitioner.", "author": "lw309637554", "createdAt": "2020-12-12T17:18:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjM1NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106354", "bodyText": "Good idea by adding this interface to keep it generic. Please add javadoc.", "author": "satishkotha", "createdAt": "2020-12-01T06:46:20Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.WorkloadProfile;\n+\n+import java.util.List;\n+", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5MTYzMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r540291633", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-10T16:05:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjM1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjgyOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r533106828", "bodyText": "Since you are doing contains, is it better to change its type to Set?", "author": "satishkotha", "createdAt": "2020-12-01T06:47:39Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+\n+  @Override\n+  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n+    List<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()\n+        .map(entry -> Pair.of(entry.getLeft().getPartitionPath(), entry.getLeft().getFileId())).collect(Collectors.toList());\n+    if (partitionPathAndFileIds.size() == 0) {\n+      return;\n+    }\n+\n+    Set<Map.Entry<String, WorkloadStat>> partitionStatEntries = workloadProfile.getPartitionPathStatMap().entrySet();\n+    for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n+      for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n+              partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n+        String partitionPath = partitionStat.getKey();\n+        String fileId = updateLocEntry.getKey();\n+        if (partitionPathAndFileIds.contains(Pair.of(partitionPath, fileId))) {", "originalCommit": "f3fc86bbf9ec02e59b9b161b99f4ee2b13a33d3b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI5MTQ4OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r540291488", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-10T16:05:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNjgyOA=="}], "type": "inlineReview"}, {"oid": "0474898d32c45a079421bd37955c52dc86f7e406", "url": "https://github.com/apache/hudi/commit/0474898d32c45a079421bd37955c52dc86f7e406", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-13T17:08:58Z", "type": "forcePushed"}, {"oid": "7991ef3e11afad79983ca4feecd9412e51368faf", "url": "https://github.com/apache/hudi/commit/7991ef3e11afad79983ca4feecd9412e51368faf", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:17:24Z", "type": "forcePushed"}, {"oid": "777ada0feeac54133e0050eb32ffbc25f2c836ea", "url": "https://github.com/apache/hudi/commit/777ada0feeac54133e0050eb32ffbc25f2c836ea", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:20:44Z", "type": "forcePushed"}, {"oid": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "url": "https://github.com/apache/hudi/commit/0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:46:13Z", "type": "commit"}, {"oid": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "url": "https://github.com/apache/hudi/commit/0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-18T07:46:13Z", "type": "forcePushed"}, {"oid": "f6c48629439c491fffbade1e12b73761b29746b0", "url": "https://github.com/apache/hudi/commit/f6c48629439c491fffbade1e12b73761b29746b0", "message": "Merge branch 'master' into HUDI-1354-2", "committedDate": "2020-12-22T09:26:25Z", "type": "commit"}, {"oid": "f6c48629439c491fffbade1e12b73761b29746b0", "url": "https://github.com/apache/hudi/commit/f6c48629439c491fffbade1e12b73761b29746b0", "message": "Merge branch 'master' into HUDI-1354-2", "committedDate": "2020-12-22T09:26:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA0ODg0Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r546048843", "bodyText": "can you please add javadoc for class and properties in class. Looks like we will need to resolve conflicts with clustering PR", "author": "satishkotha", "createdAt": "2020-12-18T19:30:00Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+import org.apache.hudi.table.action.clustering.update.RejectUpdateStrategy;\n+import org.apache.hudi.table.action.clustering.update.UpdateStrategy;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {", "originalCommit": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4Nzk2Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547987963", "bodyText": "done  have merge master", "author": "lw309637554", "createdAt": "2020-12-23T14:35:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA0ODg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA1MTU0NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r546051544", "bodyText": "Why not collect Set of HoodieFileGroupId? I'd avoid Pair as much as possible because its not that easy to read.", "author": "satishkotha", "createdAt": "2020-12-18T19:35:52Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+\n+  @Override\n+  public void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile) {\n+    Set<Pair<String, String>> partitionPathAndFileIds = fileGroupsInPendingClustering.stream()", "originalCommit": "0f5e49e1a2f55fae5958a358c14284fa0a70c78a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODA3NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988074", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:35:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjA1MTU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MjAyNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547482024", "bodyText": "I may have mentioned this earlier, consider removing WorkloadProfile and send fileGroupsWithUpdates.\nI think sending taggedRecords as additional parameter will also be useful. In future, we may want to update tagged records location to a different fileId.", "author": "satishkotha", "createdAt": "2020-12-22T20:01:57Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.WorkloadProfile;\n+\n+import java.util.List;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public interface UpdateStrategy  {\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param workloadProfile workloadProfile have the records update info,\n+   *                       just like BaseSparkCommitActionExecutor.getUpsertPartitioner use it.\n+   */\n+  void apply(List<Pair<HoodieFileGroupId, HoodieInstant>> fileGroupsInPendingClustering, WorkloadProfile workloadProfile);", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODQ0MQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988441", "bodyText": "make sense , now taggedRecords as parameter , the update file groupid get from taggedRecords", "author": "lw309637554", "createdAt": "2020-12-23T14:36:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4MjAyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NDgwNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547484804", "bodyText": "Should we add a constructor to take engineContext (similar to clustering strategy). For some of the future strategies, we need engineContext. So it'd be helpful if we start with that.", "author": "satishkotha", "createdAt": "2020-12-22T20:08:24Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/clustering/update/RejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.clustering.update;\n+\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.WorkloadProfile;\n+import org.apache.hudi.table.WorkloadStat;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+public class RejectUpdateStrategy implements UpdateStrategy {\n+  private static final Logger LOG = LogManager.getLogger(RejectUpdateStrategy.class);\n+", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODYwOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988608", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:36:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NDgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NTY5MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547485690", "bodyText": "Maybe this can also be moved into UpdateStrategy? i.e., UpdateStrategy#filterSmallFiles(List smallFiles) removes small files  in pending clustering.", "author": "satishkotha", "createdAt": "2020-12-22T20:10:49Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +140,21 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    // get the in pending clustering fileId for each partition path", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Nzk4ODcwOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547988708", "bodyText": "done", "author": "lw309637554", "createdAt": "2020-12-23T14:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NTY5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQ4NjI0OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r547486248", "bodyText": "looks like Map<String, Set> makes more sense because we only do contains check?", "author": "satishkotha", "createdAt": "2020-12-22T20:12:14Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +140,21 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    // get the in pending clustering fileId for each partition path\n+    Map<String, List<String>>  partitionPathToInPendingClusteringFileId =", "originalCommit": "f6c48629439c491fffbade1e12b73761b29746b0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d2fb59a0c6193b2db03408abfc04ced18902e009", "url": "https://github.com/apache/hudi/commit/d2fb59a0c6193b2db03408abfc04ced18902e009", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T06:17:30Z", "type": "forcePushed"}, {"oid": "7b98a2a78e6d0dd8f7a790d0918c5ff35b8f54bd", "url": "https://github.com/apache/hudi/commit/7b98a2a78e6d0dd8f7a790d0918c5ff35b8f54bd", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T07:20:32Z", "type": "forcePushed"}, {"oid": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "url": "https://github.com/apache/hudi/commit/94a20f74484458fa4bdf45f25b2612a1d75fdb35", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T14:59:40Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDE5Mg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548030192", "bodyText": "use SparkRejectUpdateStrategy.class.getName?", "author": "leesf", "createdAt": "2020-12-23T16:05:43Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -73,9 +73,13 @@\n   public static final String CLUSTERING_TARGET_FILE_MAX_BYTES = CLUSTERING_STRATEGY_PARAM_PREFIX + \"target.file.max.bytes\";\n   public static final String DEFAULT_CLUSTERING_TARGET_FILE_MAX_BYTES = String.valueOf(1 * 1024 * 1024 * 1024L); // 1GB\n   \n-  // constants related to clustering that may be used by more than 1 strategy.\n+  // Constants related to clustering that may be used by more than 1 strategy.\n   public static final String CLUSTERING_SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n \n+  // When file groups is in clustering, need to handle the update to these file groups. Default strategy just reject the update\n+  public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n+  public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0MzcyMg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548043722", "bodyText": "SparkRejectUpdateStrategy is in hudi-spark, hudi-client-common not depend hudi-spark", "author": "lw309637554", "createdAt": "2020-12-23T16:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDE5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMDkyNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548030925", "bodyText": "please use\n`/**\n\nexclude ....\n*/` ?", "author": "leesf", "createdAt": "2020-12-23T16:07:09Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);\n+\n+  public HoodieEngineContext getEngineContext() {\n+    return engineContext;\n+  }\n+\n+  // exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMTgxOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548031819", "bodyText": "If some file groups have updates or inserts, just throws exception now ?", "author": "leesf", "createdAt": "2020-12-23T16:09:09Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Update trategy Strategy based on following.\n+ * if some file group have update write ,just throw exception now", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMjIwOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548032209", "bodyText": "ditto", "author": "leesf", "createdAt": "2020-12-23T16:10:06Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -129,6 +130,16 @@ private int addUpdateBucket(String partitionPath, String fileIdHint) {\n     return bucket;\n   }\n \n+  // get the in pending clustering fileId for each partition path", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMzc0MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548033740", "bodyText": "files update are not supported in pending clustering currently?", "author": "leesf", "createdAt": "2020-12-23T16:13:46Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +151,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0Njk3OA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548046978", "bodyText": "insert to small files update will conflict to clustering. resolved this case:\nf1, f2, f3 are file groups in partition.\nAssume there is pending clustering on all file groups f1, f2, f3.\nf3 is a small file. So we buildProfile would assign inserts to f3.\napplying update strategy will throw error because f3 is included.\nInstead, we may want to change buildProfile to exclude file groups that are in pending clustering. So, new file f4 would be created in step#3 and ingestion can continue. This way inserts can continue without errors.", "author": "lw309637554", "createdAt": "2020-12-23T16:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzMzc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTIyNw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035227", "bodyText": "moving to ClusteringUtils would be more proper?", "author": "leesf", "createdAt": "2020-12-23T16:17:07Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);\n+\n+  public HoodieEngineContext getEngineContext() {\n+    return engineContext;\n+  }\n+\n+  // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+  public static List<SmallFile> filterSmallFiles(final Set<String> pendingClusteringFileGroupsId, final List<SmallFile> smallFiles) {", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA0ODAyNQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548048025", "bodyText": "filterSmallFiles is  strongly related with  clustering update strategy.  In updateStrategy.java will be better", "author": "lw309637554", "createdAt": "2020-12-23T16:45:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTIyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTYyMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035621", "bodyText": "extra space", "author": "leesf", "createdAt": "2020-12-23T16:17:59Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +151,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId,  partitionSmallFilesMap.get(partitionPath));", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODAzNTk0Nw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548035947", "bodyText": "parameters put into two lines?", "author": "leesf", "createdAt": "2020-12-23T16:18:45Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -681,6 +697,72 @@ private void assertActualAndExpectedPartitionPathRecordKeyMatches(Set<Pair<Strin\n     }\n   }\n \n+  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime, Integer recordNum, int expectStatueSize) {", "originalCommit": "94a20f74484458fa4bdf45f25b2612a1d75fdb35", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "url": "https://github.com/apache/hudi/commit/df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-23T16:53:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMzNjAxMg==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548336012", "bodyText": "why is this a static method? If clustering is not enabled on a table, this filtering is expensive. Should we add 'default' strategy for that case? The default strategy would not do any filtering.\nIf clustering is enabled, we would use RejectUpdateStrategy and that does filtering.", "author": "satishkotha", "createdAt": "2020-12-24T00:57:41Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -140,11 +154,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n     Map<String, List<SmallFile>> partitionSmallFilesMap =\n         getSmallFilesForPartitions(new ArrayList<String>(partitionPaths), context);\n \n+    Map<String, Set<String>> partitionPathToPendingClusteringFileGroupsId = getPartitionPathToPendingClusteringFileGroupsId();\n+\n     for (String partitionPath : partitionPaths) {\n       WorkloadStat pStat = profile.getWorkloadStat(partitionPath);\n       if (pStat.getNumInserts() > 0) {\n+        // exclude the small file in pending clustering, because in pending clustering file not support update now.\n+        Set<String> inPendingClusteringFileId = partitionPathToPendingClusteringFileGroupsId.getOrDefault(partitionPath, Collections.emptySet());\n+        List<SmallFile> smallFiles = UpdateStrategy.filterSmallFiles(inPendingClusteringFileId, partitionSmallFilesMap.get(partitionPath));", "originalCommit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM1MzMzOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548353338", "bodyText": "thanks\n1\u3001\"why is this a static method?\":   because transform updatestrategy into UpsertPartitioner  may be modify the UpsertPartitioner construct.  I will add updatestrategy param to UpsertPartitioner construct\n2\u3001\" If clustering is not enabled on a table, this filtering is expensive. Should we add 'default' strategy for that case? The default strategy would not do any filtering.\"  make sense", "author": "lw309637554", "createdAt": "2020-12-24T02:25:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMzNjAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2NjQ4OQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548366489", "bodyText": "can you change it to retrun taggedRecords? You can just simply return same taggedRecordsRDD. This will be helpful later on if we want to store updates in a different file group and later reconcile.", "author": "satishkotha", "createdAt": "2020-12-24T03:30:12Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.action.commit.SmallFile;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I>  {\n+\n+  private final HoodieEngineContext engineContext;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext) {\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param fileGroupsInPendingClustering\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                        future can update tagged records location to a different fileId.\n+   */\n+  public abstract void apply(Set<HoodieFileGroupId> fileGroupsInPendingClustering, I taggedRecordsRDD);", "originalCommit": "df57a7fc38dce9bc99cbabdd0fa9116ed9a61af8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3MjA1Mw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548372053", "bodyText": "ok", "author": "lw309637554", "createdAt": "2020-12-24T03:59:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM2NjQ4OQ=="}], "type": "inlineReview"}, {"oid": "4f7de715e708251303691573dcd05b1a5c24c9d8", "url": "https://github.com/apache/hudi/commit/4f7de715e708251303691573dcd05b1a5c24c9d8", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T07:29:36Z", "type": "forcePushed"}, {"oid": "e79787cf367edfd848df3b1b79894ae103edd910", "url": "https://github.com/apache/hudi/commit/e79787cf367edfd848df3b1b79894ae103edd910", "message": "[HUDI-1354] Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T07:36:36Z", "type": "forcePushed"}, {"oid": "0c55e06ac0c074314175f7bff07fc98f5bb3704a", "url": "https://github.com/apache/hudi/commit/0c55e06ac0c074314175f7bff07fc98f5bb3704a", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T08:55:56Z", "type": "forcePushed"}, {"oid": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "url": "https://github.com/apache/hudi/commit/93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-24T10:11:09Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwMzg2MA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548903860", "bodyText": "Change property to hoodie.clustering.async.enabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:17:58Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -73,9 +73,17 @@\n   public static final String CLUSTERING_TARGET_FILE_MAX_BYTES = CLUSTERING_STRATEGY_PARAM_PREFIX + \"target.file.max.bytes\";\n   public static final String DEFAULT_CLUSTERING_TARGET_FILE_MAX_BYTES = String.valueOf(1 * 1024 * 1024 * 1024L); // 1GB\n   \n-  // constants related to clustering that may be used by more than 1 strategy.\n+  // Constants related to clustering that may be used by more than 1 strategy.\n   public static final String CLUSTERING_SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n \n+  // When file groups is in clustering, need to handle the update to these file groups. Default strategy just reject the update\n+  public static final String CLUSTERING_UPDATES_STRATEGY_PROP = \"hoodie.clustering.updates.strategy\";\n+  public static final String DEFAULT_CLUSTERING_UPDATES_STRATEGY = \"org.apache.hudi.client.clustering.update.strategy.SparkRejectUpdateStrategy\";\n+\n+  // Async clustering\n+  public static final String ASYNC_CLUSTERING_ENABLE_OPT_KEY = \"hoodie.clustering.async\";", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwMzkyMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548903921", "bodyText": "nit: isClusteringEnabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:18:38Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -394,6 +395,15 @@ public boolean isInlineClustering() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n+  public boolean isAsyncClustering() {\n+    return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+  }\n+\n+  public boolean isClusteringEnable() {", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDAwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904003", "bodyText": "nit: isAsyncClusteringEnabled?", "author": "satishkotha", "createdAt": "2020-12-25T19:19:19Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -394,6 +395,15 @@ public boolean isInlineClustering() {\n     return Boolean.parseBoolean(props.getProperty(HoodieClusteringConfig.INLINE_CLUSTERING_PROP));\n   }\n \n+  public boolean isAsyncClustering() {", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDI3NQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904275", "bodyText": "What is the reason for collecting only fileId? HoodieFileGroupId will be more accurate because there may be two partitions with same fileId (Unlikely, but can happen). FileGroupId supports equals/hashcode, so it should not be a big change to do this on HoodieFileGroupId.", "author": "satishkotha", "createdAt": "2020-12-25T19:23:47Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import scala.Tuple2;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Update strategy based on following.\n+ * if some file group have update record, just throw exception now\n+ */\n+public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n+\n+  public SparkRejectUpdateStrategy(HoodieSparkEngineContext engineContext, HashSet<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    super(engineContext, fileGroupsInPendingClustering);\n+  }\n+\n+  private List<Tuple2<String, String>> getPartitionPathToFileGroupsIdWithUpdates(JavaRDD<HoodieRecord<T>> inputRecords) {\n+    List<Tuple2<String, String>> fileGroupsIdWithUpdates = inputRecords\n+        .filter(record -> record.getCurrentLocation() != null)\n+        .map(record -> new Tuple2<>(record.getPartitionPath(), record.getCurrentLocation().getFileId())).distinct().collect();\n+    return fileGroupsIdWithUpdates;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> handleRecordUpdate(JavaRDD<HoodieRecord<T>> taggedRecordsRDD) {\n+    Set<String> pendingClusteringFileGroupIds =", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkzNDIzOA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548934238", "bodyText": "okay", "author": "lw309637554", "createdAt": "2020-12-26T02:37:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDI3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDYwMA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904600", "bodyText": "the class 'org.apache.hudi.DefaultClusteringStrategy' doesnt seem to exist. could you explain what this is doing? can we use default from clustering config?", "author": "satishkotha", "createdAt": "2020-12-25T19:27:39Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -110,6 +120,13 @@\n public class TestHoodieClientOnCopyOnWriteStorage extends HoodieClientTestBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private static final String CLUSTERING_STRATEGY_CLASS = \"org.apache.hudi.DefaultClusteringStrategy\";", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDg2NA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548904864", "bodyText": "this looks out of place. move this to line 742?", "author": "satishkotha", "createdAt": "2020-12-25T19:31:22Z", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -681,6 +698,75 @@ private void assertActualAndExpectedPartitionPathRecordKeyMatches(Set<Pair<Strin\n     }\n   }\n \n+  private Pair<List<WriteStatus>, List<HoodieRecord>> insertBatchRecords(SparkRDDWriteClient client, String commitTime,\n+                                                                         Integer recordNum, int expectStatueSize) {\n+    client.startCommitWithTime(commitTime);\n+    List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime, recordNum);\n+    JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 1);\n+    List<WriteStatus> statuses = client.upsert(insertRecordsRDD1, commitTime).collect();\n+    assertNoWriteErrors(statuses);\n+    assertEquals(expectStatueSize, statuses.size(), \"check expect statue size.\");\n+    return Pair.of(statuses, inserts1);\n+  }\n+\n+  @Test\n+  public void testUpdateRejectForClustering() throws IOException {\n+    final String testPartitionPath = \"2016/09/26\";\n+    dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n+    Properties props = new Properties();\n+    props.setProperty(\"hoodie.clustering.async\", \"true\");\n+    HoodieWriteConfig config = getSmallInsertWriteConfig(100,\n+        TRIP_EXAMPLE_SCHEMA, dataGen.getEstimatedFileSizeInBytes(150), props);\n+    SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n+    HoodieSparkCopyOnWriteTable table = (HoodieSparkCopyOnWriteTable) HoodieSparkTable.create(config, context, metaClient);\n+\n+    //1. insert to generate 2 file group\n+    String commitTime1 = \"001\";\n+    Pair<List<WriteStatus>, List<HoodieRecord>> upsertResult = insertBatchRecords(client, commitTime1, 600, 2);\n+    List<WriteStatus> statuses = upsertResult.getKey();\n+    List<HoodieRecord> inserts1 = upsertResult.getValue();\n+    List<String> fileGroupIds1 = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    assertEquals(2, fileGroupIds1.size());\n+\n+    // 2. generate clustering plan for fileGroupIds1 file groups\n+    String commitTime2 = \"002\";\n+    List<List<FileSlice>> firstInsertFileSlicesList = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getAllFileSlices().collect(Collectors.toList())).collect(Collectors.toList());\n+    List<FileSlice>[] fileSlices = (List<FileSlice>[])firstInsertFileSlicesList.toArray(new List[firstInsertFileSlicesList.size()]);\n+    createRequestedReplaceInstant(this.metaClient, commitTime2, fileSlices);\n+\n+    // 3. insert one record with no updating reject exception, and not merge the small file, just generate a new file group\n+    String commitTime3 = \"003\";\n+    statuses = insertBatchRecords(client, commitTime3, 1, 1).getKey();\n+    List<String> fileGroupIds2 = table.getFileSystemView().getAllFileGroups(testPartitionPath)\n+        .map(fileGroup -> fileGroup.getFileGroupId().getFileId()).collect(Collectors.toList());\n+    assertEquals(3, fileGroupIds2.size());\n+\n+\n+    // 4. update one record for the clustering two file groups, throw reject update exception\n+    String commitTime4 = \"004\";\n+    client.startCommitWithTime(commitTime4);\n+    List<HoodieRecord> insertsAndUpdates3 = new ArrayList<>();\n+    insertsAndUpdates3.addAll(dataGen.generateUpdates(commitTime4, inserts1));\n+    assertNoWriteErrors(statuses);", "originalCommit": "93751e46cb2e730713e300f6c6bffa2a7a1a51a8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkzNDI3MQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r548934271", "bodyText": "have remove it , because in insertBatchRecords have do it .", "author": "lw309637554", "createdAt": "2020-12-26T02:38:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNDg2NA=="}], "type": "inlineReview"}, {"oid": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "url": "https://github.com/apache/hudi/commit/5372851ab6bd08c19faa7fc670fd00c8df9624b8", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-26T02:36:47Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NjcxMQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549056711", "bodyText": "This looks like will be applicable for an RDD of records, if yes, rename this to 'handleUpdate'", "author": "n3nash", "createdAt": "2020-12-27T02:52:02Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+\n+import java.util.Set;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n+\n+  protected final HoodieEngineContext engineContext;\n+  protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    this.engineContext = engineContext;\n+    this.fileGroupsInPendingClustering = fileGroupsInPendingClustering;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param taggedRecordsRDD the records will write, can get the update record,\n+   *                         future can update tagged records location to a different fileId.\n+   * @return the recordsRDD strategy updated\n+   */\n+  public abstract I handleRecordUpdate(I taggedRecordsRDD);", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1Njc0Ng==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549056746", "bodyText": "Rename to getGroupIdsWithUpdate", "author": "n3nash", "createdAt": "2020-12-27T02:52:42Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * Update strategy based on following.\n+ * if some file group have update record, just throw exception now\n+ */\n+public class SparkRejectUpdateStrategy<T extends HoodieRecordPayload<T>> extends UpdateStrategy<T, JavaRDD<HoodieRecord<T>>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkRejectUpdateStrategy.class);\n+\n+  public SparkRejectUpdateStrategy(HoodieSparkEngineContext engineContext, HashSet<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    super(engineContext, fileGroupsInPendingClustering);\n+  }\n+\n+  private List<HoodieFileGroupId> getFileGroupIdsWithRecordUpdate(JavaRDD<HoodieRecord<T>> inputRecords) {", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzIxNw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057217", "bodyText": "rename this to clusteringHandleUpdate as well, records is implied", "author": "n3nash", "createdAt": "2020-12-27T02:59:02Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/BaseSparkCommitActionExecutor.java", "diffHunk": "@@ -88,6 +93,18 @@ public BaseSparkCommitActionExecutor(HoodieEngineContext context,\n     super(context, config, table, instantTime, operationType, extraMetadata);\n   }\n \n+  private JavaRDD<HoodieRecord<T>> clusteringHandleRecordsUpdate(JavaRDD<HoodieRecord<T>> inputRecordsRDD) {", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzMwMw==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057303", "bodyText": "Nit -> remove the \"just throw exception now\" to \"throw exception\"", "author": "n3nash", "createdAt": "2020-12-27T03:00:21Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/update/strategy/SparkRejectUpdateStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.update.strategy;\n+\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.exception.HoodieClusteringUpdateException;\n+import org.apache.hudi.table.action.cluster.strategy.UpdateStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.HashSet;\n+import java.util.List;\n+\n+/**\n+ * Update strategy based on following.", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzQwNA==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057404", "bodyText": "Nit : for comments, please replace with \"the records to write, tagged with target file id\"", "author": "n3nash", "createdAt": "2020-12-27T03:02:40Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/UpdateStrategy.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+\n+import java.util.Set;\n+\n+/**\n+ * When file groups in clustering, write records to these file group need to check.\n+ */\n+public abstract class UpdateStrategy<T extends HoodieRecordPayload<T>, I> {\n+\n+  protected final HoodieEngineContext engineContext;\n+  protected Set<HoodieFileGroupId> fileGroupsInPendingClustering;\n+\n+  protected UpdateStrategy(HoodieEngineContext engineContext, Set<HoodieFileGroupId> fileGroupsInPendingClustering) {\n+    this.engineContext = engineContext;\n+    this.fileGroupsInPendingClustering = fileGroupsInPendingClustering;\n+  }\n+\n+  /**\n+   * Check the update records to the file group in clustering.\n+   * @param taggedRecordsRDD the records will write, can get the update record,", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1NzUzOQ==", "url": "https://github.com/apache/hudi/pull/2275#discussion_r549057539", "bodyText": "Please Change to : \"exclude small file handling for clustering since update path is not supported\"", "author": "n3nash", "createdAt": "2020-12-27T03:04:28Z", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -129,6 +129,34 @@ private int addUpdateBucket(String partitionPath, String fileIdHint) {\n     return bucket;\n   }\n \n+  /**\n+   * Get the in pending clustering fileId for each partition path.\n+   * @return partition path to pending clustering file groups id\n+   */\n+  private Map<String, Set<String>> getPartitionPathToPendingClusteringFileGroupsId() {\n+    Map<String, Set<String>>  partitionPathToInPendingClusteringFileId =\n+        table.getFileSystemView().getFileGroupsInPendingClustering()\n+            .map(fileGroupIdAndInstantPair ->\n+                Pair.of(fileGroupIdAndInstantPair.getKey().getPartitionPath(), fileGroupIdAndInstantPair.getKey().getFileId()))\n+            .collect(Collectors.groupingBy(Pair::getKey, Collectors.mapping(Pair::getValue, Collectors.toSet())));\n+    return partitionPathToInPendingClusteringFileId;\n+  }\n+\n+  /**\n+   * Exclude the small file in pending clustering, because in pending clustering file not support update now.", "originalCommit": "5372851ab6bd08c19faa7fc670fd00c8df9624b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "url": "https://github.com/apache/hudi/commit/589e9b10c04593c685001874ea3ebcd066ba4bb9", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-27T11:47:49Z", "type": "commit"}, {"oid": "589e9b10c04593c685001874ea3ebcd066ba4bb9", "url": "https://github.com/apache/hudi/commit/589e9b10c04593c685001874ea3ebcd066ba4bb9", "message": "[HUDI-1354]  Block updates and replace on file groups in clustering", "committedDate": "2020-12-27T11:47:49Z", "type": "forcePushed"}]}