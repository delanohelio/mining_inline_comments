{"pr_number": 2300, "pr_title": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "pr_createdAt": "2020-12-05T13:02:29Z", "pr_url": "https://github.com/apache/hudi/pull/2300", "timeline": [{"oid": "d4b29b9a98295bff1c57f0c7c72d5f7dd9223e3d", "url": "https://github.com/apache/hudi/commit/d4b29b9a98295bff1c57f0c7c72d5f7dd9223e3d", "message": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "committedDate": "2020-12-05T15:33:19Z", "type": "forcePushed"}, {"oid": "db85097aff54d3f0a6177150cde405b01e3af977", "url": "https://github.com/apache/hudi/commit/db85097aff54d3f0a6177150cde405b01e3af977", "message": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "committedDate": "2020-12-05T15:35:46Z", "type": "forcePushed"}, {"oid": "a0b89bef67bb5d0a3ed20f02af8e10d8bd65a16e", "url": "https://github.com/apache/hudi/commit/a0b89bef67bb5d0a3ed20f02af8e10d8bd65a16e", "message": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "committedDate": "2020-12-19T12:03:42Z", "type": "forcePushed"}, {"oid": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "url": "https://github.com/apache/hudi/commit/7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "message": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "committedDate": "2020-12-19T13:04:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njk5MTk3OA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r546991978", "bodyText": "while we are at it. fix this log statement? New AppendHandle for ..", "author": "vinothchandar", "createdAt": "2020-12-21T23:58:23Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -125,19 +129,26 @@ private void init(HoodieRecord record) {\n       Option<FileSlice> fileSlice = rtView.getLatestFileSlice(partitionPath, fileId);\n       // Set the base commit time as the current instantTime for new inserts into log files\n       String baseInstantTime;\n+      String baseFile = \"\";\n+      List<String> logFiles = new ArrayList<>();\n       if (fileSlice.isPresent()) {\n         baseInstantTime = fileSlice.get().getBaseInstantTime();\n+        baseFile = fileSlice.get().getBaseFile().map(BaseFile::getFileName).orElse(\"\");\n+        logFiles = fileSlice.get().getLogFiles().map(HoodieLogFile::getFileName).collect(Collectors.toList());\n       } else {\n         baseInstantTime = instantTime;\n         // This means there is no base data file, start appending to a new log file\n         fileSlice = Option.of(new FileSlice(partitionPath, baseInstantTime, this.fileId));\n         LOG.info(\"New InsertHandle for partition :\" + partitionPath);", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0MTAzMw==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547141033", "bodyText": "so this sets the baseFile and the logFiles that existed before this delta commit?", "author": "vinothchandar", "createdAt": "2020-12-22T08:37:29Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -125,19 +129,26 @@ private void init(HoodieRecord record) {\n       Option<FileSlice> fileSlice = rtView.getLatestFileSlice(partitionPath, fileId);\n       // Set the base commit time as the current instantTime for new inserts into log files\n       String baseInstantTime;\n+      String baseFile = \"\";\n+      List<String> logFiles = new ArrayList<>();\n       if (fileSlice.isPresent()) {\n         baseInstantTime = fileSlice.get().getBaseInstantTime();\n+        baseFile = fileSlice.get().getBaseFile().map(BaseFile::getFileName).orElse(\"\");\n+        logFiles = fileSlice.get().getLogFiles().map(HoodieLogFile::getFileName).collect(Collectors.toList());\n       } else {\n         baseInstantTime = instantTime;\n         // This means there is no base data file, start appending to a new log file\n         fileSlice = Option.of(new FileSlice(partitionPath, baseInstantTime, this.fileId));\n         LOG.info(\"New InsertHandle for partition :\" + partitionPath);\n       }\n-      writeStatus.getStat().setPrevCommit(baseInstantTime);\n+      HoodieDeltaWriteStat deltaWriteStat = (HoodieDeltaWriteStat) writeStatus.getStat();\n+      deltaWriteStat.setPrevCommit(baseInstantTime);\n       writeStatus.setFileId(fileId);\n       writeStatus.setPartitionPath(partitionPath);\n-      writeStatus.getStat().setPartitionPath(partitionPath);\n-      writeStatus.getStat().setFileId(fileId);\n+      deltaWriteStat.setPartitionPath(partitionPath);\n+      deltaWriteStat.setFileId(fileId);\n+      deltaWriteStat.setBaseFile(baseFile);", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4NDkwNw==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547284907", "bodyText": "right, since we got this information from the flieSlice", "author": "garyli1019", "createdAt": "2020-12-22T13:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0MTAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0MzEwMw==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547143103", "bodyText": "in general., naming these variables with some context with some documentation, would help readability of this file.\ne.g initialLogVersion ,  filePath -> logFilePathWritten etc", "author": "vinothchandar", "createdAt": "2020-12-22T08:41:48Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -87,6 +88,9 @@\n   private long averageRecordSize = 0;\n   private HoodieLogFile currentLogFile;\n   private Writer writer;\n+  private String filePath = \"null\";\n+  private int logVersion = 0;", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NDAyMA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547144020", "bodyText": "this offset would still point to the file at which the write started?", "author": "vinothchandar", "createdAt": "2020-12-22T08:43:39Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -258,20 +265,32 @@ public WriteStatus close() {\n       // flush any remaining records to disk\n       doAppend(header);\n \n+      String latestLogFile = \"\";\n       if (writer != null) {\n         sizeInBytes = writer.getCurrentSize();\n+        latestLogFile = writer.getLogFile().getFileName();\n+        filePath = partitionPath.length() == 0 ? new Path(latestLogFile).toString()\n+            : new Path(partitionPath, latestLogFile).toString();\n+        logVersion = writer.getLogFile().getLogVersion();\n         writer.close();\n       }\n \n-      HoodieWriteStat stat = writeStatus.getStat();\n+      HoodieDeltaWriteStat stat = (HoodieDeltaWriteStat) writeStatus.getStat();\n       stat.setFileId(this.fileId);\n+      stat.setPath(this.filePath);\n+      stat.setLogVersion(logVersion);\n+      stat.setLogOffset(logOffset);", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4NzMyNQ==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547287325", "bodyText": "this offset should be 0 if we write to a new log file, but the file size of the existing log file if we do append. Currently this offset seems not working because I am don't see anywhere this can get the log file size.", "author": "garyli1019", "createdAt": "2020-12-22T13:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NDAyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NDU2Mw==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547144563", "bodyText": "why is this change needed?", "author": "vinothchandar", "createdAt": "2020-12-22T08:44:41Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -303,6 +322,7 @@ private Writer createLogWriter(Option<FileSlice> fileSlice, String baseCommitTim\n         .onParentPath(FSUtils.getPartitionPath(hoodieTable.getMetaClient().getBasePath(), partitionPath))\n         .withFileId(fileId).overBaseCommit(baseCommitTime)\n         .withLogVersion(latestLogFile.map(HoodieLogFile::getLogVersion).orElse(HoodieLogFile.LOGFILE_BASE_VERSION))\n+        .withFileSize(latestLogFile.map(HoodieLogFile::getFileSize).orElse(0L))", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI5NDc3Ng==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547294776", "bodyText": "trying to store the offset in the writer, so this will be sync with the log version. when the log version being changed, we can change the offset as well.", "author": "garyli1019", "createdAt": "2020-12-22T14:03:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NDU2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI5NjYzMg==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547296632", "bodyText": "looks like I was wrong, in the picture, the log version increased but the offset is not 0. Will fix this", "author": "garyli1019", "createdAt": "2020-12-22T14:06:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NDU2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NTQxNQ==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547145415", "bodyText": "wht if we had rolled over the log files in between? i.e created more than one new log file during write. I think we should add to the list of log files, during every rollOver, not just at close", "author": "vinothchandar", "createdAt": "2020-12-22T08:46:29Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -258,20 +265,32 @@ public WriteStatus close() {\n       // flush any remaining records to disk\n       doAppend(header);\n \n+      String latestLogFile = \"\";\n       if (writer != null) {\n         sizeInBytes = writer.getCurrentSize();\n+        latestLogFile = writer.getLogFile().getFileName();\n+        filePath = partitionPath.length() == 0 ? new Path(latestLogFile).toString()\n+            : new Path(partitionPath, latestLogFile).toString();\n+        logVersion = writer.getLogFile().getLogVersion();\n         writer.close();\n       }\n \n-      HoodieWriteStat stat = writeStatus.getStat();\n+      HoodieDeltaWriteStat stat = (HoodieDeltaWriteStat) writeStatus.getStat();\n       stat.setFileId(this.fileId);\n+      stat.setPath(this.filePath);\n+      stat.setLogVersion(logVersion);\n+      stat.setLogOffset(logOffset);\n       stat.setNumWrites(recordsWritten);\n       stat.setNumUpdateWrites(updatedRecordsWritten);\n       stat.setNumInserts(insertRecordsWritten);\n       stat.setNumDeletes(recordsDeleted);\n       stat.setTotalWriteBytes(estimatedNumberOfBytesWritten);\n       stat.setFileSizeInBytes(sizeInBytes);\n       stat.setTotalWriteErrors(writeStatus.getTotalErrorRecords());\n+      // update total log file list if the latest log file was new\n+      if (!stat.getLogFiles().contains(latestLogFile)) {", "originalCommit": "7f375aaf12e0c070c7f9cebe21dc25eb5a2894f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI4ODcyNg==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r547288726", "bodyText": "Not quite sure about the rollover case. Is that possible that two log files will be written in one commit? If we will write two log files, then the path should contain 2 files as well?", "author": "garyli1019", "createdAt": "2020-12-22T13:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE0NTQxNQ=="}], "type": "inlineReview"}, {"oid": "a6c03311cab24d39908937ab5e0372dfd6c5403d", "url": "https://github.com/apache/hudi/commit/a6c03311cab24d39908937ab5e0372dfd6c5403d", "message": "[HUDI-1434] fix incorrect log file path in HoodieWriteStat", "committedDate": "2020-12-26T14:56:31Z", "type": "commit"}, {"oid": "9c25633033caed35cb45d0b9e188fa86b0ef1ca0", "url": "https://github.com/apache/hudi/commit/9c25633033caed35cb45d0b9e188fa86b0ef1ca0", "message": "HoodieWriteHandle#close() returns a list of WriteStatus objs", "committedDate": "2020-12-26T14:56:32Z", "type": "commit"}, {"oid": "34d09b7616c1b386a1bef88f07cc39c6a48bc414", "url": "https://github.com/apache/hudi/commit/34d09b7616c1b386a1bef88f07cc39c6a48bc414", "message": "Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size, consistent with other handles", "committedDate": "2020-12-26T15:05:15Z", "type": "forcePushed"}, {"oid": "2930bffaaf5bf1083f5a8e69a4883e28b8c3a9a6", "url": "https://github.com/apache/hudi/commit/2930bffaaf5bf1083f5a8e69a4883e28b8c3a9a6", "message": "Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size, consistent with other handles", "committedDate": "2020-12-26T15:07:11Z", "type": "forcePushed"}, {"oid": "207b04671876da7a64255459de00baa6a8d46477", "url": "https://github.com/apache/hudi/commit/207b04671876da7a64255459de00baa6a8d46477", "message": "Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size, consistent with other handles", "committedDate": "2020-12-26T15:19:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA4OTAxOA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549089018", "bodyText": "I am seeing the log version start from 2 in my local test. I guess it's probably we removed the write token here and initializing the token will increment the log version by 1. The HoodieLogFile.LOGFILE_BASE_VERSION is 1.", "author": "garyli1019", "createdAt": "2020-12-27T09:30:03Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -303,8 +404,8 @@ private Writer createLogWriter(Option<FileSlice> fileSlice, String baseCommitTim\n         .onParentPath(FSUtils.getPartitionPath(hoodieTable.getMetaClient().getBasePath(), partitionPath))\n         .withFileId(fileId).overBaseCommit(baseCommitTime)\n         .withLogVersion(latestLogFile.map(HoodieLogFile::getLogVersion).orElse(HoodieLogFile.LOGFILE_BASE_VERSION))\n+        .withFileSize(latestLogFile.map(HoodieLogFile::getFileSize).orElse(0L))\n         .withSizeThreshold(config.getLogFileMaxSize()).withFs(fs)\n-        .withLogWriteToken(latestLogFile.map(x -> FSUtils.getWriteTokenFromLogPath(x.getPath())).orElse(writeToken))", "originalCommit": "207b04671876da7a64255459de00baa6a8d46477", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUwOTczNg==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549509736", "bodyText": "I will fix this. should not happen", "author": "vinothchandar", "createdAt": "2020-12-28T22:47:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA4OTAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1ODYwMA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549058600", "bodyText": "Will this offset contain the offset of the last log file the append handle writes to ?", "author": "n3nash", "createdAt": "2020-12-27T03:18:41Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -203,31 +213,136 @@ private void init(HoodieRecord record) {\n     return Option.empty();\n   }\n \n+  private void initNewStatus() {\n+    HoodieDeltaWriteStat prevStat = (HoodieDeltaWriteStat) this.writeStatus.getStat();\n+    // Make a new write status and copy basic fields over.\n+    HoodieDeltaWriteStat stat = new HoodieDeltaWriteStat();\n+    stat.setFileId(fileId);\n+    stat.setPartitionPath(partitionPath);\n+    stat.setPrevCommit(prevStat.getPrevCommit());\n+    stat.setBaseFile(prevStat.getBaseFile());\n+    stat.setLogFiles(new ArrayList<>(prevStat.getLogFiles()));\n+\n+    this.writeStatus = (WriteStatus) ReflectionUtils.loadClass(config.getWriteStatusClassName(),\n+        !hoodieTable.getIndex().isImplicitWithStorage(), config.getWriteStatusFailureFraction());\n+    this.writeStatus.setFileId(fileId);\n+    this.writeStatus.setPartitionPath(partitionPath);\n+    this.writeStatus.setStat(stat);\n+  }\n+\n+  private String makeFilePath(HoodieLogFile logFile) {\n+    return partitionPath.length() == 0\n+        ? new Path(logFile.getFileName()).toString()\n+        : new Path(partitionPath, logFile.getFileName()).toString();\n+  }\n+\n+  private void resetWriteCounts() {\n+    recordsWritten = 0;\n+    updatedRecordsWritten = 0;\n+    insertRecordsWritten = 0;\n+    recordsDeleted = 0;\n+  }\n+\n+  private void updateWriteCounts(HoodieDeltaWriteStat stat, AppendResult result) {\n+    stat.setNumWrites(recordsWritten);\n+    stat.setNumUpdateWrites(updatedRecordsWritten);\n+    stat.setNumInserts(insertRecordsWritten);\n+    stat.setNumDeletes(recordsDeleted);\n+    stat.setTotalWriteBytes(result.size());\n+  }\n+\n+  private void accumulateWriteCounts(HoodieDeltaWriteStat stat, AppendResult result) {\n+    stat.setNumWrites(stat.getNumWrites() + recordsWritten);\n+    stat.setNumUpdateWrites(stat.getNumUpdateWrites() + updatedRecordsWritten);\n+    stat.setNumInserts(stat.getNumInserts() + insertRecordsWritten);\n+    stat.setNumDeletes(stat.getNumDeletes() + recordsDeleted);\n+    stat.setTotalWriteBytes(stat.getTotalWriteBytes() + result.size());\n+  }\n+\n+  private void updateWriteStat(HoodieDeltaWriteStat stat, AppendResult result) {\n+    stat.setPath(makeFilePath(result.logFile()));\n+    stat.setLogOffset(result.offset());", "originalCommit": "207b04671876da7a64255459de00baa6a8d46477", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUwOTkxNg==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549509916", "bodyText": "so the earliest offset it writes to . i.e the first time a log was written to by this append handle.", "author": "vinothchandar", "createdAt": "2020-12-28T22:48:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1ODYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA1OTAzOQ==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549059039", "bodyText": "Nit: addLogFile or appendLogFile", "author": "n3nash", "createdAt": "2020-12-27T03:25:16Z", "path": "hudi-common/src/main/java/org/apache/hudi/common/model/HoodieDeltaWriteStat.java", "diffHunk": "@@ -44,4 +49,24 @@ public void setLogOffset(long logOffset) {\n   public long getLogOffset() {\n     return logOffset;\n   }\n+\n+  public void setBaseFile(String baseFile) {\n+    this.baseFile = baseFile;\n+  }\n+\n+  public String getBaseFile() {\n+    return baseFile;\n+  }\n+\n+  public void setLogFiles(List<String> logFiles) {\n+    this.logFiles = logFiles;\n+  }\n+\n+  public void appendLogFiles(String logFile) {", "originalCommit": "207b04671876da7a64255459de00baa6a8d46477", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA2MDI3MA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549060270", "bodyText": "High level question - it looks like you are passing the information from the previous WriteStatus and HoodieWriteStat for one log file to the new one if rolled over, what is the need to return list of write statuses ?", "author": "n3nash", "createdAt": "2020-12-27T03:42:00Z", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/HoodieAppendHandle.java", "diffHunk": "@@ -203,31 +213,136 @@ private void init(HoodieRecord record) {\n     return Option.empty();\n   }\n \n+  private void initNewStatus() {", "originalCommit": "207b04671876da7a64255459de00baa6a8d46477", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUxMDI0MA==", "url": "https://github.com/apache/hudi/pull/2300#discussion_r549510240", "bodyText": "This is just the basic information that is common for all write statuses.\n\n, what is the need to return list of write statuses ?\n\nTo handle the case where rollover causes multiple logs to be generated?", "author": "vinothchandar", "createdAt": "2020-12-28T22:49:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTA2MDI3MA=="}], "type": "inlineReview"}, {"oid": "150d1832855060664a0ed76cff9a6f0c65890bc0", "url": "https://github.com/apache/hudi/commit/150d1832855060664a0ed76cff9a6f0c65890bc0", "message": "Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size, consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat", "committedDate": "2020-12-29T22:29:34Z", "type": "commit"}, {"oid": "150d1832855060664a0ed76cff9a6f0c65890bc0", "url": "https://github.com/apache/hudi/commit/150d1832855060664a0ed76cff9a6f0c65890bc0", "message": "Handle rolled-over log files and return a WriteStatus per log file written\n\n - Combined data and delete block logging into a single call\n - Lazily initialize and manage write status based on returned AppendResult\n - Use FSUtils.getFileSize() to set final file size, consistent with other handles\n - Added tests around returned values in AppendResult\n - Added validation of the file sizes returned in write stat", "committedDate": "2020-12-29T22:29:34Z", "type": "forcePushed"}]}