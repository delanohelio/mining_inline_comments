{"pr_number": 7756, "pr_title": "Add release 2.6.1 blog for website", "pr_createdAt": "2020-08-05T08:33:44Z", "pr_url": "https://github.com/apache/pulsar/pull/7756", "timeline": [{"oid": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "url": "https://github.com/apache/pulsar/commit/f25cd0aeb1f2e0813adea787ebc9896f649e8370", "message": "Add release 2.6.1 blog for website\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-05T08:32:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQzOTMzMg==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471439332", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n          \n          \n            \n            - **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:18:38Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ0MDEwMA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471440100", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n          \n          \n            \n            The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n          \n      \n    \n    \n  \n\nCheck the indentation of all bullets through the whole doc.", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:20:09Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ0MDc4Mg==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471440782", "bodyText": "This sentence is identical to the previous one. Please refine the document.", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:21:33Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ0NzI3Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471447276", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n          \n          \n            \n            2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n          \n      \n    \n    \n  \n\nsince there is only one issue, there is no need to use a sub-order.", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:34:29Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ0ODM5MA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471448390", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n          \n          \n            \n            \ta. In a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:36:36Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ0OTAzNw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471449037", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix hash range conflict issue in Key_Shared with sticky hash range**\n          \n          \n            \n            - **Fix hash range conflict issue in Key_Shared subscription with sticky hash range**", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:37:52Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ1NTU0NA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471455544", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## Pulsar Perf\n          \n          \n            \n            ## pulsar-perf", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:50:04Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**\n+\n+There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:\n+                                                                                                                  \n+```text\n+java.lang.NullPointerException: null\\n\\tat java.net.URI$Parser.parse(URI.java:3104) ~[?:?]\n+java.net.URI.<init>(URI.java:600) ~[?:?]\\n\\tat java.net.URI.create(URI.java:881) ~[?:?]\n+org.apache.pulsar.functions.worker.WorkerUtils.initializeDlogNamespace(WorkerUtils.java:160) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+org.apache.pulsar.functions.worker.Worker.initialize(Worker.java:155) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.Worker.start(Worker.java:69) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStarter.java:67) [org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+```\n+\n+This is because a 2.5 broker will response will have \"bookkeeperMetadataServiceUri\" and the admin client will return the field as null, thus causing the NPE.\n+\n+For more information about implementation details, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n+\n+- **Improve security setting of Pulsar Functions**\n+\n+For more information about implementation details, see [PR-7578](https://github.com/apache/pulsar/pull/7578).\n+\n+## Pulsar Perf", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ1NTcyMg==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471455722", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Supports `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read**\n          \n          \n            \n            - **Support `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read performance tests**", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:50:24Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**\n+\n+There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:\n+                                                                                                                  \n+```text\n+java.lang.NullPointerException: null\\n\\tat java.net.URI$Parser.parse(URI.java:3104) ~[?:?]\n+java.net.URI.<init>(URI.java:600) ~[?:?]\\n\\tat java.net.URI.create(URI.java:881) ~[?:?]\n+org.apache.pulsar.functions.worker.WorkerUtils.initializeDlogNamespace(WorkerUtils.java:160) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+org.apache.pulsar.functions.worker.Worker.initialize(Worker.java:155) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.Worker.start(Worker.java:69) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStarter.java:67) [org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+```\n+\n+This is because a 2.5 broker will response will have \"bookkeeperMetadataServiceUri\" and the admin client will return the field as null, thus causing the NPE.\n+\n+For more information about implementation details, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n+\n+- **Improve security setting of Pulsar Functions**\n+\n+For more information about implementation details, see [PR-7578](https://github.com/apache/pulsar/pull/7578).\n+\n+## Pulsar Perf\n+\n+- **Supports `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ1Nzg1NA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471457854", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Add support of `tlsAllowInsecureConnection` config to the command-line tool **pulsar-perf**, to support produce/consume/read performance tests to clusters with insecure tls connections.\n          \n          \n            \n            Add `tlsAllowInsecureConnection` config to the CLI tool **pulsar-perf**, to support produce/consume/read performance tests to clusters with insecure TLS connections.", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:54:10Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**\n+\n+There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:\n+                                                                                                                  \n+```text\n+java.lang.NullPointerException: null\\n\\tat java.net.URI$Parser.parse(URI.java:3104) ~[?:?]\n+java.net.URI.<init>(URI.java:600) ~[?:?]\\n\\tat java.net.URI.create(URI.java:881) ~[?:?]\n+org.apache.pulsar.functions.worker.WorkerUtils.initializeDlogNamespace(WorkerUtils.java:160) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+org.apache.pulsar.functions.worker.Worker.initialize(Worker.java:155) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.Worker.start(Worker.java:69) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStarter.java:67) [org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+```\n+\n+This is because a 2.5 broker will response will have \"bookkeeperMetadataServiceUri\" and the admin client will return the field as null, thus causing the NPE.\n+\n+For more information about implementation details, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n+\n+- **Improve security setting of Pulsar Functions**\n+\n+For more information about implementation details, see [PR-7578](https://github.com/apache/pulsar/pull/7578).\n+\n+## Pulsar Perf\n+\n+- **Supports `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read**\n+\n+Add support of `tlsAllowInsecureConnection` config to the command-line tool **pulsar-perf**, to support produce/consume/read performance tests to clusters with insecure tls connections.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ1ODcxNQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471458715", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix: function BC issue introduced in 2.6**\n          \n          \n            \n            - **Fix the function BC issue introduced in release 2.6.0**", "author": "Huanli-Meng", "createdAt": "2020-08-17T12:55:35Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2MTUxNw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471461517", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:\n          \n          \n            \n            There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). When the running function workers are separated from brokers, updating workers and brokers independently from release 2.5.0 to 2.6.0 will cause the following error:", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:00:19Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**\n+\n+There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2MjM5NQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471462395", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This is because a 2.5 broker will response will have \"bookkeeperMetadataServiceUri\" and the admin client will return the field as null, thus causing the NPE.\n          \n          \n            \n            This is because the broker 2.5.0 supports \"bookkeeperMetadataServiceUri\" and the admin client returns a null field, thus causing the NPE.", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:01:50Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n+\n+For more information about implementation details, see [PR-7360](https://github.com/apache/pulsar/pull/7360).\n+\n+- **Fix: function BC issue introduced in 2.6**\n+\n+There was a backwards compatibility breakage introduced in [PR-5985](https://github.com/apache/pulsar/pull/5985). If running function workers separately from brokers, updating workers and brokers independently from 2.5 to 2.6 will cause the following error:\n+                                                                                                                  \n+```text\n+java.lang.NullPointerException: null\\n\\tat java.net.URI$Parser.parse(URI.java:3104) ~[?:?]\n+java.net.URI.<init>(URI.java:600) ~[?:?]\\n\\tat java.net.URI.create(URI.java:881) ~[?:?]\n+org.apache.pulsar.functions.worker.WorkerUtils.initializeDlogNamespace(WorkerUtils.java:160) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+org.apache.pulsar.functions.worker.Worker.initialize(Worker.java:155) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.Worker.start(Worker.java:69) ~[org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT] \n+org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStarter.java:67) [org.apache.pulsar-pulsar-functions-worker-2.7.0-SNAPSHOT.jar:2.7.0-SNAPSHOT]\n+```\n+\n+This is because a 2.5 broker will response will have \"bookkeeperMetadataServiceUri\" and the admin client will return the field as null, thus causing the NPE.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2MzYwMg==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471463602", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.\n          \n          \n            \n            There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` can return the fully qualified hostname while in Java 11, it returns a simple hostname. In this case, we should use the `getCanonicalHostName()`, which returns the fully qualified hostname. This is the same method to get the advertised address for workers as well.", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:03:58Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n+\n+For more information about implementation details, see [PR-7588](https://github.com/apache/pulsar/pull/7588).\n+\n+## Functions\n+\n+- **Use fully qualified hostname as default to advertise worker**\n+\n+There is a difference in getting hostnames between `Java 8` and `Java 11`. In Java 8, `InetAddress.getLocalHost().getHostName()` was returning the fully qualified hostname while in 11 is returning the simple hostname. We should rather use the `getCanonicalHostName()` which is return the fully qualified hostname. This is the same method to get the advertised address for workers as well.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2NDAwOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471464008", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix multitopic consumer segfault on connect error**\n          \n          \n            \n            - **Fix multi-topic consumer segfault on connection error**", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:04:40Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2NDcyMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471464721", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.\n          \n          \n            \n            The multi-topic consumer triggers a segfault when there's an error in creating the consumer. This is due to the calls, which close the partial consumers with a null callback.", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:05:52Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.\n+\n+For more information about implementation details, see [PR-7606](https://github.com/apache/pulsar/pull/7606).\n+\n+- **Fix multitopic consumer segfault on connect error**\n+\n+The multi-topic consumer is triggering a segfault when there's an error in creating the consumer. This is due to the calls to close the partial consumers with a null callback.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2NDk0MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471464941", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Add support to read credentials from file**\n          \n          \n            \n            - **Support reading credentials from a file**", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:06:15Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2NTIxMA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471465210", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Add support to read credentials from file, make it align with java client.\n          \n          \n            \n            Support reading credentials from a file to make it align with the Java client.", "author": "Huanli-Meng", "createdAt": "2020-08-17T13:06:48Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n+This pull request is trying to fix this issue.\n+\n+For more information about implementation details, see [PR-7572](https://github.com/apache/pulsar/pull/7572).\n+\n+- **Add support to read credentials from file**\n+\n+Add support to read credentials from file, make it align with java client.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMDk5OQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471510999", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix segment crashes that caused by race condition of timer in cpp client**\n          \n          \n            \n            - **Fix segment crashes caused by race condition of the timer in cpp client**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:18:48Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMTUwNQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471511505", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - close operation called the `keepAliveTimer_.reset()`.\n          \n          \n            \n                - The close operation calls the `keepAliveTimer_.reset()`.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:19:33Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMTc5Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471511796", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.\n          \n          \n            \n                - At the same time, the timer is accessed in methods `startConsumerStatsTimer` and `handleKeepAliveTimeout`.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:19:58Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**\n+\n+Segment crashes happens in a race condition:\n+    - close operation called the `keepAliveTimer_.reset()`.\n+    - while at the same time, timer is accessed in method `startConsumerStatsTimer` and `handleKeepAliveTimeout`.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMjA1OQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471512059", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Cpp oauth2 auth client**\n          \n          \n            \n            - **CPP Oauth2 authentication client**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:20:20Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMjYxNA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471512614", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.\n          \n          \n            \n            Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). This change tries to support it in cpp client.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:21:12Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **Cpp oauth2 auth client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). This change tries to support it in cpp client.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMjk1Nw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471512957", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Make OAuth2 auth plugin to use AsyncHttpClient**\n          \n          \n            \n            - **Make OAuth2 authentication plugin to use AsyncHttpClient**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:21:42Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxMzc1MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471513751", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.\n          \n          \n            \n            The OAuth2 client authentication plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we are using everywhere else in client and broker. Apache HTTP client was only meant to be used for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we had better get rid of that dependency.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:22:54Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 auth plugin to use AsyncHttpClient**\n+\n+OAuth2 client auth plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we're using everywhere else in client and broker. Apache HTTP client was only meant to be use for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we should better get rid of that dependency.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxNDA2OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471514068", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Don't try to subscribe to the topic if the consumer is closed**\n          \n          \n            \n            - **Do not try to subscribe to the topic if the consumer is closed**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:23:21Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxNDI2OQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471514269", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Fix race condition on the close consumer while reconnecting to the broker.\n          \n          \n            \n            Fix race condition on the closed consumer while reconnecting to the broker.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:23:38Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUxNTEyMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471515121", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n          \n          \n            \n            The race condition happens while the consumer reconnects to the broker. The cnx of the consumer is set to null when the consumer reconnects to the broker. If the consumer is closed at this time, the client will not send close consumer command to the broker. So, when the consumer reconnects to the broker, the consumer will send the subscribe command again.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:24:56Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. ", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyMzcyOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471523728", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.\n          \n          \n            \n            This pull request adds state check when connection of the consumer is opened. If the consumer is in closing or closed state, we do not need to send the subscribe command.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:37:16Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Don't try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the close consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker, the cnx of the consumer set to null when reconnects to the broker. If close the consumer at this time, the client will not send close consumer command to the broker. So, if the consumer reconnected to the broker, the consumer will send the subscribe command again. \n+\n+This pull request add state check when connection opened of the consumer. If the consumer state is closing or closed, we don\u2019t need to send the subscribe command.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNDE1Nw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471524157", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Add authentication client with oauth2 support**\n          \n          \n            \n            - **Add authentication client with Oauth2 support**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:37:55Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNDU2Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471524566", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n          \n          \n            \n            Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic).", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:38:31Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). ", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNTQwNg==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471525406", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n          \n          \n            \n            This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, and then the client gets an `access token` from the Oauth 2.0 server, and passes this `access token` to Pulsar broker to do authentication.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:39:39Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNTgwNw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471525807", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n          \n          \n            \n            So, the broker could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:40:12Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNjA2MA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471526060", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            also user can add their own `AuthenticationProvider` to work with this module.\n          \n          \n            \n            and the user can add their own `AuthenticationProvider` to work with this module.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:40:34Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with oauth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish to a topic or consume from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. Client side communicate with Oauth 2.0 server,  then the client will get an `access token` from Oauth 2.0 server, and will pass this `access token` to Pulsar broker to do the authentication.\n+\n+So the Broker side could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+also user can add their own `AuthenticationProvider` to work with this module.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNjgyOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471526828", "bodyText": "ack set or ackset?", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:41:45Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg2OTg5NQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471869895", "bodyText": "ack set", "author": "wolfstudy", "createdAt": "2020-08-18T02:00:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNjgyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MzI5MA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471883290", "bodyText": "The following sentences use ackset instead of ack set. you should use consistent words all through the docs.", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:52:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNjgyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyNzMxNQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471527315", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix issue where HTTP header used in Athenz authentication can not be renamed**\n          \n          \n            \n            - **Fix the issue that the HTTP header used in Athenz authentication can not be renamed**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:42:29Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix issue where HTTP header used in Athenz authentication can not be renamed**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUyODU1Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471528556", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.\n          \n          \n            \n            In [PR-5607](https://github.com/apache/pulsar/pull/5607), the `useHostName()` was added with `return false`. That means that the rack-aware policy will try to resolve the Bookie's hostname into an IP address and then use that IP address to figure out to which rack the bookie belongs.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:44:25Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [#5607](https://github.com/apache/pulsar/pull/5607) the `useHostName()` was added with `return false`. That means that the rackaware policy will try to resolve the Bookies hostname into an IP and then use that IP to figure out which rack the bookie belongs.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzMTU5OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471531598", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n          \n          \n            \n            A return statement is missed in the current code when the entry is set to -1. Therefore, after the code is sent, the response is trying to read the entry and sends a second response:", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:48:57Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: ", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzMTk3Mw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471531973", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.\n          \n          \n            \n            When updating partitions on a non-persistent topic, Error 409 is returned. The pull request will fix partitions error for non-persistent topic.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:49:34Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+The current code is missing a return statement when entry is -1 and thus, after sending the response is trying to read the entry and sends a 2nd response: \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, 409 returned. The pull request will fix update partitions error for non-persistent topic.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzMjYwOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471532608", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix deduplication cursor does not delete after disabling message deduplication**\n          \n          \n            \n            - **Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:50:32Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzMzA4MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471533081", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.\n          \n          \n            \n            The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The deduplication cursor cannot be deleted.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:51:14Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix deduplication cursor does not delete after disabling message deduplication**\n+\n+Fix deduplication cursor does not delete after disabling message deduplication. The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The dedup cursor will not be deleted.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzMzkxMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471533911", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Decompression payload if needed in KeyShared subscription**\n          \n          \n            \n            - **Decompress payload if needed in Key_Shared subscription**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:52:29Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNDA4MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471534081", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Decompression payload if needed in `KeyShared` subscription.\n          \n          \n            \n            Decompress payload if needed in `Key_Shared` subscription.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:52:43Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompression payload if needed in KeyShared subscription**\n+\n+Decompression payload if needed in `KeyShared` subscription.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNDk3OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471534978", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n          \n          \n            \n            As a followup to the fix in [PR-7401](https://github.com/apache/pulsar/pull/7401), use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:54:05Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [#7401](https://github.com/apache/pulsar/pull/7401), also use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNjI4NQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471536285", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n          \n          \n            \n            NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. The unresolvable network address is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:55:56Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. Network address unresolvable is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNjczOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471536738", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n          \n          \n            \n            When the default value of the `offload-deletion-lag` is set to null, it will cause an NPE problem. To fix this bug, we add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:56:36Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+The default value of the `offload-deletion-lag` is null, this will cause an NPE problem. Add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNzM2OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471537368", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:\n          \n          \n            \n            We're getting an NPE when the creation of a non-durable cursor fails. The reason is that we fail the future but we go on in creating the subscription instance:", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:57:36Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that, we fail the future but we go on in creating the subscription instance:", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzNzc1NA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471537754", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Avoid introduce null read position for the managed cursor**\n          \n          \n            \n            - **Avoid introducing null read position for the managed cursor**", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:58:08Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzODU3MA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471538570", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n          \n          \n            \n            Avoid introducing null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If a given position is greater than the last add position, it will return a null value. This may cause the read position to become null. But I have not found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.", "author": "Huanli-Meng", "createdAt": "2020-08-17T14:59:19Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introduce null read position for the managed cursor**\n+\n+Avoid introduce null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If given a position which greater than the last add position, it will return a null value. This may cause the read position to become null. But I haven\u2019t found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzOTA1Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471539056", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n          \n          \n            \n            In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.", "author": "Huanli-Meng", "createdAt": "2020-08-17T15:00:01Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTUzOTk5OQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471539999", "bodyText": "this sentence have some grammatical error. please re-write this sentence.", "author": "Huanli-Meng", "createdAt": "2020-08-17T15:01:25Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTU0MDQxOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471540418", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.\n          \n          \n            \n            If the method `canProduce` or `canConsume` throw an exception,  the `canLookup` method will just throw the exception and will not check the other permission.", "author": "Huanli-Meng", "createdAt": "2020-08-17T15:02:06Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limiting batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is higher than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I noticed the following issues:\n+\ta. In a multiTopic (pattern) consumer the client stops receiving any messages I think it getting paused and never resumed when setting a timeout in the batch policy, only one batch is fetched and the client never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared with sticky hash range**\n+In `Key_Shared` subscription using `stickyHashRange`, consumers interleaving hashes are not allowed. For example, consumer 1 hash: [[0, 99], [400, 65535]], consumer 2 hash: [[100,399]].\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the role canProducer **or** canConsumer mean that canLookup, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+if the method `canProduce` or `canConsume` throw exception, `canLookup` will just throw the exception and won't check the other permission.", "originalCommit": "f25cd0aeb1f2e0813adea787ebc9896f649e8370", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5cbc650c9b7830526d07c8f9faa3eebf7fb6f33e", "url": "https://github.com/apache/pulsar/commit/5cbc650c9b7830526d07c8f9faa3eebf7fb6f33e", "message": "Merge branch 'master' into xiaolong/release-2.6.1-blog", "committedDate": "2020-08-18T01:39:22Z", "type": "commit"}, {"oid": "69b1470384a48a682210d50b0f300391e1735f46", "url": "https://github.com/apache/pulsar/commit/69b1470384a48a682210d50b0f300391e1735f46", "message": "fix comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-18T02:15:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg3ODQyMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471878421", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:\n          \n          \n            \n            2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:34:27Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:", "originalCommit": "69b1470384a48a682210d50b0f300391e1735f46", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MjIzOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471882238", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the user has the role canProducer **or** canConsumer means it can perform canLookup operations, but actually in the code:\n          \n          \n            \n            Currently\uff0cwhen Pulsar AuthorizationService checks lookup permission, if the user has the role canProducer **or** canConsumer, it means that the user can perform canLookup operations, but actually in the code:", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:48:57Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:\n+\ta. In a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared subscription with sticky hash range**\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the user has the role canProducer **or** canConsumer means it can perform canLookup operations, but actually in the code:", "originalCommit": "69b1470384a48a682210d50b0f300391e1735f46", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MjI3NQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471882275", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If the method `canProduce` or `canConsume` throw an exception,  the `canLookup` method will just throw the exception and will not check the other permission.\n          \n          \n            \n            If the method `canProduce` or `canConsume` throw an exception, the `canLookup` method will just throw the exception and will not check the other permission.", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:49:08Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:\n+\ta. In a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared subscription with sticky hash range**\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the user has the role canProducer **or** canConsumer means it can perform canLookup operations, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+If the method `canProduce` or `canConsume` throw an exception,  the `canLookup` method will just throw the exception and will not check the other permission.", "originalCommit": "69b1470384a48a682210d50b0f300391e1735f46", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4Mjk5OQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471882999", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n          \n          \n            \n            The exception is thrown at Line 77, since `getAddress()` returns a `null` given that the address is unresolved.", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:51:40Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:\n+\ta. In a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared subscription with sticky hash range**\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the user has the role canProducer **or** canConsumer means it can perform canLookup operations, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+If the method `canProduce` or `canConsume` throw an exception,  the `canLookup` method will just throw the exception and will not check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introducing null read position for the managed cursor**\n+\n+Avoid introducing null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If a given position is greater than the last add position, it will return a null value. This may cause the read position to become null. But I have not found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+When the default value of the `offload-deletion-lag` is set to null, it will cause an NPE problem. To fix this bug, we add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. The unresolvable network address is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [PR-7401](https://github.com/apache/pulsar/pull/7401), use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompress payload if needed in Key_Shared subscription**\n+\n+Decompress payload if needed in `Key_Shared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled**\n+\n+The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The deduplication cursor cannot be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+A return statement is missed in the current code when the entry is set to -1. Therefore, after the code is sent, the response is trying to read the entry and sends a second response:  \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, Error 409 is returned. The pull request will fix partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [PR-5607](https://github.com/apache/pulsar/pull/5607), the `useHostName()` was added with `return false`. That means that the rack-aware policy will try to resolve the Bookie's hostname into an IP address and then use that IP address to figure out to which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. ", "originalCommit": "69b1470384a48a682210d50b0f300391e1735f46", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MzUyNA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r471883524", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - **Fix segment crashes that caused by race condition of timer in cpp client**\n          \n          \n            \n            - **Fix segment crashes that caused by race condition of timer in CPP client**", "author": "Huanli-Meng", "createdAt": "2020-08-18T02:53:31Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,294 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+- **Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`**\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500) I notice the following issues:\n+\ta. In a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+- **Fix hash range conflict issue in Key_Shared subscription with sticky hash range**\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+- **Fix: get lookup permission error**\n+\n+Currently\uff0cwhen pulsar AuthorizationService check lookup permission, if the user has the role canProducer **or** canConsumer means it can perform canLookup operations, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+If the method `canProduce` or `canConsume` throw an exception,  the `canLookup` method will just throw the exception and will not check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+- **Avoid introducing null read position for the managed cursor**\n+\n+Avoid introducing null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If a given position is greater than the last add position, it will return a null value. This may cause the read position to become null. But I have not found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.\n+                                                           \n+For more information about implementation details, see [PR-7264](https://github.com/apache/pulsar/pull/7264).\n+\n+- **Handling error in creation of non-durable cursor**\n+\n+We're getting an NPE when the creation of a non-durable cursor fails. The reason is that we fail the future but we go on in creating the subscription instance:\n+                                                                      \n+```java\n+try {\n+    cursor = ledger.newNonDurableCursor(startPosition, subscriptionName);\n+} catch (ManagedLedgerException e) {\n+    subscriptionFuture.completeExceptionally(e);\n+}\n+\n+return new PersistentSubscription(this, subscriptionName, cursor, false);\n+```\n+\n+Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+\n+For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+\n+- **Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`**\n+\n+When the default value of the `offload-deletion-lag` is set to null, it will cause an NPE problem. To fix this bug, we add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+                                                                                         \n+For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+\n+- **Fix producer stuck issue due to NPE thrown when creating a new ledger**\n+\n+NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. The unresolvable network address is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+\n+This pull request does the followings:\n+\n+1. Catch the NPE when creating a new ledger.\n+2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n+\n+For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n+\n+- **Avoid NPEs at ledger creation when DNS failures happen**\n+\n+As a followup to the fix in [PR-7401](https://github.com/apache/pulsar/pull/7401), use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+\n+For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+\n+- **Decompress payload if needed in Key_Shared subscription**\n+\n+Decompress payload if needed in `Key_Shared` subscription.\n+\n+For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+\n+- **Fix NPE when using advertisedListeners**\n+\n+The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+\n+For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+\n+- **Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled**\n+\n+The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The deduplication cursor cannot be deleted.\n+\n+For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n+\n+- **Get last entry is trying to read entry -1**\n+\n+A return statement is missed in the current code when the entry is set to -1. Therefore, after the code is sent, the response is trying to read the entry and sends a second response:  \n+\n+```\n+16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n+16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n+```\n+\n+For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+\n+- **Fix update partitions error for non-persistent topic**\n+\n+When updating partitions on a non-persistent topic, Error 409 is returned. The pull request will fix partitions error for non-persistent topic.\n+\n+For more information about implementation details, see [PR-7459](https://github.com/apache/pulsar/pull/7459).\n+\n+## Zookeeper\n+\n+- **Use hostname for bookie rack awareness mapping**\n+\n+In [PR-5607](https://github.com/apache/pulsar/pull/5607), the `useHostName()` was added with `return false`. That means that the rack-aware policy will try to resolve the Bookie's hostname into an IP address and then use that IP address to figure out to which rack the bookie belongs.\n+\n+There are 2 problems: \n+ 1. The IP won't match the hostname which is recorded in the `/bookies` z-node\n+ 2. If there is an error in resolving the bookie hostname (eg: transient DNS error), an NPE exception will be triggered and the BK client will never realize that this bookie was ever seen as available in the cluster.\n+\n+The exception is thrown at 77, since `getAddress()` yealds a `null` given that the address is unresolved. \n+\n+```java\n+74        if (dnsResolver.useHostName()) {\n+75            names.add(addr.getHostName());\n+76        } else {\n+77            names.add(addr.getAddress().getHostAddress());\n+78        }\n+```\n+\n+The default implementation for the `DnsResolver.useHostName()` is to return true.\n+\n+For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+\n+## Java Client\n+\n+- **Fix the issue that the HTTP header used in Athenz authentication can not be renamed**\n+\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+                                                                                                                                                                                                    \n+For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+\n+- **Fix batch ack set recycled multiple times**\n+\n+Fix batch ackset recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ackset.\n+\n+For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+\n+- **Add authentication client with OAuth2 support**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). \n+\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, and then the client gets an `access token` from the Oauth 2.0 server, and passes this `access token` to Pulsar broker to do the authentication.\n+\n+So, the broker could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+and the user can add their own `AuthenticationProvider` to work with this module.\n+\n+For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n+\n+- **Ensure the create subscription can be completed when the operation timeout happens**\n+\n+Ensure the create subscription can be completed when the operation timeout happens.\n+\n+For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n+\n+- **Do not try to subscribe to the topic if the consumer is closed**\n+\n+Fix race condition on the closed consumer while reconnecting to the broker.\n+\n+The race condition happens while the consumer reconnects to the broker. The cnx of the consumer is set to null when the consumer reconnects to the broker. If the consumer is closed at this time, the client will not send close consumer command to the broker. So, when the consumer reconnects to the broker, the consumer will send the subscribe command again. \n+\n+This pull request adds state check when connection opened of the consumer is opened. If the consumer is in closing or closed state, we do not need to send the subscribe command.\n+\n+For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+\n+- **Make OAuth2 authentication plugin to use AsyncHttpClient**\n+\n+The OAuth2 client authentication plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we are using everywhere else in client and broker. Apache HTTP client was only meant to be used for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we had better get rid of that dependency.\n+\n+For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+\n+- **Fix batch index filter issue in Consumer**\n+\n+Fix batch index filter issue in Consumer.\n+\n+For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n+\n+## CPP Client\n+\n+- **CPP Oauth2 authentication client**\n+\n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). This change tries to support it in cpp client.\n+\n+For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+\n+- **Fix partition index error in close callback**\n+\n+In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+\n+For more information about implementation details, see [PR-7282](https://github.com/apache/pulsar/pull/7282).\n+\n+- **Fix segment crashes that caused by race condition of timer in cpp client**", "originalCommit": "69b1470384a48a682210d50b0f300391e1735f46", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a4fc4ba9406ff272475c753213381e35fed973e8", "url": "https://github.com/apache/pulsar/commit/a4fc4ba9406ff272475c753213381e35fed973e8", "message": "fix comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-18T04:05:21Z", "type": "commit"}, {"oid": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "url": "https://github.com/apache/pulsar/commit/4332b06828bb22ddf2a46c4146d204e8dbf1c433", "message": "fix comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-18T07:47:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1MDI0Mw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472050243", "bodyText": "Take care of the following:\n\nItem 1 is identical to the sentence above.\nIt's release blog, could we avoid using \"I used ..., I notice....)?", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:39:02Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1MzU2MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472053561", "bodyText": "Suggestion: When the batch size is greater than the receiveQ of the consumer (for example, the batch size is 3000 and a receiveQ is 500), the following issue occurs:", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:45:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1MDI0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NjA5NA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472056094", "bodyText": "what does \"it\" mean in \"I think it gets paused...\"?", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:49:22Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NjQwMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472056401", "bodyText": "thestickyHashRange --> the stickyHashRange", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:49:57Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+#### Fix hash range conflict issue in Key_Shared subscription with sticky hash range\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA2Njk2MQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472066961", "bodyText": "What is difference?", "author": "wolfstudy", "createdAt": "2020-08-18T10:08:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NjQwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE3ODgwNw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472178807", "bodyText": "add a space between the two words.", "author": "Jennifer88huang", "createdAt": "2020-08-18T13:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NjQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NjY5OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472056698", "bodyText": "will fix --> fixes", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:50:23Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+#### Fix hash range conflict issue in Key_Shared subscription with sticky hash range\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1NzkzOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472057938", "bodyText": "I. why do we use bold font for \"or\"?\n2. Adopt code font or bold font for  the following role/operation/method.\ncanProducer or canConsumer\ncanLookup", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:52:36Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+#### Fix hash range conflict issue in Key_Shared subscription with sticky hash range\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+#### Fix: get lookup permission error\n+\n+Currently\uff0cwhen Pulsar AuthorizationService checks lookup permission, if the user has the role canProducer **or** canConsumer, it means that the user can perform canLookup operations, but actually in the code:", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA1ODcxNA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472058714", "bodyText": "Release notes and blogs describe some objective facts, use present tense, and avoid using \"will\" if possible.", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:54:01Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+#### Fix hash range conflict issue in Key_Shared subscription with sticky hash range\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+#### Fix: get lookup permission error\n+\n+Currently\uff0cwhen Pulsar AuthorizationService checks lookup permission, if the user has the role canProducer **or** canConsumer, it means that the user can perform canLookup operations, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+If the method `canProduce` or `canConsume` throw an exception, the `canLookup` method will just throw the exception and will not check the other permission.", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA2MDc1NA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r472060754", "bodyText": "it will return --> it returns\nWho is \"I\"? Try to avoid using \"I\"/\"We\". Please check other similar cases.", "author": "Jennifer88huang", "createdAt": "2020-08-18T09:57:22Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -0,0 +1,295 @@\n+---\n+author: XiaoLong Ran\n+authorURL: https://twitter.com/wolf4j1\n+title: Apache Pulsar 2.6.1\n+---\n+We are very glad to see the Apache Pulsar community has successfully released the wonderful 2.6.1 version after accumulated hard work. It is a great milestone for this fast-growing project and the whole Pulsar community. This is the result of a huge effort from the community, with over 90 commits and a long list of improvements and bug fixes.\n+\n+Here is a selection of some of the most interesting and major features added to Pulsar 2.6.1.\n+\n+<!--truncate-->\n+\n+## Broker\n+\n+#### Limit the batch size to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages`\n+\n+The Batch size is not limited to the minimum of the `maxNumberOfMessages` and `maxSizeOfMessages` from the `BatchReceive` policy.\n+\n+1. Batch size is not limited to the minimum of the maxNumberOfMessages and maxSizeOfMessages from the BatchReceive policy.\n+2. When the batch size is greater than the receiveQ of the consumer (I used a batch size of 3000 and a receiveQ of 500), I notice the following issues:\n+\t\n+\tIn a multi-topic (pattern) consumer, the client stops receiving any messages. I think it gets paused and never resumed when setting a timeout in the batch policy. Only one batch is fetched and the client is never resumed.\n+\n+For more information about implementation details, see [PR-6865](https://github.com/apache/pulsar/pull/6865).\n+\n+#### Fix hash range conflict issue in Key_Shared subscription with sticky hash range\n+In `Key_Shared` subscription where the`stickyHashRange` is used, consumers are not allowed to use interleaving hashes.\n+\n+The pull request will fix hash range conflict issue in `Key_Shared` with sticky hash range.\n+\n+For more information about implementation details, see [PR-7231](https://github.com/apache/pulsar/pull/7231).\n+\n+#### Fix: get lookup permission error\n+\n+Currently\uff0cwhen Pulsar AuthorizationService checks lookup permission, if the user has the role canProducer **or** canConsumer, it means that the user can perform canLookup operations, but actually in the code:\n+\n+```java\n+try {\n+    return canLookupAsync(topicName, role, authenticationData)\n+            .get(conf.getZooKeeperOperationTimeoutSeconds(), SECONDS);\n+}\n+```\n+If the method `canProduce` or `canConsume` throw an exception, the `canLookup` method will just throw the exception and will not check the other permission.\n+\n+The pull request will invoke `canLookupAsync` instead.\n+\n+For more information about implementation details, see [PR-7234](https://github.com/apache/pulsar/pull/7234).\n+\n+#### Avoid introducing null read position for the managed cursor\n+\n+Avoid introducing null read position for the managed cursor. The most doubtful thing is `getNextValidPosition` method in the `ManagedLedgerImpl`. If a given position is greater than the last add position, it will return a null value. This may cause the read position to become null. But I have not found how this situation appears. So in the PR, I added a log and print the stack trace which can help us to find the root cause and fallback to the next position of the last position if the null next valid position occurs.", "originalCommit": "4332b06828bb22ddf2a46c4146d204e8dbf1c433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1d8daa0b0096542f62c1e2d84656ced1b772b517", "url": "https://github.com/apache/pulsar/commit/1d8daa0b0096542f62c1e2d84656ced1b772b517", "message": "fix comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-18T10:10:55Z", "type": "commit"}, {"oid": "c4b614f47afcf4da3be77f8baf967183be3aaea7", "url": "https://github.com/apache/pulsar/commit/c4b614f47afcf4da3be77f8baf967183be3aaea7", "message": "fix comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-18T14:49:14Z", "type": "commit"}, {"oid": "978c01c797ef8e8650807176e19acdc488214984", "url": "https://github.com/apache/pulsar/commit/978c01c797ef8e8650807176e19acdc488214984", "message": "fix docs comments\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-21T03:58:32Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5OTY0Mw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474399643", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1 and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).\n          \n          \n            \n            - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1) and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:19:43Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -263,23 +247,19 @@ org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStar\n \n This is because the broker 2.5.0 supports \"bookkeeperMetadataServiceUri\" and the admin client returns a `null` field, thus causing the NPE.\n \n-For more information about implementation details, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n-\n-#### Improve security setting of Pulsar Functions\n-\n-For more information about implementation details, see [PR-7578](https://github.com/apache/pulsar/pull/7578).\n+For more information about implementation, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n \n ## pulsar-perf\n \n-#### Support `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read performance tests\n+### Support `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read performance tests\n \n Add `tlsAllowInsecureConnection` config to the CLI tool **pulsar-perf**, to support produce/consume/read performance tests to clusters with insecure TLS connections.\n \n-For more information about implementation details, see [PR-7300](https://github.com/apache/pulsar/pull/7300).\n+For more information about implementation, see [PR-7300](https://github.com/apache/pulsar/pull/7300).\n \n ## More information\n \n-- To download Apache Pulsar 2.6.1, click [here](https://pulsar.apache.org/en/download/).\n+- To download Apache Pulsar 2.6.1, click [download](https://pulsar.apache.org/en/download/).\n - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1 and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5OTc4OA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474399788", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl`'s internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n          \n          \n            \n            In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl` internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl` close callback.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:20:18Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -148,109 +141,100 @@ The exception is thrown at Line 77, since `getAddress()` returns a `null` given\n 78        }\n ```\n \n-The default implementation for the `DnsResolver.useHostName()` is to return true.\n+The default implementation for the `DnsResolver.useHostName()` returns `true`.\n \n-For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+For more information about implementation, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n \n ## Java Client\n \n-#### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n+### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n \n-The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change uses the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and uses it directly as the header name.\n                                                                                                                                                                                                     \n-For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+For more information about implementation, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n \n-#### Fix batch ack set recycled multiple times\n+### Fix the issue that batch ack set is recycled multiple times\n \n-Fix batch ack set recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ack set.\n+The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.\n \n-For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+For more information about implementation, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n \n-#### Add authentication client with OAuth2 support\n+### Add authentication client with OAuth2 support\n \n-Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). \n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions, for example, publish messages to a topic or consume messages from a topic. \n \n-This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, and then the client gets an `access token` from the Oauth 2.0 server, and passes this `access token` to Pulsar broker to do the authentication.\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, gets an `access token` from the Oauth 2.0 server, and passes the `access token` to Pulsar broker to do the authentication.\n \n-So, the broker could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+So, the broker can use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n and the user can add their own `AuthenticationProvider` to work with this module.\n \n-For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n-\n-#### Ensure the create subscription can be completed when the operation timeout happens\n+For more information about implementation, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n \n-Ensure the create subscription can be completed when the operation timeout happens.\n \n-For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n-\n-#### Do not try to subscribe to the topic if the consumer is closed\n+### Not subscribe to the topic when the consumer is closed\n \n Fix race condition on the closed consumer while reconnecting to the broker.\n \n-The race condition happens while the consumer reconnects to the broker. The cnx of the consumer is set to null when the consumer reconnects to the broker. If the consumer is closed at this time, the client will not send close consumer command to the broker. So, when the consumer reconnects to the broker, the consumer will send the subscribe command again. \n-\n-This pull request adds state check when connection opened of the consumer is opened. If the consumer is in closing or closed state, we do not need to send the subscribe command.\n+The race condition happens when the consumer reconnects to the broker. The connection of the consumer is set to `null` when the consumer reconnects to the broker. If the consumer is not connected to broker at this time, the client does not send the consumer command to the broker. So, when the consumer reconnects to the broker, the consumer sends the subscribe command again. \n \n-For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+This pull request adds a state check when the `connectionOpened()` of the consumer opens. If the consumer is in closing or closed state, the consumer does not send the subscribe command.\n \n-#### Make OAuth2 authentication plugin to use AsyncHttpClient\n+For more information about implementation, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n \n-The OAuth2 client authentication plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we are using everywhere else in client and broker. Apache HTTP client was only meant to be used for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we had better get rid of that dependency.\n+### OAuth2 authentication plugin uses AsyncHttpClient\n \n-For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+Previously, the OAuth2 client authentication plugin used Apache HTTP client lib to make requests, Apache HTTP client is used to validate hostname. As suggested in [#7612](https://github.com/apache/pulsar/issues/7612), we get rid of the dependency of using Apache HTTP client.\n \n-#### Fix batch index filter issue in Consumer\n+In PR-7615, OAuth2 client authentication plugin uses AsyncHttpClient, which is used in client and broker. For more information about implementation, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n \n-Fix batch index filter issue in Consumer.\n-\n-For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n \n ## CPP Client\n \n-#### CPP Oauth2 authentication client\n+### CPP Oauth2 authentication client\n \n Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). This change tries to support it in cpp client.\n \n-For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+For more information about implementation, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n \n-#### Fix partition index error in close callback\n+### Fix partition index error in close callback\n \n-In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl`'s internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMDAyMQ==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474400021", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method\n          \n          \n            \n            ### Avoid an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:21:25Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -63,82 +64,74 @@ try {\n return new PersistentSubscription(this, subscriptionName, cursor, false);\n ```\n \n-Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+Additionally, the NPE leads to the topic usage count increasing to 1. When deleting a topic, the topic cannot be deleted even if you use the force flag.\n \n-For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+For more information about implementation, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n \n-#### Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`\n+### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMDEwOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474400108", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            PR-7495 adds a return statement to code, so GetLastEntry()  reads the last entry, instead of `-1`.  \n          \n          \n            \n            PR-7495 adds a return statement to code, so GetLastEntry() reads the last entry, instead of `-1`.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:21:42Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -63,82 +64,74 @@ try {\n return new PersistentSubscription(this, subscriptionName, cursor, false);\n ```\n \n-Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+Additionally, the NPE leads to the topic usage count increasing to 1. When deleting a topic, the topic cannot be deleted even if you use the force flag.\n \n-For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+For more information about implementation, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n \n-#### Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`\n+### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method\n \n-When the default value of the `offload-deletion-lag` is set to null, it will cause an NPE problem. To fix this bug, we add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+When the default value of the `offload-deletion-lag` is set to `null`, an NPE occurs. To fix the bug, null check is added in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method.\n                                                                                          \n-For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+For more information about implementation, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n \n-#### Fix producer stuck issue due to NPE thrown when creating a new ledger\n+### Fix producer stuck issue due to NPE when creating a new ledger\n \n-NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. The unresolvable network address is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+NPE occurs when creating a ledger if the network address is unresolvable. If NPE occurs before adding the timeout task, the timeout mechanism does not work. The unresolvable network address is common in the Kubernetes environment. It happens when a bookie pod or a worker node restarts.\n \n-This pull request does the followings:\n+This pull request fixes from the following perspectives:\n \n 1. Catch the NPE when creating a new ledger.\n-2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n-3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n-\n-For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n-\n-#### Avoid NPEs at ledger creation when DNS failures happen\n-\n-As a followup to the fix in [PR-7401](https://github.com/apache/pulsar/pull/7401), use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+2. When the timeout task is triggered, it always executes the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect that the `CreatingLedger` state is not moving.\n \n-For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+For more information about implementation, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n \n-#### Decompress payload if needed in Key_Shared subscription\n \n-Decompress payload if needed in `Key_Shared` subscription.\n+### Fix NPE when using advertisedListeners\n \n-For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+The broker failed to acquire ownership for the namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when TLS is not enabled.\n \n-#### Fix NPE when using advertisedListeners\n+For more information about implementation, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n \n-The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+### Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled\n \n-For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+When enabling the message deduplication in the `broker.conf` file, disabling it and then restarting the broker, the deduplication cursor is not deleted.\n \n-#### Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled\n+This PR fixes the issue, so when you disable message deduplication, you can delete the deduplication cursor.\n \n-The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The deduplication cursor cannot be deleted.\n+For more information about implementation, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n \n-For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n-\n-#### Get last entry is trying to read entry -1\n-\n-A return statement is missed in the current code when the entry is set to -1. Therefore, after the code is sent, the response is trying to read the entry and sends a second response:  \n+### Fix the issue that GetLastEntry() reads entry `-1`\n \n+Previously, the code does not include a return statement. If the entry is set to `-1`, after sending code, the response reads the entry and sends a second response, as shown in the following example.\n ```\n 16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n ```\n \n-For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+PR-7495 adds a return statement to code, so GetLastEntry()  reads the last entry, instead of `-1`.  ", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMDM2Mw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474400363", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.\n          \n          \n            \n            The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409, and fix the recycle issue.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:22:40Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -148,109 +141,100 @@ The exception is thrown at Line 77, since `getAddress()` returns a `null` given\n 78        }\n ```\n \n-The default implementation for the `DnsResolver.useHostName()` is to return true.\n+The default implementation for the `DnsResolver.useHostName()` returns `true`.\n \n-For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+For more information about implementation, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n \n ## Java Client\n \n-#### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n+### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n \n-The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change uses the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and uses it directly as the header name.\n                                                                                                                                                                                                     \n-For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+For more information about implementation, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n \n-#### Fix batch ack set recycled multiple times\n+### Fix the issue that batch ack set is recycled multiple times\n \n-Fix batch ack set recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ack set.\n+The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMDg3Ng==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474400876", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1 and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).\n          \n          \n            \n            - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1) and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:24:53Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -263,23 +247,19 @@ org.apache.pulsar.functions.worker.FunctionWorkerStarter.main(FunctionWorkerStar\n \n This is because the broker 2.5.0 supports \"bookkeeperMetadataServiceUri\" and the admin client returns a `null` field, thus causing the NPE.\n \n-For more information about implementation details, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n-\n-#### Improve security setting of Pulsar Functions\n-\n-For more information about implementation details, see [PR-7578](https://github.com/apache/pulsar/pull/7578).\n+For more information about implementation, see [PR-7528](https://github.com/apache/pulsar/pull/7528).\n \n ## pulsar-perf\n \n-#### Support `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read performance tests\n+### Support `tlsAllowInsecureConnection` in pulsar-perf produce/consume/read performance tests\n \n Add `tlsAllowInsecureConnection` config to the CLI tool **pulsar-perf**, to support produce/consume/read performance tests to clusters with insecure TLS connections.\n \n-For more information about implementation details, see [PR-7300](https://github.com/apache/pulsar/pull/7300).\n+For more information about implementation, see [PR-7300](https://github.com/apache/pulsar/pull/7300).\n \n ## More information\n \n-- To download Apache Pulsar 2.6.1, click [here](https://pulsar.apache.org/en/download/).\n+- To download Apache Pulsar 2.6.1, click [download](https://pulsar.apache.org/en/download/).\n - For more information about Apache Pulsar 2.6.1, see [2.6.1 release notes](https://pulsar.apache.org/release-notes/#2.6.1 and [2.6.1 PR list](https://github.com/apache/pulsar/pulls?q=is%3Apr+label%3Arelease%2F2.6.1+is%3Aclosed).", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMTAwOA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474401008", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method\n          \n          \n            \n            ### Avoid an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:25:21Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -63,82 +64,74 @@ try {\n return new PersistentSubscription(this, subscriptionName, cursor, false);\n ```\n \n-Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+Additionally, the NPE leads to the topic usage count increasing to 1. When deleting a topic, the topic cannot be deleted even if you use the force flag.\n \n-For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+For more information about implementation, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n \n-#### Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`\n+### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMTEwNA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474401104", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            PR-7495 adds a return statement to code, so GetLastEntry()  reads the last entry, instead of `-1`.  \n          \n          \n            \n            PR-7495 adds a return statement to code, so GetLastEntry() reads the last entry, instead of `-1`.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:25:43Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -63,82 +64,74 @@ try {\n return new PersistentSubscription(this, subscriptionName, cursor, false);\n ```\n \n-Additionally, the NPE leads to the topic usage count to not be decremented, leaking 1 usage increment. At the time of deletion, this will prevent the topic from being deleted, even when using the force flag.\n+Additionally, the NPE leads to the topic usage count increasing to 1. When deleting a topic, the topic cannot be deleted even if you use the force flag.\n \n-For more information about implementation details, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n+For more information about implementation, see [PR-7355](https://github.com/apache/pulsar/pull/7355).\n \n-#### Avoid the NPE occurs in method `ManagedLedgerImpl.isOffloadedNeedsDelete`\n+### Avoid  an NPE occurs in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method\n \n-When the default value of the `offload-deletion-lag` is set to null, it will cause an NPE problem. To fix this bug, we add null check in the method `ManagedLedgerImpl.isOffloadedNeedsDelete`.\n+When the default value of the `offload-deletion-lag` is set to `null`, an NPE occurs. To fix the bug, null check is added in the `ManagedLedgerImpl.isOffloadedNeedsDelete` method.\n                                                                                          \n-For more information about implementation details, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n+For more information about implementation, see [PR-7389](https://github.com/apache/pulsar/pull/7389).\n \n-#### Fix producer stuck issue due to NPE thrown when creating a new ledger\n+### Fix producer stuck issue due to NPE when creating a new ledger\n \n-NPE can be thrown when creating a ledger because the network address is unresolvable. If NPE is thrown before adding the timeout task, the timeout mechanism doesn't work. The unresolvable network address is commonly seen in the Kubernetes environment. It can happen when a bookie pod or a worker node restarts.\n+NPE occurs when creating a ledger if the network address is unresolvable. If NPE occurs before adding the timeout task, the timeout mechanism does not work. The unresolvable network address is common in the Kubernetes environment. It happens when a bookie pod or a worker node restarts.\n \n-This pull request does the followings:\n+This pull request fixes from the following perspectives:\n \n 1. Catch the NPE when creating a new ledger.\n-2. When the timeout task is triggered, always execute the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n-3. Add a mechanism to detect the `CreatingLedger` state is not moving.\n-\n-For more information about implementation details, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n-\n-#### Avoid NPEs at ledger creation when DNS failures happen\n-\n-As a followup to the fix in [PR-7401](https://github.com/apache/pulsar/pull/7401), use try/catch in all places where we're creating new ledgers to cover against NPEs triggered by DNS errors.\n+2. When the timeout task is triggered, it always executes the callback. It is totally fine because we already have the logic to ensure the callback is triggered only once.\n+3. Add a mechanism to detect that the `CreatingLedger` state is not moving.\n \n-For more information about implementation details, see [PR-7403](https://github.com/apache/pulsar/pull/7403).\n+For more information about implementation, see [PR-7401](https://github.com/apache/pulsar/pull/7401).\n \n-#### Decompress payload if needed in Key_Shared subscription\n \n-Decompress payload if needed in `Key_Shared` subscription.\n+### Fix NPE when using advertisedListeners\n \n-For more information about implementation details, see [PR-7416](https://github.com/apache/pulsar/pull/7416).\n+The broker failed to acquire ownership for the namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when TLS is not enabled.\n \n-#### Fix NPE when using advertisedListeners\n+For more information about implementation, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n \n-The broker failed to acquire ownership for namespace bundle when using `advertisedListeners=internal:pulsar://node1:6650,external:pulsar://node1.external:6650` with external listener name. Correct `BrokerServiceUrlTls` when tls is not enabled.\n+### Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled\n \n-For more information about implementation details, see [PR-7620](https://github.com/apache/pulsar/pull/7620).\n+When enabling the message deduplication in the `broker.conf` file, disabling it and then restarting the broker, the deduplication cursor is not deleted.\n \n-#### Fix the issue that the deduplication cursor cannot be deleted after message deduplication is disabled\n+This PR fixes the issue, so when you disable message deduplication, you can delete the deduplication cursor.\n \n-The issue occurs when enabling the message deduplication at the broker.conf and then disable it and restart the broker. The deduplication cursor cannot be deleted.\n+For more information about implementation, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n \n-For more information about implementation details, see [PR-7656](https://github.com/apache/pulsar/pull/7656).\n-\n-#### Get last entry is trying to read entry -1\n-\n-A return statement is missed in the current code when the entry is set to -1. Therefore, after the code is sent, the response is trying to read the entry and sends a second response:  \n+### Fix the issue that GetLastEntry() reads entry `-1`\n \n+Previously, the code does not include a return statement. If the entry is set to `-1`, after sending code, the response reads the entry and sends a second response, as shown in the following example.\n ```\n 16:34:25.779 [pulsar-io-54-7:org.apache.bookkeeper.client.LedgerHandle@748] ERROR org.apache.bookkeeper.client.LedgerHandle - IncorrectParameterException on ledgerId:0 firstEntry:-1 lastEntry:-1\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ConsumerImpl@1986] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [persistent://external-repl-prop/pulsar-function-admin/assignment][c-use-fw-localhost-0-function-assignment-initialize-reader-b21f7607c9] Successfully getLastMessageId 0:-1\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@602] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received error from server: Failed to get batch size for entry org.apache.bookkeeper.mledger.ManagedLedgerException: Incorrect parameter input\n 16:34:25.779 [pulsar-client-io-82-1:org.apache.pulsar.client.impl.ClientCnx@612] WARN  org.apache.pulsar.client.impl.ClientCnx - [id: 0xc78f4a0e, L:/127.0.0.1:55657 - R:localhost/127.0.0.1:55615] Received unknown request id from server: 10\n ```\n \n-For more information about implementation details, see [PR-7495](https://github.com/apache/pulsar/pull/7495).\n+PR-7495 adds a return statement to code, so GetLastEntry()  reads the last entry, instead of `-1`.  ", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMTMxNA==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474401314", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.\n          \n          \n            \n            The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409, and fix the recycle issue.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:26:31Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -148,109 +141,100 @@ The exception is thrown at Line 77, since `getAddress()` returns a `null` given\n 78        }\n ```\n \n-The default implementation for the `DnsResolver.useHostName()` is to return true.\n+The default implementation for the `DnsResolver.useHostName()` returns `true`.\n \n-For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+For more information about implementation, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n \n ## Java Client\n \n-#### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n+### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n \n-The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change uses the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and uses it directly as the header name.\n                                                                                                                                                                                                     \n-For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+For more information about implementation, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n \n-#### Fix batch ack set recycled multiple times\n+### Fix the issue that batch ack set is recycled multiple times\n \n-Fix batch ack set recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ack set.\n+The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDQwMTU2Nw==", "url": "https://github.com/apache/pulsar/pull/7756#discussion_r474401567", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl`'s internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n          \n          \n            \n            In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl` internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl` close callback.", "author": "Jennifer88huang", "createdAt": "2020-08-21T04:27:32Z", "path": "site2/website/blog/2020-08-17-Apache-Pulsar-2-6-1.md", "diffHunk": "@@ -148,109 +141,100 @@ The exception is thrown at Line 77, since `getAddress()` returns a `null` given\n 78        }\n ```\n \n-The default implementation for the `DnsResolver.useHostName()` is to return true.\n+The default implementation for the `DnsResolver.useHostName()` returns `true`.\n \n-For more information about implementation details, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n+For more information about implementation, see [PR-7361](https://github.com/apache/pulsar/pull/7361).\n \n ## Java Client\n \n-#### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n+### Fix the issue that the HTTP header used in Athenz authentication can not be renamed\n \n-The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change will hold the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and use it directly as the header name.\n+The authentication plugin for Athenz allows users to change the name of the HTTP header for sending an authentication token to a broker server with a parameter named `roleHeader`. The change uses the value of the `roleHeader` parameter on the `AuthenticationAthenz` side, and uses it directly as the header name.\n                                                                                                                                                                                                     \n-For more information about implementation details, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n+For more information about implementation, see [PR-7311](https://github.com/apache/pulsar/pull/7311).\n \n-#### Fix batch ack set recycled multiple times\n+### Fix the issue that batch ack set is recycled multiple times\n \n-Fix batch ack set recycled multiple times. The root cause is a race condition in group ack flush and cumulative Ack. So add recycled state check for the ack set.\n+The batch ack sets are recycled multiple times, due to race condition in group ack flush and cumulative Ack. So we add a recycled state check for the ack set in PR-7409,  and fix the recycle issue.\n \n-For more information about implementation details, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n+For more information about implementation, see [PR-7409](https://github.com/apache/pulsar/pull/7409).\n \n-#### Add authentication client with OAuth2 support\n+### Add authentication client with OAuth2 support\n \n-Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). \n+Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions, for example, publish messages to a topic or consume messages from a topic. \n \n-This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, and then the client gets an `access token` from the Oauth 2.0 server, and passes this `access token` to Pulsar broker to do the authentication.\n+This module is to support Pulsar Client Authentication Plugin for OAuth 2.0 directly. The client communicates with the Oauth 2.0 server, gets an `access token` from the Oauth 2.0 server, and passes the `access token` to Pulsar broker to do the authentication.\n \n-So, the broker could still use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n+So, the broker can use `org.apache.pulsar.broker.authentication.AuthenticationProviderToken`,\n and the user can add their own `AuthenticationProvider` to work with this module.\n \n-For more information about implementation details, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n-\n-#### Ensure the create subscription can be completed when the operation timeout happens\n+For more information about implementation, see [PR-7420](https://github.com/apache/pulsar/pull/7420).\n \n-Ensure the create subscription can be completed when the operation timeout happens.\n \n-For more information about implementation details, see [PR-7522](https://github.com/apache/pulsar/pull/7522).\n-\n-#### Do not try to subscribe to the topic if the consumer is closed\n+### Not subscribe to the topic when the consumer is closed\n \n Fix race condition on the closed consumer while reconnecting to the broker.\n \n-The race condition happens while the consumer reconnects to the broker. The cnx of the consumer is set to null when the consumer reconnects to the broker. If the consumer is closed at this time, the client will not send close consumer command to the broker. So, when the consumer reconnects to the broker, the consumer will send the subscribe command again. \n-\n-This pull request adds state check when connection opened of the consumer is opened. If the consumer is in closing or closed state, we do not need to send the subscribe command.\n+The race condition happens when the consumer reconnects to the broker. The connection of the consumer is set to `null` when the consumer reconnects to the broker. If the consumer is not connected to broker at this time, the client does not send the consumer command to the broker. So, when the consumer reconnects to the broker, the consumer sends the subscribe command again. \n \n-For more information about implementation details, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n+This pull request adds a state check when the `connectionOpened()` of the consumer opens. If the consumer is in closing or closed state, the consumer does not send the subscribe command.\n \n-#### Make OAuth2 authentication plugin to use AsyncHttpClient\n+For more information about implementation, see [PR-7589](https://github.com/apache/pulsar/pull/7589).\n \n-The OAuth2 client authentication plugin is using Apache HTTP client lib to make request, but it would be better to use AsyncHttpClient as we are using everywhere else in client and broker. Apache HTTP client was only meant to be used for hostname validation and, as explained in [#7612](https://github.com/apache/pulsar/issues/7612) we had better get rid of that dependency.\n+### OAuth2 authentication plugin uses AsyncHttpClient\n \n-For more information about implementation details, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n+Previously, the OAuth2 client authentication plugin used Apache HTTP client lib to make requests, Apache HTTP client is used to validate hostname. As suggested in [#7612](https://github.com/apache/pulsar/issues/7612), we get rid of the dependency of using Apache HTTP client.\n \n-#### Fix batch index filter issue in Consumer\n+In PR-7615, OAuth2 client authentication plugin uses AsyncHttpClient, which is used in client and broker. For more information about implementation, see [PR-7615](https://github.com/apache/pulsar/pull/7615).\n \n-Fix batch index filter issue in Consumer.\n-\n-For more information about implementation details, see [PR-7654](https://github.com/apache/pulsar/pull/7654).\n \n ## CPP Client\n \n-#### CPP Oauth2 authentication client\n+### CPP Oauth2 authentication client\n \n Pulsar supports authenticating clients using OAuth 2.0 access tokens. You can use tokens to identify a Pulsar client and associate with some \"principal\" (or \"role\") that is permitted to do some actions (eg: publish messages to a topic or consume messages from a topic). This change tries to support it in cpp client.\n \n-For more information about implementation details, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n+For more information about implementation, see [PR-7467](https://github.com/apache/pulsar/pull/7467).\n \n-#### Fix partition index error in close callback\n+### Fix partition index error in close callback\n \n-In partitioned producer/consumer's close callback, the partition index is always 0. We need to pass `ProducerImpl/ConsumerImpl`'s internal partition index field to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.\n+In partitioned producer/consumer's close callback, the partition index is always `0`. The `ProducerImpl/ConsumerImpl`'s internal partition index field should be passed to `PartitionedProducerImpl/PartitionedConsumerImpl`'s close callback.", "originalCommit": "978c01c797ef8e8650807176e19acdc488214984", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f2f4c1531f2074114c1c91619793fe9984ff5fed", "url": "https://github.com/apache/pulsar/commit/f2f4c1531f2074114c1c91619793fe9984ff5fed", "message": "fix docs\n\nSigned-off-by: xiaolong.ran <rxl@apache.org>", "committedDate": "2020-08-21T04:40:22Z", "type": "commit"}, {"oid": "91e4bfef2dfa759727c627d916f9e260e608706c", "url": "https://github.com/apache/pulsar/commit/91e4bfef2dfa759727c627d916f9e260e608706c", "message": "Merge branch 'master' into xiaolong/release-2.6.1-blog", "committedDate": "2020-08-21T05:20:20Z", "type": "commit"}]}