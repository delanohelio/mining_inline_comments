{"pr_number": 687, "pr_title": "Add commit strategies to support at least once delivery", "pr_createdAt": "2020-08-02T07:38:28Z", "pr_url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/687", "timeline": [{"oid": "bf36e7f024ac741b16596442e1ad5bd6b81e6da5", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/bf36e7f024ac741b16596442e1ad5bd6b81e6da5", "message": "Add commit strategies to support at least once delivery", "committedDate": "2020-08-02T07:52:39Z", "type": "forcePushed"}, {"oid": "b4f10c32bc43e4fa1af544a06624aed6da7ee37a", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/b4f10c32bc43e4fa1af544a06624aed6da7ee37a", "message": "Merge pull request #677 from smallrye/dependabot/maven/camel.version-3.4.2", "committedDate": "2020-08-02T15:45:18Z", "type": "forcePushed"}, {"oid": "02f4f7928266bcfc6533ef748219d270fa45ac2e", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/02f4f7928266bcfc6533ef748219d270fa45ac2e", "message": "Add commit strategies to support at least once delivery", "committedDate": "2020-08-03T02:50:57Z", "type": "forcePushed"}, {"oid": "84fa655e38ebbca4ce8615b28acb37a40104b421", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/84fa655e38ebbca4ce8615b28acb37a40104b421", "message": "Add commit strategies to support at least once delivery", "committedDate": "2020-08-03T03:00:48Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE3NTg4MQ==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/687#discussion_r464175881", "bodyText": "This solves the issue with compacted logs. Tested and working well, but it creates a new problem.\nThe user might decide to use the NONE Acknowledgement Strategy together with enable.auto.commit=false.\nhttps://smallrye.io/smallrye-reactive-messaging/2.1.1/apidocs/org/eclipse/microprofile/reactive/messaging/Acknowledgment.Strategy.html#NONE\nBasically the user wishes to never commit (there's a few use cases for this).\nBut with the implementation as is we will spring a leak. The received offset queue will grow without being emptied since there will be no call to ack. It looks to me that the Incoming publishers and connectors are decoupled in such a way that the connector doesn't know what strategy is configured.\nI see two possible solutions:\n\n\nHave the commit strategy be configurable like the nack strategy. Have the default depend on enable.auto.commit but let the user configure ignore if they wish.\n\n\nIf we have more than max.poll.records (default 500) received messages without committing a single one then we can assume that the user configured to never commit. In this case we can just turn the strategy off.\n\n\n1 is much simpler to maintain, but is a bit onerous on the user and requires proper documentation.\n2 requires no action from the user (should just work), but is a harder to reason and maintain.\nWhat do you think? If there was some way for the connector to know that NONE was used during setup that would be great, but I'm not seeing it.", "author": "pcasaes", "createdAt": "2020-08-03T03:26:34Z", "path": "smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/commit/KafkaThrottledLatestProcessedCommit.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package io.smallrye.reactive.messaging.kafka.commit;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionStage;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+\n+import io.smallrye.reactive.messaging.kafka.IncomingKafkaRecord;\n+import io.vertx.kafka.client.common.TopicPartition;\n+import io.vertx.kafka.client.consumer.OffsetAndMetadata;\n+import io.vertx.mutiny.core.Context;\n+import io.vertx.mutiny.kafka.client.consumer.KafkaConsumer;\n+\n+public class KafkaThrottledLatestProcessedCommit implements KafkaCommitHandler {\n+\n+    private static final long THROTTLE_TIME_IN_MILLIS = 5_000L;\n+    private static final Map<String, Map<Integer, TopicPartition>> TOPIC_PARTITIONS_CACHE = new ConcurrentHashMap<>();\n+\n+    private final Map<TopicPartition, Queue<Long>> receivedOffsetQueues = new HashMap<>();\n+    private final Map<TopicPartition, Set<Long>> processedOffsetSets = new HashMap<>();\n+\n+    private final KafkaConsumer<?, ?> consumer;\n+\n+    private volatile Context context;\n+    private long nextCommitTime;\n+\n+    public KafkaThrottledLatestProcessedCommit(KafkaConsumer<?, ?> consumer) {\n+        this.consumer = consumer;\n+    }\n+\n+    private <K, V> TopicPartition getTopicPartition(IncomingKafkaRecord<K, V> record) {\n+        return TOPIC_PARTITIONS_CACHE\n+                .computeIfAbsent(record.getTopic(), topic -> new ConcurrentHashMap<>())\n+                .computeIfAbsent(record.getPartition(), partition -> new TopicPartition(record.getTopic(), partition));\n+    }\n+\n+    private Set<Long> getProcessedOffsetSet(TopicPartition topicPartition) {\n+        return processedOffsetSets\n+                .computeIfAbsent(topicPartition, t -> new HashSet<>());\n+    }\n+\n+    private Queue<Long> getReceivedOffsetQueue(TopicPartition topicPartition) {\n+        return receivedOffsetQueues\n+                .computeIfAbsent(topicPartition, t -> new LinkedList<>());\n+    }\n+\n+    @Override\n+    public void partitionsAssigned(Context context, Set<TopicPartition> partitions) {\n+        this.context = context;\n+\n+        receivedOffsetQueues.clear();\n+        processedOffsetSets.clear();\n+\n+        resetNextCommitTime();\n+    }\n+\n+    @Override\n+    public <K, V> IncomingKafkaRecord<K, V> received(IncomingKafkaRecord<K, V> record) {\n+        TopicPartition recordsTopicPartition = getTopicPartition(record);\n+        getReceivedOffsetQueue(recordsTopicPartition).offer(record.getOffset());\n+\n+        return record;\n+    }", "originalCommit": "861859c3a18f65216b2a180bd2b34aed554a8b8b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDIxMjA5Nw==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/687#discussion_r464212097", "bodyText": "(1) definitely. We can use \"enable.auto.commit\" to determine the default. Adding another attribute to configure the strategy would be nice and allow configuring the application to use the current model.\nThat's what I wrote on the SmallRye Mailing list (for traceability purpose):\n\nTo implement this, we would need a new attribute to set the strategy:\n\ncommit every offset when the message is acked -> Today's solution\nlet kafka auto-commit -> enable the auto-commit and ignore acks from the app\nbatch/cursor tracking strategy -> the strategy described in this mail (which would need a > better name)\nEach of them has pros and cons, so they would need to be clearly documented.", "author": "cescoffier", "createdAt": "2020-08-03T06:12:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDE3NTg4MQ=="}], "type": "inlineReview"}, {"oid": "3cedf8680fd0dd1d5c11cbbefe3cb170ffa92335", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/3cedf8680fd0dd1d5c11cbbefe3cb170ffa92335", "message": "Contemplate compacted log in throttled commit strategy", "committedDate": "2020-08-03T03:52:05Z", "type": "forcePushed"}, {"oid": "235a43ee17bcf261c45ff7381e3d5c58006f95b0", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/235a43ee17bcf261c45ff7381e3d5c58006f95b0", "message": "Add commit strategy configuration", "committedDate": "2020-08-03T12:05:00Z", "type": "forcePushed"}, {"oid": "6c8ffc4a277269c475612903c6bba4d44ab5da51", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/6c8ffc4a277269c475612903c6bba4d44ab5da51", "message": "Add commit strategy configuration", "committedDate": "2020-08-03T12:31:23Z", "type": "forcePushed"}, {"oid": "f35961da8d180ae9c12994adf9c87609d873eeac", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/f35961da8d180ae9c12994adf9c87609d873eeac", "message": "Add commit strategy configuration", "committedDate": "2020-08-03T12:46:36Z", "type": "forcePushed"}, {"oid": "c566ed913bdcfa29cffaa8bf21e434c4cb0e9e1f", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/c566ed913bdcfa29cffaa8bf21e434c4cb0e9e1f", "message": "Add kafka commit strategies", "committedDate": "2020-08-11T06:43:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODc2MzExNQ==", "url": "https://github.com/smallrye/smallrye-reactive-messaging/pull/687#discussion_r468763115", "bodyText": "Line 158 and 163 are the only actual changes. We have to inform the commit handler that a re-balance took place so it can reset its internal data structures.", "author": "pcasaes", "createdAt": "2020-08-11T17:57:47Z", "path": "smallrye-reactive-messaging-kafka/src/main/java/io/smallrye/reactive/messaging/kafka/impl/KafkaSource.java", "diffHunk": "@@ -111,52 +124,59 @@ public KafkaSource(Vertx vertx,\n                         return Optional.of(rebalanceFromGroupListeners.get());\n                     }\n                     return Optional.empty();\n-                })\n-                .ifPresent(listener -> {\n-                    // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n-                    // re-balance. To do so we must wait until after the poll interval time or\n-                    // poll interval time + session timeout if group instance id is not null.\n-                    // We will retry the re-balance consumer listener on failure using an exponential backoff until\n-                    // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n-                    // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n-                    // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n-                    // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n-                    final long consumerReEnableWaitTime = Long.parseLong(\n-                            kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n-                            + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n-                                    : Long.parseLong(\n-                                            kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n-                                                    \"10000\")))\n-                            + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n-\n-                    kafkaConsumer.partitionsAssignedHandler(set -> {\n-                        kafkaConsumer.pause();\n-                        log.executingConsumerAssignedRebalanceListener(group);\n-                        listener.onPartitionsAssigned(kafkaConsumer, set)\n-                                .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n-                                .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n-                                .expireIn(consumerReEnableWaitTime)\n-                                .subscribe()\n-                                .with(\n-                                        a -> {\n-                                            log.executedConsumerAssignedRebalanceListener(group);\n-                                            kafkaConsumer.resume();\n-                                        },\n-                                        t -> {\n-                                            log.reEnablingConsumerforGroup(group);\n-                                            kafkaConsumer.resume();\n-                                        });\n-                    });\n-\n-                    kafkaConsumer.partitionsRevokedHandler(set -> {\n-                        log.executingConsumerRevokedRebalanceListener(group);\n-                        listener.onPartitionsRevoked(kafkaConsumer, set)\n-                                .subscribe()\n-                                .with(\n-                                        a -> log.executedConsumerRevokedRebalanceListener(group),\n-                                        t -> log.unableToExecuteConsumerRevokedRebalanceListener(group, t));\n-                    });\n                 });\n+\n+        if (rebalanceListener.isPresent()) {\n+            KafkaConsumerRebalanceListener listener = rebalanceListener.get();\n+            // If the re-balance assign fails we must resume the consumer in order to force a consumer group\n+            // re-balance. To do so we must wait until after the poll interval time or\n+            // poll interval time + session timeout if group instance id is not null.\n+            // We will retry the re-balance consumer listener on failure using an exponential backoff until\n+            // we can allow the kafka consumer to do it on its own. We do this because by default it would take\n+            // 5 minutes for kafka to do this which is too long. With defaults consumerReEnableWaitTime would be\n+            // 500000 millis. We also can't simply retry indefinitely because once the consumer has been paused\n+            // for consumerReEnableWaitTime kafka will force a re-balance once resumed.\n+            final long consumerReEnableWaitTime = Long.parseLong(\n+                    kafkaConfiguration.getOrDefault(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, \"300000\"))\n+                    + (kafkaConfiguration.get(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG) == null ? 0L\n+                            : Long.parseLong(\n+                                    kafkaConfiguration.getOrDefault(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,\n+                                            \"10000\")))\n+                    + 11_000L; // it's possible that it might expire 10 seconds before when we need it to\n+\n+            kafkaConsumer.partitionsAssignedHandler(set -> {\n+                kafkaConsumer.pause();\n+                log.executingConsumerAssignedRebalanceListener(group);\n+                listener.onPartitionsAssigned(kafkaConsumer, set)\n+                        .onFailure().invoke(t -> log.unableToExecuteConsumerAssignedRebalanceListener(group, t))\n+                        .onFailure().retry().withBackOff(Duration.ofSeconds(1), Duration.ofSeconds(10))\n+                        .expireIn(consumerReEnableWaitTime)\n+                        .subscribe()\n+                        .with(\n+                                a -> {\n+                                    log.executedConsumerAssignedRebalanceListener(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);\n+                                    kafkaConsumer.resume();\n+                                },\n+                                t -> {\n+                                    log.reEnablingConsumerforGroup(group);\n+                                    commitHandler.partitionsAssigned(vertx.getOrCreateContext(), set);", "originalCommit": "c566ed913bdcfa29cffaa8bf21e434c4cb0e9e1f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e5a10ac335f1b935cb2efae963b59861bf23407", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/0e5a10ac335f1b935cb2efae963b59861bf23407", "message": "Add kafka commit strategies", "committedDate": "2020-08-11T18:10:42Z", "type": "commit"}, {"oid": "0e5a10ac335f1b935cb2efae963b59861bf23407", "url": "https://github.com/smallrye/smallrye-reactive-messaging/commit/0e5a10ac335f1b935cb2efae963b59861bf23407", "message": "Add kafka commit strategies", "committedDate": "2020-08-11T18:10:42Z", "type": "forcePushed"}]}