{"pr_number": 384, "pr_title": "Add script to automate Atlas Checks Spark job on EC2", "pr_createdAt": "2020-10-16T18:22:16Z", "pr_url": "https://github.com/osmlab/atlas-checks/pull/384", "timeline": [{"oid": "7c909202f6c128ec885a5883e17e56afc73861b2", "url": "https://github.com/osmlab/atlas-checks/commit/7c909202f6c128ec885a5883e17e56afc73861b2", "message": "Add script to automate Atlas Checks Spark job on EC2", "committedDate": "2020-10-16T18:16:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0MzA5Mg==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r509543092", "bodyText": "Are these comments and TODO still useful? Consider deleting if not.", "author": "zlavergne", "createdAt": "2020-10-21T18:15:58Z", "path": "scripts/cloud-check-control/cloudAtlasCheckControl.py", "diffHunk": "@@ -0,0 +1,663 @@\n+#!/usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+execute atlas-checks on an EC2 instance\n+\"\"\"\n+import argparse\n+import logging\n+import os\n+import sys\n+import time\n+\n+import boto3.ec2\n+import paramiko\n+import scp\n+from botocore.exceptions import ClientError\n+from paramiko.auth_handler import AuthenticationException\n+\n+\n+VERSION = \"0.1.0\"\n+ec2 = boto3.client(\"ec2\")\n+\n+\n+def setup_logging(default_level=logging.INFO):\n+    \"\"\"\n+    Setup logging configuration\n+    \"\"\"\n+    logging.basicConfig(\n+        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n+        level=default_level,\n+        datefmt=\"%Y-%m-%d %H:%M:%S\",\n+    )\n+    return logging.getLogger(\"CloudAtlasChecksControl\")\n+\n+\n+def finish(error_message=None, status=0):\n+    \"\"\"exit the process\n+\n+    Method to exit the Python script. It will log the given message and then exit().\n+\n+    :param error_message: Error message to log upon exiting the process\n+    :param status: return code to exit the process with\n+    \"\"\"\n+    if error_message:\n+        logger.error(error_message)\n+    else:\n+        logger.critical(\"Done\")\n+    exit(status)\n+\n+\n+class CloudAtlasChecksControl:\n+    \"\"\"Main Class to control atlas checks spark job on EC2\"\"\"\n+\n+    def __init__(\n+        self,\n+        timeoutMinutes=6000,\n+        key=\"\",\n+        instanceId=\"\",\n+        processes=32,\n+        memory=256,\n+        formats=\"flags\",\n+        countries=\"BRA\",\n+        s3InFolder=None,\n+        s3OutFolder=None,\n+        terminate=False,\n+        templateName=\"atlas_checks-ec2-template\",\n+        atlasConfig=\"https://raw.githubusercontent.com/osmlab/atlas-checks/dev/config/configuration.json\",\n+    ):\n+        self.timeoutMinutes = timeoutMinutes\n+        self.key = key\n+        self.instanceId = instanceId\n+        self.s3InFolder = s3InFolder\n+        self.processes = processes\n+        self.memory = memory\n+        self.formats = formats\n+        self.countries = countries\n+        self.s3OutFolder = s3OutFolder\n+        self.terminate = terminate\n+        self.templateName = templateName\n+        self.homeDir = \"/home/ubuntu/\"\n+        self.atlasCheckDir = os.path.join(self.homeDir, \"atlas-checks/\")\n+        self.atlasOutDir = os.path.join(self.homeDir, \"output/\")\n+        self.atlasLogDir = os.path.join(self.homeDir, \"log/\")\n+        self.atlasCheckLogName = \"atlasCheck.log\"\n+        self.atlasCheckLog = os.path.join(self.atlasLogDir, self.atlasCheckLogName)\n+        self.atlasConfig = atlasConfig\n+\n+        self.client = None\n+        self.instanceName = \"AtlasChecks\"\n+\n+    @property\n+    def s3InBucket(self):\n+        return self.s3InFolder.strip(\"/\").split(\"/\")[0]\n+\n+    @property\n+    def atlasInDir(self):\n+        return os.path.join(self.homeDir, self.s3InFolder)\n+\n+    def atlasCheck(self):\n+        \"\"\"Submit an spark job to perform atlas checks on an EC2 instance.\n+\n+        If the CloudAtlasChecksControl includes an instance ID then atlas checks will\n+        be executed on that instance. If no instance ID is defined then it will\n+        create a new instance.\n+\n+        Dependencies:\n+          - self.instanceId - indicates a running instance or \"\" to create one\n+          - self.S3Atlas - indicates the S3 bucket and path that contains atlas files\n+        \"\"\"\n+        logger.info(\"Start Sharding Process...\")\n+        if self.instanceId == \"\":\n+            self.create_instance()\n+            self.get_instance_info()\n+\n+        if not self.is_process_running(\"SparkSubmit\"):\n+            cmd = \"mkdir -p {} {}\".format(self.atlasLogDir, self.atlasOutDir)\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to create directory {}\".format(cmd), -1)\n+\n+            # remove the success or failure files from any last run.\n+            if self.ssh_cmd(\"rm -f {}/_*\".format(self.atlasOutDir)):\n+                finish(\"Unable to clean up old status files\", -1)\n+\n+            # mount the s3 bucket\n+            cmd = \"mount |grep ~/{}\".format(self.s3InBucket)\n+            if not self.ssh_cmd(cmd, quiet=True):\n+                cmd = \"sudo umount ~/{}\".format(self.s3InBucket)\n+                if self.ssh_cmd(cmd, quiet=True):\n+                    finish(\"Unable to unmount S3 bucket\", -1)\n+            else:\n+                cmd = \"mkdir -p ~/{}\".format(self.s3InBucket)\n+                if self.ssh_cmd(cmd):\n+                    finish(\"Unable to create directory {}\".format(cmd), -1)\n+\n+            cmd = \"s3fs {0} ~/{0} -o ro,umask=222,gid=1000,uid=1000 -o use_cache=/tmp/atlas/ -o iam_role=auto\".format(\n+                self.s3InBucket\n+            )\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to mount S3 bucket\", -1)\n+\n+            atlasConfig = self.atlasConfig\n+            # if configuration.json is a file then copy it to the EC2 instance\n+            if self.atlasConfig.find(\"http\") < 0:\n+                atlasConfig = \"file://\" + os.path.join(self.homeDir, self.atlasConfig)\n+\n+            # sync local input folder with S3 input bucket and directory\n+            # cmd = \"aws s3 sync s3://{} {}\".format(self.s3InFolder, self.atlasInDir)\n+            # if self.ssh_cmd(cmd):\n+            #     finish(\"Unable to pull atlas input files from S3\", -1)\n+\n+            # TODO Call command to submit job", "originalCommit": "7c909202f6c128ec885a5883e17e56afc73861b2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDE3NzA4MQ==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r510177081", "bodyText": "Ooops. removed.", "author": "atiannicelli", "createdAt": "2020-10-22T13:49:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0MzA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU0NTcwMw==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r509545703", "bodyText": "let's specify that this is GB (\"###g\")", "author": "zlavergne", "createdAt": "2020-10-21T18:19:17Z", "path": "scripts/cloud-check-control/README.md", "diffHunk": "@@ -0,0 +1,118 @@\n+# cloudAtlasCheckControl - EC2 Atlas Checks Controller\n+\n+## Prerequisites\n+\n+### AWS EC2 and S3\n+\n+To execute the entire Atlas Check process the user will need access to an AWS account with EC2 and S3 resources. Please visit the [AWS website](https://aws.amazon.com/) to create and manage your account. This script makes use of [AWS EC2](https://aws.amazon.com/ec2/) resources to execute the Atlas Check Spark job and [AWS S3](https://aws.amazon.com/s3) object store to save the Atlas Checks results. To communicate with the AWS console and control the resources the scripts needs access the [AWS CLI](https://aws.amazon.com/cli/). To execute the AWS CLI you will need an \"Access Key\" and \"Secret Access Key\". These you will need to get from your AWS administrator. You will also need to set the default region. (e.g. \"us-west-1\").\n+\n+- [Install](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\n+  and then\n+  [configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)\n+  AWS CLI\n+\n+### AWS Key Pair\n+\n+To be able to execute commands on the EC2 instance that is created the scripts need to be able to ssh to the EC2 instance. To allow the script to ssh to the EC2 instance you need to create an AWS key pair.\n+\n+- [Create an AWS key pair](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair)\n+\n+Make sure that your \"my-key-pair.pem\" file that is produced during this process is placed in your ~/.ssh/ directory and that the permissions are set correctly as the instructions indicate. Once a key pair has been created you may also need to adjust your [AWS security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) to allow ssh access from your local server. Please see your administrator for information on whether this is required for your setup.\n+\n+### AWS EC2 Template\n+\n+This script takes advantage of the ability for AWS to start EC2 instances from templates that have been created from an image of another EC2 image. This scrip assumes that you have created a template from an image of an EC2 instance that you will use to start new instances. Information of [Launch Templates](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html?icmpid=docs_ec2_console) is available from AWS, but for the purposes of this script and document we will assume that you have installed the python environment on an EC2 instance, created an image from that and then created a launch template from that image to use in the Atlas Checks process.\n+\n+### Python3\n+\n+Make sure python3 is installed. Instructions below for Mac. Also make sure that you source your .zshrc or restart your shell after you perform these steps.\n+\n+```\n+brew install pyenv\n+brew install python\n+pyenv install 3.8.5\n+pyenv global 3.8.5\n+echo -e 'if command -v pyenv 1>/dev/null 2>&1; then\\n  eval \"$(pyenv init -)\"\\nfi' >> ~/.zshrc\n+```\n+\n+### Python libraries\n+\n+Install python libraries necessary to execute the application.\n+\n+- boto3 - the python libraries to control EC2 instances\n+- paramiko - ssh agent so the script can ssh to the EC2 instance.\n+\n+```\n+sudo pip install boto3 paramiko\n+```\n+\n+## Running the cloudAtlasCheckControl.py script\n+\n+There are three major commands you can use with the cloudAtlasCheckControl.py script. Each one has its own help. The main help display shows the flags and parameters that work with all the commands. The following parameters apply to all commands and must be used on the command line before the command.\n+\n+- `-h, --help` - Show help message and exit\n+- `-n NAME, --name NAME` - If creating an EC2 instance, this NAME will be used to override the default EC2 instance name: 'AtlasChecks'. The script doesn't use the EC2 instance name so this can be set to any value that the user would like to use.\n+- `-t TEMPLATE, --template TEMPLATE` - This parameter sets EC2 template name that will be used to create the EC2 instance from. If used, this parameter will override the default: 'atlas_checks-ec2-template'. At this time a template MUST be specified to operate properly.\n+- `-m MINUTES, --minutes MINUTES` - This parameter will set the timeout, in minutes, that the script will use when waiting for the Atlas Check spark job to complete. The default is 6000 minutes.\n+- `-v, --version` - Display the current version of the script.\n+- `-T, --terminate` - This flag indicates that the user would like to terminate the EC2 instance after successful operation. If this flag is not specified then the script will leave any EC2 instance used or created running upon completion of the script. There are a few different scenarios where the script will not terminate even if termination is requested. The script will NOT terminate an EC2 instance if an error is encountered when processing any command. The script will also not terminate an EC2 instance if the check command is performed but the sync command is skipped (see --out parameter).\n+\n+The following parameters are used by one or more of the commands. These parameters may not apply to all command so see the command help file for which of these parameters are accepted, required, or optional for each command.\n+\n+- `-k KEY, --key KEY` - This parameter is the AWS key pair name created above. This parameter specifies the name of the key as specified in the AWS console. The similarly named pem file must be located in the user's ~/.ssh/ directory. See the following URL for instructions on creating a key: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html. (e.g. `--key=aws-key`)\n+- `-i ID, --id ID` - This parameter specifies the ID of an existing VM instance to use. If this parameter is specified then the command that is being executed will NOT create a new EC2 instance and will, instead, attempt to connect to an EC2 instance with the given ID. Please note that this is the ID of the instance and not the name or description of the EC2 instance. The script will indicate the ID of the EC2 instance used in the log whether an EC2 instance is created or a running EC2 instance is used.\n+- `-o OUT, --output OUT` - The S3 Output directory to push output results upon successful completion of Atlas Check spark job processing. If this parameter is not specified when executing Atlas Checks then the output of the checks will not be pushed to an external object store and the EC2 instance will not be terminated even if -T is used. (e.g. '--out=atlas-bucket/Atlas_Checks')\n+- `-i IN, --input IN` - The S3 Input directory where atlas files are located to execute checks on. This S3 bucket will be mounted to the EC2 instance for readonly processing of the atlas files.\n+- `-p PROCESSES, --processes PROCESSES` - The number of parallel osmium processes to start. Note that when processing a large PBF file each osmium process will use a great deal of memory so small numbers of parallel processes is suggested. (Default: 32)\n+- `-j CONFIG, --config CONFIG` - The json file to use as an [Atlas configuration file](https://github.com/osmlab/atlas-checks/blob/dev/docs/configuration.md) to control the Atlas Checks process. This can either be a fully qualified URL or a full path to a json file in the S3 Input bucket. (e.g my-s3-bucket/Atlas_Checks/configurations/special_config.json) (Default: https://raw.githubusercontent.com/osmlab/atlas-checks/dev/config/configuration.json)\n+- `-f FORMATS --formats FORMATS` - A comma separated list of formats to use to determine the output format for Atlas Checks. (Default: 'flags')\n+- `-m MEMORY,--memory MEMORY` - The Maximum amount of memory for the Spark job to use. (Default: 256)", "originalCommit": "7c909202f6c128ec885a5883e17e56afc73861b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTU2MDAxNg==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r509560016", "bodyText": "Same considerations about truthiness in PBF shard script", "author": "zlavergne", "createdAt": "2020-10-21T18:33:41Z", "path": "scripts/cloud-check-control/cloudAtlasCheckControl.py", "diffHunk": "@@ -0,0 +1,663 @@\n+#!/usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+execute atlas-checks on an EC2 instance\n+\"\"\"\n+import argparse\n+import logging\n+import os\n+import sys\n+import time\n+\n+import boto3.ec2\n+import paramiko\n+import scp\n+from botocore.exceptions import ClientError\n+from paramiko.auth_handler import AuthenticationException\n+\n+\n+VERSION = \"0.1.0\"\n+ec2 = boto3.client(\"ec2\")\n+\n+\n+def setup_logging(default_level=logging.INFO):\n+    \"\"\"\n+    Setup logging configuration\n+    \"\"\"\n+    logging.basicConfig(\n+        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n+        level=default_level,\n+        datefmt=\"%Y-%m-%d %H:%M:%S\",\n+    )\n+    return logging.getLogger(\"CloudAtlasChecksControl\")\n+\n+\n+def finish(error_message=None, status=0):\n+    \"\"\"exit the process\n+\n+    Method to exit the Python script. It will log the given message and then exit().\n+\n+    :param error_message: Error message to log upon exiting the process\n+    :param status: return code to exit the process with\n+    \"\"\"\n+    if error_message:\n+        logger.error(error_message)\n+    else:\n+        logger.critical(\"Done\")\n+    exit(status)\n+\n+\n+class CloudAtlasChecksControl:\n+    \"\"\"Main Class to control atlas checks spark job on EC2\"\"\"\n+\n+    def __init__(\n+        self,\n+        timeoutMinutes=6000,\n+        key=\"\",\n+        instanceId=\"\",\n+        processes=32,\n+        memory=256,\n+        formats=\"flags\",\n+        countries=\"BRA\",\n+        s3InFolder=None,\n+        s3OutFolder=None,\n+        terminate=False,\n+        templateName=\"atlas_checks-ec2-template\",\n+        atlasConfig=\"https://raw.githubusercontent.com/osmlab/atlas-checks/dev/config/configuration.json\",\n+    ):\n+        self.timeoutMinutes = timeoutMinutes\n+        self.key = key\n+        self.instanceId = instanceId\n+        self.s3InFolder = s3InFolder\n+        self.processes = processes\n+        self.memory = memory\n+        self.formats = formats\n+        self.countries = countries\n+        self.s3OutFolder = s3OutFolder\n+        self.terminate = terminate\n+        self.templateName = templateName\n+        self.homeDir = \"/home/ubuntu/\"\n+        self.atlasCheckDir = os.path.join(self.homeDir, \"atlas-checks/\")\n+        self.atlasOutDir = os.path.join(self.homeDir, \"output/\")\n+        self.atlasLogDir = os.path.join(self.homeDir, \"log/\")\n+        self.atlasCheckLogName = \"atlasCheck.log\"\n+        self.atlasCheckLog = os.path.join(self.atlasLogDir, self.atlasCheckLogName)\n+        self.atlasConfig = atlasConfig\n+\n+        self.client = None\n+        self.instanceName = \"AtlasChecks\"\n+\n+    @property\n+    def s3InBucket(self):\n+        return self.s3InFolder.strip(\"/\").split(\"/\")[0]\n+\n+    @property\n+    def atlasInDir(self):\n+        return os.path.join(self.homeDir, self.s3InFolder)\n+\n+    def atlasCheck(self):\n+        \"\"\"Submit an spark job to perform atlas checks on an EC2 instance.\n+\n+        If the CloudAtlasChecksControl includes an instance ID then atlas checks will\n+        be executed on that instance. If no instance ID is defined then it will\n+        create a new instance.\n+\n+        Dependencies:\n+          - self.instanceId - indicates a running instance or \"\" to create one\n+          - self.S3Atlas - indicates the S3 bucket and path that contains atlas files\n+        \"\"\"\n+        logger.info(\"Start Sharding Process...\")\n+        if self.instanceId == \"\":\n+            self.create_instance()\n+            self.get_instance_info()\n+\n+        if not self.is_process_running(\"SparkSubmit\"):\n+            cmd = \"mkdir -p {} {}\".format(self.atlasLogDir, self.atlasOutDir)\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to create directory {}\".format(cmd), -1)\n+\n+            # remove the success or failure files from any last run.\n+            if self.ssh_cmd(\"rm -f {}/_*\".format(self.atlasOutDir)):\n+                finish(\"Unable to clean up old status files\", -1)\n+\n+            # mount the s3 bucket\n+            cmd = \"mount |grep ~/{}\".format(self.s3InBucket)\n+            if not self.ssh_cmd(cmd, quiet=True):\n+                cmd = \"sudo umount ~/{}\".format(self.s3InBucket)\n+                if self.ssh_cmd(cmd, quiet=True):\n+                    finish(\"Unable to unmount S3 bucket\", -1)\n+            else:\n+                cmd = \"mkdir -p ~/{}\".format(self.s3InBucket)\n+                if self.ssh_cmd(cmd):\n+                    finish(\"Unable to create directory {}\".format(cmd), -1)\n+\n+            cmd = \"s3fs {0} ~/{0} -o ro,umask=222,gid=1000,uid=1000 -o use_cache=/tmp/atlas/ -o iam_role=auto\".format(\n+                self.s3InBucket\n+            )\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to mount S3 bucket\", -1)\n+\n+            atlasConfig = self.atlasConfig\n+            # if configuration.json is a file then copy it to the EC2 instance\n+            if self.atlasConfig.find(\"http\") < 0:\n+                atlasConfig = \"file://\" + os.path.join(self.homeDir, self.atlasConfig)\n+\n+            # sync local input folder with S3 input bucket and directory\n+            # cmd = \"aws s3 sync s3://{} {}\".format(self.s3InFolder, self.atlasInDir)\n+            # if self.ssh_cmd(cmd):\n+            #     finish(\"Unable to pull atlas input files from S3\", -1)\n+\n+            # TODO Call command to submit job\n+            cmd = (\n+                \"/opt/spark/bin/spark-submit\"\n+                + \" --class=org.openstreetmap.atlas.checks.distributed.ShardedIntegrityChecksSparkJob\"\n+                + \" --master=local[{}]\".format(self.processes)\n+                + \" --conf='spark.driver.memory={}g'\".format(self.memory)\n+                + \" --conf='spark.rdd.compress=true'\"\n+                + \" /home/ubuntu/atlas-checks/build/libs/atlas-checks-6.1.3-SNAPSHOT-shadow.jar\"\n+                + \" -inputFolder='{}'\".format(self.atlasInDir)\n+                + \" -output='{}'\".format(self.atlasOutDir)\n+                + \" -outputFormats='{}'\".format(self.formats)\n+                + \" -countries='{}'\".format(self.countries)\n+                + \" -configFiles='{}'\".format(atlasConfig)\n+                + \" > {} 2>&1 &\".format(self.atlasCheckLog)\n+            )\n+\n+            logger.info(\"Submitting spark job: {}\".format(cmd))\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to execute spark job\", -1)\n+        else:\n+            logger.info(\"Detected a running sharding process.\")\n+\n+        # wait for script to complete\n+        if self.wait_for_process_to_complete():\n+            finish(\n+                \"Timeout waiting for script to complete. TODO - instructions to reconnect.\",\n+                -1,\n+            )\n+        # download log\n+        self.get_files(self.atlasCheckLog, \"./\")\n+\n+        self.sync()\n+\n+    def sync(self):\n+        \"\"\"Sync an existing instance containing already generated atlas output with s3\n+\n+        Dependencies:\n+          - self.instanceId - indicates a running instance or \"\" to create one\n+          - self.s3OutFolder - the S3 bucket and folder path to push the output\n+          - self.terminate - indicates if the EC2 instance should be terminated\n+        \"\"\"\n+        if self.s3OutFolder is None:\n+            logger.warning(\n+                \"No S3 output folder specified, skipping s3 sync. Use -o 's3folder/path' to sync to s3\"\n+            )\n+            return\n+        logger.info(\n+            \"Syncing EC2 instance atlas-checks output with S3 bucket {}.\".format(\n+                self.s3OutFolder\n+            )\n+        )\n+\n+        # push output to s3\n+        cmd = \"aws s3 sync --exclude *.crc {} s3://{} \".format(\n+            self.atlasOutDir, self.s3OutFolder\n+        )\n+        if self.ssh_cmd(cmd):\n+            finish(\"Unable to sync with S3\", -1)\n+        # terminate instance\n+        if self.terminate:\n+            self.terminate_instance()\n+\n+    def clean(self):\n+        \"\"\"Clean a running Instance of all produced folders and files\n+\n+        This readies the instance for a clean atlas check run or terminates an EC2\n+        instance completely.\n+\n+        Dependencies:\n+          - self.instanceId - indicates a running instance or \"\" to create one\n+          - self.terminate - indicates if the EC2 instance should be terminated\n+        \"\"\"\n+        if self.terminate:\n+            logger.info(\"Terminating EC2 instance.\")\n+            self.terminate_instance()\n+        else:\n+            logger.info(\"Cleaning up EC2 instance.\")\n+            cmd = \"rm -rf {}/* {}/* \".format(self.atlasOutDir, selfatlasLogDir)\n+            if self.ssh_cmd(cmd):\n+                finish(\"Unable to clean\", -1)\n+\n+    def create_instance(self):\n+        \"\"\"Create Instance from atlas_checks-ec2-template template\n+\n+        Dependencies:\n+          - self.templateId\n+          - self.instanceName\n+        :return:\n+        \"\"\"\n+        logger.info(\"Creating EC2 instance from {} template.\".format(self.templateName))\n+        try:\n+            logger.info(\"Create instance...\")\n+            response = ec2.run_instances(\n+                LaunchTemplate={\"LaunchTemplateName\": self.templateName},\n+                TagSpecifications=[\n+                    {\n+                        \"ResourceType\": \"instance\",\n+                        \"Tags\": [{\"Key\": \"Name\", \"Value\": self.instanceName}],\n+                    }\n+                ],\n+                MaxCount=1,\n+                MinCount=1,\n+                KeyName=self.key,\n+            )\n+            self.instanceId = response[\"Instances\"][0][\"InstanceId\"]\n+            logger.info(\"Instance {} was created\".format(self.instanceId))\n+        except ClientError as e:\n+            finish(e, -1)\n+\n+    def terminate_instance(self):\n+        \"\"\"Terminate Instance\n+\n+        Dependencies:\n+          - self.templateId\n+        \"\"\"\n+        logger.info(\"Terminating EC2 instance {}\".format(self.instanceId))\n+        try:\n+            response = ec2.terminate_instances(InstanceIds=[self.instanceId])\n+            logger.info(\"Instance {} was terminated\".format(self.instanceId))\n+        except ClientError as e:\n+            finish(e, -1)\n+\n+    def ssh_connect(self):\n+        \"\"\"Connect to an EC2 instance\"\"\"\n+        for timeout in range(5):\n+            try:\n+                keyFile = \"{}/.ssh/{}.pem\".format(os.environ.get(\"HOME\"), self.key)\n+                key = paramiko.RSAKey.from_private_key_file(keyFile)\n+                self.client = paramiko.SSHClient()\n+                self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n+                logger.debug(\n+                    \"Connecting to {} ... \".format(self.instance[\"PublicDnsName\"])\n+                )\n+                self.client.connect(\n+                    self.instance[\"PublicDnsName\"], username=\"ubuntu\", pkey=key\n+                )\n+                logger.info(\n+                    \"Connected to {} ... \".format(self.instance[\"PublicDnsName\"])\n+                )\n+                self.scpClient = scp.SCPClient(self.client.get_transport())\n+                break\n+            except AuthenticationException as error:\n+                logger.error(\n+                    \"Authentication failed: did you remember to create an SSH key? {error}\"\n+                )\n+                raise error\n+            except paramiko.ssh_exception.NoValidConnectionsError:\n+                time.sleep(15)\n+                continue\n+\n+    def put_files(self, localFiles, remoteDirectory):\n+        \"\"\"Put files from local system onto running EC2 instance\"\"\"\n+        if self.scpClient is None:\n+            self.ssh_connect()\n+        try:\n+            self.scpClient.put(localFiles, remoteDirectory)\n+        except scp.IOException as error:\n+            logger.error(\"Unable to copy files. {error}\")\n+            raise error\n+        logger.debug(\"Files: \" + localFiles + \" uploaded to: \" + remoteDirectory)\n+\n+    def get_files(self, remoteFiles, localDirectory):\n+        \"\"\"Get files from running ec2 instance to local system\"\"\"\n+        if self.scpClient is None:\n+            self.ssh_connect()\n+        try:\n+            self.scpClient.get(remoteFiles, localDirectory)\n+        except scp.SCPException as error:\n+            logger.error(\"Unable to copy files. {error}\")\n+            raise error\n+        logger.debug(\"Files: \" + remoteFiles + \" downloaded to: \" + localDirectory)\n+\n+    def ssh_cmd(self, cmd, quiet=False):\n+        \"\"\"Issue an ssh command on the remote EC2 instance\n+\n+        :param cmd: the command string to execute on the remote system\n+        :param quiet: If true, don't display errors on failures\n+        :returns: Returns the status of the completed ssh command.\n+        \"\"\"\n+        if self.client is None:\n+            self.ssh_connect()\n+        logger.debug(\"Issuing remote command: {} ... \".format(cmd))\n+        ssh_stdin, ssh_stdout, ssh_stderr = self.client.exec_command(cmd)\n+        if ssh_stdout.channel.recv_exit_status() and not quiet:\n+            logger.error(\" Remote command output:\")\n+            logger.error(\"\\t\".join(map(str, ssh_stderr.readlines())))\n+        return ssh_stdout.channel.recv_exit_status()\n+\n+    def wait_for_process_to_complete(self):\n+        \"\"\"Wait for process to complete\n+\n+        Will block execution while waiting for the completion of the spark\n+        submit on the EC2 instance. Upon completion of the scrip it will look\n+        at the log file produced to see if it completed successfully. If the\n+        sharding script failed then this function will exit.\n+\n+        :returns: 0 - if sharding process completed successfully\n+        :returns: 1 - if sharding process timed out\n+        \"\"\"\n+        logger.info(\"Waiting for Spark Submit process to complete...\")\n+        # wait for up to TIMEOUT seconds for the VM to be up and ready\n+        for timeout in range(self.timeoutMinutes):\n+            if not self.is_process_running(\"SparkSubmit\"):\n+                logger.info(\"Atlas Check spark job has completed.\")\n+                if self.ssh_cmd(\n+                    \"grep 'Success!' {}/_*\".format(self.atlasOutDir), quiet=True\n+                ):\n+                    logger.error(\"Atlas Check spark job Failed.\")\n+                    self.get_files(self.atlasCheckLog, \"./\")\n+                    logger.error(\n+                        \"---tail of Atlas Checks Spark job log output ({})--- \\n\".format(\n+                            self.atlasCheckLogName\n+                            + \" \".join(\n+                                map(\n+                                    str,\n+                                    open(self.atlasCheckLogName, \"r\").readlines()[-50:],\n+                                )\n+                            )\n+                        )\n+                    )\n+                    finish(status=-1)\n+                return 0\n+            time.sleep(5)\n+        return 1\n+\n+    def is_process_running(self, process):\n+        \"\"\"Indicate if process is actively running\n+\n+        Uses pgrep on the EC2 instance to detect if the process is\n+        actively running.\n+\n+        :returns: 0 - if process is NOT running\n+        :returns: 1 - if process is running\n+        \"\"\"\n+        if self.ssh_cmd(\"pgrep -P1 -f {}\".format(process), quiet=True):\n+            return 0\n+        logger.debug(\"{} is still running ... \".format(process))\n+        return 1\n+\n+    def start_ec2(self):\n+        \"\"\"Start EC2 Instance.\"\"\"\n+        logger.info(\"Starting the PBFShardGenerator EC2 instance.\")\n+\n+        try:\n+            logger.info(\"Start instance\")\n+            ec2.start_instances(InstanceIds=[self.instanceId])\n+            logger.info(response)\n+        except ClientError as e:\n+            logger.info(e)\n+\n+    def stop_ec2(self):\n+        \"\"\"Stop EC2 Instance.\"\"\"\n+        logger.info(\"Stopping the PBFShardGenerator EC2 instance.\")\n+\n+        try:\n+            response = ec2.stop_instances(InstanceIds=[self.instanceId])\n+            logger.info(response)\n+        except ClientError as e:\n+            logger.error(e)\n+\n+    def get_instance_info(self):\n+        \"\"\"Get the info for an EC2 instance.\n+\n+        Given an EC2 instance ID this function will retrieve the instance info\n+        for the instance and save it in self.instance.\n+        \"\"\"\n+        logger.info(\"Getting EC2 Instance {} Info...\".format(self.instanceId))\n+        # wait for up to TIMEOUT seconds for the VM to be up and ready\n+        for timeout in range(10):\n+            response = ec2.describe_instances(InstanceIds=[self.instanceId])\n+            if not response[\"Reservations\"]:\n+                finish(\"Instance {} not found\".format(self.instanceId), -1)\n+            if (\n+                response[\"Reservations\"][0][\"Instances\"][0].get(\"PublicIpAddress\")\n+                is None\n+            ):\n+                logger.info(\n+                    \"Waiting for EC2 instance {} to boot...\".format(self.instanceId)\n+                )\n+                time.sleep(6)\n+                continue\n+            self.instance = response[\"Reservations\"][0][\"Instances\"][0]\n+            logger.info(\n+                \"EC2 instance: {} booted with name: {}\".format(\n+                    self.instanceId, self.instance[\"PublicDnsName\"]\n+                )\n+            )\n+            break\n+        for timeout in range(100):\n+            if self.ssh_cmd(\"systemctl is-system-running\", quiet=True):\n+                logger.debug(\n+                    \"Waiting for systemd on EC2 instance to complete initialization...\"\n+                )\n+                time.sleep(6)\n+                continue\n+            return\n+        finish(\"Timeout while waiting for EC2 instance to be ready\", -1)\n+\n+\n+def parse_args(cloudctl):\n+    \"\"\"Parse user parameters\n+\n+    :returns: args\n+    \"\"\"\n+    parser = argparse.ArgumentParser(\n+        description=\"This script automates the use of EC2 instance to execute \"\n+        \"an atlas-checks spark job. It is meant to be executed on a laptop with \"\n+        \"access to the EC2 instance\"\n+    )\n+    parser.add_argument(\n+        \"-n\",\n+        \"--name\",\n+        help=\"Set EC2 instance name. (Default: {})\".format(cloudctl.instanceName),\n+    )\n+    parser.add_argument(\n+        \"-t\",\n+        \"--template\",\n+        help=\"Set EC2 template name to create instance from. (Default: {})\".format(\n+            cloudctl.templateName\n+        ),\n+    )\n+    parser.add_argument(\n+        \"-m\",\n+        \"--minutes\",\n+        type=int,\n+        help=\"Set process timeout to number of minutes. (Default: {})\".format(\n+            cloudctl.timeoutMinutes\n+        ),\n+    )\n+    parser.add_argument(\n+        \"-v\", \"--version\", help=\"Display the current version\", action=\"store_true\"\n+    )\n+    parser.add_argument(\n+        \"-T\",\n+        \"--terminate\",\n+        default=False,\n+        help=\"Terminate EC2 instance after successful execution\",\n+        action=\"store_true\",\n+    )\n+    subparsers = parser.add_subparsers(\n+        title=\"commands\",\n+        description=\"One of the following commands must be specified when executed. \"\n+        \"To see more information about each command and the parameters that \"\n+        \"are used for each command then specify the command and \"\n+        \"the --help parameter.\",\n+    )\n+\n+    parser_check = subparsers.add_parser(\n+        \"check\",\n+        help=\"Shard a pbf and, if '--output' is set, then push atlas checks output to S3 folder\",\n+    )\n+    parser_check.add_argument(\n+        \"--id\", help=\"ID - Indicates the ID of an existing EC2 instance to use\"\n+    )\n+    parser_check.add_argument(\n+        \"-k\",\n+        \"--key\",\n+        required=True,\n+        help=\"KEY - Instance key name to use to login to instance. This key \"\n+        \"is expected to be the same name as the key as defined by AWS and the \"\n+        \"corresponding pem file must be located in your local '~/.ssh/' \"\n+        \"directory and should be a pem file. See the following URL for \"\n+        \"instructions on creating a key: \"\n+        \"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html. \"\n+        \"(e.g. `--key=aws-key`)\",\n+    )\n+    parser_check.add_argument(\n+        \"-o\",\n+        \"--output\",\n+        help=\"Out - The S3 Output directory. (e.g. '--out=atlas-bucket/PBF_Sharding')\",\n+    )\n+    parser_check.add_argument(\n+        \"-i\",\n+        \"--input\",\n+        required=True,\n+        help=\"IN - The S3 Input directory that contains atlas file directories and sharing.txt. \",\n+    )\n+    parser_check.add_argument(\n+        \"-c\",\n+        \"--countries\",\n+        help=\"COUNTRIES - A comma separated list of ISO3 codes. (Default: {})\".format(\n+            cloudctl.countries\n+        ),\n+    )\n+    parser_check.add_argument(\n+        \"-m\",\n+        \"--memory\",\n+        type=int,\n+        help=\"MEMORY - Gigs of memory for spark job. (Default: {})\".format(\n+            cloudctl.memory\n+        ),\n+    )\n+    parser_check.add_argument(\n+        \"-f\",\n+        \"--formats\",\n+        help=\"FORMATS - Output format (Default: {})\".format(cloudctl.formats),\n+    )\n+    parser_check.add_argument(\n+        \"-j\",\n+        \"--config\",\n+        help=\"CONFIG - Path within the S2 Input bucket or a URL to a json file to \"\n+        \"use as configuration.json for atlas-checks (Default: Latest from atlas-config repo)\",\n+    )\n+    parser_check.add_argument(\n+        \"-p\",\n+        \"--processes\",\n+        type=int,\n+        help=\"PROCESSES - Number of parallel jobs to start. (Default: {})\".format(\n+            cloudctl.processes\n+        ),\n+    )\n+    parser_check.set_defaults(func=CloudAtlasChecksControl.atlasCheck)\n+\n+    parser_sync = subparsers.add_parser(\n+        \"sync\", help=\"Sync Atlas Check output files from instance to S3 folder\"\n+    )\n+    parser_sync.add_argument(\n+        \"--id\",\n+        required=True,\n+        help=\"ID - Indicates the ID of an existing EC2 instance to use\",\n+    )\n+    parser_sync.add_argument(\n+        \"-k\",\n+        \"--key\",\n+        required=True,\n+        help=\"KEY - Instance key name to use to login to instance. This key \"\n+        \"is expected to be the same name as the key as defined by AWS and the \"\n+        \"corresponding pem file must be located in your local '~/.ssh/' \"\n+        \"directory and should be a pem file. See the following URL for \"\n+        \"instructions on creating a key: \"\n+        \"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html. \"\n+        \"(e.g. `--key=aws-key`)\",\n+    )\n+    parser_sync.add_argument(\n+        \"-o\", \"--output\", required=True, help=\"Out - The S3 Output directory\"\n+    )\n+    parser_sync.set_defaults(func=CloudAtlasChecksControl.sync)\n+\n+    parser_clean = subparsers.add_parser(\"clean\", help=\"Clean up instance\")\n+    parser_clean.add_argument(\n+        \"--id\",\n+        required=True,\n+        help=\"ID - Indicates the ID of an existing EC2 instance to use\",\n+    )\n+    parser_clean.add_argument(\n+        \"-k\",\n+        \"--key\",\n+        required=True,\n+        help=\"KEY - Instance key name to use to login to instance. This key \"\n+        \"is expected to be the same name as the key as defined by AWS and the \"\n+        \"corresponding pem file must be located in your local '~/.ssh/' \"\n+        \"directory and should be a pem file. See the following URL for \"\n+        \"instructions on creating a key: \"\n+        \"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html. \"\n+        \"(e.g. `--key=aws-key`)\",\n+    )\n+    parser_clean.set_defaults(func=CloudAtlasChecksControl.clean)\n+\n+    args = parser.parse_args()\n+    return args\n+\n+\n+def evaluate(args, cloudctl):\n+    \"\"\"Evaluate the given arguments.\n+\n+    :param args: The user's input.\n+    :param cloudctl: An instance of CloudAtlasChecksControl to use.\n+    \"\"\"\n+    if args.terminate:\n+        cloudctl.terminate = args.terminate\n+    if args.name:\n+        cloudctl.instanceName = args.name\n+    if args.template:\n+        cloudctl.templateName = args.templateName\n+    if args.minutes:\n+        cloudctl.timeoutMinutes = args.minutes\n+    if hasattr(args, \"input\") and args.input:\n+        cloudctl.s3InFolder = args.input\n+    if hasattr(args, \"processes\") and args.processes:\n+        cloudctl.processes = args.processes\n+    if hasattr(args, \"key\") and args.key:\n+        cloudctl.key = args.key\n+    if hasattr(args, \"output\") and args.output:\n+        cloudctl.s3OutFolder = args.output\n+    if hasattr(args, \"pbf\") and args.pbf:\n+        cloudctl.pbfURL = args.pbf\n+    if hasattr(args, \"countries\") and args.countries:\n+        cloudctl.countries = args.countries\n+    if hasattr(args, \"formats\") and args.formats:\n+        cloudctl.formats = args.formats\n+    if hasattr(args, \"memory\") and args.memory:\n+        cloudctl.memory = args.memory\n+    if hasattr(args, \"config\") and args.config:\n+        cloudctl.atlasConfig = args.config\n+    if hasattr(args, \"id\") and args.id:\n+        cloudctl.instanceId = args.id\n+        cloudctl.get_instance_info()\n+    if args.version:", "originalCommit": "7c909202f6c128ec885a5883e17e56afc73861b2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7d4b1e8e655fb51eb48733e7a90b1deff240a927", "url": "https://github.com/osmlab/atlas-checks/commit/7d4b1e8e655fb51eb48733e7a90b1deff240a927", "message": "Fix parameter truthiness and fix units for memory parameter.", "committedDate": "2020-10-22T13:55:25Z", "type": "commit"}, {"oid": "ba80663ee58c4c1bfa146e6c3b237bddb1f99d80", "url": "https://github.com/osmlab/atlas-checks/commit/ba80663ee58c4c1bfa146e6c3b237bddb1f99d80", "message": "remove sharding references.", "committedDate": "2020-10-22T14:05:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjMwNTI4Ng==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r516305286", "bodyText": "Does it really make sense for this to have a default? Perhaps this should just be an empty string?", "author": "Bentleysb", "createdAt": "2020-11-02T22:48:56Z", "path": "scripts/cloud-check-control/cloudAtlasCheckControl.py", "diffHunk": "@@ -0,0 +1,656 @@\n+#!/usr/bin/env python3\n+# -*- coding: utf-8 -*-\n+\"\"\"\n+execute atlas-checks on an EC2 instance\n+\"\"\"\n+import argparse\n+import logging\n+import os\n+import sys\n+import time\n+\n+import boto3.ec2\n+import paramiko\n+import scp\n+from botocore.exceptions import ClientError\n+from paramiko.auth_handler import AuthenticationException\n+\n+\n+VERSION = \"0.1.0\"\n+ec2 = boto3.client(\"ec2\")\n+\n+\n+def setup_logging(default_level=logging.INFO):\n+    \"\"\"\n+    Setup logging configuration\n+    \"\"\"\n+    logging.basicConfig(\n+        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n+        level=default_level,\n+        datefmt=\"%Y-%m-%d %H:%M:%S\",\n+    )\n+    return logging.getLogger(\"CloudAtlasChecksControl\")\n+\n+\n+def finish(error_message=None, status=0):\n+    \"\"\"exit the process\n+\n+    Method to exit the Python script. It will log the given message and then exit().\n+\n+    :param error_message: Error message to log upon exiting the process\n+    :param status: return code to exit the process with\n+    \"\"\"\n+    if error_message:\n+        logger.error(error_message)\n+    else:\n+        logger.critical(\"Done\")\n+    exit(status)\n+\n+\n+class CloudAtlasChecksControl:\n+    \"\"\"Main Class to control atlas checks spark job on EC2\"\"\"\n+\n+    def __init__(\n+        self,\n+        timeoutMinutes=6000,\n+        key=\"\",\n+        instanceId=\"\",\n+        processes=32,\n+        memory=256,\n+        formats=\"flags\",\n+        countries=\"BRA\",", "originalCommit": "ba80663ee58c4c1bfa146e6c3b237bddb1f99d80", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjcwNDk0Nw==", "url": "https://github.com/osmlab/atlas-checks/pull/384#discussion_r516704947", "bodyText": "Removed this default and made -c or --countries a required parameter.", "author": "atiannicelli", "createdAt": "2020-11-03T14:26:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjMwNTI4Ng=="}], "type": "inlineReview"}, {"oid": "eeb9333d9ab5be7daf140c84735037388aa9f1e6", "url": "https://github.com/osmlab/atlas-checks/commit/eeb9333d9ab5be7daf140c84735037388aa9f1e6", "message": "Remove BRA as default country and make -c required", "committedDate": "2020-11-03T14:25:43Z", "type": "commit"}]}