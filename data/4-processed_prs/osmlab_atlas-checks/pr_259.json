{"pr_number": 259, "pr_title": "Sharded Integrity Checks Job", "pr_createdAt": "2020-02-13T19:45:57Z", "pr_url": "https://github.com/osmlab/atlas-checks/pull/259", "timeline": [{"oid": "2ac83c2a8a0b8960d9d74dc46a72bb7d4f1c2733", "url": "https://github.com/osmlab/atlas-checks/commit/2ac83c2a8a0b8960d9d74dc46a72bb7d4f1c2733", "message": "save work", "committedDate": "2020-02-13T18:07:54Z", "type": "commit"}, {"oid": "a881e2e458184c8f4635e1efd80752c7bfd6c4b3", "url": "https://github.com/osmlab/atlas-checks/commit/a881e2e458184c8f4635e1efd80752c7bfd6c4b3", "message": "framework up", "committedDate": "2020-02-13T18:07:54Z", "type": "commit"}, {"oid": "ca3bc628b0a3b297f7e678396769e40437bc3733", "url": "https://github.com/osmlab/atlas-checks/commit/ca3bc628b0a3b297f7e678396769e40437bc3733", "message": "formatting", "committedDate": "2020-02-13T18:07:54Z", "type": "commit"}, {"oid": "e199faaa60d9af03774c45d35f1539adefff92aa", "url": "https://github.com/osmlab/atlas-checks/commit/e199faaa60d9af03774c45d35f1539adefff92aa", "message": "processor", "committedDate": "2020-02-13T18:07:54Z", "type": "commit"}, {"oid": "b08b6530fbdd9e6a1d7324fb7ca0c99ae340c0bd", "url": "https://github.com/osmlab/atlas-checks/commit/b08b6530fbdd9e6a1d7324fb7ca0c99ae340c0bd", "message": "Produce flags set up", "committedDate": "2020-02-13T18:07:54Z", "type": "commit"}, {"oid": "67ddca41372fb17b19f57b3948bad8879b7ff7c4", "url": "https://github.com/osmlab/atlas-checks/commit/67ddca41372fb17b19f57b3948bad8879b7ff7c4", "message": "testing begining", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "b61a5a7fe832b0f07466713b0abb8a74c432798d", "url": "https://github.com/osmlab/atlas-checks/commit/b61a5a7fe832b0f07466713b0abb8a74c432798d", "message": "Testing improvements", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "823433018c08105ff96a7c5ec258ac71225b4355", "url": "https://github.com/osmlab/atlas-checks/commit/823433018c08105ff96a7c5ec258ac71225b4355", "message": "Clean up", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "163734574b61b84609fb75632f8b9062de4ed09f", "url": "https://github.com/osmlab/atlas-checks/commit/163734574b61b84609fb75632f8b9062de4ed09f", "message": "testing additions and adjustments", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "cf0a81efe06ff7e8ff6483f80452b0fcb0ee0241", "url": "https://github.com/osmlab/atlas-checks/commit/cf0a81efe06ff7e8ff6483f80452b0fcb0ee0241", "message": "Rearrangment", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "b805d318b843ef9b2fad4afa89eed6a419569de4", "url": "https://github.com/osmlab/atlas-checks/commit/b805d318b843ef9b2fad4afa89eed6a419569de4", "message": "javadoc fix", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "acb9ceaba40c8c53b76b47c55c2c0c974a08cb49", "url": "https://github.com/osmlab/atlas-checks/commit/acb9ceaba40c8c53b76b47c55c2c0c974a08cb49", "message": "update IncompleteSharding", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "44708fc7c9897d768639cd7120f4e4c641410066", "url": "https://github.com/osmlab/atlas-checks/commit/44708fc7c9897d768639cd7120f4e4c641410066", "message": "loadChecksForCountry checks white/blacklists", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "6e9db5a4aac1441de46c1a76de77ac5cd2e2bc8d", "url": "https://github.com/osmlab/atlas-checks/commit/6e9db5a4aac1441de46c1a76de77ac5cd2e2bc8d", "message": "multipolygon fix", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "f976eb2b938c29012ff294df297412c794fafdb1", "url": "https://github.com/osmlab/atlas-checks/commit/f976eb2b938c29012ff294df297412c794fafdb1", "message": "dynamic atlas", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "b53575e3125a6286b1f3ad1b73c42834acfba67b", "url": "https://github.com/osmlab/atlas-checks/commit/b53575e3125a6286b1f3ad1b73c42834acfba67b", "message": "spotless", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "7c41c680b9330a5302c62408343deb15433bd7e9", "url": "https://github.com/osmlab/atlas-checks/commit/7c41c680b9330a5302c62408343deb15433bd7e9", "message": "contrify parralelize", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "9a4c91f56d93a5537f920dab8a514c69493f7808", "url": "https://github.com/osmlab/atlas-checks/commit/9a4c91f56d93a5537f920dab8a514c69493f7808", "message": "count", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "ee18fc82f575fb0e402e4b1f7703786c89cd25c3", "url": "https://github.com/osmlab/atlas-checks/commit/ee18fc82f575fb0e402e4b1f7703786c89cd25c3", "message": "spotless", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "d79651f5bd1f2f7c3c61204bdea7702967a3f7b3", "url": "https://github.com/osmlab/atlas-checks/commit/d79651f5bd1f2f7c3c61204bdea7702967a3f7b3", "message": "para stream", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "4d4f415c5b32a0cef9824d0acb3bab3741db7bc6", "url": "https://github.com/osmlab/atlas-checks/commit/4d4f415c5b32a0cef9824d0acb3bab3741db7bc6", "message": "pool", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "d72983eec90b35d23d88c0a42fa985ff3621078e", "url": "https://github.com/osmlab/atlas-checks/commit/d72983eec90b35d23d88c0a42fa985ff3621078e", "message": "unique ids", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "7014e207bd4ac2930307ff34c42978f8a1dd1a9c", "url": "https://github.com/osmlab/atlas-checks/commit/7014e207bd4ac2930307ff34c42978f8a1dd1a9c", "message": "rm 2nd metrics", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "aa917b779d42efb44cb4dafa4055a4157423ddfb", "url": "https://github.com/osmlab/atlas-checks/commit/aa917b779d42efb44cb4dafa4055a4157423ddfb", "message": "unique container tests", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "c6ef37538923e85bb7533a1ce76b98555656d25a", "url": "https://github.com/osmlab/atlas-checks/commit/c6ef37538923e85bb7533a1ce76b98555656d25a", "message": "test rule", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "2439e1b2c0da33b079e0d20d0e613e0875fd875a", "url": "https://github.com/osmlab/atlas-checks/commit/2439e1b2c0da33b079e0d20d0e613e0875fd875a", "message": "revert MR", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "925e6a04e1df75c8bc420225735af8420b1f87d3", "url": "https://github.com/osmlab/atlas-checks/commit/925e6a04e1df75c8bc420225735af8420b1f87d3", "message": "sharded task tests", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "f2052888e2d785752d4c462c38a42ce982cab7ae", "url": "https://github.com/osmlab/atlas-checks/commit/f2052888e2d785752d4c462c38a42ce982cab7ae", "message": "flagged relation tests", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "6050f5d1a6a577ac588a3ed17e4c5afbac454e25", "url": "https://github.com/osmlab/atlas-checks/commit/6050f5d1a6a577ac588a3ed17e4c5afbac454e25", "message": "spark job tests", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "7044acd017d9a183b8543903a51e75b9b1eb59dd", "url": "https://github.com/osmlab/atlas-checks/commit/7044acd017d9a183b8543903a51e75b9b1eb59dd", "message": "sharded job clean up", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "2013666244cefc02d4125e7ec892067b93fc7916", "url": "https://github.com/osmlab/atlas-checks/commit/2013666244cefc02d4125e7ec892067b93fc7916", "message": "docs", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "4cfbe5f32ca0be371e258900f2440b59e8b6854a", "url": "https://github.com/osmlab/atlas-checks/commit/4cfbe5f32ca0be371e258900f2440b59e8b6854a", "message": "no grouping", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "750ef70c0eac43dd559cf3966330e901e9d74b2d", "url": "https://github.com/osmlab/atlas-checks/commit/750ef70c0eac43dd559cf3966330e901e9d74b2d", "message": "rm deffered loading", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "181982487541bbfdfd2eb6e241e7608d9f0eed68", "url": "https://github.com/osmlab/atlas-checks/commit/181982487541bbfdfd2eb6e241e7608d9f0eed68", "message": "pre load", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "9f67c510d849dd28a7914927e7f75ba053d3f6c3", "url": "https://github.com/osmlab/atlas-checks/commit/9f67c510d849dd28a7914927e7f75ba053d3f6c3", "message": "deffer and laod", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "e85877e00abd2944614e25a6083ddbc352bf306b", "url": "https://github.com/osmlab/atlas-checks/commit/e85877e00abd2944614e25a6083ddbc352bf306b", "message": "bring back multi", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "cf4a91abf92e68ebf6191d9adfa2abc1e67be2ed", "url": "https://github.com/osmlab/atlas-checks/commit/cf4a91abf92e68ebf6191d9adfa2abc1e67be2ed", "message": "return multi option", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "a8f0d24f2d8b8f64b5f8280f868cc97c1beae049", "url": "https://github.com/osmlab/atlas-checks/commit/a8f0d24f2d8b8f64b5f8280f868cc97c1beae049", "message": "clean up switches", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "6b4e46c0a5e9b3cd10d87876d3965c782a071f7a", "url": "https://github.com/osmlab/atlas-checks/commit/6b4e46c0a5e9b3cd10d87876d3965c782a071f7a", "message": "spotless", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"oid": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "url": "https://github.com/osmlab/atlas-checks/commit/ce93633e9228afb81fc2f6871fb5c2bfa8764130", "message": "input note", "committedDate": "2020-02-13T18:07:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE2OTE3MA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379169170", "bodyText": "Is getting Checks per country not going to be supported moving forward? // Is it depended on the being able to access the persisted RDD?", "author": "jklamer", "createdAt": "2020-02-13T23:06:31Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzIzNg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379563236", "bodyText": "I'm not sure I exactly follow the question, but all this has to do with creating job labels for the Spark UI. In order to run and label each country as a different job in spark this is persisting the RDD, calling count (just to force execution), and then unioning the results to get everything into one stream. So checks are fully by country.", "author": "Bentleysb", "createdAt": "2020-02-14T17:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE2OTE3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3MDcyOQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379170729", "bodyText": "I believe making a new one of these WILL work for every atlas fetcher, but if you can make it a transient static lazy initialized I believe that will mean each JVM working in the spark cluster will use its own, which may allow for optimizations.", "author": "jklamer", "createdAt": "2020-02-13T23:11:20Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQ1Mjg0OQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r383452849", "bodyText": "The caches are using a global namespace, so I think this should already effectively be one cache store per machine in the cluster.", "author": "Bentleysb", "createdAt": "2020-02-24T19:00:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTE3MDcyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU1ODQzOA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379558438", "bodyText": "These two variables can be local variables.", "author": "smaheshwaram", "createdAt": "2020-02-14T17:38:49Z", "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -60,6 +60,8 @@\n     private final MultiMap<String, String> countryGroups = new MultiMap<>();\n     private final Boolean enabledByDefault;\n     private final String enabledKeyTemplate;\n+    private final String countryWhitelistTemplate = \"%s.\" + BaseCheck.PARAMETER_WHITELIST_COUNTRIES;\n+    private final String countryBlacklistTemplate = \"%s.\" + BaseCheck.PARAMETER_BLACKLIST_COUNTRIES;", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDkwNjE1Mg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r380906152", "bodyText": "I am not sure why I made these like this. I think it might be best to make them static variable, as apposed to local. Any thoughts?", "author": "Bentleysb", "createdAt": "2020-02-18T20:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU1ODQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379563274", "bodyText": "We can define this String literal \"ItemId\" in GeoJsonUtils class!!", "author": "smaheshwaram", "createdAt": "2020-02-14T17:50:10Z", "path": "src/main/java/org/openstreetmap/atlas/checks/commands/AtlasChecksLogDiffSubCommand.java", "diffHunk": "@@ -78,19 +82,28 @@ public AtlasChecksLogDiffSubCommand()\n     }\n \n     /**\n-     * Get the atlas ids from an array of features.\n+     * Get the unique ids for a flag. Fall back to getting the atlas ids from the features for\n+     * reverse compatibility.\n      *\n-     * @param features\n-     *            a {@link JsonArray} of features\n+     * @param flagJson\n+     *            a {@link JsonObject} of a flag\n      * @return a {@link Set} of {@link String} ids\n      */\n-    private Set<String> getAtlasIdentifiers(final JsonArray features)\n+    private Set<String> getIdentifiers(final JsonObject flagJson)\n     {\n-        return Iterables.stream(features)\n-                .filter(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .has(IDENTIFIER))\n-                .map(object -> object.getAsJsonObject().get(PROPERTIES).getAsJsonObject()\n-                        .get(IDENTIFIER).getAsString())\n-                .collectToSet();\n+        final JsonObject flagProperties = flagJson.get(PROPERTIES).getAsJsonObject();\n+        return flagProperties.has(IDENTIFIERS)\n+                ? Iterables.stream(flagProperties.get(IDENTIFIERS).getAsJsonArray())\n+                        .map(JsonElement::getAsString).collectToSet()\n+                : Iterables.stream(flagJson.get(FEATURES).getAsJsonArray()).filter(object -> object\n+                        .getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(IDENTIFIER)\n+                        || object.getAsJsonObject().get(PROPERTIES).getAsJsonObject().has(\"ItemId\"))", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDkwNjE5OQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r380906199", "bodyText": "This is a bit of a funny one in that it is not a standard geojson key. It should probably be defined as a public constant somewhere in the output event classes. There are a bunch of similar items in the event classes that could use a refactor. It would probably be best to have a separate PR to update all of them.", "author": "Bentleysb", "createdAt": "2020-02-18T20:05:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2MTgzNQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384061835", "bodyText": "@Bentleysb do you know which class adds the \"ItemId\" to the flag?", "author": "sayas01", "createdAt": "2020-02-25T18:59:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDQ4Mw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094483", "bodyText": "In looking at this again I have realized this is the old identifier key that CheckFlagEvent used to use. It is still in this tool for reverse compatibility.", "author": "Bentleysb", "createdAt": "2020-02-25T20:00:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2MzI3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2ODk1Nw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379568957", "bodyText": "Is this method called from anywhere?", "author": "smaheshwaram", "createdAt": "2020-02-14T18:02:48Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -134,6 +81,27 @@ public static void main(final String[] args)\n         new IntegrityCheckSparkJob().run(args);\n     }\n \n+    /**\n+     * Gets complex entities\n+     *\n+     * @param check\n+     *            A {@link BaseCheck} object\n+     * @param atlas\n+     *            An {@link Atlas} object\n+     * @return An {@link Iterable} of {@link ComplexEntity}s\n+     */\n+    protected static Iterable<ComplexEntity> findComplexEntities(final BaseCheck check,", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDkwNjIzMQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r380906231", "bodyText": "Nope, its use has been replaced by IntegrityChecksCommandArguments.objectsToCheck. I will remove this.", "author": "Bentleysb", "createdAt": "2020-02-18T20:06:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU2ODk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r379574152", "bodyText": "Will POOL_DURATION_BEFORE_KILL value remains the same? Also, may I know if it is a random assigned value or any estimated value.", "author": "smaheshwaram", "createdAt": "2020-02-14T18:15:26Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -153,35 +121,11 @@ private static void executeChecks(final String country, final Atlas atlas,\n     {\n         final Pool checkExecutionPool = new Pool(checksToRun.size(), \"Check execution pool\",\n                 POOL_DURATION_BEFORE_KILL);", "originalCommit": "ce93633e9228afb81fc2f6871fb5c2bfa8764130", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDkwNjI4Mw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r380906283", "bodyText": "This is a constant value of 300 seconds. It configures the Pool to kill any check that takes longer then that time to run.", "author": "Bentleysb", "createdAt": "2020-02-18T20:06:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0MTE4Nw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384841187", "bodyText": "300 minutes, right?", "author": "seancoulter", "createdAt": "2020-02-27T00:04:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTIzNjM1Ng==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385236356", "bodyText": "Sorry, yes this is minutes.", "author": "Bentleysb", "createdAt": "2020-02-27T16:50:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTU3NDE1Mg=="}], "type": "inlineReview"}, {"oid": "e5173acf2db44ac12eb05735bf6da342ce0c9cc5", "url": "https://github.com/osmlab/atlas-checks/commit/e5173acf2db44ac12eb05735bf6da342ce0c9cc5", "message": "add md docs", "committedDate": "2020-02-18T19:48:31Z", "type": "commit"}, {"oid": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "url": "https://github.com/osmlab/atlas-checks/commit/a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "message": "clean up", "committedDate": "2020-02-24T17:45:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTMxNw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r383431317", "bodyText": "Are you able to import the event package from atlas, or is there a conflicting package issue ?", "author": "danielduhh", "createdAt": "2020-02-24T18:17:48Z", "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagEvent.java", "diffHunk": "@@ -23,25 +23,29 @@\n import org.openstreetmap.atlas.geography.geojson.GeoJsonObject;\n import org.openstreetmap.atlas.tags.HighwayTag;\n \n+import com.google.gson.Gson;\n import com.google.gson.JsonArray;\n import com.google.gson.JsonElement;\n import com.google.gson.JsonObject;\n import com.google.gson.JsonPrimitive;\n \n /**\n- * Wraps a {@link CheckFlag} for submission to the {@link EventService} for handling {@link Check}\n- * results\n+ * Wraps a {@link CheckFlag} for submission to the\n+ * {@link org.openstreetmap.atlas.event.EventService} for handling {@link Check} results\n  *\n  * @author mkalender, bbreithaupt\n  */\n-public final class CheckFlagEvent extends Event\n+public final class CheckFlagEvent extends org.openstreetmap.atlas.event.Event", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA4MjE0MQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384082141", "bodyText": "I'm able to import, not sure why that ended up like that. I will fix that.", "author": "Bentleysb", "createdAt": "2020-02-25T19:37:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMTMxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMjUyMg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r383432522", "bodyText": "Is toJsonTree converting the ids to an array? If so, what's the advantage of using an array vs string?", "author": "danielduhh", "createdAt": "2020-02-24T18:20:31Z", "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagEvent.java", "diffHunk": "@@ -143,6 +147,7 @@ else if (flaggedRelations.size() != 1 && !feature.has(GEOMETRY))\n         flagProperties.add(\"feature_properties\", featureProperties);\n         flagProperties.add(\"feature_osmids\", uniqueFeatureOsmIds);\n         flagProperties.addProperty(\"feature_count\", featureProperties.size());\n+        flagProperties.add(IDENTIFIERS, GSON.toJsonTree(flag.getUniqueIdentifiers()));", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA4MjE3OQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384082179", "bodyText": "Yes it is converting it to a json array.\nI think there are a few advantages. It makes it easier to read and prettify when in json format. There is no issue with trying to deal with gigantic unbroken strings.  A lot of languages have no problem loading a json array into a native collection format, so they can then easily use the ids in a separated (search) or combined (id) manor.\nThat said, it would probably work just about as well to have his as a csv string. For an example of how it is currently used by downstream tools look at the changes to the geojson and log diff tools in this PR.", "author": "Bentleysb", "createdAt": "2020-02-25T19:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzQzMjUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxNjk4MA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384016980", "bodyText": "Unrelated to you changes but.. whats the purpose of the configJson switch?", "author": "danielduhh", "createdAt": "2020-02-25T17:24:00Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",\n+            \"Path of folder which contains Atlas file(s)\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    protected static final String OUTPUT_ATLAS_FOLDER = \"atlas\";\n+    // Outputs\n+    protected static final String OUTPUT_FLAG_FOLDER = \"flag\";\n+    protected static final String OUTPUT_GEOJSON_FOLDER = \"geojson\";\n+    protected static final String OUTPUT_METRIC_FOLDER = \"metric\";\n+    protected static final String OUTPUT_TIPPECANOE_FOLDER = \"tippecanoe\";\n+    static final Switch<List<String>> CHECK_FILTER = new Switch<>(\"checkFilter\",\n+            \"Comma-separated list of checks to run\",\n+            checks -> Arrays.asList(checks.split(CommonConstants.COMMA)), Optionality.OPTIONAL);\n+    // Configuration\n+    static final Switch<StringList> CONFIGURATION_FILES = new Switch<>(\"configFiles\",\n+            \"Comma-separated list of configuration datasources.\",\n+            value -> StringList.split(value, CommonConstants.COMMA), Optionality.OPTIONAL);\n+    static final Switch<String> CONFIGURATION_JSON = new Switch<>(\"configJson\",", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA4MjIxMA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384082210", "bodyText": "It so you could pass the contents of a configuration file directly as json, instead of requiring the file.", "author": "Bentleysb", "createdAt": "2020-02-25T19:37:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxNjk4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxODY3OA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384018678", "bodyText": "How is this used?", "author": "danielduhh", "createdAt": "2020-02-25T17:26:42Z", "path": "src/main/java/org/openstreetmap/atlas/checks/flag/CheckFlag.java", "diffHunk": "@@ -48,18 +49,18 @@\n  * @author cuthbertm\n  * @author mgostintsev\n  * @author brian_l_davis\n+ * @author bbreithaupt\n  */\n public class CheckFlag implements Iterable<Location>, Located, Serializable\n {\n-    private static final long serialVersionUID = -1287808902452203852L;\n-    private static final Logger logger = LoggerFactory.getLogger(CheckFlag.class);\n-\n     private static final Distance TEN_METERS = Distance.meters(10);\n-\n-    private final String identifier;\n+    private static final String NULL_IDENTIFIERS = \"nullnull\";", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA4MjI1Mg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384082252", "bodyText": "It is used to filter out bad unique ids created from flagged points that are not from atlas entities.", "author": "Bentleysb", "createdAt": "2020-02-25T19:37:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAxODY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAyNTgwNA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384025804", "bodyText": "multi -> multiAtlas", "author": "danielduhh", "createdAt": "2020-02-25T17:38:59Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);\n+        final AtlasResourceLoader loader = new AtlasResourceLoader();\n+        return (Function<Shard, Optional<Atlas>> & Serializable) shard -> cache.get(country, shard)\n+                .map(loader::load);\n+    }\n+\n+    /**\n+     * Process {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s through an event service to\n+     * produce output files.\n+     *\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param outputFormats\n+     *            {@link Set} of\n+     *            {@link org.openstreetmap.atlas.checks.distributed.IntegrityChecksCommandArguments.OutputFormats}\n+     * @return {@link VoidFunction} that takes a {@link Tuple2} of a {@link String} country code and\n+     *         a {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private VoidFunction<Tuple2<String, UniqueCheckFlagContainer>> processFlags(final String output,\n+            final SparkFileHelper fileHelper, final Set<OutputFormats> outputFormats)\n+    {\n+        return tuple ->\n+        {\n+            final String country = tuple._1();\n+            final UniqueCheckFlagContainer flagContainer = tuple._2();\n+            final EventService eventService = EventService.get(country);\n+\n+            if (outputFormats.contains(OutputFormats.FLAGS))\n+            {\n+                eventService.register(new CheckFlagFileProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_FLAG_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.GEOJSON))\n+            {\n+\n+                eventService.register(new CheckFlagGeoJsonProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_GEOJSON_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.TIPPECANOE))\n+            {\n+                eventService.register(new CheckFlagTippecanoeProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_TIPPECANOE_FOLDER, country)));\n+            }\n+\n+            flagContainer.reconstructEvents().parallel().forEach(eventService::post);\n+            eventService.complete();\n+        };\n+    }\n+\n+    /**\n+     * {@link PairFunction} to run each {@link ShardedCheckFlagsTask} through to produce\n+     * {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param configurationMap\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param sharding\n+     *            spark {@link Broadcast} of the current {@link Sharding}\n+     * @param shardDistanceExpansion\n+     *            {@link Distance} to expand the shard group\n+     * @param multi\n+     *            boolean whether to use a multi or dynamic Atlas\n+     * @return {@link PairFunction} that takes {@link ShardedCheckFlagsTask} and returns a\n+     *         {@link Tuple2} of a {@link String} country code and {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private PairFunction<ShardedCheckFlagsTask, String, UniqueCheckFlagContainer> produceFlags(\n+            final String input, final String output, final Map<String, String> configurationMap,\n+            final SparkFileHelper fileHelper, final Broadcast<Sharding> sharding,\n+            final Distance shardDistanceExpansion, final boolean multi)", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDAyNzUyOA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384027528", "bodyText": "\ud83d\ude4f Thanks for adding this!", "author": "danielduhh", "createdAt": "2020-02-25T17:42:00Z", "path": "docs/shardedchecks.md", "diffHunk": "@@ -0,0 +1,58 @@\n+# Sharded Atlas Checks", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1NzMwMw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384057303", "bodyText": "Remove \"this\" keyword as COUNTRY_WHITELIST_TEMPLATE is a static variable.", "author": "sayas01", "createdAt": "2020-02-25T18:51:39Z", "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -310,4 +313,20 @@ private boolean isEnabledByConfiguration(final Configuration configuration,\n         final String key = String.format(this.enabledKeyTemplate, checkClass.getSimpleName());\n         return configuration.get(key, this.enabledByDefault).value();\n     }\n+\n+    private boolean isEnabledByConfiguration(final Configuration configuration,\n+            final Class checkClass, final String country)\n+    {\n+        final List<String> countryWhitelist = configuration\n+                .get(String.format(this.COUNTRY_WHITELIST_TEMPLATE, checkClass.getSimpleName()),", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA1NzYwNQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384057605", "bodyText": "Same as my previous comment.", "author": "sayas01", "createdAt": "2020-02-25T18:52:10Z", "path": "src/main/java/org/openstreetmap/atlas/checks/base/CheckResourceLoader.java", "diffHunk": "@@ -310,4 +313,20 @@ private boolean isEnabledByConfiguration(final Configuration configuration,\n         final String key = String.format(this.enabledKeyTemplate, checkClass.getSimpleName());\n         return configuration.get(key, this.enabledByDefault).value();\n     }\n+\n+    private boolean isEnabledByConfiguration(final Configuration configuration,\n+            final Class checkClass, final String country)\n+    {\n+        final List<String> countryWhitelist = configuration\n+                .get(String.format(this.COUNTRY_WHITELIST_TEMPLATE, checkClass.getSimpleName()),\n+                        Collections.emptyList())\n+                .value();\n+        final List<String> countryBlacklist = configuration\n+                .get(String.format(this.COUNTRY_BLACKLIST_TEMPLATE, checkClass.getSimpleName()),", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NDU2NA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384064564", "bodyText": "What are these warnings?", "author": "sayas01", "createdAt": "2020-02-25T19:04:37Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityCheckSparkJob.java", "diffHunk": "@@ -211,7 +133,7 @@ public String getName()\n         return \"Integrity Check Spark Job\";\n     }\n \n-    @SuppressWarnings({ \"rawtypes\" })\n+    @SuppressWarnings({ \"rawtypes\", \"unchecked\" })", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDYwMQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094601", "bodyText": "I'm not sure what rawtypes is for, but was already there. unchecked is for unchecked casts, that are happening when getting values from the command map.", "author": "Bentleysb", "createdAt": "2020-02-25T20:00:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NDU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NTM4NA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384065384", "bodyText": "Why is this deprecated?", "author": "sayas01", "createdAt": "2020-02-25T19:06:13Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDY0OQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094649", "bodyText": "That is a carryover from @jklamer's original PR. My assumption, and why I left it, is that it would be better to use the 'input' switch from the SparkJob base class.", "author": "Bentleysb", "createdAt": "2020-02-25T20:00:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NTM4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384066590", "bodyText": "nit: missing serialVersionUID", "author": "sayas01", "createdAt": "2020-02-25T19:08:30Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDcxMQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094711", "bodyText": "I don't think you need this for an abstract class.", "author": "Bentleysb", "createdAt": "2020-02-25T20:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0NTUxOQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384845519", "bodyText": "I think you still need the serial number if IntegrityChecksCommandArguments has any subclasses, and we plan on reading/writing objects of those classes somewhere. But since I don't think we're doing that it should be fine for now\nSee https://stackoverflow.com/questions/893259/should-an-abstract-class-have-a-serialversionuid", "author": "seancoulter", "createdAt": "2020-02-27T00:18:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MDgyOQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385270829", "bodyText": "Interesting! I don't foresee a need to serialize an entire spark job, but as the class is serializable I will add this.", "author": "Bentleysb", "createdAt": "2020-02-27T17:48:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA2NjU5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NDE5MQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384074191", "bodyText": "Couldn't we import this?", "author": "sayas01", "createdAt": "2020-02-25T19:23:04Z", "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagGeoJsonProcessor.java", "diffHunk": "@@ -18,11 +18,13 @@\n import com.google.gson.JsonObject;\n \n /**\n- * A {@link Processor} for {@link CheckFlagEvent}s to write them into GeoJson files\n+ * A {@link org.openstreetmap.atlas.event.Processor} for {@link CheckFlagEvent}s to write them into\n+ * GeoJson files\n  *\n  * @author brian_l_davis\n  */\n-public final class CheckFlagGeoJsonProcessor implements Processor<CheckFlagEvent>\n+public final class CheckFlagGeoJsonProcessor\n+        implements org.openstreetmap.atlas.event.Processor<CheckFlagEvent>", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NDU1Mg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384074552", "bodyText": "Why are these changes?", "author": "sayas01", "createdAt": "2020-02-25T19:23:41Z", "path": "src/main/java/org/openstreetmap/atlas/checks/event/CheckFlagGeoJsonProcessor.java", "diffHunk": "@@ -111,7 +113,7 @@ public void process(final CheckFlagEvent event)\n \n     @Override\n     @Subscribe\n-    public void process(final ShutdownEvent event)\n+    public void process(final org.openstreetmap.atlas.event.ShutdownEvent event)", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDc2NQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094765", "bodyText": "Changing to use the classes in atlas, instead of replicating them in atlas checks.", "author": "Bentleysb", "createdAt": "2020-02-25T20:01:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NDU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NTY1NQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384075655", "bodyText": "nit: Could we do a static import here?", "author": "sayas01", "createdAt": "2020-02-25T19:25:45Z", "path": "src/main/java/org/openstreetmap/atlas/checks/flag/CheckFlag.java", "diffHunk": "@@ -451,6 +469,22 @@ public int hashCode()\n         return new MultiIterable<>(getShapes()).iterator();\n     }\n \n+    /**\n+     * Decouple the {@link CheckFlag} from any\n+     * {@link org.openstreetmap.atlas.geography.atlas.Atlas}s by making all the", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDgyMg==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094822", "bodyText": "It would not be static, as Atlas is a class. I could import the class, but that seems excessive for just a doc reference.", "author": "Bentleysb", "createdAt": "2020-02-25T20:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3NTY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3ODI3NQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384078275", "bodyText": "What are these changes?", "author": "sayas01", "createdAt": "2020-02-25T19:30:34Z", "path": "src/main/java/org/openstreetmap/atlas/checks/flag/FlaggedObject.java", "diffHunk": "@@ -17,14 +18,14 @@\n  */\n public abstract class FlaggedObject implements Serializable, Located\n {\n-    protected static final String COUNTRY_MISSING = \"NA\";\n     protected static final String AREA_TAG = \"Area\";\n+    protected static final String COUNTRY_MISSING = \"NA\";", "originalCommit": "a9e0b787695d1a6abff8aef2638e0cdc2b79f1ef", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA5NDg2NA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384094864", "bodyText": "Automatic code rearrangement.", "author": "Bentleysb", "createdAt": "2020-02-25T20:01:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDA3ODI3NQ=="}], "type": "inlineReview"}, {"oid": "a5df81546617e673ea2e312c314d6b01e6d0b9eb", "url": "https://github.com/osmlab/atlas-checks/commit/a5df81546617e673ea2e312c314d6b01e6d0b9eb", "message": "clean up 2", "committedDate": "2020-02-25T19:37:23Z", "type": "commit"}, {"oid": "ef91ee49f8e3173833c52b0709658024619c0603", "url": "https://github.com/osmlab/atlas-checks/commit/ef91ee49f8e3173833c52b0709658024619c0603", "message": "clean up 3", "committedDate": "2020-02-25T20:00:17Z", "type": "commit"}, {"oid": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "url": "https://github.com/osmlab/atlas-checks/commit/1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "message": "clean up 4", "committedDate": "2020-02-25T20:03:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0ODI3Ng==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384848276", "bodyText": "If this condition fails we could log that we're skipping the file. Could be useful for debugging in case a shard file has a bad name and doesn't get loaded", "author": "seancoulter", "createdAt": "2020-02-27T00:27:26Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/IntegrityChecksCommandArguments.java", "diffHunk": "@@ -0,0 +1,189 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Predicate;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.openstreetmap.atlas.checks.atlas.CountrySpecificAtlasFilePathFilter;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.maproulette.MapRouletteConfiguration;\n+import org.openstreetmap.atlas.generator.tools.filesystem.FileSystemHelper;\n+import org.openstreetmap.atlas.generator.tools.spark.SparkJob;\n+import org.openstreetmap.atlas.geography.Rectangle;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasEntity;\n+import org.openstreetmap.atlas.geography.atlas.items.AtlasObject;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.SlippyTile;\n+import org.openstreetmap.atlas.utilities.collections.Iterables;\n+import org.openstreetmap.atlas.utilities.collections.MultiIterable;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Handles arguments and base functionality for integrity check sparkjobs generating commands\n+ *\n+ * @author jklamer\n+ */\n+public abstract class IntegrityChecksCommandArguments extends SparkJob\n+{\n+    /**\n+     * @author brian_l_davis\n+     */\n+    protected enum OutputFormats\n+    {\n+        FLAGS,\n+        GEOJSON,\n+        METRICS,\n+        TIPPECANOE\n+    }\n+\n+    @Deprecated\n+    protected static final Switch<String> ATLAS_FOLDER = new Switch<>(\"inputFolder\",\n+            \"Path of folder which contains Atlas file(s)\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    protected static final String OUTPUT_ATLAS_FOLDER = \"atlas\";\n+    // Outputs\n+    protected static final String OUTPUT_FLAG_FOLDER = \"flag\";\n+    protected static final String OUTPUT_GEOJSON_FOLDER = \"geojson\";\n+    protected static final String OUTPUT_METRIC_FOLDER = \"metric\";\n+    protected static final String OUTPUT_TIPPECANOE_FOLDER = \"tippecanoe\";\n+    static final Switch<List<String>> CHECK_FILTER = new Switch<>(\"checkFilter\",\n+            \"Comma-separated list of checks to run\",\n+            checks -> Arrays.asList(checks.split(CommonConstants.COMMA)), Optionality.OPTIONAL);\n+    // Configuration\n+    static final Switch<StringList> CONFIGURATION_FILES = new Switch<>(\"configFiles\",\n+            \"Comma-separated list of configuration datasources.\",\n+            value -> StringList.split(value, CommonConstants.COMMA), Optionality.OPTIONAL);\n+    static final Switch<String> CONFIGURATION_JSON = new Switch<>(\"configJson\",\n+            \"Json formatted configuration.\", StringConverter.IDENTITY, Optionality.OPTIONAL);\n+    static final Switch<String> COUNTRIES = new Switch<>(\"countries\",\n+            \"Comma-separated list of country ISO3 codes to be processed\", StringConverter.IDENTITY,\n+            Optionality.REQUIRED);\n+    static final Switch<MapRouletteConfiguration> MAP_ROULETTE = new Switch<>(\"maproulette\",\n+            \"Map roulette server information, format <Host>:<Port>:<ProjectName>:<ApiKey>, projectName is optional.\",\n+            MapRouletteConfiguration::parse, Optionality.OPTIONAL);\n+    static final Switch<Set<OutputFormats>> OUTPUT_FORMATS = new Switch<>(\"outputFormats\",\n+            \"Comma-separated list of output formats (flags, metrics, geojson, tippecanoe).\",\n+            csvFormats -> Stream.of(csvFormats.split(\",\"))\n+                    .map(format -> Enum.valueOf(OutputFormats.class, format.toUpperCase()))\n+                    .collect(Collectors.toSet()),\n+            Optionality.OPTIONAL, \"flags,metrics\");\n+    static final Switch<Rectangle> PBF_BOUNDING_BOX = new Switch<>(\"pbfBoundingBox\",\n+            \"OSM protobuf data will be loaded only in this bounding box\", Rectangle::forString,\n+            Optionality.OPTIONAL);\n+    static final Switch<Boolean> PBF_SAVE_INTERMEDIATE_ATLAS = new Switch<>(\"savePbfAtlas\",\n+            \"Saves intermediate atlas files created when processing OSM protobuf data.\",\n+            Boolean::valueOf, Optionality.OPTIONAL, \"false\");\n+    private static final String ATLAS_FILENAME_PATTERN_FORMAT = \"^%s_([0-9]+)-([0-9]+)-([0-9]+)\";\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(IntegrityChecksCommandArguments.class);\n+\n+    /**\n+     * Creates a map from country name to {@link List} of {@link Shard} definitions from\n+     * {@link Atlas} files.\n+     *\n+     * @param countries\n+     *            Set of countries to find out shards for\n+     * @param pathResolver\n+     *            {@link AtlasFilePathResolver} to search for {@link Atlas} files\n+     * @param atlasFolder\n+     *            Path to {@link Atlas} folder\n+     * @param sparkContext\n+     *            Spark context (or configuration) as a key-value map\n+     * @return A map from country name to {@link List} of {@link Shard} definitions\n+     */\n+    public static MultiMap<String, Shard> countryShardMapFromShardFiles(final Set<String> countries,\n+            final AtlasFilePathResolver pathResolver, final String atlasFolder,\n+            final Map<String, String> sparkContext)\n+    {\n+        final MultiMap<String, Shard> countryShardMap = new MultiMap<>();\n+        logger.info(\"Building country shard map from country shard files.\");\n+\n+        countries.forEach(country ->\n+        {\n+            final String countryDirectory = pathResolver.resolvePath(atlasFolder, country);\n+            final CountrySpecificAtlasFilePathFilter atlasFilter = new CountrySpecificAtlasFilePathFilter(\n+                    country);\n+            final Pattern atlasFilePattern = Pattern\n+                    .compile(String.format(ATLAS_FILENAME_PATTERN_FORMAT, country));\n+\n+            // Go over shard files for the country and use file name pattern to find out shards\n+            FileSystemHelper.listResourcesRecursively(countryDirectory, sparkContext, atlasFilter)\n+                    .forEach(shardFile ->\n+                    {\n+                        final String shardFileName = shardFile.getName();\n+                        final Matcher matcher = atlasFilePattern.matcher(shardFileName);\n+                        if (matcher.find())", "originalCommit": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MDkxNw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385270917", "bodyText": "I agree, I have added that.", "author": "Bentleysb", "createdAt": "2020-02-27T17:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg0ODI3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1MzQwNA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384853404", "bodyText": "Since we're reading in the raw country string are we handling invalid country strings?", "author": "seancoulter", "createdAt": "2020-02-27T00:44:46Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);", "originalCommit": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MDk2NA==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385270964", "bodyText": "Country strings are only invalid if they don't have corresponding files. If that is the case then that is handled by the block on line 194.", "author": "Bentleysb", "createdAt": "2020-02-27T17:49:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1MzQwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NDY0NQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384854645", "bodyText": "What if we load shards for countries that are given in the configuration but have no checks in countryChecks?", "author": "seancoulter", "createdAt": "2020-02-27T00:48:44Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(", "originalCommit": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NTcyMw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384855723", "bodyText": "I suppose this could happen if we specify a country in the config and a checkFilter but the checks are disabled in that country, so we'd want to exit", "author": "seancoulter", "createdAt": "2020-02-27T00:52:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NDY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MTAwNw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385271007", "bodyText": "There is a check for this on line 171.", "author": "Bentleysb", "createdAt": "2020-02-27T17:49:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg1NDY0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg2MTYzMw==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r384861633", "bodyText": "Wondering how useful it'd be to have a configurable DynamicAtlasPolicy, or one defined in a file", "author": "seancoulter", "createdAt": "2020-02-27T01:12:40Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,434 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;\n+        }\n+        for (final String country : countries)\n+        {\n+            final Set<Check> checksLoadedForCountry = checkLoader.loadChecksForCountry(country);\n+            if (checksLoadedForCountry.isEmpty())\n+            {\n+                logger.warn(\"No checks loaded for country {}. Skipping execution\", country);\n+            }\n+            else\n+            {\n+                checksLoadedForCountry.forEach(check -> this.countryChecks.add(country, check));\n+            }\n+        }\n+        if (this.countryChecks.isEmpty())\n+        {\n+            logger.error(\"No checks loaded for any of the countries provided. Skipping execution\");\n+            return;\n+        }\n+\n+        // Find the shards for each country atlas files\n+        final MultiMap<String, Shard> countryShards = countryShardMapFromShardFiles(\n+                countries.stream().collect(Collectors.toSet()), resolver, input, sparkContext);\n+        if (countryShards.isEmpty())\n+        {\n+            logger.info(\"No atlas files found in input \");\n+        }\n+\n+        if (!countries.stream().allMatch(countryShards::containsKey))\n+        {\n+            final Set<String> missingCountries = countries.stream()\n+                    .filter(aCountry -> !countryShards.containsKey(aCountry))\n+                    .collect(Collectors.toSet());\n+            logger.error(\n+                    \"Unable to find standardized named shard files in the path {}/<countryName> for the countries {}. \\n Files must be in format <country>_<zoom>_<x>_<y>.atlas\",\n+                    input, missingCountries);\n+            return;\n+        }\n+\n+        // List of spark RDDS/tasks\n+        final List<JavaPairRDD<String, UniqueCheckFlagContainer>> countryRdds = new ArrayList<>();\n+\n+        // Countrify spark parallelization for better debugging\n+        try (Pool checkPool = new Pool(countryShards.size(), \"Countries Execution Pool\"))\n+        {\n+            for (final Map.Entry<String, List<Shard>> countryShard : countryShards.entrySet())\n+            {\n+                checkPool.queue(() ->\n+                {\n+                    // Generate a task for each shard group\n+                    final List<ShardedCheckFlagsTask> tasksForCountry = countryShard.getValue()\n+                            .stream()\n+                            .map(group -> new ShardedCheckFlagsTask(countryShard.getKey(), group,\n+                                    this.countryChecks.get(countryShard.getKey())))\n+                            .collect(Collectors.toList());\n+\n+                    // Set spark UI job title\n+                    this.getContext().setJobGroup(\"0\",\n+                            String.format(\"Running checks on %s\",\n+                                    tasksForCountry.get(0).getCountry()));\n+                    // Run each task in spark, producing UniqueCheckFlagContainers\n+                    final JavaPairRDD<String, UniqueCheckFlagContainer> rdd = this.getContext()\n+                            .parallelize(tasksForCountry, tasksForCountry.size())\n+                            .mapToPair(produceFlags(input, output, this.configurationMap(),\n+                                    fileHelper, shardingBroadcast, distanceToLoadShards,\n+                                    (Boolean) commandMap.get(MULTI_ATLAS)));\n+                    // Save the RDD\n+                    rdd.persist(storageLevel);\n+                    // Use a fast spark action to overcome spark lazy elocution. This is necessary\n+                    // to get the UI title to appear in the right spot.\n+                    rdd.count();\n+                    // Add the RDDs to the list\n+                    countryRdds.add(rdd);\n+                });\n+            }\n+        }\n+\n+        // Set up for unioning\n+        final JavaPairRDD<String, UniqueCheckFlagContainer> firstCountryRdd = countryRdds.get(0);\n+        countryRdds.remove(0);\n+\n+        // Add UI title\n+        this.getContext().setJobGroup(\"0\", \"Conflate flags and generate outputs\");\n+        // union the RDDs, combine the UniqueCheckFlagContainers by country, and proccess the flags\n+        // through the output event service\n+        this.getContext().union(firstCountryRdd, countryRdds)\n+                .reduceByKey(UniqueCheckFlagContainer::combine)\n+                .foreach(processFlags(output, fileHelper, outputFormats));\n+\n+        logger.info(\"Sharded checks completed in {}\", start.elapsedSince());\n+    }\n+\n+    @Override\n+    protected SwitchList switches()\n+    {\n+        return super.switches().with(EXPANSION_DISTANCE, MULTI_ATLAS, SPARK_STORAGE_DISK_ONLY,\n+                SHARDING);\n+    }\n+\n+    /**\n+     * Get the fetcher to use for Atlas files. The fetcher uses a hadoop cache to reduce remote\n+     * reads.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param country\n+     *            {@link String} country code\n+     * @param configuration\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @return {@link Function} that fetches atlases/\n+     */\n+    private Function<Shard, Optional<Atlas>> atlasFetcher(final String input, final String country,\n+            final Map<String, String> configuration)\n+    {\n+        final HadoopAtlasFileCache cache = new HadoopAtlasFileCache(input, configuration);\n+        final AtlasResourceLoader loader = new AtlasResourceLoader();\n+        return (Function<Shard, Optional<Atlas>> & Serializable) shard -> cache.get(country, shard)\n+                .map(loader::load);\n+    }\n+\n+    /**\n+     * Process {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s through an event service to\n+     * produce output files.\n+     *\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param outputFormats\n+     *            {@link Set} of\n+     *            {@link org.openstreetmap.atlas.checks.distributed.IntegrityChecksCommandArguments.OutputFormats}\n+     * @return {@link VoidFunction} that takes a {@link Tuple2} of a {@link String} country code and\n+     *         a {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private VoidFunction<Tuple2<String, UniqueCheckFlagContainer>> processFlags(final String output,\n+            final SparkFileHelper fileHelper, final Set<OutputFormats> outputFormats)\n+    {\n+        return tuple ->\n+        {\n+            final String country = tuple._1();\n+            final UniqueCheckFlagContainer flagContainer = tuple._2();\n+            final EventService eventService = EventService.get(country);\n+\n+            if (outputFormats.contains(OutputFormats.FLAGS))\n+            {\n+                eventService.register(new CheckFlagFileProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_FLAG_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.GEOJSON))\n+            {\n+\n+                eventService.register(new CheckFlagGeoJsonProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_GEOJSON_FOLDER, country)));\n+            }\n+\n+            if (outputFormats.contains(OutputFormats.TIPPECANOE))\n+            {\n+                eventService.register(new CheckFlagTippecanoeProcessor(fileHelper,\n+                        SparkFileHelper.combine(output, OUTPUT_TIPPECANOE_FOLDER, country)));\n+            }\n+\n+            flagContainer.reconstructEvents().parallel().forEach(eventService::post);\n+            eventService.complete();\n+        };\n+    }\n+\n+    /**\n+     * {@link PairFunction} to run each {@link ShardedCheckFlagsTask} through to produce\n+     * {@link org.openstreetmap.atlas.checks.flag.CheckFlag}s.\n+     *\n+     * @param input\n+     *            {@link String} input folder path\n+     * @param output\n+     *            {@link String} output folder path\n+     * @param configurationMap\n+     *            {@link org.openstreetmap.atlas.generator.tools.spark.SparkJob} configuration map\n+     * @param fileHelper\n+     *            {@link SparkFileHelper}\n+     * @param sharding\n+     *            spark {@link Broadcast} of the current {@link Sharding}\n+     * @param shardDistanceExpansion\n+     *            {@link Distance} to expand the shard group\n+     * @param multiAtlas\n+     *            boolean whether to use a multi or dynamic Atlas\n+     * @return {@link PairFunction} that takes {@link ShardedCheckFlagsTask} and returns a\n+     *         {@link Tuple2} of a {@link String} country code and {@link UniqueCheckFlagContainer}\n+     */\n+    @SuppressWarnings(\"unchecked\")\n+    private PairFunction<ShardedCheckFlagsTask, String, UniqueCheckFlagContainer> produceFlags(\n+            final String input, final String output, final Map<String, String> configurationMap,\n+            final SparkFileHelper fileHelper, final Broadcast<Sharding> sharding,\n+            final Distance shardDistanceExpansion, final boolean multiAtlas)\n+    {\n+        return task ->\n+        {\n+            // Get the atlas\n+            final Function<Shard, Optional<Atlas>> fetcher = this.atlasFetcher(input,\n+                    task.getCountry(), configurationMap);\n+            final Atlas atlas;\n+\n+            // Use dynamic or multi atlas (multi runs faster locally)\n+            if (multiAtlas)\n+            {\n+                atlas = new MultiAtlas(\n+                        StreamSupport\n+                                .stream(sharding.getValue()\n+                                        .shards(task.getShard().bounds()\n+                                                .expand(shardDistanceExpansion))\n+                                        .spliterator(), true)\n+                                .map(fetcher).filter(Optional::isPresent).map(Optional::get)\n+                                .collect(Collectors.toList()));\n+            }\n+            else\n+            {\n+                final DynamicAtlasPolicy policy = new DynamicAtlasPolicy(fetcher,\n+                        sharding.getValue(), Collections.singleton(task.getShard()),\n+                        task.getShard().bounds().expand(shardDistanceExpansion))\n+                                .withDeferredLoading(true).withAggressivelyExploreRelations(true)\n+                                .withExtendIndefinitely(false);", "originalCommit": "1f413b4855b96a2af68f1af565a87e6bf1ced1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3MTA0Ng==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385271046", "bodyText": "This is definitely something I want to have eventually. That may be part of the solution for dealing with large relations. For now I think it is fine to have it hard coded while that solution is architected.", "author": "Bentleysb", "createdAt": "2020-02-27T17:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDg2MTYzMw=="}], "type": "inlineReview"}, {"oid": "e0d2a7c70308b20a7cc15ef1c95654a474f6bb6c", "url": "https://github.com/osmlab/atlas-checks/commit/e0d2a7c70308b20a7cc15ef1c95654a474f6bb6c", "message": "Merge branch 'dev' into shardedChecks-dynamic-noGroup", "committedDate": "2020-02-27T01:54:50Z", "type": "commit"}, {"oid": "baf9f93d9ccb5a2895dcea01631b53d8179c2fc0", "url": "https://github.com/osmlab/atlas-checks/commit/baf9f93d9ccb5a2895dcea01631b53d8179c2fc0", "message": "clean up 5", "committedDate": "2020-02-27T17:48:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTI3NDEzMQ==", "url": "https://github.com/osmlab/atlas-checks/pull/259#discussion_r385274131", "bodyText": "One of @seancoulter's comments made me realize that there are a lot of soft failures used in the start method to short circuit execution of the new spark job. These were added in the original PR, so I had just left them alone. I am now wondering if it might be better to throw exceptions, to make it more evident why the job did not do anything. Does anyone have an opinion on this?", "author": "Bentleysb", "createdAt": "2020-02-27T17:55:01Z", "path": "src/main/java/org/openstreetmap/atlas/checks/distributed/ShardedIntegrityChecksSparkJob.java", "diffHunk": "@@ -0,0 +1,435 @@\n+package org.openstreetmap.atlas.checks.distributed;\n+\n+import static org.openstreetmap.atlas.checks.distributed.IntegrityCheckSparkJob.METRICS_FILENAME;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.api.java.function.VoidFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.openstreetmap.atlas.checks.base.Check;\n+import org.openstreetmap.atlas.checks.base.CheckResourceLoader;\n+import org.openstreetmap.atlas.checks.configuration.ConfigurationResolver;\n+import org.openstreetmap.atlas.checks.constants.CommonConstants;\n+import org.openstreetmap.atlas.checks.event.CheckFlagEvent;\n+import org.openstreetmap.atlas.checks.event.CheckFlagFileProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagGeoJsonProcessor;\n+import org.openstreetmap.atlas.checks.event.CheckFlagTippecanoeProcessor;\n+import org.openstreetmap.atlas.checks.event.MetricFileGenerator;\n+import org.openstreetmap.atlas.checks.utility.UniqueCheckFlagContainer;\n+import org.openstreetmap.atlas.event.EventService;\n+import org.openstreetmap.atlas.event.Processor;\n+import org.openstreetmap.atlas.event.ShutdownEvent;\n+import org.openstreetmap.atlas.generator.sharding.AtlasSharding;\n+import org.openstreetmap.atlas.generator.tools.caching.HadoopAtlasFileCache;\n+import org.openstreetmap.atlas.generator.tools.spark.utilities.SparkFileHelper;\n+import org.openstreetmap.atlas.geography.atlas.Atlas;\n+import org.openstreetmap.atlas.geography.atlas.AtlasResourceLoader;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.DynamicAtlas;\n+import org.openstreetmap.atlas.geography.atlas.dynamic.policy.DynamicAtlasPolicy;\n+import org.openstreetmap.atlas.geography.atlas.multi.MultiAtlas;\n+import org.openstreetmap.atlas.geography.sharding.Shard;\n+import org.openstreetmap.atlas.geography.sharding.Sharding;\n+import org.openstreetmap.atlas.utilities.collections.StringList;\n+import org.openstreetmap.atlas.utilities.configuration.Configuration;\n+import org.openstreetmap.atlas.utilities.configuration.MergedConfiguration;\n+import org.openstreetmap.atlas.utilities.configuration.StandardConfiguration;\n+import org.openstreetmap.atlas.utilities.conversion.StringConverter;\n+import org.openstreetmap.atlas.utilities.filters.AtlasEntityPolygonsFilter;\n+import org.openstreetmap.atlas.utilities.maps.MultiMap;\n+import org.openstreetmap.atlas.utilities.runtime.CommandMap;\n+import org.openstreetmap.atlas.utilities.scalars.Distance;\n+import org.openstreetmap.atlas.utilities.threads.Pool;\n+import org.openstreetmap.atlas.utilities.time.Time;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.eventbus.AllowConcurrentEvents;\n+import com.google.common.eventbus.Subscribe;\n+\n+import scala.Serializable;\n+import scala.Tuple2;\n+\n+/**\n+ * A spark job for generating integrity checks in a sharded fashion. This allows for a lower local\n+ * memory profile as well as better parallelization.<br>\n+ * This implementation currently only supports Atlas files as an input data source. They must be in\n+ * the structure: folder/iso_code/iso_z-x-y.atlas<br>\n+ * Also required is a reference for how the Atlases are sharded. This can be provided as a\n+ * sharding.txt file in the input path, or through the {@code sharding} argument.\n+ *\n+ * @author jklamer\n+ * @author bbreithaupt\n+ */\n+public class ShardedIntegrityChecksSparkJob extends IntegrityChecksCommandArguments\n+{\n+    private static final Switch<Distance> EXPANSION_DISTANCE = new Switch<>(\"shardBufferDistance\",\n+            \"Distance to expand the bounds of the shard group to create a network in kilometers\",\n+            distanceString -> Distance.kilometers(Double.valueOf(distanceString)),\n+            Optionality.OPTIONAL, \"10.0\");\n+    private static final String ATLAS_SHARDING_FILE = \"sharding.txt\";\n+    private static final Switch<String> SHARDING = new Switch<>(\"sharding\",\n+            \"Sharding to load in place of sharding file in Atlas path\", StringConverter.IDENTITY,\n+            Optionality.OPTIONAL);\n+    private static final Switch<Boolean> MULTI_ATLAS = new Switch<>(\"multiAtlas\",\n+            \"If true then use a multi atlas, else use a dynamic atlas. This works better for running on a single machine\",\n+            Boolean::new, Optionality.OPTIONAL, \"false\");\n+    private static final Switch<String> SPARK_STORAGE_DISK_ONLY = new Switch<>(\n+            \"sparkStorageDiskOnly\", \"Store cached RDDs exclusively on disk\",\n+            StringConverter.IDENTITY, Optionality.OPTIONAL);\n+\n+    private static final Logger logger = LoggerFactory\n+            .getLogger(ShardedIntegrityChecksSparkJob.class);\n+    private static final long serialVersionUID = -8038802870994470017L;\n+\n+    private final MultiMap<String, Check> countryChecks = new MultiMap<>();\n+\n+    public static void main(final String[] args)\n+    {\n+        new ShardedIntegrityChecksSparkJob().run(args);\n+    }\n+\n+    @Override\n+    public String getName()\n+    {\n+        return \"Sharded Integrity Checks Spark Job\";\n+    }\n+\n+    @Override\n+    public void start(final CommandMap commandMap)\n+    {\n+        final Time start = Time.now();\n+        final String atlasDirectory = (String) commandMap.get(ATLAS_FOLDER);\n+        final String input = Optional.ofNullable(input(commandMap)).orElse(atlasDirectory);\n+\n+        // Gather arguments\n+        final String output = output(commandMap);\n+        @SuppressWarnings(\"unchecked\")\n+        final Set<OutputFormats> outputFormats = (Set<OutputFormats>) commandMap\n+                .get(OUTPUT_FORMATS);\n+        final StringList countries = StringList.split((String) commandMap.get(COUNTRIES),\n+                CommonConstants.COMMA);\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<List<String>> checkFilter = (Optional<List<String>>) commandMap\n+                .getOption(CHECK_FILTER);\n+\n+        final Configuration checksConfiguration = new MergedConfiguration(Stream\n+                .concat(Stream.of(ConfigurationResolver.loadConfiguration(commandMap,\n+                        CONFIGURATION_FILES, CONFIGURATION_JSON)),\n+                        Stream.of(checkFilter\n+                                .<Configuration> map(whitelist -> new StandardConfiguration(\n+                                        \"WhiteListConfiguration\",\n+                                        Collections.singletonMap(\n+                                                \"CheckResourceLoader.checks.whitelist\", whitelist)))\n+                                .orElse(ConfigurationResolver.emptyConfiguration())))\n+                .collect(Collectors.toList()));\n+\n+        final Map<String, String> sparkContext = configurationMap();\n+\n+        // File loading helpers\n+        final AtlasFilePathResolver resolver = new AtlasFilePathResolver(checksConfiguration);\n+        final SparkFileHelper fileHelper = new SparkFileHelper(sparkContext);\n+        final CheckResourceLoader checkLoader = new CheckResourceLoader(checksConfiguration);\n+\n+        // Spark storage argument\n+        final StorageLevel storageLevel = Optional\n+                .ofNullable(commandMap.get(SPARK_STORAGE_DISK_ONLY)).orElse(\"false\").equals(\"true\")\n+                        ? StorageLevel.DISK_ONLY()\n+                        : StorageLevel.MEMORY_AND_DISK();\n+\n+        // Get sharding\n+        @SuppressWarnings(\"unchecked\")\n+        final Optional<String> alternateShardingFile = (Optional<String>) commandMap\n+                .getOption(SHARDING);\n+        final String shardingPathInAtlas = \"dynamic@\"\n+                + SparkFileHelper.combine(input, ATLAS_SHARDING_FILE);\n+        final String shardingFilePath = alternateShardingFile.orElse(shardingPathInAtlas);\n+        final Sharding sharding = AtlasSharding.forString(shardingFilePath,\n+                this.configurationMap());\n+        final Broadcast<Sharding> shardingBroadcast = this.getContext().broadcast(sharding);\n+        final Distance distanceToLoadShards = (Distance) commandMap.get(EXPANSION_DISTANCE);\n+\n+        // Check inputs\n+        if (countries.isEmpty())\n+        {\n+            logger.error(\"No countries found to run\");\n+            return;", "originalCommit": "baf9f93d9ccb5a2895dcea01631b53d8179c2fc0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8da8b39f52ea9d122b28c0b1e4084935efca9c73", "url": "https://github.com/osmlab/atlas-checks/commit/8da8b39f52ea9d122b28c0b1e4084935efca9c73", "message": "code smells", "committedDate": "2020-02-28T04:41:22Z", "type": "commit"}, {"oid": "bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324", "url": "https://github.com/osmlab/atlas-checks/commit/bebcb42f96042aa6f6bd64f7c2c3b3c743ea4324", "message": "hard fails", "committedDate": "2020-02-28T19:55:27Z", "type": "commit"}]}