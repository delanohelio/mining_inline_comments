{"pr_number": 11568, "pr_title": "Improve SecureHdfsValidationTask", "pr_createdAt": "2020-06-16T08:29:01Z", "pr_url": "https://github.com/Alluxio/alluxio/pull/11568", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDY3Nzk4NA==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r440677984", "bodyText": "Changed to a more lenient format following https://github.com/cloudera/hadoop-common/blob/ca2ff489eb805da4700fb15fa49e539f1c195b89/src/java/org/apache/hadoop/security/KerberosName.java#L53 and the implementation of zookeeper org.apache.zookeeper.server.auth.KerberosName.\nThe previous pattern does not allow dot in the realm like @ALLUXIO.COM", "author": "jiacheliu3", "createdAt": "2020-06-16T08:30:22Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -39,7 +38,7 @@\n    * for more details.\n    */\n   private static final Pattern PRINCIPAL_PATTERN =\n-      Pattern.compile(\"(?<primary>[\\\\w][\\\\w-]*\\\\$?)(/(?<instance>[\\\\w]+))?(@(?<realm>[\\\\w]+))?\");\n+      Pattern.compile(\"(?<primary>[^/@]*)(/(?<instance>[^/@]*))?@(?<realm>[^/@]*)\");", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNTU4MQ==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441235581", "bodyText": "Can we use org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_AUTHORIZATION instead?", "author": "bf8086", "createdAt": "2020-06-17T01:52:40Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -51,6 +50,13 @@\n       PRINCIPAL_MAP_MASTER_KEY, PropertyKey.MASTER_KEYTAB_KEY_FILE,\n       PRINCIPAL_MAP_WORKER_KEY, PropertyKey.WORKER_KEYTAB_FILE);\n \n+  private static final String HDFS_AUTHORIZATION_KEY = \"hadoop.security.authorization\";", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI1MjA3OA==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441252078", "bodyText": "I'm a bit concerned this introduces hadoop-common to shell/ and might lead to us shading jars to not break the downstream. @LuQQiu What do you think? If it's okay we add hadoop common directly to shell, i'll make this change. Otherwise I prefer to leave a TODO here and revisit it after 2.3.", "author": "jiacheliu3", "createdAt": "2020-06-17T02:56:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNTU4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI4MTkwNw==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441281907", "bodyText": "Discussed with @LuQQiu. Since there's no quick way to verify if we have to shade it in shell/, I added a TODO.", "author": "jiacheliu3", "createdAt": "2020-06-17T05:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNTU4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNTc2MA==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441235760", "bodyText": "Same here.", "author": "bf8086", "createdAt": "2020-06-17T01:53:20Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -51,6 +50,13 @@\n       PRINCIPAL_MAP_MASTER_KEY, PropertyKey.MASTER_KEYTAB_KEY_FILE,\n       PRINCIPAL_MAP_WORKER_KEY, PropertyKey.WORKER_KEYTAB_FILE);\n \n+  private static final String HDFS_AUTHORIZATION_KEY = \"hadoop.security.authorization\";\n+  private static final String HDFS_AUTHORIZATION_VALUE = \"true\";\n+  private static final String HDFS_AUTHENTICATION_KEY = \"hadoop.security.authentication\";", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzNjYxOA==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441236618", "bodyText": "StringUtils.capitalize(mProcess)?", "author": "bf8086", "createdAt": "2020-06-17T01:56:52Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -79,36 +86,72 @@ public SecureHdfsValidationTask(String process, String path, AlluxioConfiguratio\n \n   @Override\n   public String getName() {\n-    return String.format(\"ValidateKerberosForSecureHdfs%s\", mProcess.toUpperCase());\n+    // Convert the first char to upper case\n+    char first = Character.toUpperCase(mProcess.charAt(0));\n+    return String.format(\"ValidateKerberosForSecureHdfs%s\", first + mProcess.substring(1));", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzODI5Nw==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441238297", "bodyText": "These properties are not bound to be enabled at the same time, you can have authentication set to \"simple\" but have the authorization on.", "author": "bf8086", "createdAt": "2020-06-17T02:03:16Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -79,36 +86,72 @@ public SecureHdfsValidationTask(String process, String path, AlluxioConfiguratio\n \n   @Override\n   public String getName() {\n-    return String.format(\"ValidateKerberosForSecureHdfs%s\", mProcess.toUpperCase());\n+    // Convert the first char to upper case\n+    char first = Character.toUpperCase(mProcess.charAt(0));\n+    return String.format(\"ValidateKerberosForSecureHdfs%s\", first + mProcess.substring(1));\n   }\n \n   @Override\n   public ValidationUtils.TaskResult validate(Map<String, String> optionsMap) {\n-    if (shouldSkip()) {\n+    if (!HdfsConfValidationTask.isHdfsScheme(mPath)) {\n+      mMsg.append(\"Skip this check as the UFS is not HDFS.\\n\");\n       return new ValidationUtils.TaskResult(ValidationUtils.State.SKIPPED, getName(),\n               mMsg.toString(), mAdvice.toString());\n     }\n+\n+    ValidationUtils.TaskResult loadConfig = loadHdfsConfig();\n+    if (loadConfig.getState() != ValidationUtils.State.OK) {\n+      return loadConfig;\n+    }\n+\n+    // The state is OK when the HDFS is secured\n+    ValidationUtils.TaskResult hdfsSecured = validateSecureHdfs();\n+    if (hdfsSecured.getState() != ValidationUtils.State.OK) {\n+      return hdfsSecured;\n+    }\n+\n     return validatePrincipalLogin();\n   }\n \n-  protected boolean shouldSkip() {\n-    if (!HdfsConfValidationTask.isHdfsScheme(mPath)) {\n-      mMsg.append(\"Skip this check as the UFS is not HDFS.\\n\");\n-      return true;\n-    }\n-    String principal = null;\n-    if (mConf.isSet(mPrincipalProperty)) {\n-      principal = mConf.get(mPrincipalProperty);\n+  private ValidationUtils.TaskResult validateSecureHdfs() {\n+    // Skipped if HDFS is not Kerberized\n+    // Ref: https://docs.cloudera.com/documentation/enterprise/5-16-x/topics\n+    // /cdh_sg_hadoop_security_enable.html\n+    String hadoopAuthentication = mCoreConf.getOrDefault(HDFS_AUTHENTICATION_KEY, \"\");\n+    boolean authenticationEnabled =\n+            hadoopAuthentication.equalsIgnoreCase(HDFS_AUTHENTICATION_VALUE);\n+    String hadoopAuthorization = mCoreConf.getOrDefault(HDFS_AUTHORIZATION_KEY, \"\");\n+    boolean authorizationEnabled = hadoopAuthorization.equals(HDFS_AUTHORIZATION_VALUE);\n+    if (!authenticationEnabled && !authorizationEnabled) {\n+      mMsg.append(\"HDFS is not Kerberized. Skip this test.\");\n+      return new ValidationUtils.TaskResult(ValidationUtils.State.SKIPPED, getName(),\n+              mMsg.toString(), mAdvice.toString());\n     }\n-    if (principal == null || principal.isEmpty()) {\n-      mMsg.append(String.format(\"Skip validation for secure HDFS. %s is not specified.%n\",\n-          PRINCIPAL_MAP.get(mProcess).getName()));\n-      return true;\n+\n+    // Issue an error if the secured HDFS is not configured properly\n+    if (!authenticationEnabled || !authorizationEnabled) {\n+      mMsg.append(String.format(\"Found inconsistent configuration for Hadoop security.\"", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI1Mjg3OQ==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441252879", "bodyText": "OK. Removed the authorization check as only authentication matters in this check", "author": "jiacheliu3", "createdAt": "2020-06-17T02:59:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzODI5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzODY2OA==", "url": "https://github.com/Alluxio/alluxio/pull/11568#discussion_r441238668", "bodyText": "Would be nice to be more specific on which property is missing.", "author": "bf8086", "createdAt": "2020-06-17T02:04:44Z", "path": "shell/src/main/java/alluxio/cli/validation/hdfs/SecureHdfsValidationTask.java", "diffHunk": "@@ -79,36 +86,72 @@ public SecureHdfsValidationTask(String process, String path, AlluxioConfiguratio\n \n   @Override\n   public String getName() {\n-    return String.format(\"ValidateKerberosForSecureHdfs%s\", mProcess.toUpperCase());\n+    // Convert the first char to upper case\n+    char first = Character.toUpperCase(mProcess.charAt(0));\n+    return String.format(\"ValidateKerberosForSecureHdfs%s\", first + mProcess.substring(1));\n   }\n \n   @Override\n   public ValidationUtils.TaskResult validate(Map<String, String> optionsMap) {\n-    if (shouldSkip()) {\n+    if (!HdfsConfValidationTask.isHdfsScheme(mPath)) {\n+      mMsg.append(\"Skip this check as the UFS is not HDFS.\\n\");\n       return new ValidationUtils.TaskResult(ValidationUtils.State.SKIPPED, getName(),\n               mMsg.toString(), mAdvice.toString());\n     }\n+\n+    ValidationUtils.TaskResult loadConfig = loadHdfsConfig();\n+    if (loadConfig.getState() != ValidationUtils.State.OK) {\n+      return loadConfig;\n+    }\n+\n+    // The state is OK when the HDFS is secured\n+    ValidationUtils.TaskResult hdfsSecured = validateSecureHdfs();\n+    if (hdfsSecured.getState() != ValidationUtils.State.OK) {\n+      return hdfsSecured;\n+    }\n+\n     return validatePrincipalLogin();\n   }\n \n-  protected boolean shouldSkip() {\n-    if (!HdfsConfValidationTask.isHdfsScheme(mPath)) {\n-      mMsg.append(\"Skip this check as the UFS is not HDFS.\\n\");\n-      return true;\n-    }\n-    String principal = null;\n-    if (mConf.isSet(mPrincipalProperty)) {\n-      principal = mConf.get(mPrincipalProperty);\n+  private ValidationUtils.TaskResult validateSecureHdfs() {\n+    // Skipped if HDFS is not Kerberized\n+    // Ref: https://docs.cloudera.com/documentation/enterprise/5-16-x/topics\n+    // /cdh_sg_hadoop_security_enable.html\n+    String hadoopAuthentication = mCoreConf.getOrDefault(HDFS_AUTHENTICATION_KEY, \"\");\n+    boolean authenticationEnabled =\n+            hadoopAuthentication.equalsIgnoreCase(HDFS_AUTHENTICATION_VALUE);\n+    String hadoopAuthorization = mCoreConf.getOrDefault(HDFS_AUTHORIZATION_KEY, \"\");\n+    boolean authorizationEnabled = hadoopAuthorization.equals(HDFS_AUTHORIZATION_VALUE);\n+    if (!authenticationEnabled && !authorizationEnabled) {\n+      mMsg.append(\"HDFS is not Kerberized. Skip this test.\");\n+      return new ValidationUtils.TaskResult(ValidationUtils.State.SKIPPED, getName(),\n+              mMsg.toString(), mAdvice.toString());\n     }\n-    if (principal == null || principal.isEmpty()) {\n-      mMsg.append(String.format(\"Skip validation for secure HDFS. %s is not specified.%n\",\n-          PRINCIPAL_MAP.get(mProcess).getName()));\n-      return true;\n+\n+    // Issue an error if the secured HDFS is not configured properly\n+    if (!authenticationEnabled || !authorizationEnabled) {\n+      mMsg.append(String.format(\"Found inconsistent configuration for Hadoop security.\"\n+                      + \" %s=%s but %s=%s.%n\", HDFS_AUTHENTICATION_KEY, hadoopAuthentication,\n+              HDFS_AUTHORIZATION_KEY, hadoopAuthorization));\n+      mAdvice.append(String.format(\"Please enable Hadoop security by setting %s=%s and %s=%s%n.\",\n+              HDFS_AUTHENTICATION_KEY, HDFS_AUTHENTICATION_VALUE, HDFS_AUTHORIZATION_KEY,\n+              HDFS_AUTHORIZATION_VALUE));\n+      return new ValidationUtils.TaskResult(ValidationUtils.State.FAILED, getName(),\n+              mMsg.toString(), mAdvice.toString());\n     }\n-    return false;\n+    return new ValidationUtils.TaskResult(ValidationUtils.State.OK, getName(),\n+            mMsg.toString(), mAdvice.toString());\n   }\n \n   private ValidationUtils.TaskResult validatePrincipalLogin() {\n+    if (!mConf.isSet(mPrincipalProperty) || !mConf.isSet(mKeytabProperty)) {", "originalCommit": "3ce6dbc2f0e5407883f2d78ad74dc15607f13c96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1cca9be40e5372d567690ee1b26b05ffbb361079", "url": "https://github.com/Alluxio/alluxio/commit/1cca9be40e5372d567690ee1b26b05ffbb361079", "message": "improve SecureHdfsCheck", "committedDate": "2020-06-19T14:27:08Z", "type": "commit"}, {"oid": "e54b09bc841db3245b8d5984793d969bcaeb2dde", "url": "https://github.com/Alluxio/alluxio/commit/e54b09bc841db3245b8d5984793d969bcaeb2dde", "message": "update pattern", "committedDate": "2020-06-19T14:28:20Z", "type": "commit"}, {"oid": "0a412ce3bf75f993bc96fb590a05f62fe761f748", "url": "https://github.com/Alluxio/alluxio/commit/0a412ce3bf75f993bc96fb590a05f62fe761f748", "message": "address comments", "committedDate": "2020-06-19T14:28:20Z", "type": "commit"}, {"oid": "e491d5fcdddcad1c255a968557be8e96185ad013", "url": "https://github.com/Alluxio/alluxio/commit/e491d5fcdddcad1c255a968557be8e96185ad013", "message": "add todo", "committedDate": "2020-06-19T14:28:21Z", "type": "commit"}, {"oid": "e491d5fcdddcad1c255a968557be8e96185ad013", "url": "https://github.com/Alluxio/alluxio/commit/e491d5fcdddcad1c255a968557be8e96185ad013", "message": "add todo", "committedDate": "2020-06-19T14:28:21Z", "type": "forcePushed"}]}