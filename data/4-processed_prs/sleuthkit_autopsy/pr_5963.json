{"pr_number": 5963, "pr_title": "6415 streaming ingest pipeline2", "pr_createdAt": "2020-06-09T23:57:46Z", "pr_url": "https://github.com/sleuthkit/autopsy/pull/5963", "timeline": [{"oid": "5e1b13f85a976242187880dd0e531baacc046c64", "url": "https://github.com/sleuthkit/autopsy/commit/5e1b13f85a976242187880dd0e531baacc046c64", "message": "Renamed DataSourceIngestJob.\nWork on scheduling streaming ingest tasks.", "committedDate": "2020-06-08T14:32:24Z", "type": "commit"}, {"oid": "d8baf3469608b73efe5531454cb6843278a6ac99", "url": "https://github.com/sleuthkit/autopsy/commit/d8baf3469608b73efe5531454cb6843278a6ac99", "message": "Hard coded test of streaming ingest is working.", "committedDate": "2020-06-09T23:53:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEwNjgwNg==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438106806", "bodyText": "Let's change variable names, etc. to reflect \"pipeline\" instead of job.", "author": "rcordovano", "createdAt": "2020-06-10T13:07:16Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestTask.java", "diffHunk": "@@ -23,15 +23,15 @@\n abstract class IngestTask {\n \n     private final static long NOT_SET = Long.MIN_VALUE;\n-    private final DataSourceIngestJob job;\n+    private final IngestJobPipeline job;", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODExMDE0OA==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438110148", "bodyText": "Change \"job\" to something else.", "author": "rcordovano", "createdAt": "2020-06-10T13:12:29Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/DataSourceIngestPipeline.java", "diffHunk": "@@ -52,7 +52,7 @@\n      * @param moduleTemplates Templates for the creating the ingest modules that\n      *                        make up this pipeline.\n      */\n-    DataSourceIngestPipeline(DataSourceIngestJob job, List<IngestModuleTemplate> moduleTemplates) {\n+    DataSourceIngestPipeline(IngestJobPipeline job, List<IngestModuleTemplate> moduleTemplates) {", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODExNTI0MQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438115241", "bodyText": "This may not be true anymore.", "author": "rcordovano", "createdAt": "2020-06-10T13:19:58Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -340,7 +347,7 @@ String getExecutionContext() {\n     /**\n      * Gets the data source to be ingested by this job.\n      *\n-     * @return A Content object representing the data source.\n+     * @return A Content object representing the data source. May return null.", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODExODIyNQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438118225", "bodyText": "If this code stays the same, we can invert the conditionals and reduce code duplication.", "author": "rcordovano", "createdAt": "2020-06-10T13:24:09Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -414,20 +421,35 @@ private boolean hasFileIngestPipeline() {\n      * @return A collection of ingest module startup errors, empty on success.\n      */\n     List<IngestModuleError> start() {\n+\tif (dataSource == null) {\n+\t    throw new IllegalStateException(\"Ingest started before setting data source\"); // TODO remove?\n+\t}\n         List<IngestModuleError> errors = startUpIngestPipelines();\n         if (errors.isEmpty()) {\n             try {\n                 this.ingestJob = Case.getCurrentCaseThrows().getSleuthkitCase().addIngestJob(dataSource, NetworkUtils.getLocalHostName(), ingestModules, new Date(this.createTime), new Date(0), IngestJobStatusType.STARTED, \"\");\n             } catch (TskCoreException | NoCurrentCaseException ex) {\n                 logErrorMessage(Level.WARNING, \"Failed to add ingest job info to case database\", ex); //NON-NLS\n             }\n-            if (this.hasFirstStageDataSourceIngestPipeline() || this.hasFileIngestPipeline()) {\n-                logInfoMessage(\"Starting first stage analysis\"); //NON-NLS\n-                this.startFirstStage();\n-            } else if (this.hasSecondStageDataSourceIngestPipeline()) {\n-                logInfoMessage(\"Starting second stage analysis\"); //NON-NLS\n-                this.startSecondStage();\n-            }\n+\t    if (ingestMode == IngestJob.Mode.BATCH) {\n+\t\tif (this.hasFirstStageDataSourceIngestPipeline() || this.hasFileIngestPipeline()) {\n+\t\t    logInfoMessage(\"Starting first stage analysis\"); //NON-NLS\n+\t\t    this.startFirstStage();\n+\t\t} else if (this.hasSecondStageDataSourceIngestPipeline()) {\n+\t\t    logInfoMessage(\"Starting second stage analysis\"); //NON-NLS\n+\t\t    this.startSecondStage();\n+\t\t}\n+\t    } else {\n+\t\t// TODO figure out logic when we're starting with second stage\n+\t\tif (this.hasFirstStageDataSourceIngestPipeline() || this.hasFileIngestPipeline()) {\n+\t\t    logInfoMessage(\"Preparing for first stage analysis\"); //NON-NLS\n+\t\t    this.startFileIngestStreaming();\n+\t\t} else if (this.hasSecondStageDataSourceIngestPipeline()) {\n+\t\t    // TODO Test this part\n+\t\t    logInfoMessage(\"Starting second stage analysis\"); //NON-NLS\n+\t\t    this.startSecondStage();\n+\t\t}\n+\t    }", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODExOTcwOQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438119709", "bodyText": "We may want to make the start/start stage, etc.. method names match up better.", "author": "rcordovano", "createdAt": "2020-06-10T13:26:14Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -539,21 +561,77 @@ private void startFirstStage() {\n             this.checkForStageCompleted();\n         }\n     }\n+    \n+    /**\n+     * Prepares for file ingest.\n+     * Used for streaming ingest. Does not schedule any file tasks - those\n+     * will come from calls to addStreamingIngestFiles().\n+     */\n+    private void startFileIngestStreaming() {", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODgzNDIwMA==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438834200", "bodyText": "Discussion in Jira", "author": "APriestman", "createdAt": "2020-06-11T14:36:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODExOTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEyMTI0Nw==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438121247", "bodyText": "We should check over the rather old thread safety code and maybe make it less granular.", "author": "rcordovano", "createdAt": "2020-06-10T13:28:24Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -539,21 +561,77 @@ private void startFirstStage() {\n             this.checkForStageCompleted();\n         }\n     }\n+    \n+    /**\n+     * Prepares for file ingest.\n+     * Used for streaming ingest. Does not schedule any file tasks - those\n+     * will come from calls to addStreamingIngestFiles().\n+     */\n+    private void startFileIngestStreaming() {\n+\tsynchronized (this.stageCompletionCheckLock) {\n+\t    this.stage = IngestJobPipeline.Stages.STREAMING_FILE_INGEST;\n+\t}", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEyMTcwMw==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438121703", "bodyText": "We need to decide how we want to show progress for streaming.", "author": "rcordovano", "createdAt": "2020-06-10T13:28:59Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -539,21 +561,77 @@ private void startFirstStage() {\n             this.checkForStageCompleted();\n         }\n     }\n+    \n+    /**\n+     * Prepares for file ingest.\n+     * Used for streaming ingest. Does not schedule any file tasks - those\n+     * will come from calls to addStreamingIngestFiles().\n+     */\n+    private void startFileIngestStreaming() {\n+\tsynchronized (this.stageCompletionCheckLock) {\n+\t    this.stage = IngestJobPipeline.Stages.STREAMING_FILE_INGEST;\n+\t}\n+\n+        if (this.hasFileIngestPipeline()) {\n+            synchronized (this.fileIngestProgressLock) {\n+                this.estimatedFilesToProcess = 0; // There's no way to estimate file count for a streaming data source\n+            }\n+        }", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc5ODUyMQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438798521", "bodyText": "I'm going to make a story for this since I don't think it's an easy question.", "author": "APriestman", "createdAt": "2020-06-11T13:48:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODEyMTcwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE1NDY5MA==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438154690", "bodyText": "There will always be a data source now.", "author": "rcordovano", "createdAt": "2020-06-10T14:13:37Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -599,15 +677,15 @@ private void startFileIngestProgressBar() {\n             synchronized (this.fileIngestProgressLock) {\n                 String displayName = NbBundle.getMessage(this.getClass(),\n                         \"IngestJob.progress.fileIngest.displayName\",\n-                        this.dataSource.getName());\n+                        getDataSourceName());", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE1NTUxOA==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438155518", "bodyText": "I think I'd like to see two methods here.", "author": "rcordovano", "createdAt": "2020-06-10T14:14:46Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -623,16 +701,41 @@ public boolean cancel() {\n      */\n     private void checkForStageCompleted() {\n         synchronized (this.stageCompletionCheckLock) {\n-            if (DataSourceIngestJob.taskScheduler.tasksForJobAreCompleted(this)) {\n-                switch (this.stage) {\n-                    case FIRST:\n-                        this.finishFirstStage();\n-                        break;\n-                    case SECOND:\n-                        this.finish();\n-                        break;\n-                }\n-            }\n+\t    if (ingestMode == IngestJob.Mode.BATCH) {\n+\t\tif (IngestJobPipeline.taskScheduler.currentTasksAreCompleted(this)) {\n+\t\t    switch (this.stage) {\n+\t\t\tcase FIRST:\n+\t\t\t    this.finishFirstStage();\n+\t\t\t    break;\n+\t\t\tcase SECOND:\n+\t\t\t    this.finish();\n+\t\t\t    break;\n+\t\t    }\n+\t\t}\n+\t    } else {\n+\t\tSystem.out.println(\"IngestJobPipeline.checkForStageCompleted() - current stage: \" + this.stage.toString());\n+\n+\t\tif (IngestJobPipeline.taskScheduler.currentTasksAreCompleted(this)) {\n+\t\t    System.out.println(\"IngestJobPipeline.checkForStageCompleted() - current tasks are completed\");\n+\t\t    switch (this.stage) {\n+\t\t\tcase STREAMING_FILE_INGEST:\n+\t\t\t    // Nothing to do here - need to wait for the data source\n+\t\t\t    System.out.println(\"IngestJobPipeline.checkForStageCompleted() - do nothing\");\n+\t\t\t    break;\n+\t\t\tcase STREAMING_FIRST_STAGE_DATA_SOURCE_INGEST:\n+\t\t\t    // Finish file and data source ingest, start second stage (if applicable)\n+\t\t\t    System.out.println(\"IngestJobPipeline.checkForStageCompleted() - finishFirstStage\");\n+\t\t\t    this.finishFirstStage();\n+\t\t\t    break;\n+\t\t\tcase SECOND:\n+\t\t\t    System.out.println(\"IngestJobPipeline.checkForStageCompleted() - finish\");\n+\t\t\t    this.finish();\n+\t\t\t    break;\n+\t\t    }\n+\t\t} else {\n+\t\t    System.out.println(\"IngestJobPipeline.checkForStageCompleted() - current tasks are not complete\");\n+\t\t}\n+\t    }", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE2Mzk3Ng==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438163976", "bodyText": "Another place to clear up the job vs pipeline dichotomy...", "author": "rcordovano", "createdAt": "2020-06-10T14:25:36Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -1071,7 +1198,7 @@ boolean isCancelled() {\n      * @param message The message.\n      */\n     private void logInfoMessage(String message) {\n-        logger.log(Level.INFO, String.format(\"%s (data source = %s, objId = %d, jobId = %d)\", message, dataSource.getName(), dataSource.getId(), id)); //NON-NLS        \n+        logger.log(Level.INFO, String.format(\"%s (data source = %s, objId = %d, jobId = %d)\", message, getDataSourceName(), getDataSourceId(), id)); //NON-NLS        ", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODE2NzkxOQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438167919", "bodyText": "We might want to try to be sure that we put at least as many tasks as there are ingest threads into the fileIngestThreadsQueue.\nMaybe, just maybe, we can think about merging the loops?", "author": "rcordovano", "createdAt": "2020-06-10T14:30:31Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestTasksScheduler.java", "diffHunk": "@@ -323,7 +357,32 @@ synchronized void cancelPendingTasksForIngestJob(DataSourceIngestJob job) {\n      * files derived from prioritized files.\n      */\n     synchronized private void shuffleFileTaskQueues() {\n-        while (this.fileIngestThreadsQueue.isEmpty()) {\n+\t\n+\twhile (fileIngestThreadsQueue.isEmpty()) {\n+\t    final FileIngestTask streamingTask = streamedTasksQueue.poll();\n+\t    if (streamingTask == null) {\n+\t\tSystem.out.println(\"### IngestTasksScheduler.shuffleFileTaskQueues(): failed to get file from streamedTasksQueue\");\n+\t\tbreak; // No streaming tasks right now\n+\t    }\n+\t    \n+\t    System.out.println(\"### IngestTasksScheduler.shuffleFileTaskQueues(): Testing file ID \" + streamingTask.getFile().getId() + \" and name \" + streamingTask.getFile().getName() + \" to pending queue\");\n+\t    try {\n+\t\tif (shouldEnqueueFileTask(streamingTask)) {\n+\t\t    System.out.println(\"### IngestTasksScheduler.shuffleFileTaskQueues(): Adding file ID \" + streamingTask.getFile().getId()\n+\t\t\t+ \" to fileIngestThreadsQueue\");\n+\t\t    fileIngestThreadsQueue.putLast(streamingTask);\n+\t\t} else {\n+\t\t    System.out.println(\"### IngestTasksScheduler.shuffleFileTaskQueues(): File ID \" + streamingTask.getFile().getId()\n+\t\t\t+ \" is not valid for ingest\");\n+\t\t}\n+\t    } catch (InterruptedException ex) {\n+\t\tIngestTasksScheduler.logger.log(Level.INFO, \"Ingest tasks scheduler interrupted while blocked adding a task to the file level ingest task queue\", ex);\n+\t\tThread.currentThread().interrupt();\n+\t\treturn;\n+\t    }\n+\t}\n+\t", "originalCommit": "d8baf3469608b73efe5531454cb6843278a6ac99", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "221e335b8a9835f7811000477e8cefa2ce17384f", "url": "https://github.com/sleuthkit/autopsy/commit/221e335b8a9835f7811000477e8cefa2ce17384f", "message": "review comments", "committedDate": "2020-06-10T17:28:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NjYxNA==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438396614", "bodyText": "We should rename this method as well, it's package private.", "author": "rcordovano", "createdAt": "2020-06-10T20:42:44Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestTask.java", "diffHunk": "@@ -23,16 +23,16 @@\n abstract class IngestTask {\n \n     private final static long NOT_SET = Long.MIN_VALUE;\n-    private final DataSourceIngestJob job;\n+    private final IngestJobPipeline ingestJobPipeline;\n     private long threadId;\n \n-    IngestTask(DataSourceIngestJob job) {\n-        this.job = job;\n+    IngestTask(IngestJobPipeline ingestJobPipeline) {\n+        this.ingestJobPipeline = ingestJobPipeline;\n         threadId = NOT_SET;\n     }\n \n-    DataSourceIngestJob getIngestJob() {\n-        return job;\n+    IngestJobPipeline getIngestJob() {\n+        return ingestJobPipeline;", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5OTk3Ng==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438399976", "bodyText": "Whoops.", "author": "rcordovano", "createdAt": "2020-06-10T20:49:37Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/DataSourceIngestPipeline.java", "diffHunk": "@@ -24,12 +24,11 @@\n import java.util.logging.Level;\n import org.openide.util.NbBundle;\n import org.sleuthkit.autopsy.coreutils.Logger;\n-import org.sleuthkit.autopsy.coreutils.MessageNotifyUtil;\n import org.sleuthkit.datamodel.Content;\n \n /**\n  * This class manages a sequence of data source level ingest modules for a data\n- * source ingest job. It starts the modules, runs data sources through them, and\n+ source ingest ingestJobPipeline. It starts the modules, runs data sources through them, and", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwMDEyOQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438400129", "bodyText": "Whoops.", "author": "rcordovano", "createdAt": "2020-06-10T20:49:55Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/DataSourceIngestPipeline.java", "diffHunk": "@@ -47,13 +46,13 @@\n      * modules. It starts the modules, runs data sources through them, and shuts\n      * them down when data source level ingest is complete.\n      *\n-     * @param job             The data source ingest job that owns this\n-     *                        pipeline.\n+     * @param ingestJobPipeline  The data source ingest ingestJobPipeline that owns this\n+                        pipeline.", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwMDc5Nw==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438400797", "bodyText": "It would help when debugging with the logs if we listed both the IngestJob ID and the IngestJobPipeline ID. This applies throughout the package.", "author": "rcordovano", "createdAt": "2020-06-10T20:51:18Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/DataSourceIngestPipeline.java", "diffHunk": "@@ -98,27 +97,27 @@ boolean isEmpty() {\n      */\n     synchronized List<IngestModuleError> process(DataSourceIngestTask task) {\n         List<IngestModuleError> errors = new ArrayList<>();\n-        if (!this.job.isCancelled()) {\n+        if (!this.ingestJobPipeline.isCancelled()) {\n             Content dataSource = task.getDataSource();\n             for (PipelineModule module : modules) {\n                 try {\n                     this.currentModule = module;\n                     String displayName = NbBundle.getMessage(this.getClass(),\n                             \"IngestJob.progress.dataSourceIngest.displayName\",\n                             module.getDisplayName(), dataSource.getName());\n-                    this.job.updateDataSourceIngestProgressBarDisplayName(displayName);\n-                    this.job.switchDataSourceIngestProgressBarToIndeterminate();\n+                    this.ingestJobPipeline.updateDataSourceIngestProgressBarDisplayName(displayName);\n+                    this.ingestJobPipeline.switchDataSourceIngestProgressBarToIndeterminate();\n                     DataSourceIngestPipeline.ingestManager.setIngestTaskProgress(task, module.getDisplayName());\n-                    logger.log(Level.INFO, \"{0} analysis of {1} (jobId={2}) starting\", new Object[]{module.getDisplayName(), this.job.getDataSource().getName(), this.job.getId()}); //NON-NLS\n-                    module.process(dataSource, new DataSourceIngestModuleProgress(this.job));\n-                    logger.log(Level.INFO, \"{0} analysis of {1} (jobId={2}) finished\", new Object[]{module.getDisplayName(), this.job.getDataSource().getName(), this.job.getId()}); //NON-NLS\n+                    logger.log(Level.INFO, \"{0} analysis of {1} (jobId={2}) starting\", new Object[]{module.getDisplayName(), this.ingestJobPipeline.getDataSource().getName(), this.ingestJobPipeline.getId()}); //NON-NLS", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwOTM4Mw==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438409383", "bodyText": "Whoops.", "author": "rcordovano", "createdAt": "2020-06-10T21:08:23Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestStream.java", "diffHunk": "@@ -55,35 +43,32 @@\n      * @throws IngestStreamClosedException \n      */\n     void addFiles(List<Long> fileObjectIds) throws IngestStreamClosedException;\n-    \n-    /**\n-     * Returns the next set of files that are ready for ingest.\n-     * Abstract files will be returned in the order they were added through\n-     * addFiles().\n-     * \n-     * @param numberOfFiles Maximum number of files to return.\n-     * \n-     * @return A list of abstract files for ingest. List may be empty or contain less\n-     *         files than requested.\n-     * \n-     * @throws TskCoreException \n-     */\n-    List<AbstractFile> getNextFiles(int numberOfFiles) throws TskCoreException;\n \n     /**\n      * Closes the ingest stream.\n-     * Adding a data source or a set of files to a closed ingest stream\n-     * will generate an error, but getDataSource() and getNextFiles() will work.\n-     * \n-     * @param completed True if the data source processing was complete, false \n-     *                  if canceled or an error occurred\n+     * Should be called after all files from data source have\n+     * been sent to the stream.\n      */\n-    void close(boolean completed);\n+    void close();\n     \n     /**\n      * Check whether the ingest stream is closed.\n      * \n      * @return True if closed, false otherwise.\n      */\n     boolean isClosed();\n+    \n+    /**\n+     * Stops the ingest stream. The stream will no longer accept new data\n+     * and should no longer be used. Will also close the ingest stream.\n+     */\n+    void stop();\n+    \n+    /**\n+     * Check whether the ingest stream was stopped before completion.\n+     * If this returns true, data should not be written to or read from the stream.\n+     * \n+     * @return True the ingest stream was stopped, false otherwise.", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwOTc4OQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438409789", "bodyText": "Whoops. Check the whole file...", "author": "rcordovano", "createdAt": "2020-06-10T21:09:12Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJob.java", "diffHunk": "@@ -29,17 +29,21 @@\n import java.util.concurrent.atomic.AtomicLong;\n \n import org.openide.util.NbBundle;\n+import org.sleuthkit.autopsy.casemodule.Case;\n import org.sleuthkit.datamodel.AbstractFile;\n import org.sleuthkit.datamodel.Content;\n+import org.sleuthkit.datamodel.DataSource;\n+import org.sleuthkit.datamodel.TskCoreException;\n+import org.sleuthkit.datamodel.TskDataException;\n \n /**\n  * Analyzes one or more data sources using a set of ingest modules specified via\n- * ingest job settings.\n+ ingest ingestJobPipeline settings.", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQxMjM3NQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438412375", "bodyText": "The first two constructors have a lot of duplicated code. Perhaps this can be reduced by having one call the other?", "author": "rcordovano", "createdAt": "2020-06-10T21:14:57Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJob.java", "diffHunk": "@@ -60,102 +64,194 @@ public String getDisplayName() {\n             return displayName;\n         }\n     }\n+    \n+    enum Mode {\n+\tBATCH,\n+\tSTREAMING\n+    }\n+    \n \n     private final static AtomicLong nextId = new AtomicLong(0L);\n     private final long id;\n-    private final Map<Long, DataSourceIngestJob> dataSourceJobs;\n+    private final List<Content> dataSources = new ArrayList<>();\n+    private final List<AbstractFile> files;\n+    private final Mode ingestMode;\n+    private DataSource streamingIngestDataSource = null;\n+    private final Map<Long, IngestJobPipeline> ingestJobPipelines;\n     private final AtomicInteger incompleteJobsCount;\n+    private final IngestJobSettings settings;\n     private volatile CancellationReason cancellationReason;\n \n     /**\n-     * Constructs an ingest job that analyzes one or more data sources using a\n-     * set of ingest modules specified via ingest job settings.\n+     * Constructs an ingest ingestJobPipeline that analyzes one or more data sources using a\n+ set of ingest modules specified via ingest ingestJobPipeline settings.\n      *\n      * @param dataSources The data sources to be ingested.\n-     * @param settings    The ingest job settings.\n-     * @param doUI        Whether or not this job should use progress bars,\n-     *                    message boxes for errors, etc.\n+     * @param settings    The ingest ingestJobPipeline settings.\n      */\n-    IngestJob(Collection<Content> dataSources, IngestJobSettings settings, boolean doUI) {\n+    IngestJob(Collection<Content> dataSources, IngestJobSettings settings) {\n         this.id = IngestJob.nextId.getAndIncrement();\n-        this.dataSourceJobs = new ConcurrentHashMap<>();\n-        for (Content dataSource : dataSources) {\n-            DataSourceIngestJob dataSourceIngestJob = new DataSourceIngestJob(this, dataSource, settings, doUI);\n-            this.dataSourceJobs.put(dataSourceIngestJob.getId(), dataSourceIngestJob);\n-        }\n-        incompleteJobsCount = new AtomicInteger(dataSourceJobs.size());\n+\tthis.settings = settings;\n+        this.ingestJobPipelines = new ConcurrentHashMap<>();\n+\tthis.ingestMode = Mode.BATCH;\n+\tthis.dataSources.addAll(dataSources);\n+\tthis.files = null;\n+        incompleteJobsCount = new AtomicInteger(dataSources.size());\n         cancellationReason = CancellationReason.NOT_CANCELLED;\n     }\n \n     /**\n-     * Constructs an ingest job that analyzes one data source using a set of\n-     * ingest modules specified via ingest job settings. Either all of the files\n+     * Constructs an ingest ingestJobPipeline that analyzes one data source using a set of\n+ ingest modules specified via ingest ingestJobPipeline settings. Either all of the files\n      * in the data source or a given subset of the files will be analyzed.\n      *\n      * @param dataSource The data source to be analyzed\n      * @param files      A subset of the files for the data source.\n-     * @param settings   The ingest job settings.\n-     * @param doUI       Whether or not this job should use progress bars,\n-     *                   message boxes for errors, etc.\n+     * @param settings   The ingest ingestJobPipeline settings.\n+     */\n+    IngestJob(Content dataSource, List<AbstractFile> files, IngestJobSettings settings) {\n+        this.id = IngestJob.nextId.getAndIncrement();\n+        this.ingestJobPipelines = new ConcurrentHashMap<>();\n+\tthis.settings = settings;\n+\tthis.files = files;\n+\tthis.dataSources.add(dataSource);\n+\tthis.ingestMode = Mode.BATCH;\n+        incompleteJobsCount = new AtomicInteger(1);\n+        cancellationReason = CancellationReason.NOT_CANCELLED;\n+    }", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMDIzMQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438420231", "bodyText": "This comment looks like the bit about a \"not unwarranted assumption\" is probably no longer true.", "author": "rcordovano", "createdAt": "2020-06-10T21:32:23Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJob.java", "diffHunk": "@@ -60,102 +64,194 @@ public String getDisplayName() {\n             return displayName;\n         }\n     }\n+    \n+    enum Mode {\n+\tBATCH,\n+\tSTREAMING\n+    }\n+    \n \n     private final static AtomicLong nextId = new AtomicLong(0L);\n     private final long id;\n-    private final Map<Long, DataSourceIngestJob> dataSourceJobs;\n+    private final List<Content> dataSources = new ArrayList<>();\n+    private final List<AbstractFile> files;\n+    private final Mode ingestMode;\n+    private DataSource streamingIngestDataSource = null;\n+    private final Map<Long, IngestJobPipeline> ingestJobPipelines;\n     private final AtomicInteger incompleteJobsCount;\n+    private final IngestJobSettings settings;\n     private volatile CancellationReason cancellationReason;\n \n     /**\n-     * Constructs an ingest job that analyzes one or more data sources using a\n-     * set of ingest modules specified via ingest job settings.\n+     * Constructs an ingest ingestJobPipeline that analyzes one or more data sources using a\n+ set of ingest modules specified via ingest ingestJobPipeline settings.\n      *\n      * @param dataSources The data sources to be ingested.\n-     * @param settings    The ingest job settings.\n-     * @param doUI        Whether or not this job should use progress bars,\n-     *                    message boxes for errors, etc.\n+     * @param settings    The ingest ingestJobPipeline settings.\n      */\n-    IngestJob(Collection<Content> dataSources, IngestJobSettings settings, boolean doUI) {\n+    IngestJob(Collection<Content> dataSources, IngestJobSettings settings) {\n         this.id = IngestJob.nextId.getAndIncrement();\n-        this.dataSourceJobs = new ConcurrentHashMap<>();\n-        for (Content dataSource : dataSources) {\n-            DataSourceIngestJob dataSourceIngestJob = new DataSourceIngestJob(this, dataSource, settings, doUI);\n-            this.dataSourceJobs.put(dataSourceIngestJob.getId(), dataSourceIngestJob);\n-        }\n-        incompleteJobsCount = new AtomicInteger(dataSourceJobs.size());\n+\tthis.settings = settings;\n+        this.ingestJobPipelines = new ConcurrentHashMap<>();\n+\tthis.ingestMode = Mode.BATCH;\n+\tthis.dataSources.addAll(dataSources);\n+\tthis.files = null;\n+        incompleteJobsCount = new AtomicInteger(dataSources.size());\n         cancellationReason = CancellationReason.NOT_CANCELLED;\n     }\n \n     /**\n-     * Constructs an ingest job that analyzes one data source using a set of\n-     * ingest modules specified via ingest job settings. Either all of the files\n+     * Constructs an ingest ingestJobPipeline that analyzes one data source using a set of\n+ ingest modules specified via ingest ingestJobPipeline settings. Either all of the files\n      * in the data source or a given subset of the files will be analyzed.\n      *\n      * @param dataSource The data source to be analyzed\n      * @param files      A subset of the files for the data source.\n-     * @param settings   The ingest job settings.\n-     * @param doUI       Whether or not this job should use progress bars,\n-     *                   message boxes for errors, etc.\n+     * @param settings   The ingest ingestJobPipeline settings.\n+     */\n+    IngestJob(Content dataSource, List<AbstractFile> files, IngestJobSettings settings) {\n+        this.id = IngestJob.nextId.getAndIncrement();\n+        this.ingestJobPipelines = new ConcurrentHashMap<>();\n+\tthis.settings = settings;\n+\tthis.files = files;\n+\tthis.dataSources.add(dataSource);\n+\tthis.ingestMode = Mode.BATCH;\n+        incompleteJobsCount = new AtomicInteger(1);\n+        cancellationReason = CancellationReason.NOT_CANCELLED;\n+    }\n+    \n+    /**\n+     * Constructs an ingest ingestJobPipeline that analyzes one data source using an\n+ ingest stream.\n+     *\n+     * @param settings   The ingest ingestJobPipeline settings.\n      */\n-    IngestJob(Content dataSource, List<AbstractFile> files, IngestJobSettings settings, boolean doUI) {\n+    public IngestJob(IngestJobSettings settings) { // TODO revert public\n         this.id = IngestJob.nextId.getAndIncrement();\n-        this.dataSourceJobs = new ConcurrentHashMap<>();\n-        DataSourceIngestJob dataSourceIngestJob = new DataSourceIngestJob(this, dataSource, files, settings, doUI);\n-        this.dataSourceJobs.put(dataSourceIngestJob.getId(), dataSourceIngestJob);\n-        incompleteJobsCount = new AtomicInteger(dataSourceJobs.size());\n+        this.ingestJobPipelines = new ConcurrentHashMap<>();\n+\tthis.settings = settings;\n+\tthis.files = null;\n+\tthis.ingestMode = Mode.STREAMING;\n+        incompleteJobsCount = new AtomicInteger(1);\n         cancellationReason = CancellationReason.NOT_CANCELLED;\n     }\n \n     /**\n-     * Gets the unique identifier assigned to this ingest job.\n+     * Gets the unique identifier assigned to this ingest ingestJobPipeline.\n      *\n-     * @return The job identifier.\n+     * @return The ingestJobPipeline identifier.\n      */\n     public long getId() {\n         return this.id;\n     }\n \n     /**\n-     * Checks to see if this ingest job has at least one non-empty ingest module\n-     * pipeline (first or second stage data-source-level pipeline or file-level\n-     * pipeline).\n+     * Checks to see if this ingest ingestJobPipeline has at least one non-empty ingest module\n+ pipeline (first or second stage data-source-level pipeline or file-level\n+ pipeline).\n      *\n      * @return True or false.\n      */\n     boolean hasIngestPipeline() {\n-        /**\n-         * TODO: This could actually be done more simply by adding a method to\n-         * the IngestJobSettings to check for at least one enabled ingest module\n-         * template. The test could then be done in the ingest manager before\n-         * even constructing an ingest job.\n-         */\n-        for (DataSourceIngestJob dataSourceJob : this.dataSourceJobs.values()) {\n-            if (dataSourceJob.hasIngestPipeline()) {\n-                return true;\n-            }\n-        }\n-        return false;\n+\treturn (!settings.getEnabledIngestModuleTemplates().isEmpty());\n+    }\n+    \n+    /**\n+     * Add a set of files (by object ID) to be ingested.\n+     * \n+     * @param fileObjIds the list of file IDs\n+     */\n+    void addStreamingIngestFiles(List<Long> fileObjIds) {\n+\t// TODO error if using when not streaming or if there are multiple pipelines\n+\tif (ingestJobPipelines.keySet().size() != 1) {\n+\t    // TODO fix error handling\n+\t    System.out.println(\"\\n###addStreamingIngestDataSource() error: pipeline key size = \" + ingestJobPipelines.keySet().size());\n+\t    //throw new RuntimeException(\"Wrong number of pipelines\");\n+\t}\n+\t\n+\tgetStreamingIngestPipeline().addStreamingIngestFiles(fileObjIds);\n+    }\n+    \n+    /**\n+     * Start data source processing for streaming ingest.\n+     */\n+    void addStreamingIngestDataSource() {\n+\t// TODO error if using when not streaming or if there are multiple pipelines\n+\tif (ingestJobPipelines.keySet().size() != 1) {\n+\t    // TODO fix error handling\n+\t    System.out.println(\"\\n###addStreamingIngestDataSource() error: pipeline key size = \" + ingestJobPipelines.keySet().size());\n+\t    //throw new RuntimeException(\"Wrong number of pipelines\");\n+\t}\n+\t\n+\tgetStreamingIngestPipeline().addStreamingIngestDataSource();\n+    }\n+    \n+    /**\n+     * TODO this is a workaround to get the right pipeline. For streaming ingest\n+ there will only be one. Might be able to pass/store the ingestJobPipeline ID to avoid this.\n+     * \n+     * @return \n+     */\n+    private IngestJobPipeline getStreamingIngestPipeline() {\n+\treturn ingestJobPipelines.values().iterator().next();\n+    }\n+    \n+    /**\n+     * TODO possibly move setting the data source ID elsewhere so the calls\n+     * to start can be the same for batch and streaming. This may not be necessary\n+     * if the data source is created earlier. \n+     * \n+     * @param dataSourceObjId\n+     * \n+     * @return \n+     */\n+    List<IngestModuleError> start(long dataSourceObjId) {\n+\ttry {\n+\t    streamingIngestDataSource = Case.getCurrentCase().getSleuthkitCase().getDataSource(dataSourceObjId);\n+\t    return start();\n+\t} catch (TskCoreException | TskDataException ex) {\n+\t    // TODO figure out how to handle an error here\n+\t    ex.printStackTrace();\n+\t    return new ArrayList<>();\n+\t}\n     }\n \n     /**\n-     * Starts this ingest job by starting its ingest module pipelines and\n-     * scheduling the ingest tasks that make up the job.\n+     * Starts this ingest ingestJobPipeline by starting its ingest module pipelines and\n+ scheduling the ingest tasks that make up the ingestJobPipeline.\n      *\n      * @return A collection of ingest module start up errors, empty on success.\n      */\n     List<IngestModuleError> start() {\n+\t\n+\t/*\n+\t * Set up the pipeline(s)\n+\t */\n+\tif (ingestMode == Mode.STREAMING) {\n+\t    IngestJobPipeline ingestJobPipeline = new IngestJobPipeline(this, streamingIngestDataSource, settings);\n+\t    this.ingestJobPipelines.put(ingestJobPipeline.getId(), ingestJobPipeline);\n+\t} else if (files != null && dataSources.size() == 1) {\n+\t    IngestJobPipeline ingestJobPipeline = new IngestJobPipeline(this, dataSources.get(0), files, settings);\n+\t    this.ingestJobPipelines.put(ingestJobPipeline.getId(), ingestJobPipeline);\n+\t} else {\n+\t    for (Content dataSource : dataSources) {\n+\t\tIngestJobPipeline ingestJobPipeline = new IngestJobPipeline(this, dataSource, settings);\n+\t\tthis.ingestJobPipelines.put(ingestJobPipeline.getId(), ingestJobPipeline);\n+\t    }\n+        }\n+\tincompleteJobsCount.set(ingestJobPipelines.size());\n+\t\n         /*\n-         * Try to start each data source ingest job. Note that there is a not\n+         * Try to start each data source ingest ingestJobPipeline. Note that there is a not\n          * unwarranted assumption here that if there is going to be a module\n-         * startup failure, it will be for the first data source ingest job.\n+         * startup failure, it will be for the first data source ingest ingestJobPipeline.", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMTE1MQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438421151", "bodyText": "This method name should be changed.", "author": "rcordovano", "createdAt": "2020-06-10T21:34:32Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJob.java", "diffHunk": "@@ -256,24 +363,24 @@ public boolean isCancelled() {\n \n     /**\n      * Provides a callback for completed data source ingest jobs, allowing this\n-     * ingest job to notify the ingest manager when it is complete.\n+ ingest ingestJobPipeline to notify the ingest manager when it is complete.\n      *\n-     * @param job A completed data source ingest job.\n+     * @param ingestJobPipeline A completed data source ingest ingestJobPipeline.\n      */\n-    void dataSourceJobFinished(DataSourceIngestJob job) {\n+    void dataSourceJobFinished(IngestJobPipeline ingestJobPipeline) {", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQyMjQ5Ng==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r438422496", "bodyText": "We need a better name for this method.", "author": "rcordovano", "createdAt": "2020-06-10T21:37:42Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobInputStream.java", "diffHunk": "@@ -18,122 +18,69 @@\n  */\n package org.sleuthkit.autopsy.ingest;\n \n-import java.util.ArrayList;\n-import java.util.LinkedList;\n import java.util.List;\n-import java.util.Queue;\n-import org.sleuthkit.autopsy.casemodule.Case;\n-import org.sleuthkit.autopsy.casemodule.NoCurrentCaseException;\n-import org.sleuthkit.datamodel.AbstractFile;\n-import org.sleuthkit.datamodel.DataSource;\n-import org.sleuthkit.datamodel.TskCoreException;\n-import org.sleuthkit.datamodel.TskDataException;\n \n /**\n  * Implementation of IngestStream. Will collect data from the data source\n  * processor to be sent to the ingest pipeline.\n  */\n-class IngestJobInputStream implements IngestStream {\n+// TODO revert to package private\n+public class IngestJobInputStream implements IngestStream {\n     private final IngestJob ingestJob;\n     private long dataSourceObjectId = -1;\n-    private DataSource dataSource = null;\n-    private final Queue<Long> fileIdQueue = new LinkedList<>();\n     private boolean isClosed = false;\n+    private boolean isStopped = false;\n \n     /**\n      * Create an ingest stream object, saving a reference to the associated IngestJob;\n      * \n      * @param ingestJob The IngestJob associated with this stream.\n      */\n-    IngestJobInputStream(IngestJob ingestJob) {\n+    public IngestJobInputStream(IngestJob ingestJob) { // TODO revert public\n         this.ingestJob = ingestJob;\n     }\n     \n     @Override\n-    public void addDataSource(long dataSourceObjectId) throws IngestStreamClosedException {\n-        synchronized(this) {\n-            if (isClosed) {\n-               throw new IngestStreamClosedException(\"Can not add data source - ingest stream is closed\");\n-            }\n-            this.dataSourceObjectId = dataSourceObjectId;\n-            ingestJob.start();\n-        }\n+    public synchronized void addDataSource(long dataSourceObjectId) throws IngestStreamClosedException {\n+\tif (isClosed) {\n+\t   throw new IngestStreamClosedException(\"Can not add data source - ingest stream is closed\");\n+\t}\n+\tthis.dataSourceObjectId = dataSourceObjectId;\n+\tingestJob.start(dataSourceObjectId);\n     }\n \n     @Override\n-    public DataSource getDataSource() throws TskCoreException {\n-        synchronized(this) {\n-            // If we've already loaded it, return it\n-            if (dataSource != null) {\n-                return dataSource;\n-            }\n-            \n-            // Make sure the data source object ID has been set\n-            if (dataSourceObjectId == -1) {\n-                throw new TskCoreException(\"Data source object ID has not been set\");\n-            }\n-            \n-            try {\n-                dataSource = Case.getCurrentCaseThrows().getSleuthkitCase().getDataSource(dataSourceObjectId);\n-                return dataSource;\n-            } catch (NoCurrentCaseException ex) {\n-                throw new TskCoreException(\"Case is closed\");\n-            } catch (TskDataException ex) {\n-                throw new TskCoreException(\"Error loading data source with object ID \" + dataSourceObjectId, ex);\n-            }\n-        }\n+    public synchronized void addFiles(List<Long> fileObjectIds) throws IngestStreamClosedException {\n+\tif (isClosed) {\n+\t    throw new IngestStreamClosedException(\"Can not add files - ingest stream is closed\");\n+\t}\n+\tif (dataSourceObjectId < 0) {\n+\t    throw new IllegalStateException(\"Files can not be added before a data source\");\n+\t}\n+\tingestJob.addStreamingIngestFiles(fileObjectIds);\n     }\n \n     @Override\n-    public void addFiles(List<Long> fileObjectIds) throws IngestStreamClosedException {\n-        synchronized(this) {\n-            if (isClosed) {\n-                throw new IngestStreamClosedException(\"Can not add files - ingest stream is closed\");\n-            }\n-            if (dataSource == null) {\n-                throw new IllegalStateException(\"Files can not be added before a data source\");\n-            }\n-            fileIdQueue.addAll(fileObjectIds);\n-        }\n+    public synchronized void close() {\n+\tisClosed = true;\n+\tingestJob.addStreamingIngestDataSource();", "originalCommit": "221e335b8a9835f7811000477e8cefa2ce17384f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "920eeed8c61f4939eba955f6555f67c0a7a5ad3c", "url": "https://github.com/sleuthkit/autopsy/commit/920eeed8c61f4939eba955f6555f67c0a7a5ad3c", "message": "Review changes", "committedDate": "2020-06-11T14:17:10Z", "type": "commit"}, {"oid": "acca051e1443056d372010924e5e314673224be9", "url": "https://github.com/sleuthkit/autopsy/commit/acca051e1443056d372010924e5e314673224be9", "message": "Review changes", "committedDate": "2020-06-12T14:27:59Z", "type": "commit"}, {"oid": "c1c7f6168ca1edd6d3aae4268744b62458142298", "url": "https://github.com/sleuthkit/autopsy/commit/c1c7f6168ca1edd6d3aae4268744b62458142298", "message": "Review comments", "committedDate": "2020-06-12T17:02:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4MDU4OQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r439480589", "bodyText": "Stages.FIRST_STAGE_FILES_AND_DATASOURCE applies here?", "author": "rcordovano", "createdAt": "2020-06-12T15:14:01Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestJobPipeline.java", "diffHunk": "@@ -951,8 +946,12 @@ void process(FileIngestTask task) throws InterruptedException {\n      * @param fileObjIds List of newly added file IDs\n      */\n     void addStreamingIngestFiles(List<Long> fileObjIds) {\n-\t// TODO error checking for stage and streaming (should be in streaming files or datasource 1)\n-\tIngestJobPipeline.taskScheduler.scheduleStreamedFileIngestTasks(this, fileObjIds);\n+\tif (stage.equals(Stages.FIRST_STAGE_FILES_ONLY)\n+\t\t|| stage.equals(Stages.FIRST_STAGE_FILES_AND_DATASOURCE)) {\n+\t    IngestJobPipeline.taskScheduler.scheduleStreamedFileIngestTasks(this, fileObjIds);", "originalCommit": "acca051e1443056d372010924e5e314673224be9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1ODU2NQ==", "url": "https://github.com/sleuthkit/autopsy/pull/5963#discussion_r439558565", "bodyText": "Name changes we talked about...\nshuffleFileTaskQueues => refillIngestThreadQueue\nshuffleStreamingFileTaskQueues =>takeFromStreamingTaskQueue\nshuffleRootFileTaskQueues =>takeFromBatchTasksQueues", "author": "rcordovano", "createdAt": "2020-06-12T17:43:00Z", "path": "Core/src/org/sleuthkit/autopsy/ingest/IngestTasksScheduler.java", "diffHunk": "@@ -318,6 +318,47 @@ synchronized void cancelPendingTasksForIngestJob(IngestJobPipeline ingestJobPipe\n         }\n         return topLevelFiles;\n     }\n+    \n+    /**\n+     * Schedules file ingest tasks for the ingest manager's file ingest threads.\n+     * Files from streaming ingest will be prioritized.\n+     */\n+    synchronized private void shuffleFileTaskQueues() {\n+\ttry {\n+\t    shuffleStreamingFileTaskQueues();\n+\t    shuffleRootFileTaskQueues();\n+\t} catch (InterruptedException ex) {\n+\t    IngestTasksScheduler.logger.log(Level.INFO, \"Ingest tasks scheduler interrupted while blocked adding a task to the file level ingest task queue\", ex);\n+\t    Thread.currentThread().interrupt();\n+\t}\n+    }", "originalCommit": "acca051e1443056d372010924e5e314673224be9", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a1bd0275e17b78950ca22ca8e821f8dd128870bd", "url": "https://github.com/sleuthkit/autopsy/commit/a1bd0275e17b78950ca22ca8e821f8dd128870bd", "message": "Review comments", "committedDate": "2020-06-15T13:28:14Z", "type": "commit"}, {"oid": "ab3fa4f6d83b865f969a8c3a12efab505df2895b", "url": "https://github.com/sleuthkit/autopsy/commit/ab3fa4f6d83b865f969a8c3a12efab505df2895b", "message": "Revert temporary changes to run streaming ingest from the wizard.", "committedDate": "2020-06-15T13:40:41Z", "type": "commit"}]}