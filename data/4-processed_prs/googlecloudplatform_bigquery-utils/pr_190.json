{"pr_number": 190, "pr_title": "Cloud functions ingest: backfill CLI tool + minor improvements", "pr_createdAt": "2020-11-11T21:47:36Z", "pr_url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190", "timeline": [{"oid": "18970ad78f262c24980cee43f5de47bfde2a0a4b", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/18970ad78f262c24980cee43f5de47bfde2a0a4b", "message": "gcs ingest cloud function", "committedDate": "2020-10-23T20:08:03Z", "type": "commit"}, {"oid": "e9dd912856fea330b05ccbab2234bf40c3e087ef", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/e9dd912856fea330b05ccbab2234bf40c3e087ef", "message": "address review feedback", "committedDate": "2020-10-27T02:17:50Z", "type": "commit"}, {"oid": "843f8714b1bc4623b19b011474f86e14f495ab09", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/843f8714b1bc4623b19b011474f86e14f495ab09", "message": "remove extra newline", "committedDate": "2020-10-27T02:18:51Z", "type": "commit"}, {"oid": "bf1fd011b27fbb9c2d3e011840de0b482e0255d6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bf1fd011b27fbb9c2d3e011840de0b482e0255d6", "message": "remove project files", "committedDate": "2020-10-27T02:19:28Z", "type": "commit"}, {"oid": "1e831070123750e853b5ab36e9f271aa1a734eaa", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/1e831070123750e853b5ab36e9f271aa1a734eaa", "message": "regex destination details env var", "committedDate": "2020-10-28T17:46:19Z", "type": "commit"}, {"oid": "64c026c5e5dfc5413d0bfd9a6c0b303e0537d709", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/64c026c5e5dfc5413d0bfd9a6c0b303e0537d709", "message": "ignore intellij .idea/ configs", "committedDate": "2020-10-28T17:47:02Z", "type": "commit"}, {"oid": "242283f78d21bff8c638b37dee7a9ac20e8260df", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/242283f78d21bff8c638b37dee7a9ac20e8260df", "message": "incremental loads docs", "committedDate": "2020-10-28T18:07:28Z", "type": "commit"}, {"oid": "152f4b777dedb18c1e29bb42b7c6a8a85227bed9", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/152f4b777dedb18c1e29bb42b7c6a8a85227bed9", "message": "update TODOs", "committedDate": "2020-10-28T18:09:45Z", "type": "commit"}, {"oid": "db1f2bf1fefa7d7a6da7c1d2f00e7ee0ecc034b2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/db1f2bf1fefa7d7a6da7c1d2f00e7ee0ecc034b2", "message": "Merge branch 'master' into cloud-functions-ingest", "committedDate": "2020-10-28T18:12:13Z", "type": "commit"}, {"oid": "899203dd95157c513c3abf3dc0907528fc4c860a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/899203dd95157c513c3abf3dc0907528fc4c860a", "message": "remove bad labels", "committedDate": "2020-10-28T21:05:28Z", "type": "commit"}, {"oid": "79665f8563f375fe1616f5e2ca7371385061fea5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/79665f8563f375fe1616f5e2ca7371385061fea5", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest", "committedDate": "2020-10-28T21:07:39Z", "type": "commit"}, {"oid": "6335be9ae8cbc309841383a9522c136020b0912d", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6335be9ae8cbc309841383a9522c136020b0912d", "message": "move external query recursive search todo", "committedDate": "2020-10-28T21:12:47Z", "type": "commit"}, {"oid": "36eb155bdccfaa6eed00f93b4b655f326ec05618", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/36eb155bdccfaa6eed00f93b4b655f326ec05618", "message": "passing integration tests", "committedDate": "2020-10-30T17:59:52Z", "type": "commit"}, {"oid": "d2d4dc9e7e4579eca0495f5115a555c9e06b6dd1", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d2d4dc9e7e4579eca0495f5115a555c9e06b6dd1", "message": "add ci for gcs_event_based_ingest cloud function", "committedDate": "2020-10-31T00:03:04Z", "type": "commit"}, {"oid": "4a345c8d3aa4068c1c9631cb052cd316f5039c85", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4a345c8d3aa4068c1c9631cb052cd316f5039c85", "message": "handle duplicate pubsub notification", "committedDate": "2020-10-31T01:25:34Z", "type": "commit"}, {"oid": "0d6fdaf2b2e01f5ccf54ab2204d1ee6e2aca5d2f", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0d6fdaf2b2e01f5ccf54ab2204d1ee6e2aca5d2f", "message": "Merge branch 'master' into cloud-functions-ingest", "committedDate": "2020-11-02T14:08:11Z", "type": "commit"}, {"oid": "a542f385f6bce2e07d65b9d0a6e214523517570d", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a542f385f6bce2e07d65b9d0a6e214523517570d", "message": "fixup ci dockerfile", "committedDate": "2020-11-02T19:34:59Z", "type": "commit"}, {"oid": "c4da1e34622ee6afa332195dcb74fcb88153ce5e", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/c4da1e34622ee6afa332195dcb74fcb88153ce5e", "message": "more ci", "committedDate": "2020-11-02T19:59:01Z", "type": "commit"}, {"oid": "bfc9622038f1ba8d1006e9057b69990fcedb5981", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bfc9622038f1ba8d1006e9057b69990fcedb5981", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest", "committedDate": "2020-11-02T20:01:59Z", "type": "commit"}, {"oid": "41dc1cd6abc540cecbab21223e49e7060695d5f7", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/41dc1cd6abc540cecbab21223e49e7060695d5f7", "message": "better defaults, faster ci", "committedDate": "2020-11-03T20:36:08Z", "type": "commit"}, {"oid": "54390f6198e25cdc4ddb2a5486979e3f7bd4e32c", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/54390f6198e25cdc4ddb2a5486979e3f7bd4e32c", "message": "add integration test for partitioned tables / data", "committedDate": "2020-11-03T23:14:19Z", "type": "commit"}, {"oid": "7e7272db359c6b30ebfbb05870f522a9f0695aed", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/7e7272db359c6b30ebfbb05870f522a9f0695aed", "message": "parallelize tests 4x+ speed up in ci", "committedDate": "2020-11-04T19:17:50Z", "type": "commit"}, {"oid": "ddc265615275c72a983c5030ff9b08e5b296860b", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ddc265615275c72a983c5030ff9b08e5b296860b", "message": "fixup docs", "committedDate": "2020-11-04T19:46:31Z", "type": "commit"}, {"oid": "b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/b0bafe3e4ddcdfa5b4de35912b81d7e69677d89e", "message": "add terraform module for deployment", "committedDate": "2020-11-05T02:07:31Z", "type": "commit"}, {"oid": "3da292d49b236f94d8ac2328b3d2cb8aa323b06b", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/3da292d49b236f94d8ac2328b3d2cb8aa323b06b", "message": "remove useless gitignore", "committedDate": "2020-11-05T02:10:33Z", "type": "commit"}, {"oid": "f1b5659f3b4b5cccd1277b55ea0f95fa99e6d6a2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/f1b5659f3b4b5cccd1277b55ea0f95fa99e6d6a2", "message": "add pytest.mark.IT", "committedDate": "2020-11-05T22:26:18Z", "type": "commit"}, {"oid": "a88939b1d8d5b9f4bbc05ceecdee661345d44b25", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a88939b1d8d5b9f4bbc05ceecdee661345d44b25", "message": "add note on alternatives", "committedDate": "2020-11-09T21:50:15Z", "type": "commit"}, {"oid": "aeab1ad06015cbd55e4d204fe523340fc13177f5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/aeab1ad06015cbd55e4d204fe523340fc13177f5", "message": "wip", "committedDate": "2020-11-11T16:22:02Z", "type": "commit"}, {"oid": "c4eaa19be1268ae66547f85b44156bf77c78c999", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/c4eaa19be1268ae66547f85b44156bf77c78c999", "message": "move cloud functions to tools", "committedDate": "2020-11-11T17:16:37Z", "type": "commit"}, {"oid": "ea45ad883e84a4d5ad1919a0ade7b358443be057", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ea45ad883e84a4d5ad1919a0ade7b358443be057", "message": "Merge branch 'chore/move-gcf-to-tools' into cloud-functions-ingest", "committedDate": "2020-11-11T17:28:53Z", "type": "commit"}, {"oid": "cae43a46b45ddb6ccd01733e24ab263c61325e93", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/cae43a46b45ddb6ccd01733e24ab263c61325e93", "message": "wip backfill cli", "committedDate": "2020-11-11T17:30:33Z", "type": "commit"}, {"oid": "85a2ec020a5d94f9dd6afe56eab49f119cbcbae5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/85a2ec020a5d94f9dd6afe56eab49f119cbcbae5", "message": "backfill cli", "committedDate": "2020-11-11T20:51:43Z", "type": "commit"}, {"oid": "eddbfd696356c6f70cd938752936e1cb95297b06", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/eddbfd696356c6f70cd938752936e1cb95297b06", "message": "update docs", "committedDate": "2020-11-11T21:41:55Z", "type": "commit"}, {"oid": "32c25705dfa56785abe0e5016ae7685d9fd09659", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/32c25705dfa56785abe0e5016ae7685d9fd09659", "message": "mock function name", "committedDate": "2020-11-11T21:46:16Z", "type": "commit"}, {"oid": "d610365025de23661e39d8d0670c735ff38dd99b", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d610365025de23661e39d8d0670c735ff38dd99b", "message": "Merge branch 'master' into cloud-functions-ingest", "committedDate": "2020-11-11T21:50:31Z", "type": "commit"}, {"oid": "29d41a60bdb32b93ed9885ac0d78de0befa39e37", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/29d41a60bdb32b93ed9885ac0d78de0befa39e37", "message": "restructure tests", "committedDate": "2020-11-11T22:28:39Z", "type": "commit"}, {"oid": "324b8e414f3ed387d1a2127bdc14ccfae8c7cd74", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/324b8e414f3ed387d1a2127bdc14ccfae8c7cd74", "message": "add backfill cli test", "committedDate": "2020-11-11T23:37:58Z", "type": "commit"}, {"oid": "f617f91e39dc19c380a5741d7e92afa2689a4629", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/f617f91e39dc19c380a5741d7e92afa2689a4629", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest", "committedDate": "2020-11-11T23:41:18Z", "type": "commit"}, {"oid": "13cc0423d4753fea48e6a8859afbfa11d2211fdb", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/13cc0423d4753fea48e6a8859afbfa11d2211fdb", "message": "update test docstring", "committedDate": "2020-11-11T23:43:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTcwODM3OQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r521708379", "bodyText": "these fixtures are all duplicated from test_gcs_ocn_bq_ingest_it.py should DRY them up into a common fixtures module.", "author": "jaketf", "createdAt": "2020-11-11T23:45:17Z", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "diffHunk": "@@ -0,0 +1,257 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n+import json\n+import os\n+import uuid\n+from time import monotonic\n+from typing import List\n+\n+import backfill\n+import google.cloud.storage as storage\n+import pytest\n+from google.cloud import bigquery\n+\n+from gcs_ocn_bq_ingest import main\n+\n+TEST_DIR = os.path.realpath(os.path.dirname(__file__) + \"/..\")\n+LOAD_JOB_POLLING_TIMEOUT = 20  # seconds\n+\n+\n+@pytest.fixture(scope=\"module\")\n+def bq() -> bigquery.Client:", "originalCommit": "f617f91e39dc19c380a5741d7e92afa2689a4629", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e85af56f9b949f46dd83ce6fa80304d0a3f77e8e", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/e85af56f9b949f46dd83ce6fa80304d0a3f77e8e", "message": "dry up pytest fixtures", "committedDate": "2020-11-12T00:07:41Z", "type": "commit"}, {"oid": "668cf4a70e1a3872add80b75cd901dee49146a93", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/668cf4a70e1a3872add80b75cd901dee49146a93", "message": "docs on tests fixtures", "committedDate": "2020-11-12T00:10:48Z", "type": "commit"}, {"oid": "33ff20192980fc1d0bfd999ce920a36cba0382c4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/33ff20192980fc1d0bfd999ce920a36cba0382c4", "message": "fixup docs formatting", "committedDate": "2020-11-12T00:20:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjYwMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412600", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--success_filename\",\n          \n          \n            \n                    \"--success-filename\",\n          \n      \n    \n    \n  \n\nThis is the common convention.", "author": "mik-laj", "createdAt": "2020-11-12T20:40:25Z", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",\n+        \"-p\",\n+        help=\"GCS path (e.g. gs://bucket/prefix/to/search/)to search for \"\n+        \"existing _SUCCESS files\",\n+        required=True,\n+    )\n+\n+    parser.add_argument(\n+        \"--mode\",\n+        \"-m\",\n+        help=\"How to perform the backfill: LOCAL run cloud function main\"\n+        \" method locally (in concurrent threads) or NOTIFICATIONS just push\"\n+        \" notifications to Pub/Sub for a deployed version of the cloud function\"\n+        \" to pick up. Default is NOTIFICATIONS.\",\n+        required=False,\n+        type=str.upper,\n+        choices=[\"LOCAL\", \"NOTIFICATIONS\"],\n+        default=\"NOTIFICATIONS\",\n+    )\n+\n+    parser.add_argument(\n+        \"--pubsub_topic\",\n+        \"--topic\",\n+        \"-t\",\n+        help=\"Pub/Sub notifications topic to post notifications for. \"\n+        \"i.e. projects/{PROJECT_ID}/topics/{TOPIC_ID} \"\n+        \"Required if using NOTIFICATIONS mode.\",\n+        required=False,\n+        default=None,\n+    )\n+\n+    parser.add_argument(\n+        \"--success_filename\",", "originalCommit": "33ff20192980fc1d0bfd999ce920a36cba0382c4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjY2OQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412669", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--pubsub_topic\",\n          \n          \n            \n                    \"--pubsub-topic\",", "author": "mik-laj", "createdAt": "2020-11-12T20:40:34Z", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",\n+        \"-p\",\n+        help=\"GCS path (e.g. gs://bucket/prefix/to/search/)to search for \"\n+        \"existing _SUCCESS files\",\n+        required=True,\n+    )\n+\n+    parser.add_argument(\n+        \"--mode\",\n+        \"-m\",\n+        help=\"How to perform the backfill: LOCAL run cloud function main\"\n+        \" method locally (in concurrent threads) or NOTIFICATIONS just push\"\n+        \" notifications to Pub/Sub for a deployed version of the cloud function\"\n+        \" to pick up. Default is NOTIFICATIONS.\",\n+        required=False,\n+        type=str.upper,\n+        choices=[\"LOCAL\", \"NOTIFICATIONS\"],\n+        default=\"NOTIFICATIONS\",\n+    )\n+\n+    parser.add_argument(\n+        \"--pubsub_topic\",", "originalCommit": "33ff20192980fc1d0bfd999ce920a36cba0382c4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQxMjcxMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r522412710", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"--gcs_path\",\n          \n          \n            \n                    \"--gcs-path\",", "author": "mik-laj", "createdAt": "2020-11-12T20:40:41Z", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,162 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(prefix)  # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None\n+    suffix = args.success_filename\n+    if args.mode == \"NOTIFICATIONS\":\n+        if not args.pubsub_topic:\n+            raise ValueError(\"when passing mode=NOTIFICATIONS\"\n+                             \"you must also pass pubsub_topic.\")\n+        # import is here because this utility can be used without\n+        # google-cloud-pubsub dependency in LOCAL mode.\n+        # pylint: disable=import-outside-toplevel\n+        from google.cloud import pubsub\n+        ps_cli = pubsub.PublisherClient()\n+\n+    # These are all I/O bound tasks so use Thread Pool concurrency for speed.\n+    with concurrent.futures.ThreadPoolExecutor() as executor:\n+        future_to_gsurl = {}\n+        for blob in find_blobs_with_suffix(gcs_cli, args.gcs_path, suffix):\n+            if ps_cli:\n+                # kwargs are message attributes\n+                # https://googleapis.dev/python/pubsub/latest/publisher/index.html#publish-a-message\n+                logging.info(\"sending pubsub message for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    ps_cli.publish,\n+                    args.pubsub_topic,\n+                    b'',    # cloud function ignores message body\n+                    bucketId=blob.bucket.name,\n+                    objectId=blob.name,\n+                    _metaInfo=\"this message was submitted with \"\n+                    \"gcs_ocn_bq_ingest backfill.py utility\"\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+            else:\n+                logging.info(\"running  cloud function locally for: %s\",\n+                             f\"gs://{blob.bucket.name}/{blob.name}\")\n+                future_to_gsurl[executor.submit(\n+                    gcs_ocn_bq_ingest.main.main,\n+                    {\n+                        \"attributes\": {\n+                            \"bucketId\": blob.bucket.name,\n+                            \"objectId\": blob.name\n+                        }\n+                    },\n+                    None,\n+                )] = f\"gs://{blob.bucket.name}/{blob.name}\"\n+        exceptions: Dict[str, Exception] = dict()\n+        for future in concurrent.futures.as_completed(future_to_gsurl):\n+            gsurl = future_to_gsurl[future]\n+            try:\n+                future.result()\n+            except Exception as err:  # pylint: disable=broad-except\n+                logging.error(\"Error processing %s: %s\", gsurl, err)\n+                exceptions[gsurl] = err\n+        if exceptions:\n+            raise RuntimeError(\"The following errors were encountered:\\n\" +\n+                               pprint.pformat(exceptions))\n+\n+\n+def parse_args(args: List[str]) -> Namespace:\n+    \"\"\"argument parser for backfill CLI\"\"\"\n+    parser = ArgumentParser(\n+        description=\"utility to backfill success file notifications \"\n+                    \"or run the cloud function locally in concurrent threads.\")\n+\n+    parser.add_argument(\n+        \"--gcs_path\",", "originalCommit": "33ff20192980fc1d0bfd999ce920a36cba0382c4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9943926dd4853eb47ddaf688c62c563ab8b1c62c", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/9943926dd4853eb47ddaf688c62c563ab8b1c62c", "message": "fix default regex", "committedDate": "2020-11-12T22:07:23Z", "type": "commit"}, {"oid": "2b63418c01db69e04356b5724c38f138475e927d", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/2b63418c01db69e04356b5724c38f138475e927d", "message": "fixup regex docs", "committedDate": "2020-11-12T22:18:14Z", "type": "commit"}, {"oid": "dbb1612b172c00440b5de389ad4b87f51d810997", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/dbb1612b172c00440b5de389ad4b87f51d810997", "message": "Update tools/cloud_functions/gcs_event_based_ingest/backfill.py\n\nCo-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "committedDate": "2020-11-13T20:03:35Z", "type": "commit"}, {"oid": "321f997d59c8048b301259677433de7d0bdf354d", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/321f997d59c8048b301259677433de7d0bdf354d", "message": "Update tools/cloud_functions/gcs_event_based_ingest/backfill.py\n\nCo-authored-by: Kamil Bregu\u0142a <mik-laj@users.noreply.github.com>", "committedDate": "2020-11-13T20:03:50Z", "type": "commit"}, {"oid": "ff4687f1272b5aa4e1bfb9b872c6cbbde7cd63af", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ff4687f1272b5aa4e1bfb9b872c6cbbde7cd63af", "message": "fixup! tests\n\n* fix regex to handle hourly partitions\n* fix test_backfill.py to use cli arg w/ s/_/-/g", "committedDate": "2020-11-13T21:54:52Z", "type": "commit"}, {"oid": "be672fc400134c6cbc25f582cd39d47082deed24", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/be672fc400134c6cbc25f582cd39d47082deed24", "message": "add docs example / guidance for overriding destination regex", "committedDate": "2020-11-13T22:15:04Z", "type": "commit"}, {"oid": "be672fc400134c6cbc25f582cd39d47082deed24", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/be672fc400134c6cbc25f582cd39d47082deed24", "message": "add docs example / guidance for overriding destination regex", "committedDate": "2020-11-13T22:15:04Z", "type": "forcePushed"}, {"oid": "8356f931c4621e91fb6d544edc6802883656be66", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/8356f931c4621e91fb6d544edc6802883656be66", "message": "fixup backfill usage docs", "committedDate": "2020-11-13T22:18:12Z", "type": "commit"}, {"oid": "fe5245a57afaabbec300f0551dadb2655cee47ec", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/fe5245a57afaabbec300f0551dadb2655cee47ec", "message": "fixup pylint", "committedDate": "2020-11-13T22:35:39Z", "type": "commit"}, {"oid": "a58088fdeceb2353e3ac75b37b4fcf45f6c51ef6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a58088fdeceb2353e3ac75b37b4fcf45f6c51ef6", "message": "fixup regex docs", "committedDate": "2020-11-13T22:39:15Z", "type": "commit"}, {"oid": "50d84e8639cdbbebc4c52cc5426df198ee6bae48", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/50d84e8639cdbbebc4c52cc5426df198ee6bae48", "message": "fixup document and support more flexible deployment scenarios", "committedDate": "2020-11-13T23:56:54Z", "type": "commit"}, {"oid": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/9e3e963fba00e1757c2dc8b7cf212565bab582d3", "message": "add support for Cloud Functions direct triggering", "committedDate": "2020-11-17T00:58:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MDU4Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524860582", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n          \n          \n            \n                r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decorator (optional)", "author": "danieldeleo", "createdAt": "2020-11-17T03:25:00Z", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -52,10 +52,18 @@\n     \"labels\": DEFAULT_JOB_LABELS,\n }\n \n-DEFAULT_DESTINATION_REGEX = r\"(?P<dataset>[\\w\\-_0-9]+)/\" \\\n-                            r\"(?P<table>[\\w\\-_0-9]+)/\" \\\n-                            r\"?(?P<partition>\\$[0-9]+)?/\" \\\n-                            r\"?(?P<batch>[\\w\\-_0-9]+)?/\"\n+# yapf: disable\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MDg2Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524860862", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            an hourly partition might have mutliple directories that upload to it.\n          \n          \n            \n            an hourly partition might have multiple directories that upload to it.", "author": "danieldeleo", "createdAt": "2020-11-17T03:26:03Z", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -16,6 +16,77 @@ like so:\n Note, the table prefix can contain multiple sub-prefixes for handling partitions\n or for configuring historical / incremental loads differently.\n \n+### Configurable Naming Convention with Regex\n+By Default we try to read dataset, table, partition (or yyyy/mm/dd/hh) and\n+batch id using the following python regex:\n+```python3\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n+    r\"(?P<yyyy>[0-9]{4})?/?\"         # partition year (yyyy) (optional)\n+    r\"(?P<mm>[0-9]{2})?/?\"           # partition month (mm) (optional)\n+    r\"(?P<dd>[0-9]{2})?/?\"           # partition day (dd)  (optional)\n+    r\"(?P<hh>[0-9]{2})?/?\"           # partition hour (hh) (optional)\n+    r\"(?P<batch>[\\w\\-_0-9]+)?/\"      # batch id (optional)\n+)\n+```\n+you can see if this meets your needs in this [regex playground](https://regex101.com/r/5Y9TDh/2)\n+Otherwise you can override the regex by setting the `DESTINATION_REGEX` to\n+better fit your naming convention on GCS. Your regex must include\n+[Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for destination `dataset`, and `table`.\n+Note, that `dataset` can optionally, explicitly specify destination project\n+(i.e. `gs://${BUCKET}/project_id.dataset_id/table/....`) otherwise the default\n+project will be inferred from Application Default Credential (the project in\n+which the Cloud Function is running, or the ADC configured in Google Cloud SDK\n+if invoked locally). This is useful in scenarios where a single deployment of\n+the Cloud Function is responsible for ingesting data into BigQuery tables in\n+projects other than the one it is deployed in. In these cases it is crucial to\n+ensure the service account that Cloud Functions is impersonating has the correct\n+permissions on all destination projects.\n+\n+Your regex can optionally include  for \n+- `partition` must be BigQuery Partition decorator with leading `$`\n+- `yyyy`, `mm`, `dd`, `hr` partition year, month, day, and hour\n+(depending on your partition granularity)\n+- `batch` an optional batch id to indicate multiple uploads for this partition.\n+\n+For example, if your datafiles were laid out like this:\n+```text\n+gs://${BUCKET}/${SOURCE_SYSTEM}/${DATASET}/${TABLE}/region=${LOCATION}/yyyy=${YEAR}/mm=${MONTH}/dd=${DAY}/hh=${HOUR}\n+```\n+i.e.\n+```text\n+gs://my-bucket/on-prem-edw/my_product/transactions/region=US/yyyy=2020/mm=01/dd=02/hh=03/_SUCCESS\n+```\n+Then you could use [this regex](https://regex101.com/r/OLpmg4/2):\n+```text\n+DESTINATION_REGEX='(?:[\\w\\-_0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/region=(?P<batch>[\\w]+)/yyyy=(?P<yyyy>[0-9]{4})/mm=(?P<mm>[0-9]{2})/dd=(?P<dd>[0-9]{2})/hh=(?P<hh>[0-9]{2})/'\n+```\n+In this case we can take advantage of a more known rigid structure so our regex \n+is simpler (no optional capturing groups, optional slashes).\n+Note, we can use the `region=` string (which may have been partitioned on\n+in an  upstream system such as Hive) as a batch ID because we might expect that\n+an hourly partition might have mutliple directories that upload to it.", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MTIwMg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524861202", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In most cases, it would be recommeneded to have separate buckets / deployment \n          \n          \n            \n            In most cases, it would be recommended to have separate buckets / deployment", "author": "danieldeleo", "createdAt": "2020-11-17T03:27:20Z", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -16,6 +16,77 @@ like so:\n Note, the table prefix can contain multiple sub-prefixes for handling partitions\n or for configuring historical / incremental loads differently.\n \n+### Configurable Naming Convention with Regex\n+By Default we try to read dataset, table, partition (or yyyy/mm/dd/hh) and\n+batch id using the following python regex:\n+```python3\n+DEFAULT_DESTINATION_REGEX = (\n+    r\"^(?P<dataset>[\\w\\-\\._0-9]+)/\"  # dataset (required)\n+    r\"(?P<table>[\\w\\-_0-9]+)/?\"      # table name (required)\n+    r\"(?P<partition>\\$[0-9]+)?/?\"    # partition decortator (optional)\n+    r\"(?P<yyyy>[0-9]{4})?/?\"         # partition year (yyyy) (optional)\n+    r\"(?P<mm>[0-9]{2})?/?\"           # partition month (mm) (optional)\n+    r\"(?P<dd>[0-9]{2})?/?\"           # partition day (dd)  (optional)\n+    r\"(?P<hh>[0-9]{2})?/?\"           # partition hour (hh) (optional)\n+    r\"(?P<batch>[\\w\\-_0-9]+)?/\"      # batch id (optional)\n+)\n+```\n+you can see if this meets your needs in this [regex playground](https://regex101.com/r/5Y9TDh/2)\n+Otherwise you can override the regex by setting the `DESTINATION_REGEX` to\n+better fit your naming convention on GCS. Your regex must include\n+[Python Regex with named capturing groups](https://docs.python.org/3/howto/regex.html#non-capturing-and-named-groups)\n+for destination `dataset`, and `table`.\n+Note, that `dataset` can optionally, explicitly specify destination project\n+(i.e. `gs://${BUCKET}/project_id.dataset_id/table/....`) otherwise the default\n+project will be inferred from Application Default Credential (the project in\n+which the Cloud Function is running, or the ADC configured in Google Cloud SDK\n+if invoked locally). This is useful in scenarios where a single deployment of\n+the Cloud Function is responsible for ingesting data into BigQuery tables in\n+projects other than the one it is deployed in. In these cases it is crucial to\n+ensure the service account that Cloud Functions is impersonating has the correct\n+permissions on all destination projects.\n+\n+Your regex can optionally include  for \n+- `partition` must be BigQuery Partition decorator with leading `$`\n+- `yyyy`, `mm`, `dd`, `hr` partition year, month, day, and hour\n+(depending on your partition granularity)\n+- `batch` an optional batch id to indicate multiple uploads for this partition.\n+\n+For example, if your datafiles were laid out like this:\n+```text\n+gs://${BUCKET}/${SOURCE_SYSTEM}/${DATASET}/${TABLE}/region=${LOCATION}/yyyy=${YEAR}/mm=${MONTH}/dd=${DAY}/hh=${HOUR}\n+```\n+i.e.\n+```text\n+gs://my-bucket/on-prem-edw/my_product/transactions/region=US/yyyy=2020/mm=01/dd=02/hh=03/_SUCCESS\n+```\n+Then you could use [this regex](https://regex101.com/r/OLpmg4/2):\n+```text\n+DESTINATION_REGEX='(?:[\\w\\-_0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/region=(?P<batch>[\\w]+)/yyyy=(?P<yyyy>[0-9]{4})/mm=(?P<mm>[0-9]{2})/dd=(?P<dd>[0-9]{2})/hh=(?P<hh>[0-9]{2})/'\n+```\n+In this case we can take advantage of a more known rigid structure so our regex \n+is simpler (no optional capturing groups, optional slashes).\n+Note, we can use the `region=` string (which may have been partitioned on\n+in an  upstream system such as Hive) as a batch ID because we might expect that\n+an hourly partition might have mutliple directories that upload to it.\n+(e.g. US, GB, etc). Because it is all named capturing groups we don't have any\n+strict ordering restrictions about batch id appearing before / after partition\n+information.\n+\n+### Dealing with Different Naming Conventions in the Same Bucket\n+In most cases, it would be recommeneded to have separate buckets / deployment ", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MjEyNA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524862124", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            All load or external query jobs will have a job id witha  prefix following this convention:\n          \n          \n            \n            All load or external query jobs will have a job id with a  prefix following this convention:", "author": "danieldeleo", "createdAt": "2020-11-17T03:31:04Z", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -135,16 +206,19 @@ If more granular data is needed about a particular job id\n ### Job Naming Convention\n All load or external query jobs will have a job id witha  prefix following this convention:", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MjQxMw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524862413", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            and the integrationt tests for the backfill CLI.\n          \n          \n            \n            and the integration tests for the backfill CLI.", "author": "danieldeleo", "createdAt": "2020-11-17T03:32:09Z", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -225,6 +306,12 @@ Note that integration tests will spin up / tear down cloud resources that can\n incur a small cost. These resources will be spun up based on your Google Cloud SDK\n [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default)\n \n+#### Pytest Fixtures\n+All Pytest fixtures are DRY-ed up into `tests/conftest.py`\n+This is mostly to share fixtures between the main integration test for the cloud function\n+and the integrationt tests for the backfill CLI.", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2MzkwMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524863900", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    Override the default destination regex for determining BigQuerydestination based on information encoded in the GCS path of thesuccess file\n          \n          \n            \n                                    Override the default destination regex for determining BigQuery destination based on information encoded in the GCS path of the success file", "author": "danieldeleo", "createdAt": "2020-11-17T03:37:46Z", "path": "tools/cloud_functions/gcs_event_based_ingest/README.md", "diffHunk": "@@ -240,7 +327,85 @@ pytest -m IT\n ```\n \n ## Deployment\n-It is suggested to deploy this Cloud Function with the [accompanying terraform module](terraform_module/gcs_ocn_bq_ingest_function/README.md)\n+It is suggested to deploy this Cloud Function with the\n+[accompanying terraform module](terraform_module/gcs_ocn_bq_ingest_function/README.md)\n+\n+### Google Cloud SDK \n+Alternatively, you can deploy with Google Cloud SDK:\n+\n+#### Pub/Sub Notifications\n+```bash\n+PROJECT_ID=your-project-id\n+TOPIC_ID=test-gcs-ocn\n+PUBSUB_TOPIC=projects/${PROJECT_ID/topics/${TOPIC_ID}\n+\n+# Create Pub/Sub Object Change Notifications\n+gsutil notification create -f json -t ${PUBSUB_TOPIC} -e OBJECT_FINALIZE gs://${INGESTION_BUCKET}\n+\n+# Deploy Cloud Function\n+gcloud functions deploy test-gcs-bq-ingest \\\n+  --region=us-west4 \\\n+  --source=gcs_ocn_bq_ingest \\\n+  --entrypoint=main \\\n+  --runtime=python38 \\\n+  --trigger-topic=${PUBSUB_TOPIC} \\\n+  --service-account=${SERVICE_ACCOUNT_EMAIL} \\\n+  --timeout=540 \\\n+  --set-env-vars='DESTINATION_REGEX=^(?:[\\w\\-0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?:incremental|history)?/?(?P<yyyy>[0-9]{4})?/?(?P<mm>[0-9]{2})?/?(?P<dd>[0-9]{2})?/?(?P<hh>[0-9]{2})?/?(?P<batch>[0-9]+)?/?'\n+```\n+\n+#### Cloud Functions Events\n+```bash\n+PROJECT_ID=your-project-id\n+\n+# Deploy Cloud Function\n+gcloud functions deploy test-gcs-bq-ingest \\\n+  --region=us-west4 \\\n+  --source=gcs_ocn_bq_ingest \\\n+  --entrypoint=main \\\n+  --runtime=python38 \\\n+  --trigger-resource ${INGESTION_BUCKET} \\\n+  --trigger-event google.storage.object.finalize\n+  --service-account=${SERVICE_ACCOUNT_EMAIL} \\\n+  --timeout=540 \\\n+  --set-env-vars='DESTINATION_REGEX=^(?:[\\w\\-0-9]+)/(?P<dataset>[\\w\\-_0-9]+)/(?P<table>[\\w\\-_0-9]+)/?(?:incremental|history)?/?(?P<yyyy>[0-9]{4})?/?(?P<mm>[0-9]{2})?/?(?P<dd>[0-9]{2})?/?(?P<hh>[0-9]{2})?/?(?P<batch>[0-9]+)?/?'\n+```\n+\n+In theory, one could set up Pub/Sub notifications from multiple GCS Buckets \n+(owned by different teams but following a common naming convention) to the same\n+Pub/Sub topic so that data uploaded to any of these buckets could get\n+automatically loaded to BigQuery by a single deployment of the Cloud Function.\n+\n+## Backfill\n+There are some cases where you may have data already copied to GCS according to\n+the naming convention / with success files before the Object Change\n+Notifications or Cloud Function have been set up. In these cases, you can use\n+the `backfill.py` CLI utility to crawl an existing bucket searching for success\n+files. The utility supports either invoking the Cloud Function main method\n+locally (in concurrent threads) or publishing notifications for the success \n+files (for a deployed Cloud Function to pick up).\n+\n+### Usage\n+```\n+python3 -m backfill -h\n+usage: backfill.py [-h] --gcs-path GCS_PATH [--mode {LOCAL,NOTIFICATIONS}] [--pubsub-topic PUBSUB_TOPIC] [--success-filename SUCCESS_FILENAME] [--destination-regex DESTINATION_REGEX]\n+\n+utility to backfill success file notifications or run the cloud function locally in concurrent threads.\n+\n+optional arguments:\n+  -h, --help            show this help message and exit\n+  --gcs-path GCS_PATH, -p GCS_PATH\n+                        GCS path (e.g. gs://bucket/prefix/to/search/)to search for existing _SUCCESS files\n+  --mode {LOCAL,NOTIFICATIONS}, -m {LOCAL,NOTIFICATIONS}\n+                        How to perform the backfill: LOCAL run cloud function main method locally (in concurrent threads) or NOTIFICATIONS just push notifications to Pub/Sub for a deployed\n+                        version of the cloud function to pick up. Default is NOTIFICATIONS.\n+  --pubsub-topic PUBSUB_TOPIC, --topic PUBSUB_TOPIC, -t PUBSUB_TOPIC\n+                        Pub/Sub notifications topic to post notifications for. i.e. projects/{PROJECT_ID}/topics/{TOPIC_ID} Required if using NOTIFICATIONS mode.\n+  --success-filename SUCCESS_FILENAME, -f SUCCESS_FILENAME\n+                        Override the default success filename '_SUCCESS'\n+  --destination-regex DESTINATION_REGEX, -r DESTINATION_REGEX\n+                        Override the default destination regex for determining BigQuerydestination based on information encoded in the GCS path of thesuccess file", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524865503", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            from argparse import ArgumentParser, Namespace\n          \n          \n            \n            from argparse import ArgumentParser\n          \n          \n            \n            from argparse import Namespace", "author": "danieldeleo", "createdAt": "2020-11-17T03:43:38Z", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4MzQ2NA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525383464", "bodyText": "reasoning? this looks like java style suggestion on a python import. Many python communities will import several things from a single module in a single statement (under the hood it is slightly more performant IIUC).\nExample in airflow even splitting on to multiple lines:\nhttps://github.com/apache/airflow/blob/master/airflow/operators/check_operator.py#L23-L28", "author": "jaketf", "createdAt": "2020-11-17T18:17:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUxMzEwNQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525513105", "bodyText": "tl;dr I'll refactor imports to just import full modules directly to be more explicit and change yapf to style google.\nAlright I did some research:\nIt seems PEP-8 says this style fine (link)\nInterestingly Google style says always import entire module for namespacing reasons (https://google.github.io/styleguide/pyguide.html#22-imports)\nFor posterity I was totally wrong about any performance penalty of importing a whole module according to this SO post", "author": "jaketf", "createdAt": "2020-11-17T20:45:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUyOTExOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525529119", "bodyText": "refactored imports to match google style recommendation in 4ae77a2", "author": "jaketf", "createdAt": "2020-11-17T21:15:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2NTUwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDg2OTI3Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r524869272", "bodyText": "Rename to a more descriptive variable name like \"pubsub_client\".\ncli is more commonly understood as \"command line interface\"", "author": "danieldeleo", "createdAt": "2020-11-17T03:58:09Z", "path": "tools/cloud_functions/gcs_event_based_ingest/backfill.py", "diffHunk": "@@ -0,0 +1,175 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\" Command Line utility for backfilling gcs_ocn_bq_ingest cloud function\n+\"\"\"\n+import concurrent.futures\n+import logging\n+import os\n+import pprint\n+import sys\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, Iterator, List\n+\n+from google.api_core.client_info import ClientInfo\n+from google.cloud import storage\n+\n+import gcs_ocn_bq_ingest  # pylint: disable=import-error\n+\n+CLIENT_INFO = ClientInfo(user_agent=\"google-pso-tool/bq-severless-loader\")\n+\n+os.environ[\"FUNCTION_NAME\"] = \"backfill-cli\"\n+\n+\n+def find_blobs_with_suffix(\n+    gcs_cli: storage.Client,\n+    prefix: str,\n+    suffix: str = \"_SUCCESS\",\n+) -> Iterator[storage.Blob]:\n+    \"\"\"\n+    Find GCS blobs with a given suffix.\n+\n+    :param gcs_cli:  storage.Client\n+    :param prefix: A GCS prefix to search i.e. gs://bucket/prefix/to/search\n+    :param suffix: A suffix in blob name to match\n+    :return:  Iterable of blobs matching the suffix.\n+    \"\"\"\n+    bucket_name, prefix = gcs_ocn_bq_ingest.main.parse_gcs_url(\n+        prefix)    # noqa\n+    bucket: storage.Bucket = gcs_cli.lookup_bucket(bucket_name)\n+    # filter passes on scalability / laziness advantages of iterator.\n+    return filter(lambda blob: blob.name.endswith(suffix),\n+                  bucket.list_blobs(prefix=prefix))\n+\n+\n+def main(args: Namespace):\n+    \"\"\"main entry point for backfill CLI.\"\"\"\n+    gcs_cli: storage.Client = storage.Client(client_info=CLIENT_INFO)\n+    ps_cli = None", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525239475", "bodyText": "Check and raise error if bkt is None before calling blob()", "author": "danieldeleo", "createdAt": "2020-11-17T15:21:19Z", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -103,6 +110,7 @@ def main(event: Dict, context):    # pylint: disable=unused-argument\n     prefix_to_load = removesuffix(object_id, SUCCESS_FILENAME)\n     gsurl = f\"gs://{bucket_id}/{prefix_to_load}\"\n     gcs_client = storage.Client(client_info=CLIENT_INFO)\n+    project = gcs_client.project\n     bkt = gcs_client.lookup_bucket(bucket_id)\n     success_blob: storage.Blob = bkt.blob(object_id)", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4NTIwNQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525385205", "bodyText": "goooood point mypy should have caught this if there was a proper typehint lookup_bucket() -> Optional[storage.Bucket].", "author": "jaketf", "createdAt": "2020-11-17T18:19:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM5Mjg2NA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525392864", "bodyText": "Opened googleapis/python-storage#318", "author": "jaketf", "createdAt": "2020-11-17T18:31:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTUwNTk5Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525505996", "bodyText": "implemented this in a wrapper function and added local type hint.", "author": "jaketf", "createdAt": "2020-11-17T20:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTIzOTQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODM2MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525258361", "bodyText": "I'd recommend using as much of the cloud storage python client as possible. You can get rid of your parsing function and use:\nhttps://googleapis.dev/python/storage/latest/blobs.html#google.cloud.storage.blob.Blob.from_string", "author": "danieldeleo", "createdAt": "2020-11-17T15:36:14Z", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -376,7 +389,7 @@ def get_batches_for_prefix(storage_client,\n     (one batch has an array of multiple GCS uris)\n     \"\"\"\n     batches = []\n-    bucket_name, prefix_name = _parse_gcs_url(prefix_path)\n+    bucket_name, prefix_name = parse_gcs_url(prefix_path)", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM4NTYyNw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525385627", "bodyText": "TIL this existed thanks <3", "author": "jaketf", "createdAt": "2020-11-17T18:20:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI1ODM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3MzM3Nw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525273377", "bodyText": "Cleaner to check for:\nnotification.get(\"kind\") == \"storage#object\"", "author": "danieldeleo", "createdAt": "2020-11-17T15:54:26Z", "path": "tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py", "diffHunk": "@@ -426,19 +439,38 @@ def parse_notification(notification: dict) -> Tuple[str, str]:\n     Args:\n         notification(dict): Pub/Sub Storage Notification\n         https://cloud.google.com/storage/docs/pubsub-notifications\n+        Or Cloud Functions direct trigger\n+        https://cloud.google.com/functions/docs/tutorials/storage\n+        with notification schema\n+        https://cloud.google.com/storage/docs/json_api/v1/objects#resource\n     Returns:\n         tuple of bucketId and objectId attributes\n     Raises:\n         KeyError if the input notification does not contain the expected\n         attributes.\n     \"\"\"\n-    try:\n-        attributes = notification[\"attributes\"]\n-        return attributes[\"bucketId\"], attributes[\"objectId\"]\n-    except KeyError:\n-        raise RuntimeError(\n-            \"Issue with payload, did not contain expected attributes\"\n-            f\"'bucketId' and 'objectId': {notification}\") from KeyError\n+    if {\"bucket\", \"name\", } <= notification.keys():", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTQwMDA0Nw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525400047", "bodyText": "so much more explicit, love it.", "author": "jaketf", "createdAt": "2020-11-17T18:42:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3MzM3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI3OTA5MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525279091", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                This is an adapatation of test_load_job_partitioned but instead uses the\n          \n          \n            \n                This is an adaptation of test_load_job_partitioned but instead uses the", "author": "danieldeleo", "createdAt": "2020-11-17T16:01:20Z", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py", "diffHunk": "@@ -0,0 +1,90 @@\n+# Copyright 2020 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n+import os\n+from time import monotonic\n+\n+import backfill\n+import pytest\n+from google.cloud import bigquery\n+\n+TEST_DIR = os.path.realpath(os.path.dirname(__file__) + \"/..\")\n+LOAD_JOB_POLLING_TIMEOUT = 20    # seconds\n+\n+\n+@pytest.mark.IT\n+@pytest.mark.CLI\n+def test_backfill(bq, gcs_partitioned_data, gcs_truncating_load_config,\n+                  gcs_bucket, dest_dataset, dest_partitioned_table, mock_env):\n+    \"\"\"\n+    This is an adapatation of test_load_job_partitioned but instead uses the", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTI4MDU1Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525280552", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"\n          \n          \n            \n            \"\"\"Integration tests for gcs_ocn_bq_ingest\"\"\"", "author": "danieldeleo", "createdAt": "2020-11-17T16:03:17Z", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "diffHunk": "@@ -11,23 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+\"\"\"integrtion tests for gcs_ocn_bq_ingest\"\"\"", "originalCommit": "9e3e963fba00e1757c2dc8b7cf212565bab582d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5752459c4f5d9855cf9721b95a32ff591cf162f5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/5752459c4f5d9855cf9721b95a32ff591cf162f5", "message": "Update tools/cloud_functions/gcs_event_based_ingest/gcs_ocn_bq_ingest/main.py\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:00:52Z", "type": "commit"}, {"oid": "98287ef8df763988d0a9a3371d6617fd0f0cf249", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/98287ef8df763988d0a9a3371d6617fd0f0cf249", "message": "Update tools/cloud_functions/gcs_event_based_ingest/README.md\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:01:02Z", "type": "commit"}, {"oid": "b6d9323ea5b39901fa919c4a4a2681b2bd853ead", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/b6d9323ea5b39901fa919c4a4a2681b2bd853ead", "message": "Update tools/cloud_functions/gcs_event_based_ingest/README.md\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:01:13Z", "type": "commit"}, {"oid": "f311fcff83b8d5c2306785cd0f6bd63ff5929e46", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/f311fcff83b8d5c2306785cd0f6bd63ff5929e46", "message": "Update tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:01:24Z", "type": "commit"}, {"oid": "df025362ee37fd0386687ffeb073219da70ce199", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/df025362ee37fd0386687ffeb073219da70ce199", "message": "Update tools/cloud_functions/gcs_event_based_ingest/tests/cli/test_backfill.py\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:01:37Z", "type": "commit"}, {"oid": "bcf3b961bd82f2536a6af9d865dc72644b00d521", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bcf3b961bd82f2536a6af9d865dc72644b00d521", "message": "Update tools/cloud_functions/gcs_event_based_ingest/README.md\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:13:08Z", "type": "commit"}, {"oid": "72f7a4fbbb800a312c5b262630146cf6700c35d5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/72f7a4fbbb800a312c5b262630146cf6700c35d5", "message": "Update tools/cloud_functions/gcs_event_based_ingest/README.md\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:13:18Z", "type": "commit"}, {"oid": "17d7356cfce721cb4fd151b07914b943488062f6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/17d7356cfce721cb4fd151b07914b943488062f6", "message": "Update tools/cloud_functions/gcs_event_based_ingest/README.md\n\nCo-authored-by: Daniel De Leo <danieldeleo@users.noreply.github.com>", "committedDate": "2020-11-17T18:13:30Z", "type": "commit"}, {"oid": "50d9400e30e3b44b5f10540ef27435598e5c69ea", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/50d9400e30e3b44b5f10540ef27435598e5c69ea", "message": "Fixup address review feedback and fix linters\n\n* use client libraray gsurl parser\n* explicitly check kind == \"storage#object\"\n* fix yapf / isort contention / non-determinism\n* briefly cache redundant GCS API calls\n* naming nits", "committedDate": "2020-11-17T20:29:00Z", "type": "commit"}, {"oid": "24d380c07130543ff49bcbc05592ae55678cf9cf", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/24d380c07130543ff49bcbc05592ae55678cf9cf", "message": "Merge branch 'cloud-functions-ingest' of github.com:jaketf/bigquery-utils into cloud-functions-ingest", "committedDate": "2020-11-17T20:32:09Z", "type": "commit"}, {"oid": "4ae77a206e6d2e1223636ec039a4ddada658d1f4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4ae77a206e6d2e1223636ec039a4ddada658d1f4", "message": "fixup google style imports everywhere", "committedDate": "2020-11-17T21:09:51Z", "type": "commit"}, {"oid": "ab8eac80132e14b72715156e86075dc0a7c663d2", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ab8eac80132e14b72715156e86075dc0a7c663d2", "message": "set pytest workers to auto", "committedDate": "2020-11-17T21:14:02Z", "type": "commit"}, {"oid": "6da5ddda30b540e2c6e8aeec36ddb1cb8032f1e6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6da5ddda30b540e2c6e8aeec36ddb1cb8032f1e6", "message": "don't alias use full import paths", "committedDate": "2020-11-17T21:21:53Z", "type": "commit"}, {"oid": "eef025606b510a84ff925c76df75cde23e597cef", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/eef025606b510a84ff925c76df75cde23e597cef", "message": "fixup! move --workers=auto to pytest.ini\n\nThis way it affects cloudbuild or local invocations", "committedDate": "2020-11-17T21:30:15Z", "type": "commit"}, {"oid": "8ccc387de0242f7dc4b94f3d535544fa660eb691", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/8ccc387de0242f7dc4b94f3d535544fa660eb691", "message": "fixup don't reverse implement storage.Client.get_bucket", "committedDate": "2020-11-17T21:47:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NjczNA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r525576734", "bodyText": "Imports should be on separate lines\nhttps://google.github.io/styleguide/pyguide.html#313-imports-formatting", "author": "danieldeleo", "createdAt": "2020-11-17T22:48:33Z", "path": "tools/cloud_functions/gcs_event_based_ingest/tests/conftest.py", "diffHunk": "@@ -11,24 +11,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"unit tests for gcs_ocn_bq_ingest\"\"\"\n+\"\"\"Integration tests for gcs_ocn_bq_ingest\"\"\"\n import json\n import os\n-import sys\n+import time\n import uuid\n-from time import monotonic\n from typing import List\n \n-import google.cloud.storage as storage\n import pytest\n-from google.cloud import bigquery\n-from google.cloud.exceptions import NotFound\n+from google.cloud import bigquery, storage", "originalCommit": "8ccc387de0242f7dc4b94f3d535544fa660eb691", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5Njg3MA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/190#discussion_r526296870", "bodyText": "Doh! we were just talking about this too!\nFixed here and in main.py in 348287f\nI will look into if there is a pylint or isort plugin / configuration we can use to check this import style.", "author": "jaketf", "createdAt": "2020-11-18T17:46:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NjczNA=="}], "type": "inlineReview"}, {"oid": "348287f646ec5313c372ccd64b8ab63bd64395f8", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/348287f646ec5313c372ccd64b8ab63bd64395f8", "message": "fixup import google style", "committedDate": "2020-11-18T17:45:04Z", "type": "commit"}, {"oid": "ba79922eef17a5d05d8c62bd8015b13d36f13287", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ba79922eef17a5d05d8c62bd8015b13d36f13287", "message": "fixup ci cloudbuild", "committedDate": "2020-11-18T18:54:32Z", "type": "commit"}, {"oid": "91d1dc606247edb6c496379ffe94887f24b0f19a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/91d1dc606247edb6c496379ffe94887f24b0f19a", "message": "remove trailing whitespace", "committedDate": "2020-11-18T19:48:37Z", "type": "commit"}]}