{"pr_number": 187, "pr_title": "Adding CI/CD process for hosting minified javascript libraries", "pr_createdAt": "2020-10-29T18:19:11Z", "pr_url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187", "timeline": [{"oid": "a9e642d75f4bb0f90f1b38b9e601dd4c1231dad4", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a9e642d75f4bb0f90f1b38b9e601dd4c1231dad4", "message": "Adding CI/CD process for building minified javascript libraries and hosting them on a GCS bucket for use in BigQuery UDFs", "committedDate": "2020-10-30T13:58:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMTMxNg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515211316", "bodyText": "any reason for this to be a separate cloud build rather than add steps here?", "author": "jaketf", "createdAt": "2020-10-30T16:09:10Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -15,14 +15,26 @@\n \n steps:\n ###########################################################\n-# Step 1: Run UDF Unit Tests\n+# Step 1: Build and host all js libs in GCS\n+###########################################################\n+- name: gcr.io/google.com/cloudsdktool/cloud-sdk\n+  entrypoint: 'gcloud'\n+  args:\n+    - builds\n+    - submit\n+    - udfs/js_libs\n+    - --config=udfs/js_libs/cloudbuild.yaml", "originalCommit": "2212e23165fe51518d52364edd4077c2050fd712", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTM5OTc5MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521399791", "bodyText": "Steps have been added to udfs/cloudbuild.yaml", "author": "danieldeleo", "createdAt": "2020-11-11T14:34:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMTMxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMjE4Nw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515212187", "bodyText": "nit: In theory, BQ could support other languages in the future. Let's add a javascript/ prefix.", "author": "jaketf", "createdAt": "2020-10-30T16:10:34Z", "path": "udfs/js_libs/README.md", "diffHunk": "@@ -0,0 +1,79 @@\n+# Adding a Javascript Library for use in a BigQuery UDF\n+\n+Complete the following 2 steps to host an npm package in the bigquery-utils GCS bucket. \\\n+You can then import the npm package into your BigQuery UDF using the following syntax:\n+```\n+CREATE FUNCTION myFunc(a FLOAT64, b STRING)\n+  RETURNS STRING\n+  LANGUAGE js\n+  OPTIONS ( \n+    library=[\"gs://bqutil-lib/LIBRARY_NAME-vA.B.C.min.js\"] ", "originalCommit": "2212e23165fe51518d52364edd4077c2050fd712", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMTEwNQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521401105", "bodyText": "Done. Javascript libs will be added to: gs://bqutil-lib/bq_js_libs/", "author": "danieldeleo", "createdAt": "2020-11-11T14:36:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMjE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMzA5Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515213096", "bodyText": "this seems like an unintentional change.", "author": "jaketf", "createdAt": "2020-10-30T16:12:09Z", "path": "udfs/tests/run.sh", "diffHunk": "@@ -14,6 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+", "originalCommit": "2212e23165fe51518d52364edd4077c2050fd712", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMTQxNg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521401416", "bodyText": "Reverted.", "author": "danieldeleo", "createdAt": "2020-11-11T14:36:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMzA5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMzcyMw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515213723", "bodyText": "this PR should also update the UDF Definition queries to point to these \"managed\" GCS URIs.", "author": "jaketf", "createdAt": "2020-10-30T16:13:22Z", "path": "udfs/js_libs/package.json", "diffHunk": "@@ -0,0 +1,22 @@\n+{\n+  \"name\": \"js-bq-libs\",\n+  \"version\": \"1.0.0\",\n+  \"scripts\": {\n+    \"build-all-libs\": \"concurrently \\\"npm:webpack-*\\\"\",\n+    \"webpack-compromise-v11.14.0\": \"webpack --config compromise-v11.14.0-webpack.config.js\",\n+    \"webpack-js-levenshtein-v1.1.6\": \"webpack --config js-levenshtein-v1.1.6-webpack.config.js\",", "originalCommit": "2212e23165fe51518d52364edd4077c2050fd712", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyNTI2NQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515225265", "bodyText": "oops looks like it already does! sorry I missed it.", "author": "jaketf", "createdAt": "2020-10-30T16:32:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxMzcyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxNTE2MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r515215161", "bodyText": "It might be nice to have a small script that does all 3 of these steps.\nThis way  you can use the logic for pushing to a (randomly named) GCS bucket when running the UDF tests.", "author": "jaketf", "createdAt": "2020-10-30T16:15:41Z", "path": "udfs/js_libs/cloudbuild.yaml", "diffHunk": "@@ -0,0 +1,36 @@\n+# Google Cloud Build script for bqutils\n+#\n+# This build script is used to unit test the BigQuery UDFs for every\n+# change pushed to the udfs/ directory.\n+#\n+# Manual Execution:\n+# Use the below command to invoke the build manually. Note the substitutions for\n+# BRANCH_NAME and REVISION_ID. These variables are normally populated when the\n+# build is executed via build triggers but will be empty during manual\n+# execution. Dummy branch and revisions can be passed during manual execution so\n+# the artifacts can be uploaded upon build completion.\n+#\n+# gcloud builds submit . --config=cloudbuild.yaml\n+#\n+\n+steps:", "originalCommit": "2212e23165fe51518d52364edd4077c2050fd712", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMjI1NQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521402255", "bodyText": "GCS bucket is now a substitution variable _JS_BUCKET", "author": "danieldeleo", "createdAt": "2020-11-11T14:37:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIxNTE2MQ=="}], "type": "inlineReview"}, {"oid": "b3eafcdbf7ae33fad182f0e667864f57b2019bc9", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/b3eafcdbf7ae33fad182f0e667864f57b2019bc9", "message": "Adding CI/CD process for building minified javascript libraries and hosting them on a GCS bucket for use in BigQuery UDFs", "committedDate": "2020-10-31T03:14:12Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzMzQxNg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516233416", "bodyText": "nit: remove step_${num}_ prefix\nreasoning:\n\ncloud build already prefixes all logs w. Step # ${num} - and this will create stutter in the logs.\nIf a contributor add steps they  have to go in and update all these id numbers.", "author": "jaketf", "createdAt": "2020-11-02T20:24:20Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,143 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ###########################################################\n+  # Step 0: Install npm packages\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_0_install_npm_packages'", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMjQzMw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521402433", "bodyText": "Good point. Removed", "author": "danieldeleo", "createdAt": "2020-11-11T14:38:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzMzQxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNDA1Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516234052", "bodyText": "speed it up! use gsutil -m to parallelize uploads\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              args:\n          \n          \n            \n              args:\n          \n          \n            \n                - \"-m\"", "author": "jaketf", "createdAt": "2020-11-02T20:25:32Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,143 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ###########################################################\n+  # Step 0: Install npm packages\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_0_install_npm_packages'\n+  entrypoint: 'npm'\n+  args:\n+    - install\n+  dir: js_libs\n+  ###########################################################\n+  # Step 1: Build all js libraries for BigQuery UDFs\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_1_build_bq_js_libs'\n+  entrypoint: 'npm'\n+  args:\n+    - run-script\n+    - build-all-libs\n+  dir: js_libs\n+  waitFor:\n+    - 'step_0_install_npm_packages'\n+  ###########################################################\n+  # Step 2: Copy libs to GCS bucket\n+  ###########################################################\n - name: gcr.io/google.com/cloudsdktool/cloud-sdk\n-  entrypoint: 'bash'\n-  args: ['release/build.sh', '$BRANCH_NAME', '$_PR_NUMBER']\n+  id: 'step_2_copy_js_to_gcs'\n+  entrypoint: 'gsutil'\n+  args:", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMjYxNw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521402617", "bodyText": "Good catch! Done", "author": "danieldeleo", "createdAt": "2020-11-11T14:38:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNDA1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNjYwMg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516236602", "bodyText": "All these cloud build steps are extremely inefficient (slow / expensive) and not easy for a contributor to replicate locally (for example if they do not have python 3.8 installed locally).\nInstead define a dockerfile and build / cache it (for speed). This way the contributor can just build your dockerfile and use it to run tests. This will make it very easy to contribute from a browser in github codespaces once it goes GA (or from any machine w/ docker installed).\nYou can read about the advantages of this approach in my other PR here.\nTo give you and idea of what it would look like here (of course I'd also recommend adding more python linters to catch common issues):\nDockerfile.ci\nFROM python:3.8.0-slim\nWORKDIR /ci\nCOPY . /ci/\nRUN pip3 install -r tests/requirements.txt\n# This can be overriden in cloud build\nENTRYPOINT [\"tests/udf_test_utils.py\"] \ncloudbuild.yaml\n# steps 1 & 2 ...\n- name: 'hadolint/hadolint'  # optional\n  entrypoint: '/bin/hadolint'\n  args:\n    - 'Dockerfile.ci'\n  id: 'lint-ci-docker-image'\n- name: 'gcr.io/kaniko-project/executor:latest'\n  waitFor:\n    - 'lint-ci-docker-image'\n  args:\n    - '--dockerfile=Dockerfile.ci'\n    - '--destination=gcr.io/$PROJECT_ID/bq_udf_ci'\n    - '--cache=true'\n  id: 'build-ci-image-from-kaniko-cache'\n- name: gcr.io/$PROJECT_ID/bq_udf_ci\n  id: 'create_test_datasets'  # use Dockerfile's ENTRYPOINT\n  args:\n    - --create_test_datasets\n  env:\n    - SHORT_SHA=${SHORT_SHA}\n  waitFor:\n    - 'build-ci-image-from-kaniko-cache'\n  ###########################################################\n  # Step 6: Create UDF signatures with empty bodies to prevent\n  # UDF creation errors in next step when one UDF depends\n  # on another UDF.\n  ###########################################################\n- name: gcr.io/$PROJECT_ID/bq_udf_ci\n  id: 'create_udf_signatures'\n  entrypoint: 'pytest'   # Overide entrypoint locally\n  args:\n    - --workers\n    - '100'\n    - tests/create_udf_signatures.py\n  env:\n    - SHORT_SHA=${SHORT_SHA}\n  waitFor:\n    - 'copy_js_to_gcs'\n    - 'create_test_datasets'\n # etc...", "author": "jaketf", "createdAt": "2020-11-02T20:30:51Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,143 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ###########################################################\n+  # Step 0: Install npm packages\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_0_install_npm_packages'\n+  entrypoint: 'npm'\n+  args:\n+    - install\n+  dir: js_libs\n+  ###########################################################\n+  # Step 1: Build all js libraries for BigQuery UDFs\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_1_build_bq_js_libs'\n+  entrypoint: 'npm'\n+  args:\n+    - run-script\n+    - build-all-libs\n+  dir: js_libs\n+  waitFor:\n+    - 'step_0_install_npm_packages'\n+  ###########################################################\n+  # Step 2: Copy libs to GCS bucket\n+  ###########################################################\n - name: gcr.io/google.com/cloudsdktool/cloud-sdk\n-  entrypoint: 'bash'\n-  args: ['release/build.sh', '$BRANCH_NAME', '$_PR_NUMBER']\n+  id: 'step_2_copy_js_to_gcs'\n+  entrypoint: 'gsutil'\n+  args:\n+    - cp\n+    - builds/*\n+    - ${_JS_BUCKET}\n+  dir: js_libs\n+  waitFor:\n+    - 'step_1_build_bq_js_libs'\n+  ###########################################################\n+  # Step 3: Create python virtual environment\n+  ###########################################################\n+- name: python:3.8.0-slim", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwMzM0OQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521403349", "bodyText": "Great suggestions thanks! I've modified to follow your recommendations", "author": "danieldeleo", "createdAt": "2020-11-11T14:39:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIzNjYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NzczMQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516247731", "bodyText": "Is this typo of FUNCTION?", "author": "jaketf", "createdAt": "2020-11-02T20:53:35Z", "path": "udfs/community/nlp_compromise_number.sql", "diffHunk": "@@ -14,10 +14,10 @@\n  * limitations under the License.\n  */\n \n-CREATE OR REPLACE FUNCTION fn.nlp_compromise_number(str STRING)\n+CREATE OR REPLACE FUNCION fn.nlp_compromise_number(str STRING)", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwNjY1Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521406656", "bodyText": "it was a temporary intentional typo introduced to test the build process. reverted now", "author": "danieldeleo", "createdAt": "2020-11-11T14:44:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0NzczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0ODM3Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516248373", "bodyText": "does this need JS_BUCKET?", "author": "jaketf", "createdAt": "2020-11-02T20:54:52Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,143 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ###########################################################\n+  # Step 0: Install npm packages\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_0_install_npm_packages'\n+  entrypoint: 'npm'\n+  args:\n+    - install\n+  dir: js_libs\n+  ###########################################################\n+  # Step 1: Build all js libraries for BigQuery UDFs\n+  ###########################################################\n+- name: 'node'\n+  id: 'step_1_build_bq_js_libs'\n+  entrypoint: 'npm'\n+  args:\n+    - run-script\n+    - build-all-libs\n+  dir: js_libs\n+  waitFor:\n+    - 'step_0_install_npm_packages'\n+  ###########################################################\n+  # Step 2: Copy libs to GCS bucket\n+  ###########################################################\n - name: gcr.io/google.com/cloudsdktool/cloud-sdk\n-  entrypoint: 'bash'\n-  args: ['release/build.sh', '$BRANCH_NAME', '$_PR_NUMBER']\n+  id: 'step_2_copy_js_to_gcs'\n+  entrypoint: 'gsutil'\n+  args:\n+    - cp\n+    - builds/*\n+    - ${_JS_BUCKET}\n+  dir: js_libs\n+  waitFor:\n+    - 'step_1_build_bq_js_libs'\n+  ###########################################################\n+  # Step 3: Create python virtual environment\n+  ###########################################################\n+- name: python:3.8.0-slim\n+  id: 'step_3_create_python_venv'\n+  entrypoint: 'python3'\n+  args:\n+    - -m\n+    - venv\n+    - /workspace/venv\n+  waitFor:\n+    - '-'\n+  ###########################################################\n+  # Step 4: Install python requirements for testing\n+  ###########################################################\n+- name: python:3.8.0-slim\n+  id: 'step_4_pip_install'\n+  entrypoint: '/workspace/venv/bin/python3'\n+  args:\n+    - -m\n+    - pip\n+    - install\n+    - -r\n+    - tests/requirements.txt\n+  waitFor:\n+    - 'step_3_create_python_venv'\n+  ###########################################################\n+  # Step 5: Create BigQuery datasets specifically for testing\n+  ###########################################################\n+- name: python:3.8.0-slim\n+  id: 'step_5_create_test_datasets'\n+  entrypoint: '/workspace/venv/bin/python3'\n+  args:\n+    - tests/udf_test_utils.py\n+    - --create_test_datasets\n+  env:\n+    - SHORT_SHA=${SHORT_SHA}\n+  waitFor:\n+    - 'step_4_pip_install'\n+  ###########################################################\n+  # Step 6: Create UDF signatures with empty bodies to prevent\n+  # UDF creation errors in next step when one UDF depends\n+  # on another UDF.\n+  ###########################################################\n+- name: python:3.8.0-slim\n+  id: 'step_6_create_udf_signatures'\n+  entrypoint: '/workspace/venv/bin/python3'\n+  args:\n+    - -m\n+    - pytest\n+    - --workers\n+    - '100'\n+    - tests/create_udf_signatures.py\n+  env:\n+    - SHORT_SHA=${SHORT_SHA}\n+  waitFor:\n+    - 'step_2_copy_js_to_gcs'\n+    - 'step_5_create_test_datasets'\n+  ###########################################################\n+  # Step 7: Create UDFs in the test dataset from step 5\n+  ###########################################################\n+- name: python:3.8.0-slim\n+  id: 'step_7_create_udfs'\n+  entrypoint: '/workspace/venv/bin/python3'\n+  args:\n+    - -m\n+    - pytest\n+    - --workers\n+    - '100'\n+    - tests/test_create_udfs.py\n+  env:\n+    - SHORT_SHA=${SHORT_SHA}", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwNjA1Nw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521406057", "bodyText": "Not needed. These tests simply execute the SQL. The _JS_BUCKET variable in UDFs is replaced with the actual value by the release/build.sh script.", "author": "danieldeleo", "createdAt": "2020-11-11T14:43:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0ODM3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0OTkzOA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516249938", "bodyText": "nit: Fix shellcheck issues for safety\nshellcheck build.sh \n\nIn build.sh line 18:\nSCRIPT_DIR=\"$( cd \"$(dirname \"$0\")\" ; pwd -P )\"\n               ^------------------^ SC2164: Use 'cd ... || exit' or 'cd ... || return' in case cd fails.\n\nDid you mean: \nSCRIPT_DIR=\"$( cd \"$(dirname \"$0\")\" || exit ; pwd -P )\"\n\n\nIn build.sh line 81:\n  if ! bq show --headless $dataset > /dev/null 2>&1; then\n                          ^------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n  if ! bq show --headless \"$dataset\" > /dev/null 2>&1; then\n\n\nIn build.sh line 82:\n    bq mk --headless -d --data_location=${LOCATION} ${dataset}\n                                                    ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    bq mk --headless -d --data_location=${LOCATION} \"${dataset}\"\n\n\nIn build.sh line 105:\n  printf \"${BOLD}${file}${NORMAL}\\n\"\n         ^-------------------------^ SC2059: Don't use variables in the printf format string. Use printf '..%s..' \"$foo\".\n\n\nIn build.sh line 107:\n    bq query --dataset_id ${dataset} --headless --nouse_legacy_sql --dry_run \"$(cat ${file})\"\n                          ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n                                                                                    ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    bq query --dataset_id \"${dataset}\" --headless --nouse_legacy_sql --dry_run \"$(cat \"${file}\")\"\n\n\nIn build.sh line 109:\n    bq query --dataset_id ${dataset} --headless --nouse_legacy_sql \"$(cat ${file})\"\n                          ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n                                                                          ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    bq query --dataset_id \"${dataset}\" --headless --nouse_legacy_sql \"$(cat \"${file}\")\"\n\n\nIn build.sh line 112:\n  if [[ $? -gt 0 ]]; then\n        ^-- SC2181: Check exit code directly with e.g. 'if mycmd;', not indirectly with $?.\n\n\nIn build.sh line 113:\n    printf \"Failed to create: $file\"\n           ^-----------------------^ SC2059: Don't use variables in the printf format string. Use printf '..%s..' \"$foo\".\n\n\nIn build.sh line 131:\n  local sql_files=$(find ${UDF_DIR} -type f -name \"*.sql\")\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 132:\n  local num_files=$(echo \"$sql_files\" | wc -l)\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 134:\n    sed -i \"s|\\${JS_BUCKET}|${JS_BUCKET}|g\" ${file}\n                                            ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    sed -i \"s|\\${JS_BUCKET}|${JS_BUCKET}|g\" \"${file}\"\n\n\nIn build.sh line 142:\n  --substitutions _JS_BUCKET=${JS_BUCKET},SHORT_SHA=${SHORT_SHA}\n                             ^----------^ SC2086: Double quote to prevent globbing and word splitting.\n                                                    ^----------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n  --substitutions _JS_BUCKET=\"${JS_BUCKET}\",SHORT_SHA=\"${SHORT_SHA}\"\n\n\nIn build.sh line 145:\n  if [[ $? -gt 0 ]]; then\n        ^-- SC2181: Check exit code directly with e.g. 'if mycmd;', not indirectly with $?.\n\n\nIn build.sh line 173:\n  local sql_files=$(find . -type f -name \"*.sql\")\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 174:\n  local num_files=$(echo \"$sql_files\" | wc -l)\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 176:\n  printf \"Building $num_files database objects...\\n\"\n         ^-- SC2059: Don't use variables in the printf format string. Use printf '..%s..' \"$foo\".\n\n\nIn build.sh line 178:\n    local dataset=$(get_dataset ${file})\n          ^-----^ SC2155: Declare and assign separately to avoid masking return values.\n                                ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    local dataset=$(get_dataset \"${file}\")\n\n\nIn build.sh line 180:\n    if [[ ! -z ${dataset} ]]; then\n          ^-- SC2236: Use -n instead of ! -z.\n\n\nIn build.sh line 181:\n      create_dataset_if_not_exists ${dataset}\n                                   ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n      create_dataset_if_not_exists \"${dataset}\"\n\n\nIn build.sh line 182:\n      execute_query ${file} true ${dataset}\n                    ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n                                 ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n      execute_query \"${file}\" true \"${dataset}\"\n\n\nIn build.sh line 200:\n  local files_changed=$(git diff --name-only origin/master)\n        ^-----------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 204:\n  if [[ ! -z $(echo ${files_changed} | grep udfs/) ]]; then\n        ^-- SC2236: Use -n instead of ! -z.\n          ^-- SC2143: Use ! grep -q instead of comparing output with [ -z .. ].\n                    ^--------------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n  if [[ ! -z $(echo \"${files_changed}\" | grep udfs/) ]]; then\n\n\nIn build.sh line 223:\n  local sql_files=$(find ${UDF_DIR} -type f -name \"*.sql\")\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 224:\n  local num_files=$(echo \"$sql_files\" | wc -l)\n        ^-------^ SC2155: Declare and assign separately to avoid masking return values.\n\n\nIn build.sh line 226:\n  printf \"Creating or updating $num_files database objects...\\n\"\n         ^-- SC2059: Don't use variables in the printf format string. Use printf '..%s..' \"$foo\".\n\n\nIn build.sh line 228:\n    local dataset=$(get_dataset ${file})\n          ^-----^ SC2155: Declare and assign separately to avoid masking return values.\n                                ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n    local dataset=$(get_dataset \"${file}\")\n\n\nIn build.sh line 231:\n      create_dataset_if_not_exists ${dataset}\n                                   ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n      create_dataset_if_not_exists \"${dataset}\"\n\n\nIn build.sh line 232:\n      execute_query ${file} false ${dataset}\n                    ^-----^ SC2086: Double quote to prevent globbing and word splitting.\n                                  ^--------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n      execute_query \"${file}\" false \"${dataset}\"\n\n\nIn build.sh line 250:\n  cd ${SCRIPT_DIR}/.. || exit\n     ^-----------^ SC2086: Double quote to prevent globbing and word splitting.\n\nDid you mean: \n  cd \"${SCRIPT_DIR}\"/.. || exit\n\nFor more information:\n  https://www.shellcheck.net/wiki/SC2155 -- Declare and assign separately to ...\n  https://www.shellcheck.net/wiki/SC2164 -- Use 'cd ... || exit' or 'cd ... |...\n  https://www.shellcheck.net/wiki/SC2059 -- Don't use variables in the printf...", "author": "jaketf", "createdAt": "2020-11-02T20:57:59Z", "path": "release/build.sh", "diffHunk": "@@ -25,7 +25,7 @@ LOCATION=US\n \n # Set colors if terminal supports it", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQwNjkyOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r521406929", "bodyText": "Done.", "author": "danieldeleo", "createdAt": "2020-11-11T14:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI0OTkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjI1MTEzOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r516251139", "bodyText": "TIL  you can do this for just a single PR (rather than all PRs)\nsource: https://gist.github.com/piscisaureus/3342247\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  git fetch origin +refs/pull/*/merge:refs/remotes/origin/pr/*\n          \n          \n            \n                  git fetch origin +refs/pull/${_PR_NUMBER}/merge:refs/remotes/origin/pr/${_PR_NUMBER}", "author": "jaketf", "createdAt": "2020-11-02T21:00:34Z", "path": "cloudbuild.yaml", "diffHunk": "@@ -14,12 +14,36 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Deploy\n-###########################################################\n+  ############################################################\n+  # Step 0: Initialize git repo to enable checking for changes\n+  ############################################################\n+- name: 'gcr.io/cloud-builders/git'\n+  entrypoint: 'bash'\n+  args:\n+    - '-c'\n+    - |\n+      git init\n+      git config user.email \"builder@bigquery-utils.repo\"\n+      git config user.name \"builder\"\n+\n+      git commit -m \"empty commit\"\n+      git remote add origin https://github.com/GoogleCloudPlatform/bigquery-utils.git\n+      git fetch origin master\n+\n+      # Fetch all PRs to get history for PRs created from forked repos\n+      git fetch origin +refs/pull/*/merge:refs/remotes/origin/pr/*", "originalCommit": "dd2598a8610c73d9626f9fa6e0c14041a4402343", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "cdbf3ea3a23b020d0f2c81f89070ea9abe24582a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/cdbf3ea3a23b020d0f2c81f89070ea9abe24582a", "message": "Added CI/CD process for building minified javascript libraries and hosting them on a GCS bucket for use in BigQuery UDFs.\nOptimized UDF deployment by performing UDF creation concurrently using python test framework.", "committedDate": "2020-11-13T22:48:04Z", "type": "commit"}, {"oid": "cdbf3ea3a23b020d0f2c81f89070ea9abe24582a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/cdbf3ea3a23b020d0f2c81f89070ea9abe24582a", "message": "Added CI/CD process for building minified javascript libraries and hosting them on a GCS bucket for use in BigQuery UDFs.\nOptimized UDF deployment by performing UDF creation concurrently using python test framework.", "committedDate": "2020-11-13T22:48:04Z", "type": "forcePushed"}, {"oid": "8ab182c17e31a7292c66867159adeac482c36dce", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/8ab182c17e31a7292c66867159adeac482c36dce", "message": "Fix naming of javascript libs that are written to GCS\nFixing version of compromise npm package", "committedDate": "2020-11-14T00:04:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzMwMDA5OQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r523300099", "bodyText": "I recently had a look at this when adapting for current customer and noticed  a few issues:\n\nnot DRY (3x essentially the same code for no reason): solution shift\nonly supports -k pytest option (should easily support all pytest options e.g. -x for fail fast): solution $@ Special Parameter\nThe test scripts assumes SHORT_SHA env var is set (not the case when not in cloud build)\nMy suggestion is this (though you'd have to change references to SHORT_SHA to TEST_ID):\n\nTEST_ID=$SHORT_SHA\n\n# fall back on random string if SHORT_SHA not set\nif [ -z \"$TEST_ID\" ]\nthen\n  # shellcheck disable=SC2002\n  TEST_ID=$(cat /dev/urandom | tr -dc 'a-z0-9_' | fold -w 32 | head -n 1)\nfi\n\necho \"Test ID: ${TEST_ID}\"\n\nif [[ $1 == \"--pip_install_before_run\" ]]; then\n  shift\n  python3 -m pip install -r tests/requirements.txt\nfi\npython3 tests/udf_test_utils.py --create_test_datasets\npython3 -m pytest --workers 100 tests/create_udf_signatures.py $@\npython3 -m pytest --workers 100 tests/test_create_udfs.py $@\npython3 -m pytest --workers 100 tests/test_run_udfs.py $@\npython3 tests/udf_test_utils.py --delete_test_datasets\nNote that I might want to locally run the tests BEFORE committing so using short SHA of the most recent commit (git rev-parse --short HEAD) may be misleading.", "author": "jaketf", "createdAt": "2020-11-14T00:32:40Z", "path": "udfs/tests/run.sh", "diffHunk": "@@ -15,23 +15,23 @@\n # limitations under the License.\n \n if [[ $1 == \"--pip_install_before_run\" ]]; then", "originalCommit": "cdbf3ea3a23b020d0f2c81f89070ea9abe24582a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMyNDk5Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r524324993", "bodyText": "Great points and I didn't know about shift, thanks!\nWith the build process now using a custom image and the testing steps being executed by the yaml file, the run.sh script is now just a helper script for the user to run tests locally.\nI've made the following updates to run.sh:\n\nRemoved the pip install step since the contributing guide has users manually perform that step.\nExplicitly set the SHORT_SHA env variable in run.sh to a dummy value since it's only needed to create tests datasets in the users GCP project.", "author": "danieldeleo", "createdAt": "2020-11-16T14:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzMwMDA5OQ=="}], "type": "inlineReview"}, {"oid": "77e913b3e82cacdb5429adb3e7c6fd8ba9236958", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/77e913b3e82cacdb5429adb3e7c6fd8ba9236958", "message": "Follow up on review comment to DRY-up helper run.sh script", "committedDate": "2020-11-16T14:42:28Z", "type": "commit"}, {"oid": "6c7eaa3cda7cedb357555ca75547bbd2f148d770", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6c7eaa3cda7cedb357555ca75547bbd2f148d770", "message": "Changed pytest worker count to be automatically determined based on environment core count\nAdded pytest option --tests-per-worker and set to 'auto' which will allocate at most 50 tests per worker", "committedDate": "2020-11-16T15:29:01Z", "type": "commit"}, {"oid": "4228411eba5f164a9a010b75cc89c5708a159695", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4228411eba5f164a9a010b75cc89c5708a159695", "message": "Added test cases for nlp_compromise_number and nlp_compromise_people javascript-based UDFs.", "committedDate": "2020-11-16T18:56:28Z", "type": "commit"}, {"oid": "da52c2fbf92d266fe074f63be6059691dd217b76", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/da52c2fbf92d266fe074f63be6059691dd217b76", "message": "Changed underscores in python argument names to hyphens.", "committedDate": "2020-11-16T22:34:51Z", "type": "commit"}, {"oid": "b5957cac036a1f39e59d6a1fb02097c39d833a76", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/b5957cac036a1f39e59d6a1fb02097c39d833a76", "message": "Removing unintentional space", "committedDate": "2020-11-16T23:01:52Z", "type": "commit"}, {"oid": "09d9b34647bdca8377e41e8e00b5aa8f7188fb87", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/09d9b34647bdca8377e41e8e00b5aa8f7188fb87", "message": "Only create NULL function signatures for newly added UDFs when deploying in public facing dataset.\nThis prevents deployment process from temporarily changing existing UDFs to a NULL implementations.", "committedDate": "2020-12-03T23:16:23Z", "type": "commit"}, {"oid": "6612de0448bfc609d69555c3d9309291f1610cd7", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6612de0448bfc609d69555c3d9309291f1610cd7", "message": "Increase build timeout from default value of 10min to 30min", "committedDate": "2020-12-07T23:50:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzUyMDM0Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553520343", "bodyText": "build is failing because this relative path under udfs/ but need to specify dir: \"udfs\".", "author": "jaketf", "createdAt": "2021-01-07T18:56:44Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,147 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ############################################################\n+  # Dynamically create the package.json file based off the libs\n+  # specified in the js_libs/js_libs.yaml file.\n+  ############################################################\n+- name: gcr.io/$PROJECT_ID/bq_udf_ci\n+  id: generate_js_libs_package_json\n+  entrypoint: python3\n+  args:\n+    - tests/udf_test_utils.py", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTEzMjMyMQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555132321", "bodyText": "This is expected. The current build that's failing will be removed once this PR is merged.", "author": "danieldeleo", "createdAt": "2021-01-11T15:36:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzUyMDM0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU3NDI5OA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553574298", "bodyText": "nit: move this into it's own script git_init.sh and invoke here.", "author": "jaketf", "createdAt": "2021-01-07T20:36:49Z", "path": "cloudbuild.yaml", "diffHunk": "@@ -14,12 +14,60 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Deploy\n-###########################################################\n+  ############################################################\n+  # Fetches master branch from GitHub and then hard \"resets\"\n+  # to the _PR_NUMBER which triggered this build. This enables\n+  # build.sh to build only what's changed relative to the\n+  # master branch by checking the output of \"git diff\".\n+  ############################################################\n+- name: gcr.io/cloud-builders/git\n+  entrypoint: bash\n+  args:\n+    - '-c'", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU3NTYyNw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553575627", "bodyText": "seems we have several cloudbuild.yamls could we name them a bit more descriptively?", "author": "jaketf", "createdAt": "2021-01-07T20:39:58Z", "path": "release/build.sh", "diffHunk": "@@ -14,133 +14,196 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-# Directory of this script\n-SCRIPT_DIR=\"$( cd \"$(dirname \"$0\")\" ; pwd -P )\"\n-\n # Directory of the UDFs\n UDF_DIR=udfs\n \n-# Location for new datasets\n-LOCATION=US\n-\n # Set colors if terminal supports it\n ncolors=$(tput colors)\n-if [[ $ncolors -gt 7 ]]; then\n+if [[ \"${ncolors}\" -gt 7 ]]; then\n   BOLD=$(tput bold)\n   NORMAL=$(tput sgr0)\n fi\n \n #######################################\n-# Retrieves the dataset of a file by\n-# evaluating the mapping of the file's\n-# parent directory to an associated\n-# dataset.\n+# Executes a query file. If the execution\n+# fails, the script will exit with an error\n+# code of 1.\n # Globals:\n-#   None\n+#   BOLD\n+#   NORMAL\n+#   LOCATION\n # Arguments:\n #   file\n+#   dry_run\n # Returns:\n-#   The dataset\n+#   None\n #######################################\n-function get_dataset() {\n+function execute_query() {\n   local file=$1\n-  local dataset=\"\"\n-  case $file in\n-    *\"community\"*)\n-      dataset=\"fn\";;\n-    *\"netezza\"*)\n-      dataset=\"nz\";;\n-    *\"oracle\"*)\n-      dataset=\"or\";;\n-    *\"redshift\"*)\n-      dataset=\"rs\";;\n-    *\"snowflake\"*)\n-      dataset=\"sf\";;      \n-    *\"teradata\"*)\n-      dataset=\"td\";;\n-    *\"vertica\"*)\n-      dataset=\"ve\";;\n-  esac\n-  echo \"$dataset\"\n-}\n+  local dry_run=$2\n \n+  printf \"%s%s%s\\n\" \"${BOLD}\" \"${file}\" \"${NORMAL}\"\n+  if [[ ${dry_run} = true ]]; then\n+    if ! bq query \\\n+    --headless --nouse_legacy_sql --dry_run \"$(cat \"${file}\")\" ; then\n+      printf \"Failed to dry run: %s\" \"${file}\"\n+      # exit 1 is not called here because some dry-runs may fail due to\n+      # variable placeholders which a user must replace with their own values.\n+      # These dry-runs require manual revision of results.\n+    fi\n+  else\n+    if ! bq query \\\n+    --headless --nouse_legacy_sql \"$(cat \"${file}\")\" ; then\n+      printf \"Failed to create: %s\" \"${file}\"\n+      exit 1\n+    fi\n+  fi\n+}\n \n #######################################\n-# Creates a dataset if it doesn't exist\n-# already.\n+# Builds and hosts the Cloud Build image\n+# used to test BigQuery UDFs.\n # Globals:\n-#   LOCATION\n+#   UDF_DIR\n # Arguments:\n-#   dataset\n+#   None\n # Returns:\n #   None\n #######################################\n-function create_dataset_if_not_exists() {\n-  local dataset=$1\n-  # If we failed to show the dataset, it doesn't exist so create it.\n-  if ! bq show --headless $dataset > /dev/null 2>&1; then\n-    bq mk --headless -d --data_location=$LOCATION $dataset\n-  fi\n+function build_udf_testing_image() {\n+  gcloud builds submit \"${UDF_DIR}\"/tests/ \\\n+    --config=\"${UDF_DIR}\"/tests/cloudbuild.yaml", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTUxMzg5Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555513896", "bodyText": "I think it might be ok to have multiple cloudbuild.yaml files since the directory path signals what it's building (e.g. the cloudbuild.yaml in udfs directory builds the udfs).\nreference: https://github.com/GoogleCloudPlatform/cloud-builders", "author": "danieldeleo", "createdAt": "2021-01-12T04:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU3NTYyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU4NTkxOA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553585918", "bodyText": "you do a lot of creating / deleteing fixtures for your test environment in cloud build / shell scripts when it would be better to use pytest fixtures to make things easier to run and ensure that things get cleaned up even if tests fail. this can be refactored in a future PR but might make things easier to reason about as your cloud builds can be simplified to \"just running pytest\".", "author": "jaketf", "createdAt": "2021-01-07T21:02:57Z", "path": "udfs/cloudbuild.yaml", "diffHunk": "@@ -14,19 +14,147 @@\n #\n \n steps:\n-###########################################################\n-# Step 1: Run UDF Unit Tests\n-###########################################################\n-- name: python:3.8.0-slim\n-  entrypoint: 'bash'\n-  args: ['udfs/tests/run.sh', '--pip_install_before_run']\n-  env: ['SHORT_SHA=$SHORT_SHA']\n-###########################################################\n-# Step 2: Deploy if all tests pass\n-###########################################################\n+  ############################################################\n+  # Dynamically create the package.json file based off the libs\n+  # specified in the js_libs/js_libs.yaml file.\n+  ############################################################\n+- name: gcr.io/$PROJECT_ID/bq_udf_ci\n+  id: generate_js_libs_package_json\n+  entrypoint: python3\n+  args:\n+    - tests/udf_test_utils.py\n+    - --generate-js-libs-package-json\n+  ###########################################################\n+  # Install npm packages based off the package.json file\n+  # created in the previous step.\n+  ###########################################################\n+- name: node\n+  id: install_npm_packages\n+  entrypoint: npm\n+  args:\n+    - install\n+  ############################################################\n+  # Dynamically create webpack config files needed by webpack\n+  # to build npm packages into single .js files which will be\n+  # hosted on GCS and used by BigQuery UDFs.\n+  ############################################################\n+- name: gcr.io/$PROJECT_ID/bq_udf_ci\n+  id: generate_webpack_configs\n+  entrypoint: python3\n+  args:\n+    - tests/udf_test_utils.py\n+    - --generate-webpack-configs\n+  waitFor:\n+    - install_npm_packages\n+    - generate_js_libs_package_json\n+  ###########################################################\n+  # Build (via webpack) all js libraries for BigQuery UDFs\n+  ###########################################################\n+- name: node\n+  id: build_bq_js_libs\n+  entrypoint: npm\n+  args:\n+    - run-script\n+    - build-all-libs\n+  waitFor:\n+    - generate_webpack_configs\n+    - install_npm_packages\n+  ###########################################################\n+  # Copy all libs to GCS bucket\n+  ###########################################################\n - name: gcr.io/google.com/cloudsdktool/cloud-sdk\n-  entrypoint: 'bash'\n-  args: ['release/build.sh', '$BRANCH_NAME', '$_PR_NUMBER']\n+  id: copy_js_to_gcs\n+  entrypoint: gsutil\n+  args:\n+    - '-m'\n+    - cp\n+    - js_builds/*\n+    - ${_JS_BUCKET}\n+  waitFor:\n+    - build_bq_js_libs\n+  ###########################################################\n+  # Create BigQuery datasets specifically for testing", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTE0NzkwMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555147900", "bodyText": "This is a great idea. I'll investigate this in a followup PR", "author": "danieldeleo", "createdAt": "2021-01-11T15:54:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU4NTkxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ3MDY0MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555470641", "bodyText": "Talk to me before you do this work.\nI think i have this 90% implemented in some branch I can dig up.", "author": "jaketf", "createdAt": "2021-01-12T02:28:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU4NTkxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTUxNDAzNg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555514036", "bodyText": "awesome thanks!", "author": "danieldeleo", "createdAt": "2021-01-12T04:57:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzU4NTkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY3MjMzOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553672339", "bodyText": "nit trailing newline", "author": "jaketf", "createdAt": "2021-01-08T00:34:21Z", "path": "udfs/dir_to_dataset_map.yaml", "diffHunk": "@@ -0,0 +1,7 @@\n+netezza: nz\n+oracle: or\n+redshift: rs\n+snowflake: sf\n+teradata: td\n+vertica: ve\n+community: fn", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxNzA5OA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r553717098", "bodyText": "this function is extremely hard to read / follow.\nI'd recommend editing this as a dict object before writing to file.\nI'm not sure I quite got this right but it will illustrate how to clean this up.\npackage_json_dict = \n{\n    \"name\": \"js-bq-libs\",\n    \"version\": \"1.0.0\",\n    \"scripts\": {\n        \"build-all-libs\": \"concurrently \\\\\"npm:webpack-*\\\\\"\"\n    },\n    \"dependencies\": {\n        f'{lib_name}-v{version}\" ': f'npm:{lib_name}@^{version}'\n        for lib_name in js_libs_dict\n        for version in js_libs_dict.get(lib_name).get('versions')\n    },\n    \"devDependencies\": {\n        \"webpack\": \"^5.3.1\",\n        \"webpack-cli\": \"^4.1.0\",\n        \"concurrently\": \"^5.3.0\"\n        }'\n}\n\npackage_json['scripts'].update({\n    f'webpack-{lib_name}-v{version}':\n    f'webpack --config {lib_name}-v{version}-webpack.config.js'\n    for lib_name in js_libs_dict\n    for version in js_libs_dict.get(lib_name).get('versions')                \n})\n\nwith open('./package.json', 'w') as js_libs_package_json:\n    json.dump(package_json, js_libs_package_json)\nAlternatively you could make a template string with sections.", "author": "jaketf", "createdAt": "2021-01-08T03:11:25Z", "path": "udfs/tests/udf_test_utils.py", "diffHunk": "@@ -64,63 +122,205 @@ def extract_udf_name(udf_path):\n         return None\n \n \n-def extract_udf_signature(udf_path):\n+def replace_with_null_body(udf_path):\n+    \"\"\"\n+    For a given path to a UDF DDL file, parse the SQL and return\n+    the UDF with the body entirely replaced with NULL.\n+\n+    :param udf_path: Path to the UDF DDL .sql file\n+    :return: Input UDF DDL with a NULL body\n+    \"\"\"\n     with open(udf_path) as udf_file:\n         udf_sql = udf_file.read()\n     udf_sql = udf_sql.replace('\\n', ' ')\n     pattern = re.compile(r'FUNCTION\\s+(`?.+?`?.*?\\).*?\\s+)AS')\n     match = pattern.search(udf_sql)\n     if match:\n-        udf_name = match[1].replace('LANGUAGE js', '')\n-        return udf_name\n+        udf_signature = match[1].replace('LANGUAGE js', '')\n+        udf_null_body = (f'CREATE FUNCTION IF NOT EXISTS {udf_signature}'\n+                         f' AS (NULL)')\n+        return udf_null_body\n     else:\n-        return udf_path\n+        return None\n+\n \n+def replace_with_test_datasets(project_id, udf_sql):\n+    \"\"\"\n+    For a given path to a UDF DDL file, parse the SQL and return the UDF\n+    with the dataset name changed with an added suffix for testing. The suffix\n+    value is defined in the global variable, DATASET_SUFFIX, and includes the\n+    commit's SHORT_SHA to prevent different commits from interfering\n+    with UDF builds.\n \n-def replace_with_test_datasets(udf_path=None, project_id=None, udf_sql=None):\n-    if udf_path:\n-        with open(udf_path) as udf_file:\n-            udf_sql = udf_file.read()\n-    udf_length_before_replacement = len(udf_sql)\n-    udf_sql = re.sub(\n+    :param project_id: Project in which to create the UDF\n+    :param udf_sql: SQL DDL of the UDF\n+    :return: Same SQL DDL as input udf_sql but replaced with testing dataset\n+    \"\"\"\n+    udf_sql_with_test_dataset = re.sub(\n         r'(\\w+\\.)?(?P<bq_dataset>\\w+)(?P<udf_name>\\.\\w+)\\(',\n-        f'`{project_id}.\\\\g<bq_dataset>_test_{os.getenv(\"SHORT_SHA\")}\\\\g<udf_name>`(',\n+        f'`{project_id}.\\\\g<bq_dataset>{DATASET_SUFFIX}\\\\g<udf_name>`(',\n         udf_sql)\n-    if udf_length_before_replacement == len(udf_sql):\n+    if udf_sql_with_test_dataset == udf_sql:\n+        # Replacement failed, return None to prevent overwriting prod dataset\n         return None\n     else:\n-        return udf_sql\n+        return udf_sql_with_test_dataset\n \n \n-def get_target_bq_dataset(udf_path):\n-    parent_dir_name = Path(udf_path).parent.name\n-    return f'{BIGQUERY_TEST_DATASET_MAPPINGS.get(parent_dir_name)}_{os.getenv(\"SHORT_SHA\")}'\n+def get_test_bq_dataset(udf_path):\n+    \"\"\"\n+    For a given path to a UDF DDL file, return the BigQuery dataset name in\n+    which to test the UDF. The test dataset name is the same as production\n+    but with the suffix, _test_$SHORT_SHA, appended. The $SHORT_SHA value comes\n+    from the commit which triggered the build.\n+    :param udf_path: Path to the UDF DDL .sql file\n+    :return: Name of test dataset\n+    \"\"\"\n+    udf_parent_dir_name = Path(udf_path).parent.name\n+    if os.getenv('SHORT_SHA') is not None:\n+        bq_dataset = get_dir_to_dataset_mappings().get(udf_parent_dir_name)\n+        return f'{bq_dataset}{DATASET_SUFFIX}'\n+    else:\n+        return None\n \n \n def delete_datasets(client):\n-    for dataset in BIGQUERY_TEST_DATASET_MAPPINGS.values():\n-        dataset = f'{dataset}_{os.getenv(\"SHORT_SHA\")}'\n+    for dataset in get_dir_to_dataset_mappings().values():\n+        dataset = f'{dataset}{DATASET_SUFFIX}'\n         client.delete_dataset(dataset, delete_contents=True, not_found_ok=True)\n \n \n-def create_datasets(client):\n-    for dataset in BIGQUERY_TEST_DATASET_MAPPINGS.values():\n-        dataset = f'{dataset}_{os.getenv(\"SHORT_SHA\")}'\n+def create_datasets(client, dataset_suffix=None):\n+    for dataset in get_dir_to_dataset_mappings().values():\n+        if dataset_suffix:\n+            dataset = f'{dataset}{dataset_suffix}'\n         client.create_dataset(dataset, exists_ok=True)\n \n \n+def generate_js_libs_package_json():", "originalCommit": "6612de0448bfc609d69555c3d9309291f1610cd7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTIwMDYxMg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555200612", "bodyText": "fantastic recommendation! much cleaner", "author": "danieldeleo", "createdAt": "2021-01-11T17:03:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxNzA5OA=="}], "type": "inlineReview"}, {"oid": "16471623c3dad39f78897545ee9109fe33ec5433", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/16471623c3dad39f78897545ee9109fe33ec5433", "message": "Addressing review comments", "committedDate": "2021-01-11T17:49:05Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTMxMzU4NQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r555313585", "bodyText": "add newline", "author": "jaketf", "createdAt": "2021-01-11T20:20:03Z", "path": "release/git_init.sh", "diffHunk": "@@ -0,0 +1,40 @@\n+#!/usr/bin/env bash\n+\n+# Copyright 2021 Google LLC\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Only compare with master branch if this build has been triggered\n+# by either a non-master branch on origin repo or a pull request.\n+if [[ ! \"${BRANCH_NAME}\" = \"master\" || -n \"${_PR_NUMBER}\" ]]; then\n+  git init\n+  git config user.email \"builder@bigquery-utils.repo\"\n+  git config user.name \"builder\"\n+  printf \"Adding repo %s as a remote\" \"${_REPO_URL}\"\n+  git remote remove origin # Removing necessary when testing locally\n+  git remote add origin \"${_REPO_URL}\"\n+  git fetch origin master\n+\n+  # Fetch the pull request which triggered this build and then\n+  # hard reset to it. The build.sh script can then call \"git diff\"\n+  # to figure out what to build based on what's changed.\n+  if [[ ! \"${BRANCH_NAME}\" = \"master\" && -z \"${_PR_NUMBER}\" ]]; then\n+    printf \"Fetching and --hard resetting to (%s) branch from origin repo.\" \"${BRANCH_NAME}\"\n+    git fetch origin \"${BRANCH_NAME}\"\n+    git reset --hard origin/\"${BRANCH_NAME}\"\n+  elif [[ -n \"${_PR_NUMBER}\" ]]; then\n+    printf \"Fetching and --hard resetting to the merge commit of pull request #%s which triggered this build.\" \"${_PR_NUMBER}\"\n+    git fetch origin +refs/pull/\"${_PR_NUMBER}\"/merge:refs/remotes/origin/pr/\"${_PR_NUMBER}\"\n+    git reset --hard origin/pr/\"${_PR_NUMBER}\"\n+  fi\n+fi", "originalCommit": "16471623c3dad39f78897545ee9109fe33ec5433", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "6cf560b8fd876981f95ac7709ed9fe2bf75348e6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6cf560b8fd876981f95ac7709ed9fe2bf75348e6", "message": "Addressing review comments", "committedDate": "2021-01-11T21:18:33Z", "type": "commit"}, {"oid": "0e2fe7c569be304a56a7e3100bfe7e122ed5e554", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0e2fe7c569be304a56a7e3100bfe7e122ed5e554", "message": "Resolving conflict with master branch", "committedDate": "2021-01-11T23:04:10Z", "type": "commit"}, {"oid": "7b822d58b5cb30c2a8d4f03ce59d8519f3cd5c44", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/7b822d58b5cb30c2a8d4f03ce59d8519f3cd5c44", "message": "Resolving conflict with master branch", "committedDate": "2021-01-11T23:08:56Z", "type": "commit"}, {"oid": "dbded6c16cf5e5bf71b2b613141c2aa9374ad26f", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/dbded6c16cf5e5bf71b2b613141c2aa9374ad26f", "message": "Fixing git_init script to refer to head of PR when fetching changes", "committedDate": "2021-01-12T04:05:44Z", "type": "commit"}, {"oid": "ae8ea95bbfac4c679016b3e13c1ad9ba16844169", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ae8ea95bbfac4c679016b3e13c1ad9ba16844169", "message": "Merging latest tests from main branch", "committedDate": "2021-01-12T04:24:19Z", "type": "commit"}, {"oid": "2d913d0bc216530f538091b767a68ff64974b953", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/2d913d0bc216530f538091b767a68ff64974b953", "message": "Merge branch 'master' into jsudf_cicd", "committedDate": "2021-01-12T04:28:23Z", "type": "commit"}, {"oid": "ccfc67da34133ce055f4cd6101c40058b865a0fc", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ccfc67da34133ce055f4cd6101c40058b865a0fc", "message": "Comment fixes", "committedDate": "2021-01-12T21:43:03Z", "type": "commit"}, {"oid": "d598fa1687a73ce48dd49ecca057595d4f344b09", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d598fa1687a73ce48dd49ecca057595d4f344b09", "message": "Adding more descriptive name for cloudbuild file used to build test image", "committedDate": "2021-01-19T15:47:02Z", "type": "commit"}, {"oid": "9390cd6c42e8c08299a172f95b98f96883040092", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/9390cd6c42e8c08299a172f95b98f96883040092", "message": "Merge branch 'jsudf_cicd' of https://github.com/danieldeleo/bigquery-utils into jsudf_cicd", "committedDate": "2021-01-19T15:47:43Z", "type": "commit"}, {"oid": "0247dd1201647456fb2244956429af329989e3b8", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0247dd1201647456fb2244956429af329989e3b8", "message": "Adding no-cache-dir flag to dockerfile", "committedDate": "2021-01-19T15:59:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDM5OTIwMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/187#discussion_r560399200", "bodyText": "nit: add newline", "author": "jaketf", "createdAt": "2021-01-19T18:38:32Z", "path": "udfs/tests/Dockerfile.ci", "diffHunk": "@@ -1,4 +1,4 @@\n FROM python:3.8.0-slim\n COPY requirements.txt ./\n-RUN pip3 install -r requirements.txt\n+RUN pip3 install --no-cache-dir -r requirements.txt\n ENTRYPOINT [\"pytest\"]", "originalCommit": "0247dd1201647456fb2244956429af329989e3b8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d5b8e1777226ec3e739114aa9f58bf9fa91e4e7d", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d5b8e1777226ec3e739114aa9f58bf9fa91e4e7d", "message": "Add trailing new line", "committedDate": "2021-01-19T18:42:23Z", "type": "commit"}, {"oid": "a3fab3e228fa638157555c2894f3fb4dff82a464", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/a3fab3e228fa638157555c2894f3fb4dff82a464", "message": "Merge branch 'master' into jsudf_cicd", "committedDate": "2021-01-19T18:47:25Z", "type": "commit"}, {"oid": "020657937ec93e24e630f1614cbef3f03072bfa9", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/020657937ec93e24e630f1614cbef3f03072bfa9", "message": "Adding Note to README for proper invocation of javascript libraries", "committedDate": "2021-01-19T19:45:05Z", "type": "commit"}]}