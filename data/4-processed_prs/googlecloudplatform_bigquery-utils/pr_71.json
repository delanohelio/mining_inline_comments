{"pr_number": 71, "pr_title": "SQL Crawler: Added crawler architecture and files", "pr_createdAt": "2020-06-22T23:34:12Z", "pr_url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71", "timeline": [{"oid": "52750e8f8330016a4c9c45310413c77ab665413f", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/52750e8f8330016a4c9c45310413c77ab665413f", "message": "SQL Crawler: Added crawler architecture and files", "committedDate": "2020-06-22T23:30:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MDA1NQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443970055", "bodyText": "This can just be set()", "author": "yzhvictor", "createdAt": "2020-06-23T05:27:25Z", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjE5Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972196", "bodyText": "not self.linkQueue.empty()", "author": "yzhvictor", "createdAt": "2020-06-23T05:34:57Z", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MjYwNw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443972607", "bodyText": "Instead of printing to console, try to print to some logger, such as https://docs.python.org/2/library/logging.html", "author": "yzhvictor", "createdAt": "2020-06-23T05:36:20Z", "path": "tools/unsupervised_dataset/Crawler/Crawler.py", "diffHunk": "@@ -0,0 +1,101 @@\n+import requests\n+import queue\n+import CQNode\n+import Extractor\n+import CrawlerLog\n+\n+class Crawler:\n+    \"\"\" Contains the functions and logic to run and coordinate the crawling process. Given\n+        initial starting URLs and maximum size, the crawler will explore websites to look\n+        for SQL queries.\n+    \"\"\"\n+    \n+    def __init__(self, links, maxDepth=3, maxSize=500):\n+        \"\"\" Initializes the crawler and instance variables.\n+        \n+        Args:\n+            links: The root URLs to begin crawling from. Can be one or more.\n+            maxDepth: The maximum depth for the crawler to explore.\n+            maxSize: THe maximum number of links the crawler should explore.\n+    \n+        \"\"\"\n+        self.linkQueue = queue.Queue()\n+        self.seen = set([])\n+        self.maxDepth = maxDepth\n+        self.maxSize = maxSize\n+        self.log = CrawlerLog.CrawlerLog()\n+        self.count = 0\n+        \n+        for link in links:\n+            self.linkQueue.put(CQNode.CQNode(link, 0))\n+            self.seen.add(link)\n+        \n+    def crawl(self):\n+        \"\"\" Begins the crawling process using variables set earlier. Extracts\n+            queries by locating website-specific HTML tags or searching for\n+            common expression patterns. Writes queries to output after finishing\n+            each site.\n+        \"\"\"\n+        \n+        while (self.linkQueue.empty() == False):\n+            # Retrieve the next link in the queue\n+            nextNode = self.linkQueue.get()\n+            nodeURL = nextNode.getURL()\n+            nodeDepth = nextNode.getDepth()\n+            \n+            # Check if crawler has exceeded maximum depth or maximum count\n+            if nodeDepth >= self.maxDepth or self.count >= self.maxSize:\n+                print(\"Crawled {0} websites.\".format(self.count))", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3MzcwOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443973709", "bodyText": "Instead of creating customized log mechanism, why not use python's logging library to log messages?", "author": "yzhvictor", "createdAt": "2020-06-23T05:40:15Z", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NDk3Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443974973", "bodyText": "I think we need to handle this case more carefully. Imagine people mistakenly run the tool with an old path, it may unexpectedly overwrite some existing crawled data?", "author": "yzhvictor", "createdAt": "2020-06-23T05:44:24Z", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:\n+            pass", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NTc5MA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r443975790", "bodyText": "In general, it's good to have tests covering the actual logic. So maybe you can consider adding some corresponding tests for each source files.", "author": "yzhvictor", "createdAt": "2020-06-23T05:47:03Z", "path": "tools/unsupervised_dataset/Crawler/CQNode.py", "diffHunk": "@@ -0,0 +1,32 @@\n+class CQNode:", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NjUzOQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444476539", "bodyText": "+1 Please add tests0\nAssume pylint has been run on this?\nAlso do we already have some format checker integrated, it is easier to have people's code more consistent.", "author": "kikkyo", "createdAt": "2020-06-23T20:06:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk3NTc5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTczMA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469730", "bodyText": "I am not expert in Python, is this recommended practice?\nI think we usually check for existence before making directories, instead of ignoring exceptions?", "author": "kikkyo", "createdAt": "2020-06-23T19:53:36Z", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ2OTg3Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444469873", "bodyText": "same here", "author": "kikkyo", "createdAt": "2020-06-23T19:53:54Z", "path": "tools/unsupervised_dataset/Crawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,71 @@\n+import datetime\n+import csv\n+import os\n+\n+class CrawlerLog:\n+    \"\"\" The CrawlerLog keeps track of which websites were explored, how many queries were found,\n+        and creates a CSV with all the queries. It also logs any errors encountered. The log is\n+        saved into Logs subdirectory with name based on start time. Queries are saved into Queries\n+        subdirectory.\n+    \"\"\"\n+    \n+    def __init__(self):\n+        \"\"\" Initializes crawler log to keep track of crawler progress and instantiates\n+            instance variables.\n+        \"\"\"\n+        \n+        self.startTime = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M\")\n+        \n+        # Create directory for logs if it does not already exists\n+        try:\n+            os.mkdir(\"Logs\")\n+        except FileExistsError as e:\n+            pass\n+        \n+        self.log = open(\"Logs/log-\" + self.startTime + \".txt\", \"a\")\n+        self.log.write(\"Beginning crawl at time {0}.\\n\".format(self.startTime))\n+        \n+        try:\n+            os.mkdir(\"Queries\")\n+        except FileExistsError as e:", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3MzY2Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444473662", "bodyText": "nit: seems style guide recommends us using: # TODO(kl@gmail.com): Use a \"*\" here for string repetition.", "author": "kikkyo", "createdAt": "2020-06-23T20:00:48Z", "path": "tools/unsupervised_dataset/Crawler/run_crawler.py", "diffHunk": "@@ -0,0 +1,16 @@\n+import requests\n+import sys\n+import Crawler\n+\n+# Initializes a crawler and starts the crawling process using command line arguments\n+def start_crawler():\n+    # TO-DO: Allow user to optionally define max size or max depth", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ3NDc1OA==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r444474758", "bodyText": "nit: add a comment on \"URL should be  separated by comma or space/something else\" ?", "author": "kikkyo", "createdAt": "2020-06-23T20:03:06Z", "path": "tools/unsupervised_dataset/Crawler/README.md", "diffHunk": "@@ -0,0 +1,11 @@\n+# SQL Crawler\n+\n+This directory contains the code to run a universal, unsupervised SQL web crawler. The user provides a starting target URL from which to begin crawling, and has the option to set the maximum depth or size of the crawler.\n+\n+## Usage\n+To run the crawler, run the following command:\n+\n+```\n+python3 run_crawler.py <starting URLs>", "originalCommit": "52750e8f8330016a4c9c45310413c77ab665413f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9f93dc65b95a7a165f7a1e2b4df9610b82f2a9f9", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/9f93dc65b95a7a165f7a1e2b4df9610b82f2a9f9", "message": "Edited formatting and made fixes, set up testing architecture", "committedDate": "2020-06-23T23:47:49Z", "type": "commit"}, {"oid": "cb6e33705d656dfaace43675e9d5f29521dd5fc3", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/cb6e33705d656dfaace43675e9d5f29521dd5fc3", "message": "Renamed directory structure, added additional tests", "committedDate": "2020-06-24T23:49:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTMyNDQ3OQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/71#discussion_r445324479", "bodyText": "just a second thought on the output file format (not necessarily to be resolved in the cl:)), if we make it as CSV, later would it be problematic to read the query for later processing? For example, if a multi-line query is not well formatted.", "author": "kikkyo", "createdAt": "2020-06-25T06:01:12Z", "path": "tools/unsupervised_dataset/SQLCrawler/CrawlerLog.py", "diffHunk": "@@ -0,0 +1,76 @@\n+import datetime\n+import csv\n+import os\n+import logging\n+import pathlib\n+\n+class CrawlerLog(object):\n+    \"\"\" Logs the status of the SQL crawler, including websites and queries.\n+        \n+        The CrawlerLog keeps track of which websites were explored, how many\n+        queries were found, and creates a CSV with all the queries. It also", "originalCommit": "cb6e33705d656dfaace43675e9d5f29521dd5fc3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ab00612bcbc8fa90f23c447efe3b89ad323b1640", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/ab00612bcbc8fa90f23c447efe3b89ad323b1640", "message": "Made style and formatting changes to file and variable naming", "committedDate": "2020-06-25T17:57:08Z", "type": "commit"}]}