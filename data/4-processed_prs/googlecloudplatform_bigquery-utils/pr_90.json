{"pr_number": 90, "pr_title": "SQL Crawler: Support for command-line arguments and more SQL expressions, additional test cases", "pr_createdAt": "2020-07-09T20:57:00Z", "pr_url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90", "timeline": [{"oid": "4ece76db6e11336c7092362cbf3b4902b9eee3f5", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/4ece76db6e11336c7092362cbf3b4902b9eee3f5", "message": "Simplified generic extraction by making regex pattern stricter", "committedDate": "2020-07-07T00:05:04Z", "type": "commit"}, {"oid": "d71580d5ec2c2598fdda3e4eb949721c9be25df0", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d71580d5ec2c2598fdda3e4eb949721c9be25df0", "message": "Added more command line args and adjusted defaults", "committedDate": "2020-07-07T00:06:28Z", "type": "commit"}, {"oid": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "message": "Added additional SQL extraction keywords and further tests", "committedDate": "2020-07-09T20:55:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDI4Mg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510282", "bodyText": "Is the list separated by comma or something? maybe add that as well in description", "author": "yzhvictor", "createdAt": "2020-07-09T21:50:48Z", "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')", "originalCommit": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjUxMDc0Mw==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r452510743", "bodyText": "Is this the standard? looks like an array, but it should be an int I think.", "author": "yzhvictor", "createdAt": "2020-07-09T21:51:48Z", "path": "tools/unsupervised_dataset/run_crawler.py", "diffHunk": "@@ -1,11 +1,16 @@\n \"\"\" Script to initialize the SQL crawler on a website of the user's choice \"\"\"\n \n import sys\n+import argparse\n from sql_crawler import crawler\n \n def start_crawler():\n-    urls = sys.argv[1:]\n-    new_crawler = crawler.Crawler(urls, max_size=50)\n+    parser = argparse.ArgumentParser(description=\"SQL Web Crawler\")\n+    parser.add_argument(\"urls\", help=\"A list of URLs to be crawled\", nargs='+')\n+    parser.add_argument(\"--max_depth\", help=\"The max depth of the crawler (default=3)\", type=int, nargs=1, default=[3])\n+    parser.add_argument(\"--max_size\", help=\"The maximum number of links to be crawled (default=100)\", type=int, nargs=1, default=[100])\n+    args = parser.parse_args()\n+    new_crawler = crawler.Crawler(args.urls, max_size=args.max_size[0], max_depth=args.max_depth[0])", "originalCommit": "d5d44cb64260fde4b40b3f6308313a7c5d50c40f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0e6f164af159ef8383c030f20958260fc8ac4854", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/0e6f164af159ef8383c030f20958260fc8ac4854", "message": "Fixed command-line args and updated README", "committedDate": "2020-07-09T22:34:33Z", "type": "commit"}, {"oid": "bcc2fde0767ecc57b9d1e6e61d7976c95343d585", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/bcc2fde0767ecc57b9d1e6e61d7976c95343d585", "message": "Added cloud output integration and tests", "committedDate": "2020-07-16T18:45:09Z", "type": "commit"}, {"oid": "d41c07830266e40768ae5e7f16183de1472c188a", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/d41c07830266e40768ae5e7f16183de1472c188a", "message": "Merge branch 'master' into crawler", "committedDate": "2020-07-16T18:50:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTIxMg==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005212", "bodyText": "Will there be some exceptions such as permission issue, not found issue, should we handle them?", "author": "yzhvictor", "createdAt": "2020-07-16T18:58:47Z", "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()\n+    \n+    return \"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_id, table_id)\n+\n+def upload_gcs_file(project_id, bucket_id, destination_blob_name, filename):\n+    \"\"\" Uploads a file to Google Cloud Storage.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        bucket_id: Name of destination bucket.\n+        destination_blob_name: Name of destination file.\n+        filename: Name of file to be uploaded.\n+\n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+\n+    storage_client = storage.Client(project=project_id)\n+    bucket = storage_client.bucket(bucket_id)\n+    blob = bucket.blob(destination_blob_name)\n+\n+    blob.upload_from_filename(filename)", "originalCommit": "d41c07830266e40768ae5e7f16183de1472c188a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNTg2MQ==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456005861", "bodyText": "Do we get anything from the return of this method call? Or it's just to make sure thread is blocked.", "author": "yzhvictor", "createdAt": "2020-07-16T18:59:52Z", "path": "tools/unsupervised_dataset/sql_crawler/cloud_integration.py", "diffHunk": "@@ -0,0 +1,61 @@\n+\"\"\" Contains functions to upload data to Google Cloud Storage and Google\n+Bigquery. These functions take input specifying where to store the file\n+and return a message denoting success or error.\n+\"\"\"\n+\n+from google.cloud import bigquery, storage\n+\n+def load_bigquery_table(project_id, dataset_id, table_id, filename):\n+    \"\"\" Uploads a file to Google BigQuery.\n+    \n+    Args:\n+        project_id: Google Cloud Project ID for destination.\n+        dataset_id: Name of destination dataset.\n+        table_id: Name of destination table.\n+        filename: Name of CSV file to be uploaded.\n+    \n+    Returns:\n+        A log message depending on success or failure.\n+    \"\"\"\n+    \n+    client = bigquery.Client(project=project_id)\n+    \n+    dataset_ref = client.dataset(dataset_id)\n+    table_ref = dataset_ref.table(table_id)\n+    job_config = bigquery.LoadJobConfig()\n+    job_config.source_format = bigquery.SourceFormat.CSV\n+    job_config.skip_leading_rows = 1\n+    job_config.autodetect = True\n+\n+    job_config.schema = [\n+        bigquery.SchemaField(\"query\", \"STRING\", mode=\"NULLABLE\"),\n+        bigquery.SchemaField(\"url\", \"STRING\", mode=\"NULLABLE\"),\n+    ]\n+    \n+    with open(filename, \"rb\") as source_file:\n+        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n+    \n+    job.result()", "originalCommit": "d41c07830266e40768ae5e7f16183de1472c188a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjAwNzY3Ng==", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/pull/90#discussion_r456007676", "bodyText": "Do we also need to check gcs_bucket, and bq_dataset for the below method?", "author": "yzhvictor", "createdAt": "2020-07-16T19:03:11Z", "path": "tools/unsupervised_dataset/sql_crawler/crawler_log.py", "diffHunk": "@@ -66,11 +71,68 @@ def log_error(self, errorMessage):\n         Args:\n             str: Error message to be logged.\n         \"\"\"\n-\n+        \n+        self.error_log_count += 1\n         logging.error(\"ERROR: %s\", errorMessage)\n+        \n+    def parse_location_arg(self, location):\n+        \"\"\" Validates and splits location argument for cloud upload\n+        into two parts. Should be formatted as project_id.dataset.\n+        \n+        Args:\n+            location: String with name of project ID and dataset.\n+            \n+        Returns\n+            List of separate strings after splitting location.\n+        \"\"\"\n+        if location.count(\".\") != 1:\n+            self.log_error(\"Argument not formatted correctly: {0}\".format(location))\n+            return None, None\n+        \n+        return location.split(\".\")\n+        \n+    def set_gcs(self, location):\n+        \"\"\" Sets variables for uploading data to Google Cloud Storage.\n+            \n+        Args:\n+            location: String with name of project ID and bucket name,\n+            separated by a period.\n+        \"\"\"\n+\n+        self.gcs_project, self.gcs_bucket = self.parse_location_arg(location)\n+        if self.gcs_project:", "originalCommit": "d41c07830266e40768ae5e7f16183de1472c188a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "641e69949e055897527fc874dca797c88a13a759", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/641e69949e055897527fc874dca797c88a13a759", "message": "Added additional error handling", "committedDate": "2020-07-16T20:02:51Z", "type": "commit"}, {"oid": "8b285a472db9e7abb25b0b8778caf3323aa7c20c", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/8b285a472db9e7abb25b0b8778caf3323aa7c20c", "message": "Merge branch 'crawler' of https://github.com/noah-kuo/bigquery-utils into crawler", "committedDate": "2020-07-16T20:43:22Z", "type": "commit"}, {"oid": "6878d722683b8495e95f442401e76626066127a6", "url": "https://github.com/GoogleCloudPlatform/bigquery-utils/commit/6878d722683b8495e95f442401e76626066127a6", "message": "Merge branch 'master' into crawler", "committedDate": "2020-07-20T21:40:21Z", "type": "commit"}]}