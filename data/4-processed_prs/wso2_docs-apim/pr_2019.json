{"pr_number": 2019, "pr_title": "Performance Test Results - Doc for 3.2.0", "pr_createdAt": "2020-10-08T10:43:45Z", "pr_url": "https://github.com/wso2/docs-apim/pull/2019", "timeline": [{"oid": "0074b19220127ed931f8f409281ac1a13f7ec843", "url": "https://github.com/wso2/docs-apim/commit/0074b19220127ed931f8f409281ac1a13f7ec843", "message": "Add draft for performance test results", "committedDate": "2020-10-08T08:48:56Z", "type": "commit"}, {"oid": "12b2e74b77a46ec523cf51bf33b63425d234fbbb", "url": "https://github.com/wso2/docs-apim/commit/12b2e74b77a46ec523cf51bf33b63425d234fbbb", "message": "Add images", "committedDate": "2020-10-08T08:49:22Z", "type": "commit"}, {"oid": "3508efe1e085151c951a14da1ec78c7e4cbecc63", "url": "https://github.com/wso2/docs-apim/commit/3508efe1e085151c951a14da1ec78c7e4cbecc63", "message": "Update doc", "committedDate": "2020-10-08T10:41:17Z", "type": "commit"}, {"oid": "4029fe8d29fa737ed6aa49d5fe8fdb0a3544c480", "url": "https://github.com/wso2/docs-apim/commit/4029fe8d29fa737ed6aa49d5fe8fdb0a3544c480", "message": "Merge branch '3.2.0' of https://github.com/wso2/docs-apim into 3.2.0-perf", "committedDate": "2020-10-08T10:41:24Z", "type": "commit"}, {"oid": "2e2c6c977c3c2e77a284d7d26fcda3893646f0bc", "url": "https://github.com/wso2/docs-apim/commit/2e2c6c977c3c2e77a284d7d26fcda3893646f0bc", "message": "Add doc to ToC", "committedDate": "2020-10-08T11:22:01Z", "type": "commit"}, {"oid": "93d340b7b3b3563132f8d94554b34bac59e70743", "url": "https://github.com/wso2/docs-apim/commit/93d340b7b3b3563132f8d94554b34bac59e70743", "message": "Reviewed and updated full doc", "committedDate": "2020-10-16T15:00:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjUyODM0Mg==", "url": "https://github.com/wso2/docs-apim/pull/2019#discussion_r506528342", "bodyText": "In 3.1.0 and 3.0.0 we have the following breakdown.\n      - Performance Tuning and Test Results:  \n        - Tuning Performance: install-and-setup/setup/deployment-best-practices/tuning-performance.md\n        - Performance Test Results: install-and-setup/setup/deployment-best-practices/performance-test-results.md\n\nDo we need to rename this page to performance-test-results.md and change the heading of the page?\nOr do we need to add a redirection as the file name has changed from 3.1.0?", "author": "Mariangela", "createdAt": "2020-10-16T15:09:04Z", "path": "en/mkdocs.yml", "diffHunk": "@@ -442,6 +442,7 @@ nav:\n                 - Production Deployment Guidelines: install-and-setup/setup/deployment-best-practices/production-deployment-guidelines.md\n                 - Security Guidelines for Production Deployment: install-and-setup/setup/deployment-best-practices/security-guidelines-for-production-deployment.md\n                 - Tuning Performance: install-and-setup/setup/deployment-best-practices/tuning-performance.md\n+                - Performance and Capacity Planning: install-and-setup/setup/deployment-best-practices/performance-and-capacity-planning.md", "originalCommit": "93d340b7b3b3563132f8d94554b34bac59e70743", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjUzMDAyNg==", "url": "https://github.com/wso2/docs-apim/pull/2019#discussion_r506530026", "bodyText": "The file names of the images should not have uppercase letters in it, because based on the current mkdocs setup such images will not appear. Therefore, the actual image names and the references to them in this file need to be changed.", "author": "Mariangela", "createdAt": "2020-10-16T15:11:01Z", "path": "en/docs/install-and-setup/setup/deployment-best-practices/performance-and-capacity-planning.md", "diffHunk": "@@ -0,0 +1,260 @@\n+# WSO2 API-M Performance and Capacity Planning\n+\n+The following sections analyze the results of WSO2 API Manager performance tests.\n+\n+## Summary\n+\n+During each release, WSO2 executes various automated performance test scenarios and publishes the results.\n+\n+| **Test Scenarios** | **Description**                                                      |\n+|------------------|--------------------------------------------------------------------------|\n+| Passthrough      |   A secured API, which directly invokes the back\\-end service\\.          |\n+| Transformation   |   A secured API, which has a mediation extension to modify the message\\. |\n+ \n+\n+WSO2 uses [Apache JMeter](https://jmeter.apache.org/index.html) as the test client. WSO2 tests each scenario for a fixed duration of time. Thereafter, WSO2 splits the test results into warmup and measurement parts and uses the measurement part to compute the performance metrics.\n+\n+Test scenarios use a [Netty](https://netty.io/) based back-end service which echoes back any request posted to it after a specified period of time.\n+\n+WSO2 runs the performance tests with different concurrent user loads, message sizes (payloads), and back-end service delays.\n+\n+The main performance metrics:\n+\n+- **Throughput**: The number of requests that the WSO2 API Manager processes during a specific time interval (e.g. per second).\n+- **Response Time**: The end-to-end latency for an operation of invoking an API. The complete distribution of response times was recorded.\n+\n+In addition to the above metrics, WSO2 measures the load average and several memory-related metrics.\n+\n+The duration of each test is 900 seconds. The warm-up period is 300 seconds. The measurement results are collected after the warm-up period.\n+\n+A [c5.large Amazon EC2 instance](https://aws.amazon.com/ec2/instance-types/) was used to install WSO2 API Manager.\n+\n+## Test parameters\n+<table>\n+  <tr>\n+   <th><strong>Test Parameter</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+   <th><strong>Values</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Scenario Name\n+   </td>\n+   <td>The name of the test scenario.\n+   </td>\n+   <td> - \n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Heap Size\n+   </td>\n+   <td>The amount of memory allocated to the application\n+   </td>\n+   <td>2G\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Concurrent Users\n+   </td>\n+   <td>The number of users accessing the application at the same time.\n+   </td>\n+   <td>50, 100, 200, 300, 500, 1000\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Message Size (Bytes)\n+   </td>\n+   <td>The request payload size in Bytes.\n+   </td>\n+   <td>50, 1024, 10240, 102400\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Back-end Delay (ms)\n+   </td>\n+   <td>The delay added by the back-end service.\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+</table>\n+\n+## Measurements collected\n+The following are the measurements collected from each performance test conducted for a given combination of test parameters.\n+\n+<table>\n+  <tr>\n+   <th><strong>Measurement</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Error %\n+   </td>\n+   <td>The percentage of requests with errors.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Response Time (ms)\n+   </td>\n+   <td>The average response time of a set of results.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Standard Deviation of Response Time (ms)\n+   </td>\n+   <td>The \u201cStandard Deviation\u201d of the response time.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>99th Percentile of Response Time (ms)\n+   </td>\n+   <td>99% of the requests took no more than this time. The remaining samples took at least as long as this.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Throughput (Requests/sec)\n+   </td>\n+   <td>The throughput is measured in requests per second.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Memory Footprint After Full GC (M)\n+   </td>\n+   <td>The average memory consumed by the application after a full garbage collection event.\n+   </td>\n+  </tr>\n+</table>\n+\n+For a detailed analysis of the performance of API-M 3.2.0, see [API-M 3.2.0 Performance graphs on Github](https://github.com/wso2/performance-apim/tree/performance-test-276-2020-08-03_08-47-25/performance/benchmarks/3.2.0%20graphs-all).\n+\n+## Observations from all results\n+\n+There are key observations for the average user scenario of accessing APIs with 1KiB messages and the back-end service having 30ms delay.\n+\n+The following are the key observations from the all performance tests done with different message sizes and different backend delays. (See **Comparison of results** for all charts used to derive the pointed mentioned below)\n+\n+#### Throughput comparison:\n+\n+A throughput increase is observed in the transformation scenario in API-M 3.2.0, in comparison to API-M 3.1.0\n+\n+The throughput increases up to a certain limit when the number of concurrent users increases. The Mediation API throughput increase rate is much lower than the Echo API. Throughput decreases when the message sizes increase. Throughput decreases when the backend sleep time increase. This observation is similar to both APIs. This means that if the backend takes more time, the request processing rate at the API Manager Gateway will be less.\n+\n+#### Key observations related to response time:\n+\n+The average response time increases when the number of concurrent users increases. The increasing rate of average response time for both API-M 3.2.0 and API-M 3.1.0 is similar.\n+\n+The average response time increases when the number of concurrent users increases. The average response time increases considerably for Mediation API when the message sizes increase due to the message processing. The average response time of the Echo API does not increase as much as the Mediation API. The average response time increases when the backend sleep time increases. This observation is similar to both APIs.\n+\n+#### Key observations related to GC throughput:\n+\n+The GC throughput decreases when the number of concurrent users increases. When there are more concurrent users, the object allocation rate increases. The GC throughput increases when the message size increases. The request processing rate slows down due to the time taken to process large messages. Therefore, the object allocation rate decreases  when the message size increases. The GC throughput increases when the backend sleep time increases. The object allocation rate will be low when the backend takes more time to respond.\n+\n+## Comparison of 3.1.0 and 3.2.0\n+\n+### Average response time comparison\n+\n+#### Average response time vs concurrent users\n+\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_1KiB.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_1KiB.png)\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_10KiB.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_10KiB.png)\n+\n+### GC Throughput comparison \n+\n+ [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/gc_throughput_0ms_1KiB.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/gc_throughput_0ms_1KiB.png)", "originalCommit": "93d340b7b3b3563132f8d94554b34bac59e70743", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjUzMTM0OQ==", "url": "https://github.com/wso2/docs-apim/pull/2019#discussion_r506531349", "bodyText": "Best to add values within the [   ] tags which will serve as ALT for the images.", "author": "Mariangela", "createdAt": "2020-10-16T15:12:43Z", "path": "en/docs/install-and-setup/setup/deployment-best-practices/performance-and-capacity-planning.md", "diffHunk": "@@ -0,0 +1,260 @@\n+# WSO2 API-M Performance and Capacity Planning\n+\n+The following sections analyze the results of WSO2 API Manager performance tests.\n+\n+## Summary\n+\n+During each release, WSO2 executes various automated performance test scenarios and publishes the results.\n+\n+| **Test Scenarios** | **Description**                                                      |\n+|------------------|--------------------------------------------------------------------------|\n+| Passthrough      |   A secured API, which directly invokes the back\\-end service\\.          |\n+| Transformation   |   A secured API, which has a mediation extension to modify the message\\. |\n+ \n+\n+WSO2 uses [Apache JMeter](https://jmeter.apache.org/index.html) as the test client. WSO2 tests each scenario for a fixed duration of time. Thereafter, WSO2 splits the test results into warmup and measurement parts and uses the measurement part to compute the performance metrics.\n+\n+Test scenarios use a [Netty](https://netty.io/) based back-end service which echoes back any request posted to it after a specified period of time.\n+\n+WSO2 runs the performance tests with different concurrent user loads, message sizes (payloads), and back-end service delays.\n+\n+The main performance metrics:\n+\n+- **Throughput**: The number of requests that the WSO2 API Manager processes during a specific time interval (e.g. per second).\n+- **Response Time**: The end-to-end latency for an operation of invoking an API. The complete distribution of response times was recorded.\n+\n+In addition to the above metrics, WSO2 measures the load average and several memory-related metrics.\n+\n+The duration of each test is 900 seconds. The warm-up period is 300 seconds. The measurement results are collected after the warm-up period.\n+\n+A [c5.large Amazon EC2 instance](https://aws.amazon.com/ec2/instance-types/) was used to install WSO2 API Manager.\n+\n+## Test parameters\n+<table>\n+  <tr>\n+   <th><strong>Test Parameter</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+   <th><strong>Values</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Scenario Name\n+   </td>\n+   <td>The name of the test scenario.\n+   </td>\n+   <td> - \n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Heap Size\n+   </td>\n+   <td>The amount of memory allocated to the application\n+   </td>\n+   <td>2G\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Concurrent Users\n+   </td>\n+   <td>The number of users accessing the application at the same time.\n+   </td>\n+   <td>50, 100, 200, 300, 500, 1000\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Message Size (Bytes)\n+   </td>\n+   <td>The request payload size in Bytes.\n+   </td>\n+   <td>50, 1024, 10240, 102400\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Back-end Delay (ms)\n+   </td>\n+   <td>The delay added by the back-end service.\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+</table>\n+\n+## Measurements collected\n+The following are the measurements collected from each performance test conducted for a given combination of test parameters.\n+\n+<table>\n+  <tr>\n+   <th><strong>Measurement</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Error %\n+   </td>\n+   <td>The percentage of requests with errors.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Response Time (ms)\n+   </td>\n+   <td>The average response time of a set of results.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Standard Deviation of Response Time (ms)\n+   </td>\n+   <td>The \u201cStandard Deviation\u201d of the response time.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>99th Percentile of Response Time (ms)\n+   </td>\n+   <td>99% of the requests took no more than this time. The remaining samples took at least as long as this.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Throughput (Requests/sec)\n+   </td>\n+   <td>The throughput is measured in requests per second.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Memory Footprint After Full GC (M)\n+   </td>\n+   <td>The average memory consumed by the application after a full garbage collection event.\n+   </td>\n+  </tr>\n+</table>\n+\n+For a detailed analysis of the performance of API-M 3.2.0, see [API-M 3.2.0 Performance graphs on Github](https://github.com/wso2/performance-apim/tree/performance-test-276-2020-08-03_08-47-25/performance/benchmarks/3.2.0%20graphs-all).\n+\n+## Observations from all results\n+\n+There are key observations for the average user scenario of accessing APIs with 1KiB messages and the back-end service having 30ms delay.\n+\n+The following are the key observations from the all performance tests done with different message sizes and different backend delays. (See **Comparison of results** for all charts used to derive the pointed mentioned below)\n+\n+#### Throughput comparison:\n+\n+A throughput increase is observed in the transformation scenario in API-M 3.2.0, in comparison to API-M 3.1.0\n+\n+The throughput increases up to a certain limit when the number of concurrent users increases. The Mediation API throughput increase rate is much lower than the Echo API. Throughput decreases when the message sizes increase. Throughput decreases when the backend sleep time increase. This observation is similar to both APIs. This means that if the backend takes more time, the request processing rate at the API Manager Gateway will be less.\n+\n+#### Key observations related to response time:\n+\n+The average response time increases when the number of concurrent users increases. The increasing rate of average response time for both API-M 3.2.0 and API-M 3.1.0 is similar.\n+\n+The average response time increases when the number of concurrent users increases. The average response time increases considerably for Mediation API when the message sizes increase due to the message processing. The average response time of the Echo API does not increase as much as the Mediation API. The average response time increases when the backend sleep time increases. This observation is similar to both APIs.\n+\n+#### Key observations related to GC throughput:\n+\n+The GC throughput decreases when the number of concurrent users increases. When there are more concurrent users, the object allocation rate increases. The GC throughput increases when the message size increases. The request processing rate slows down due to the time taken to process large messages. Therefore, the object allocation rate decreases  when the message size increases. The GC throughput increases when the backend sleep time increases. The object allocation rate will be low when the backend takes more time to respond.\n+\n+## Comparison of 3.1.0 and 3.2.0\n+\n+### Average response time comparison\n+\n+#### Average response time vs concurrent users\n+\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)", "originalCommit": "93d340b7b3b3563132f8d94554b34bac59e70743", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjU1NTQ5Ng==", "url": "https://github.com/wso2/docs-apim/pull/2019#discussion_r506555496", "bodyText": "Do we need any information from [1] like the \"Performance test scripts\" section, some info from the \"Deployment used for the test\" section?\n[1] https://github.com/wso2/docs-apim/blob/3.1.0/en/docs/install-and-setup/setup/deployment-best-practices/performance-test-results.md", "author": "Mariangela", "createdAt": "2020-10-16T15:39:37Z", "path": "en/docs/install-and-setup/setup/deployment-best-practices/performance-and-capacity-planning.md", "diffHunk": "@@ -0,0 +1,260 @@\n+# WSO2 API-M Performance and Capacity Planning\n+\n+The following sections analyze the results of WSO2 API Manager performance tests.\n+\n+## Summary\n+\n+During each release, WSO2 executes various automated performance test scenarios and publishes the results.\n+\n+| **Test Scenarios** | **Description**                                                      |\n+|------------------|--------------------------------------------------------------------------|\n+| Passthrough      |   A secured API, which directly invokes the back\\-end service\\.          |\n+| Transformation   |   A secured API, which has a mediation extension to modify the message\\. |\n+ \n+\n+WSO2 uses [Apache JMeter](https://jmeter.apache.org/index.html) as the test client. WSO2 tests each scenario for a fixed duration of time. Thereafter, WSO2 splits the test results into warmup and measurement parts and uses the measurement part to compute the performance metrics.\n+\n+Test scenarios use a [Netty](https://netty.io/) based back-end service which echoes back any request posted to it after a specified period of time.\n+\n+WSO2 runs the performance tests with different concurrent user loads, message sizes (payloads), and back-end service delays.\n+\n+The main performance metrics:\n+\n+- **Throughput**: The number of requests that the WSO2 API Manager processes during a specific time interval (e.g. per second).\n+- **Response Time**: The end-to-end latency for an operation of invoking an API. The complete distribution of response times was recorded.\n+\n+In addition to the above metrics, WSO2 measures the load average and several memory-related metrics.\n+\n+The duration of each test is 900 seconds. The warm-up period is 300 seconds. The measurement results are collected after the warm-up period.\n+\n+A [c5.large Amazon EC2 instance](https://aws.amazon.com/ec2/instance-types/) was used to install WSO2 API Manager.", "originalCommit": "93d340b7b3b3563132f8d94554b34bac59e70743", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjU1ODM0Mw==", "url": "https://github.com/wso2/docs-apim/pull/2019#discussion_r506558343", "bodyText": "When defining images, always make sure to add the zoom option using the following format similar to how I have updated all the image definitions in this PR.\n[![ALT text](image-path/image-name)](image-path/image-name)", "author": "Mariangela", "createdAt": "2020-10-16T15:42:45Z", "path": "en/docs/install-and-setup/setup/deployment-best-practices/performance-and-capacity-planning.md", "diffHunk": "@@ -0,0 +1,260 @@\n+# WSO2 API-M Performance and Capacity Planning\n+\n+The following sections analyze the results of WSO2 API Manager performance tests.\n+\n+## Summary\n+\n+During each release, WSO2 executes various automated performance test scenarios and publishes the results.\n+\n+| **Test Scenarios** | **Description**                                                      |\n+|------------------|--------------------------------------------------------------------------|\n+| Passthrough      |   A secured API, which directly invokes the back\\-end service\\.          |\n+| Transformation   |   A secured API, which has a mediation extension to modify the message\\. |\n+ \n+\n+WSO2 uses [Apache JMeter](https://jmeter.apache.org/index.html) as the test client. WSO2 tests each scenario for a fixed duration of time. Thereafter, WSO2 splits the test results into warmup and measurement parts and uses the measurement part to compute the performance metrics.\n+\n+Test scenarios use a [Netty](https://netty.io/) based back-end service which echoes back any request posted to it after a specified period of time.\n+\n+WSO2 runs the performance tests with different concurrent user loads, message sizes (payloads), and back-end service delays.\n+\n+The main performance metrics:\n+\n+- **Throughput**: The number of requests that the WSO2 API Manager processes during a specific time interval (e.g. per second).\n+- **Response Time**: The end-to-end latency for an operation of invoking an API. The complete distribution of response times was recorded.\n+\n+In addition to the above metrics, WSO2 measures the load average and several memory-related metrics.\n+\n+The duration of each test is 900 seconds. The warm-up period is 300 seconds. The measurement results are collected after the warm-up period.\n+\n+A [c5.large Amazon EC2 instance](https://aws.amazon.com/ec2/instance-types/) was used to install WSO2 API Manager.\n+\n+## Test parameters\n+<table>\n+  <tr>\n+   <th><strong>Test Parameter</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+   <th><strong>Values</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Scenario Name\n+   </td>\n+   <td>The name of the test scenario.\n+   </td>\n+   <td> - \n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Heap Size\n+   </td>\n+   <td>The amount of memory allocated to the application\n+   </td>\n+   <td>2G\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Concurrent Users\n+   </td>\n+   <td>The number of users accessing the application at the same time.\n+   </td>\n+   <td>50, 100, 200, 300, 500, 1000\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Message Size (Bytes)\n+   </td>\n+   <td>The request payload size in Bytes.\n+   </td>\n+   <td>50, 1024, 10240, 102400\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Back-end Delay (ms)\n+   </td>\n+   <td>The delay added by the back-end service.\n+   </td>\n+   <td>0\n+   </td>\n+  </tr>\n+</table>\n+\n+## Measurements collected\n+The following are the measurements collected from each performance test conducted for a given combination of test parameters.\n+\n+<table>\n+  <tr>\n+   <th><strong>Measurement</strong>\n+   </th>\n+   <th><strong>Description</strong>\n+   </th>\n+  </tr>\n+  <tr>\n+   <td>Error %\n+   </td>\n+   <td>The percentage of requests with errors.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Response Time (ms)\n+   </td>\n+   <td>The average response time of a set of results.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Standard Deviation of Response Time (ms)\n+   </td>\n+   <td>The \u201cStandard Deviation\u201d of the response time.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>99th Percentile of Response Time (ms)\n+   </td>\n+   <td>99% of the requests took no more than this time. The remaining samples took at least as long as this.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Throughput (Requests/sec)\n+   </td>\n+   <td>The throughput is measured in requests per second.\n+   </td>\n+  </tr>\n+  <tr>\n+   <td>Average Memory Footprint After Full GC (M)\n+   </td>\n+   <td>The average memory consumed by the application after a full garbage collection event.\n+   </td>\n+  </tr>\n+</table>\n+\n+For a detailed analysis of the performance of API-M 3.2.0, see [API-M 3.2.0 Performance graphs on Github](https://github.com/wso2/performance-apim/tree/performance-test-276-2020-08-03_08-47-25/performance/benchmarks/3.2.0%20graphs-all).\n+\n+## Observations from all results\n+\n+There are key observations for the average user scenario of accessing APIs with 1KiB messages and the back-end service having 30ms delay.\n+\n+The following are the key observations from the all performance tests done with different message sizes and different backend delays. (See **Comparison of results** for all charts used to derive the pointed mentioned below)\n+\n+#### Throughput comparison:\n+\n+A throughput increase is observed in the transformation scenario in API-M 3.2.0, in comparison to API-M 3.1.0\n+\n+The throughput increases up to a certain limit when the number of concurrent users increases. The Mediation API throughput increase rate is much lower than the Echo API. Throughput decreases when the message sizes increase. Throughput decreases when the backend sleep time increase. This observation is similar to both APIs. This means that if the backend takes more time, the request processing rate at the API Manager Gateway will be less.\n+\n+#### Key observations related to response time:\n+\n+The average response time increases when the number of concurrent users increases. The increasing rate of average response time for both API-M 3.2.0 and API-M 3.1.0 is similar.\n+\n+The average response time increases when the number of concurrent users increases. The average response time increases considerably for Mediation API when the message sizes increase due to the message processing. The average response time of the Echo API does not increase as much as the Mediation API. The average response time increases when the backend sleep time increases. This observation is similar to both APIs.\n+\n+#### Key observations related to GC throughput:\n+\n+The GC throughput decreases when the number of concurrent users increases. When there are more concurrent users, the object allocation rate increases. The GC throughput increases when the message size increases. The request processing rate slows down due to the time taken to process large messages. Therefore, the object allocation rate decreases  when the message size increases. The GC throughput increases when the backend sleep time increases. The object allocation rate will be low when the backend takes more time to respond.\n+\n+## Comparison of 3.1.0 and 3.2.0\n+\n+### Average response time comparison\n+\n+#### Average response time vs concurrent users\n+\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_50B.png)\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_1KiB.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_1KiB.png)\n+  [![]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_10KiB.png)]({{base_path}}/assets/img/setup-and-install/performance-test-results/average_time_0ms_10KiB.png)", "originalCommit": "93d340b7b3b3563132f8d94554b34bac59e70743", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4149db5a544213a5a42008033e38d9ee0c0da259", "url": "https://github.com/wso2/docs-apim/commit/4149db5a544213a5a42008033e38d9ee0c0da259", "message": "Change image names to lowercase", "committedDate": "2020-11-04T15:02:13Z", "type": "commit"}, {"oid": "eb4c668d389a2205a6484adb04e27ddb7db37df5", "url": "https://github.com/wso2/docs-apim/commit/eb4c668d389a2205a6484adb04e27ddb7db37df5", "message": "Merge branch '3.2.0' of https://github.com/wso2/docs-apim into 3.2.0-perf", "committedDate": "2020-11-04T15:07:00Z", "type": "commit"}]}