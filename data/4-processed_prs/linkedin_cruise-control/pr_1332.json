{"pr_number": 1332, "pr_title": "Support handling planned maintenance events submitted via a topic", "pr_createdAt": "2020-09-26T04:38:07Z", "pr_url": "https://github.com/linkedin/cruise-control/pull/1332", "timeline": [{"oid": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "url": "https://github.com/linkedin/cruise-control/commit/aaf061405c443864f36d2f804a05ee1a02a6dde5", "message": "Support handling planned maintenance events submitted via a topic", "committedDate": "2020-09-26T04:33:53Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NDQ5Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496344496", "bodyText": "KMF aka XinfraMonitor has an interface which abstracts away topic creation.  This is sometimes useful if you need to stub out a noop topic creation or specific environments require additional steps for topic creation.", "author": "smccauliff", "createdAt": "2020-09-29T02:39:38Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/KafkaCruiseControlUtils.java", "diffHunk": "@@ -168,6 +195,194 @@ public static String getRequiredConfig(Map<String, ?> configs, String configName\n     return value;\n   }\n \n+  /**\n+   * Creates the given topic if it does not exist.\n+   *\n+   * @param adminClient The adminClient to send createTopics request.\n+   * @param topicToBeCreated A wrapper around the topic to be created.\n+   * @return {@code false} if the topic to be created already exists, {@code true} otherwise.\n+   */\n+  public static boolean createTopic(AdminClient adminClient, NewTopic topicToBeCreated) {", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTc3OA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215778", "bodyText": "In Cruise Control, components that create topics are already pluggable. Hence, it abstracts away the topic creation and update logic on the relevant caller end (e.g. SampleStore, MaintenanceEventReader).", "author": "efeg", "createdAt": "2020-09-30T03:01:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NDQ5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0ODAyMw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496348023", "bodyText": "What happens when the partition is empty, not containing any offsets?", "author": "smccauliff", "createdAt": "2020-09-29T02:53:45Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/KafkaCruiseControlUtils.java", "diffHunk": "@@ -168,6 +195,194 @@ public static String getRequiredConfig(Map<String, ?> configs, String configName\n     return value;\n   }\n \n+  /**\n+   * Creates the given topic if it does not exist.\n+   *\n+   * @param adminClient The adminClient to send createTopics request.\n+   * @param topicToBeCreated A wrapper around the topic to be created.\n+   * @return {@code false} if the topic to be created already exists, {@code true} otherwise.\n+   */\n+  public static boolean createTopic(AdminClient adminClient, NewTopic topicToBeCreated) {\n+    try {\n+      CreateTopicsResult createTopicsResult = adminClient.createTopics(Collections.singletonList(topicToBeCreated));\n+      createTopicsResult.values().get(topicToBeCreated.name()).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+      LOG.info(\"Topic {} has been created.\", topicToBeCreated.name());\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      if (e.getCause() instanceof TopicExistsException) {\n+        return false;\n+      }\n+      throw new IllegalStateException(String.format(\"Unable to create topic %s.\", topicToBeCreated.name()), e);\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   *\n+   * @param topic The name of the topic.\n+   * @param partitionCount Desired partition count.\n+   * @param replicationFactor Desired replication factor.\n+   * @param retentionMs Desired retention in milliseconds.\n+   * @return A wrapper around the topic with the given desired properties.\n+   */\n+  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n+    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n+      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n+                                                       + \" must be positive for the topic (%s).\", partitionCount,\n+                                                       replicationFactor, retentionMs, topic));\n+    }\n+\n+    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n+    Map<String, String> config = new HashMap<>(2);\n+    config.put(RetentionMsProp(), Long.toString(retentionMs));\n+    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n+    newTopic.configs(config);\n+\n+    return newTopic;\n+  }\n+\n+  /**\n+   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n+   *\n+   * @param configsToAlter A set of config altering operations to be populated.\n+   * @param desiredConfig Desired config value by name.\n+   * @param currentConfig Current config.\n+   */\n+  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n+    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n+      String configName = entry.getKey();\n+      String targetConfigValue = entry.getValue();\n+      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n+      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n+        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Update topic configurations with the desired configs specified in the given topicToUpdateConfigs.\n+   *\n+   * @param adminClient The adminClient to send describeConfigs and incrementalAlterConfigs requests.\n+   * @param topicToUpdateConfigs Existing topic to update selected configs if needed -- cannot be {@code null}.\n+   * @return {@code true} if the request is completed successfully, {@code false} if there are any exceptions.\n+   */\n+  public static boolean maybeUpdateTopicConfig(AdminClient adminClient, NewTopic topicToUpdateConfigs) {\n+    String topicName = topicToUpdateConfigs.name();\n+    // Retrieve topic config to check if it needs an update.\n+    ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topicName);\n+    DescribeConfigsResult describeConfigsResult = adminClient.describeConfigs(Collections.singleton(topicResource));\n+    Config topicConfig;\n+    try {\n+      topicConfig = describeConfigsResult.values().get(topicResource).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      LOG.warn(\"Config check for topic {} failed due to failure to describe its configs.\", topicName, e);\n+      return false;\n+    }\n+\n+    // Update configs if needed.\n+    Map<String, String> desiredConfig = topicToUpdateConfigs.configs();\n+    if (desiredConfig != null) {\n+      Set<AlterConfigOp> alterConfigOps = new HashSet<>(desiredConfig.size());\n+      maybeUpdateConfig(alterConfigOps, desiredConfig, topicConfig);\n+      if (!alterConfigOps.isEmpty()) {\n+        AlterConfigsResult alterConfigsResult\n+            = adminClient.incrementalAlterConfigs(Collections.singletonMap(topicResource, alterConfigOps));\n+        try {\n+          alterConfigsResult.values().get(topicResource).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+        } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+          LOG.warn(\"Config change for topic {} failed.\", topicName, e);\n+          return false;\n+        }\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Increase the partition count of the given existing topic to the desired partition count (if needed).\n+   *\n+   * @param adminClient The adminClient to send describeTopics and createPartitions requests.\n+   * @param topicToAddPartitions Existing topic to add more partitions if needed -- cannot be {@code null}.\n+   * @return {@code true} if the request is completed successfully, {@code false} if there are any exceptions.\n+   */\n+  public static boolean maybeIncreasePartitionCount(AdminClient adminClient, NewTopic topicToAddPartitions) {\n+    String topicName = topicToAddPartitions.name();\n+\n+    // Retrieve partition count of topic to check if it needs a partition count update.\n+    TopicDescription topicDescription;\n+    try {\n+      topicDescription = adminClient.describeTopics(Collections.singletonList(topicName)).values()\n+                                    .get(topicName).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+    } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+      LOG.warn(\"Partition count increase check for topic {} failed due to failure to describe cluster.\", topicName, e);\n+      return false;\n+    }\n+\n+    // Update partition count of topic if needed.\n+    if (topicDescription.partitions().size() < topicToAddPartitions.numPartitions()) {\n+      CreatePartitionsResult createPartitionsResult = adminClient.createPartitions(\n+          Collections.singletonMap(topicName, NewPartitions.increaseTo(topicToAddPartitions.numPartitions())));\n+\n+      try {\n+        createPartitionsResult.values().get(topicName).get(CLIENT_REQUEST_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n+      } catch (InterruptedException | ExecutionException | TimeoutException e) {\n+        LOG.warn(\"Partition count increase to {} for topic {} failed{}.\", topicToAddPartitions.numPartitions(), topicName,\n+                 (e.getCause() instanceof ReassignmentInProgressException) ? \" due to ongoing reassignment\" : \"\", e);\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Sanity check whether there are failures in partition offsets fetched by a consumer. This typically happens due to\n+   * transient network failures (e.g. Error sending fetch request XXX to node XXX: org.apache.kafka.common.errors.DisconnectException.)\n+   * that prevents the consumer from getting offset from some brokers for as long as reconnect.backoff.ms.\n+   *\n+   * @param endOffsets End offsets retrieved by consumer.\n+   * @param offsetsForTimes Offsets for times retrieved by consumer.\n+   */\n+  public static void sanityCheckOffsetFetch(Map<TopicPartition, Long> endOffsets,\n+                                            Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes)\n+      throws SamplingException {\n+    Set<TopicPartition> failedToFetchOffsets = new HashSet<>();\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      if (entry.getValue() == null && endOffsets.get(entry.getKey()) == null) {", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTc4Nw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215787", "bodyText": "If a partition has never been written to, its end offset is expected to be0. (see KafkaConsumer#endOffsets(Collection)).\nIf earliest offset whose timestamp is greater than or equal to the given timestamp in the corresponding partition does not exist, null is expected to be returned for the partition. (see KafkaConsumer#offsetsForTimes(Map)).\n\nConsequently, this sanity check will succeed. -- updated the JavaDoc to indicate these parameter assumptions.", "author": "efeg", "createdAt": "2020-09-30T03:01:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0ODAyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MjI5MA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496352290", "bodyText": "Do you want to add a magic number and/or CRC to detect corruption?", "author": "smccauliff", "createdAt": "2020-09-29T03:10:22Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AddBrokerPlan.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+\n+public class AddBrokerPlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+  private final Set<Integer> _brokers;\n+\n+  public AddBrokerPlan(long timeMs, int brokerId, Set<Integer> brokers) {\n+    super(MaintenanceEventType.ADD_BROKER, timeMs, brokerId);\n+    if (brokers == null || brokers.isEmpty()) {\n+      throw new IllegalArgumentException(\"Missing brokers for the plan.\");\n+    }\n+    if (brokers.size() > Short.MAX_VALUE) {\n+      throw new IllegalArgumentException(String.format(\"Cannot add more than %d brokers (attempt: %d).\",\n+                                                       Short.MAX_VALUE, brokers.size()));\n+    }\n+    _brokers = brokers;\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  public Set<Integer> brokers() {\n+    return _brokers;\n+  }\n+\n+  @Override\n+  public ByteBuffer toBuffer(int headerSize) {", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTgwMQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215801", "bodyText": "I believe we already have a magic number for each plan (please see MaintenancePlan#planVersion()) -- am I missing something?\n-- Added crc to verify the integrity -- thanks for the suggestion!", "author": "efeg", "createdAt": "2020-09-30T03:01:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MjI5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzNjMxOQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497236319", "bodyText": "The magic number is there just incase someone produces garbage into your topic.  This dosen't happen often but it does happen.  For example our internal message format has a magic byte '0' there have been times when 1/256 of the invalid messages that where incorrectly produced into a topic would pass this check.  This can sometimes lead to OOM when the bytes are deserialized as the length field is bad.  For example this happens when client is not configured with TLS then connects to the TLS port on the broker.  It interprets the TLS handshake as some length field and the client OOMs. Having a magic number (preferably more than one byte) helps prevent this.  CRC should prevent this as well.", "author": "smccauliff", "createdAt": "2020-09-30T04:28:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MjI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496353768", "bodyText": "Randomly generating the group id makes it impossible to track offset commits using lagmontor.  Randomly generating the client id breaks quotas based on client ids.  Also each time there is a new client id a new set of metrics are generated on both the broker and the client itself.  Since the brokers are probably longer lived than the clients this results is accumulated garbage.  Open source kafka does periodically clean it out but for some reason we've opted to keep them around in memory forever.  Recommend never having random numbers be part of the group or client ids.", "author": "smccauliff", "createdAt": "2020-09-29T03:16:34Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AnomalyDetectorUtils.java", "diffHunk": "@@ -28,10 +38,42 @@\n   public static final String ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG = \"anomaly.detection.time.ms.object\";\n   public static final long MAX_METADATA_WAIT_MS = 60000L;\n   public static final Anomaly SHUTDOWN_ANOMALY = new BrokerFailures();\n+  public static final Random RANDOM = new Random();\n \n   private AnomalyDetectorUtils() {\n   }\n \n+  /**\n+   * Create a Kafka consumer for retrieving reported Maintenance plans.\n+   * The consumer uses {@link String} for keys and {@link MaintenancePlan} for values.\n+   *\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n+   */\n+  public static Consumer<String, MaintenancePlan> createMaintenanceEventConsumer(Map<String, ?> configs, String groupIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.\n+    if (bootstrapServers.length() > 2) {\n+      bootstrapServers = bootstrapServers.substring(1, bootstrapServers.length() - 1);\n+    }\n+\n+    // Create consumer\n+    long randomToken = RANDOM.nextLong();\n+    Properties consumerProps = new Properties();\n+    consumerProps.putAll(configs);\n+    consumerProps.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+    consumerProps.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupIdPrefix + randomToken);\n+    consumerProps.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, groupIdPrefix + \"-consumer-\" + randomToken);", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTgyOQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215829", "bodyText": "TL;DR Dropping group ids, but keeping random client ids.\n--\n\nRandomly generating the group id makes it impossible to track offset commits using lagmontor\n\nThis consumer uses neither the group management functionality by using subscribe(topic) nor the Kafka-based offset management strategy. Hence, the group.id config is irrelevant. (Added a clarification to JavaDoc and dropped this setting).\n\nRandomly generating the client id ...\n\nI see your point, but making this config deterministic is tricky:\nIf multiple CC instances point to the same cluster, which is a valid usage today (i.e. one active and multiple passive CC instances, for which all self-healing is disabled and ReadOnlySampleStore is used), then this will result in duplicate sensors for different clients. I will also share example internal issues caused by using the same client id for different consumers.", "author": "efeg", "createdAt": "2020-09-30T03:01:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzMDgwMA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497230800", "bodyText": "There's usually some i00X number used to identify different instances, this can be part of the client id.  Other people can have other solutions to this problem.  Host name?\nSo CC never commits offsets into Kafka?  The offset management and the consumer group coordination can be used independently from each other.", "author": "smccauliff", "createdAt": "2020-09-30T04:04:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzMTg3Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497231876", "bodyText": "The previous code review was based on emitting the output in log messages.  I guess it's fine, but it seems strange because it's already a List it's possible to do the following in single line.\n  list.stream().collect(Collectors.joining( \",\" ) )", "author": "smccauliff", "createdAt": "2020-09-30T04:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzMjk0NA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497232944", "bodyText": "The i00X string can be added to different cruise control instances.  For those outside of LinkedIn their deployment system may have something like this or they can add hostname or something else.", "author": "smccauliff", "createdAt": "2020-09-30T04:13:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MTYzNg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497761636", "bodyText": "There's usually some i00X number used to identify different instances, this can be part of the client id. Other people can have other solutions to this problem. Host name?\n\n\nThe i00X string can be added to different cruise control instances. For those outside of LinkedIn their deployment system may have something like this or they can add hostname or something else.\n\nEven if a local (i.e. dev) deployment has a i00X string, I suspect that this would not suffice to distinguish clients for multiple local deployments pointing to randomly configured (i.e. different) destination clusters (e.g. for testing).\nIf possible, I would prefer a solution that is applicable to both internal and open source. I believe there are at least 3 pieces that should go into the client id: (1) source host, (2) destination cluster, and (3) whether CC is the active instance or a passive one (and if passive, which specific passive instance).\nPresumably, this change is not directly relevant to this PR other than continue using the existing approach. I created an issue about this discussion and propose to address this on a later patch: #1337\n\nSo CC never commits offsets into Kafka? The offset management and the consumer group coordination can be used independently from each other.\n\nThat is correct.", "author": "efeg", "createdAt": "2020-09-30T19:51:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Mzc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496354558", "bodyText": "The output of this method is not guaranteed to be stable across Java versions.", "author": "smccauliff", "createdAt": "2020-09-29T03:19:43Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/AnomalyDetectorUtils.java", "diffHunk": "@@ -28,10 +38,42 @@\n   public static final String ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG = \"anomaly.detection.time.ms.object\";\n   public static final long MAX_METADATA_WAIT_MS = 60000L;\n   public static final Anomaly SHUTDOWN_ANOMALY = new BrokerFailures();\n+  public static final Random RANDOM = new Random();\n \n   private AnomalyDetectorUtils() {\n   }\n \n+  /**\n+   * Create a Kafka consumer for retrieving reported Maintenance plans.\n+   * The consumer uses {@link String} for keys and {@link MaintenancePlan} for values.\n+   *\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n+   */\n+  public static Consumer<String, MaintenancePlan> createMaintenanceEventConsumer(Map<String, ?> configs, String groupIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTg1NA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496359854", "bodyText": "ArrayList.toString() has a specification, but that's not true for all implementations.", "author": "smccauliff", "createdAt": "2020-09-29T03:42:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTg1Mg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215852", "bodyText": "I believe we discussed a similar issue under a different PR: #1223 (comment)\nIt is true that the List interface itself does not enforce a specific toString() response format, but I don't suppose we expect to use / support collections that are not based on the standard https://docs.oracle.com/javase/7/docs/api/java/util/AbstractCollection.html#toString(), whose openJDK implementation for toString() hasn't changed from 8 to 11?", "author": "efeg", "createdAt": "2020-09-30T03:01:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzMjE1NA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497232154", "bodyText": "The previous code review was based on emitting the output in log messages.  I guess it's fine, but it seems strange because it's already a List it's possible to do the following in single line.\n  list.stream().collect(Collectors.joining( \",\" ) )", "author": "smccauliff", "createdAt": "2020-09-30T04:10:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MTcxMg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497761712", "bodyText": "I propose the following, which simplifies the statement w/o using the stream API.\n    String bootstrapServers = String.join(\",\", (List<String>) configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG));", "author": "efeg", "createdAt": "2020-09-30T19:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NDU1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Njk2MQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496356961", "bodyText": "I've always thought this was strange as this can be implemented as\n  byte id() {\n    return (byte) ordinal();\n  }", "author": "smccauliff", "createdAt": "2020-09-29T03:30:01Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventType.java", "diffHunk": "@@ -23,7 +23,38 @@\n  * </ul>\n  */\n public enum MaintenanceEventType {\n-  ADD_BROKER, REMOVE_BROKER, FIX_OFFLINE_REPLICAS, REBALANCE, DEMOTE_BROKER, TOPIC_REPLICATION_FACTOR;\n+  // Do not change the order of enums. Append new ones to the end.\n+  ADD_BROKER((byte) 0),\n+  REMOVE_BROKER((byte) 1),\n+  FIX_OFFLINE_REPLICAS((byte) 2),\n+  REBALANCE((byte) 3),\n+  DEMOTE_BROKER((byte) 4),\n+  TOPIC_REPLICATION_FACTOR((byte) 5);\n+\n+  // This id helps with serialization and deserialization of event types\n+  private final byte _id;", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTg3Mg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215872", "bodyText": "This is indeed cleaner -- thanks.", "author": "efeg", "createdAt": "2020-09-30T03:01:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1Njk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzM5NQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496357395", "bodyText": "Seems like this protocol is going to have many different types of messages, but low QPS.  Seems like a good use case for some kind of serialization library.", "author": "smccauliff", "createdAt": "2020-09-29T03:31:46Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenancePlan.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import java.nio.ByteBuffer;\n+\n+\n+/**\n+ * An abstract class for all maintenance plans.\n+ */\n+public abstract class MaintenancePlan {\n+  protected final MaintenanceEventType _maintenanceEventType;\n+  protected final long _timeMs;\n+  protected final int _brokerId;\n+\n+  public MaintenancePlan(MaintenanceEventType maintenanceEventType, long timeMs, int brokerId) {\n+    _maintenanceEventType = maintenanceEventType;\n+    _timeMs = timeMs;\n+    _brokerId = brokerId;\n+  }\n+\n+  /**\n+   * Retrieve the maintenance event type of this maintenance plan.\n+   *\n+   * @return the maintenance event type of this maintenance plan, which is stored in the serialized metrics so\n+   * that the deserializer will know which class should be used to deserialize the data.\n+   */\n+  public MaintenanceEventType maintenanceEventType() {\n+    return _maintenanceEventType;\n+  }\n+\n+  /**\n+   * @return The timestamp in ms that corresponds to the generation of this plan.\n+   */\n+  public long timeMs() {\n+    return _timeMs;\n+  }\n+\n+  /**\n+   * @return The id of the broker that reported this maintenance event.\n+   */\n+  public int brokerId() {\n+    return _brokerId;\n+  }\n+\n+  /**\n+   * @return The plan version for serialization/deserialization.\n+   */\n+  public abstract byte planVersion();\n+\n+  /**\n+   * Serialize the maintenance plan to a byte buffer with the header size reserved.\n+   *\n+   * @param headerSize the header size to reserve.\n+   * @return A {@link ByteBuffer} with header size reserved at the beginning.\n+   */\n+  public ByteBuffer toBuffer(int headerSize) {", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTg4Mw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215883", "bodyText": "Can you comment a little more on the advantages of using a serialization library as opposed to the current approach?\nI feel that we may not get the same level of efficiency and may need to require users to have an extra dependency on their end -- and potentially on our end depending on the selected library.", "author": "efeg", "createdAt": "2020-09-30T03:01:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzNDUzMw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497234533", "bodyText": "You probably won't gain any efficiency.  Maybe protobuf since the integers are variable length encoded.  You gain maintainability.   Since this is low qps there's no need for efficiency here.  Gson looks like it is easy to use and has minimal dependencies. The real win is not when you write the initial code it's when you need to change the code: add additional messages, add additional fields, need to reason about versioning, need to debug messages by actually reading their contents.", "author": "smccauliff", "createdAt": "2020-09-30T04:21:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzkxOTM4MQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497919381", "bodyText": "Makes sense -- updated the code to use Gson for de/serialization of maintenance plans.", "author": "efeg", "createdAt": "2020-10-01T01:35:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NzM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTE2MA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496359160", "bodyText": "See comment about group id", "author": "smccauliff", "createdAt": "2020-09-29T03:39:29Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/SamplingUtils.java", "diffHunk": "@@ -346,124 +315,70 @@ private static boolean skipBuildingBrokerMetricSample(BrokerLoad brokerLoad, int\n   }\n \n   /**\n-   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   * Create a Kafka consumer for retrieving reported Cruise Control metrics.\n+   * The consumer uses {@link String} for keys and {@link CruiseControlMetric} for values.\n    *\n-   * @param topic The name of the topic.\n-   * @param partitionCount Desired partition count.\n-   * @param replicationFactor Desired replication factor.\n-   * @param retentionMs Desired retention in milliseconds.\n-   * @return A wrapper around the topic with the given desired properties.\n+   * @param configs The configurations for Cruise Control.\n+   * @return A new Kafka consumer\n    */\n-  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n-    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n-      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n-                                                       + \" must be positive for the topic (%s).\", partitionCount,\n-                                                       replicationFactor, retentionMs, topic));\n+  public static Consumer<String, CruiseControlMetric> createMetricConsumer(Map<String, ?> configs) {\n+    // Get bootstrap servers\n+    String bootstrapServers = (String) configs.get(METRIC_REPORTER_SAMPLER_BOOTSTRAP_SERVERS);\n+    if (bootstrapServers == null) {\n+      bootstrapServers = bootstrapServers(configs);\n     }\n-\n-    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n-    Map<String, String> config = new HashMap<>(2);\n-    config.put(RetentionMsProp(), Long.toString(retentionMs));\n-    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n-    newTopic.configs(config);\n-\n-    return newTopic;\n-  }\n-\n-  /**\n-   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n-   *\n-   * @param configsToAlter A set of config altering operations to be populated.\n-   * @param desiredConfig Desired config value by name.\n-   * @param currentConfig Current config.\n-   */\n-  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n-    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n-      String configName = entry.getKey();\n-      String targetConfigValue = entry.getValue();\n-      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n-      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n-        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n-      }\n+    // Get group id\n+    long randomToken = RANDOM.nextLong();", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTg5NQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215895", "bodyText": "Addressed this similar to #1332 (comment).", "author": "efeg", "createdAt": "2020-09-30T03:02:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTQ4Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r496359486", "bodyText": "This can probably be a much smaller topic.", "author": "smccauliff", "createdAt": "2020-09-29T03:40:48Z", "path": "docs/wiki/User Guide/Configurations.md", "diffHunk": "@@ -245,6 +246,14 @@ We are still trying to improve cruise control. And following are some configurat\n | min.broker.sample.store.topic.retention.time.ms       | Integer | N         | 3600000       | The config for the minimal retention time for Kafka broker sample store topic                                                                                                                           |\n | skip.sample.store.topic.rack.awareness.check          | Boolean | N         | false         | The config to skip rack awareness sanity check for sample store topics                                                                                                                                  |\n \n+### MaintenanceEventTopicReader configurations\n+| Name                                          | Type    | Required? | Default Value           | Description                                                       |\n+|-----------------------------------------------|---------|-----------|-------------------------|-------------------------------------------------------------------|\n+| maintenance.event.topic                       | String  | N         | __MaintenanceEvent      | The name of the Kafka topic to consume maintenance events from    |\n+| maintenance.event.topic.replication.factor    | Short   | N         | min(2, #alive-brokers)  | The replication factor of the maintenance event topic             |\n+| maintenance.event.topic.partition.count       | Integer | N         | 32                      | The partition count of the maintenance event topic                |", "originalCommit": "aaf061405c443864f36d2f804a05ee1a02a6dde5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIxNTkwNQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497215905", "bodyText": "Changed this to 8.", "author": "efeg", "createdAt": "2020-09-30T03:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1OTQ4Ng=="}], "type": "inlineReview"}, {"oid": "174414248d750a9b9bd7ebe449348efb0ae7921d", "url": "https://github.com/linkedin/cruise-control/commit/174414248d750a9b9bd7ebe449348efb0ae7921d", "message": "Address the feedback", "committedDate": "2020-09-30T03:01:21Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzOTEzNg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497239136", "bodyText": "What's the advantage of doing offset management this way?", "author": "smccauliff", "createdAt": "2020-09-30T04:41:10Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;\n+  // A maintenance event has a certain validity period after which it expires and becomes invalid. A delay in handling\n+  // could be introduced by the Kafka producer that generates the plan, the consumer that retrieves the plan, or the\n+  // network that connects these clients to the Kafka cluster. Maintenance event topic reader discards the expired events.\n+  protected long _maintenancePlanExpirationMs;\n+\n+  public static final String MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG = \"maintenance.plan.expiration.ms\";\n+  public static final long DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS = Duration.ofMinutes(15).toMillis();\n+  public static final String MAINTENANCE_EVENT_TOPIC_CONFIG = \"maintenance.event.topic\";\n+  public static final String DEFAULT_MAINTENANCE_EVENT_TOPIC = \"__MaintenanceEvent\";\n+  public static final String MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG = \"maintenance.event.topic.replication.factor\";\n+  public static final short DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR = 2;\n+  public static final String MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG = \"maintenance.event.topic.partition.count\";\n+  public static final int DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT = 8;\n+  public static final String MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG = \"maintenance.event.topic.retention.ms\";\n+  public static final long DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS = Duration.ofHours(6).toMillis();\n+  public static final Duration CONSUMER_CLOSE_TIMEOUT = Duration.ofSeconds(2);\n+  public static final String CONSUMER_CLIENT_ID_PREFIX = MaintenanceEventTopicReader.class.getSimpleName();\n+  // How far should topic reader initially (i.e. upon startup) look back in the history for maintenance events.\n+  public static final long INIT_MAINTENANCE_HISTORY_MS = 60000L;\n+\n+  /**\n+   * Seek to the relevant offsets (i.e. either (1) end time of the last event read period or (2) end offset) that the\n+   * consumer will use on the next poll from each partition.\n+   *\n+   * @return End offsets by the partitions to be consumed.\n+   */\n+  protected Map<TopicPartition, Long> seekToRelevantOffsets() throws SamplingException {\n+    Map<TopicPartition, Long> timestampToSeek = new HashMap<>(_currentPartitionAssignment.size());\n+    for (TopicPartition tp : _currentPartitionAssignment) {\n+      timestampToSeek.put(tp, _lastEventReadPeriodEndTimeMs);\n+    }\n+    Set<TopicPartition> assignment = new HashSet<>(_currentPartitionAssignment);\n+    Map<TopicPartition, Long> endOffsets = _consumer.endOffsets(assignment);\n+    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = _consumer.offsetsForTimes(timestampToSeek);\n+    sanityCheckOffsetFetch(endOffsets, offsetsForTimes);\n+\n+    // If offsets for times are provided for a partition, seek to the returned offset. Otherwise, i.e. for partitions\n+    // without a record timestamp greater than or equal to the target timestamp, seek to the end offset.\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      TopicPartition tp = entry.getKey();\n+      OffsetAndTimestamp offsetAndTimestamp = entry.getValue();\n+      _consumer.seek(tp, offsetAndTimestamp != null ? offsetAndTimestamp.offset() : endOffsets.get(tp));\n+    }", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MTkxNg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497761916", "bodyText": "I assume by this way you mean the  manual management of offsets?\nThe first iteration of CC used subscribe-based group management for its consumers -- i.e. both for sample store consumers and the metrics reporter sampler. However, we observed frequent consumer rebalances typically under unhealthy cluster state. This overhead has adversely affected CC, which is supposed to be robust under unhealthy cluster state; hence, we manage offsets ourselves.", "author": "efeg", "createdAt": "2020-09-30T19:52:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzOTEzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MTEzMA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497981130", "bodyText": "Assign can be used with offset commit without needing to do consumer group coordination.", "author": "smccauliff", "createdAt": "2020-10-01T04:46:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzIzOTEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0MTAyNA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497241024", "bodyText": "This creates some kind of strange circular dependency.  Consider replacing this with relevant interfaces.  For example time can be abstracted with https://docs.oracle.com/javase/8/docs/api/java/time/Clock.html presumably the config object related to the KCC object also has some interface or an object of that class can just be referenced directly.", "author": "smccauliff", "createdAt": "2020-09-30T04:49:04Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MTk0NQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497761945", "bodyText": "This class does not just retrieve the time and config from the KafkaCruiseControl object (i.e. as you mentioned, referencing them via their corresponding objects/interfaces would be trivial), but it also passes this config to MaintenanceEvent upon its creation for its API to be used by the underlying OperationRunnable objects. Going forward, we can further polish the existing code to abstract away the relevant logic that is used by them from KafkaCruiseControl; however, I feel that this is orthogonal to the goal of this PR.", "author": "efeg", "createdAt": "2020-09-30T19:52:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0MTAyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjQyMA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497246420", "bodyText": "Not all the producers will have the same clock and so time is not monotonically increasing.  Does this still work if that is the case?", "author": "smccauliff", "createdAt": "2020-09-30T05:10:30Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReader.java", "diffHunk": "@@ -0,0 +1,356 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.OffsetAndTimestamp;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.CLIENT_REQUEST_TIMEOUT_MS;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.consumptionDone;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.createTopic;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeIncreasePartitionCount;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.maybeUpdateTopicConfig;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.sanityCheckOffsetFetch;\n+import static com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils.wrapTopic;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.createMaintenanceEventConsumer;\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyUtils.extractKafkaCruiseControlObjectFromConfig;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.BROKERS_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.MAINTENANCE_EVENT_TYPE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEvent.TOPICS_WITH_RF_UPDATE_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+\n+\n+/**\n+ * A maintenance event reader that retrieves events from the configured Kafka topic.\n+ *\n+ * Required configurations for this class.\n+ * <ul>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_CONFIG}: The config for the name of the Kafka topic to consume maintenance events\n+ *   from (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG}: The config for the replication factor of the maintenance\n+ *   event topic (default: min({@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR}, broker-count-in-the-cluster)).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG}: The config for the partition count of the maintenance\n+ *   event topic (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT}).</li>\n+ *   <li>{@link #MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG}: The config for the retention of the maintenance event topic\n+ *   (default: {@link #DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS}).</li>\n+ *   <li>{@link #MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG}: The config for the validity period of a maintenance plan\n+ *   (default: {@link #DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS}).</li>\n+ * </ul>\n+ */\n+public class MaintenanceEventTopicReader implements MaintenanceEventReader {\n+  private static final Logger LOG = LoggerFactory.getLogger(MaintenanceEventTopicReader.class);\n+  protected String _maintenanceEventTopic;\n+  protected Consumer<String, MaintenancePlan> _consumer;\n+  protected Set<TopicPartition> _currentPartitionAssignment;\n+  protected volatile boolean _shutdown = false;\n+  protected long _lastEventReadPeriodEndTimeMs;\n+  protected KafkaCruiseControl _kafkaCruiseControl;\n+  // A maintenance event has a certain validity period after which it expires and becomes invalid. A delay in handling\n+  // could be introduced by the Kafka producer that generates the plan, the consumer that retrieves the plan, or the\n+  // network that connects these clients to the Kafka cluster. Maintenance event topic reader discards the expired events.\n+  protected long _maintenancePlanExpirationMs;\n+\n+  public static final String MAINTENANCE_PLAN_EXPIRATION_MS_CONFIG = \"maintenance.plan.expiration.ms\";\n+  public static final long DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS = Duration.ofMinutes(15).toMillis();\n+  public static final String MAINTENANCE_EVENT_TOPIC_CONFIG = \"maintenance.event.topic\";\n+  public static final String DEFAULT_MAINTENANCE_EVENT_TOPIC = \"__MaintenanceEvent\";\n+  public static final String MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG = \"maintenance.event.topic.replication.factor\";\n+  public static final short DEFAULT_MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR = 2;\n+  public static final String MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG = \"maintenance.event.topic.partition.count\";\n+  public static final int DEFAULT_MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT = 8;\n+  public static final String MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG = \"maintenance.event.topic.retention.ms\";\n+  public static final long DEFAULT_MAINTENANCE_EVENT_TOPIC_RETENTION_TIME_MS = Duration.ofHours(6).toMillis();\n+  public static final Duration CONSUMER_CLOSE_TIMEOUT = Duration.ofSeconds(2);\n+  public static final String CONSUMER_CLIENT_ID_PREFIX = MaintenanceEventTopicReader.class.getSimpleName();\n+  // How far should topic reader initially (i.e. upon startup) look back in the history for maintenance events.\n+  public static final long INIT_MAINTENANCE_HISTORY_MS = 60000L;\n+\n+  /**\n+   * Seek to the relevant offsets (i.e. either (1) end time of the last event read period or (2) end offset) that the\n+   * consumer will use on the next poll from each partition.\n+   *\n+   * @return End offsets by the partitions to be consumed.\n+   */\n+  protected Map<TopicPartition, Long> seekToRelevantOffsets() throws SamplingException {\n+    Map<TopicPartition, Long> timestampToSeek = new HashMap<>(_currentPartitionAssignment.size());\n+    for (TopicPartition tp : _currentPartitionAssignment) {\n+      timestampToSeek.put(tp, _lastEventReadPeriodEndTimeMs);\n+    }\n+    Set<TopicPartition> assignment = new HashSet<>(_currentPartitionAssignment);\n+    Map<TopicPartition, Long> endOffsets = _consumer.endOffsets(assignment);\n+    Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = _consumer.offsetsForTimes(timestampToSeek);\n+    sanityCheckOffsetFetch(endOffsets, offsetsForTimes);\n+\n+    // If offsets for times are provided for a partition, seek to the returned offset. Otherwise, i.e. for partitions\n+    // without a record timestamp greater than or equal to the target timestamp, seek to the end offset.\n+    for (Map.Entry<TopicPartition, OffsetAndTimestamp> entry : offsetsForTimes.entrySet()) {\n+      TopicPartition tp = entry.getKey();\n+      OffsetAndTimestamp offsetAndTimestamp = entry.getValue();\n+      _consumer.seek(tp, offsetAndTimestamp != null ? offsetAndTimestamp.offset() : endOffsets.get(tp));\n+    }\n+\n+    return endOffsets;\n+  }\n+\n+  protected void addMaintenancePlan(MaintenancePlan maintenancePlan, Set<MaintenanceEvent> maintenanceEvents) {\n+    LOG.debug(\"Retrieved maintenance plan {}.\", maintenancePlan);\n+    Map<String, Object> parameterConfigOverrides = new HashMap<>(4);\n+    parameterConfigOverrides.put(KAFKA_CRUISE_CONTROL_OBJECT_CONFIG, _kafkaCruiseControl);\n+    parameterConfigOverrides.put(ANOMALY_DETECTION_TIME_MS_OBJECT_CONFIG, _kafkaCruiseControl.timeMs());\n+    parameterConfigOverrides.put(MAINTENANCE_EVENT_TYPE_CONFIG, maintenancePlan.maintenanceEventType());\n+    switch (maintenancePlan.maintenanceEventType()) {\n+      case ADD_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((AddBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case REMOVE_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((RemoveBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case FIX_OFFLINE_REPLICAS:\n+      case REBALANCE:\n+        break;\n+      case DEMOTE_BROKER:\n+        parameterConfigOverrides.put(BROKERS_OBJECT_CONFIG, ((DemoteBrokerPlan) maintenancePlan).brokers());\n+        break;\n+      case TOPIC_REPLICATION_FACTOR:\n+        parameterConfigOverrides.put(TOPICS_WITH_RF_UPDATE_CONFIG, ((TopicReplicationFactorPlan) maintenancePlan).topicRegexWithRFUpdate());\n+        break;\n+      default:\n+        throw new IllegalStateException(String.format(\"Unrecognized event type %s\", maintenancePlan.maintenanceEventType()));\n+    }\n+    maintenanceEvents.add(_kafkaCruiseControl.config().getConfiguredInstance(AnomalyDetectorConfig.MAINTENANCE_EVENT_CLASS_CONFIG,\n+                                                                             MaintenanceEvent.class,\n+                                                                             parameterConfigOverrides));\n+  }\n+\n+  /**\n+   * See {@link MaintenanceEventReader#readEvents(Duration)}\n+   * This method may block beyond the timeout in order to execute additional logic to create maintenance events from the\n+   * maintenance plans retrieved from the relevant topic.\n+   *\n+   * @param timeout The maximum time to block for retrieving records from the relevant topic.\n+   * @return Set of maintenance events, or empty set if none is available after the given timeout expires.\n+   */\n+  @Override\n+  public Set<MaintenanceEvent> readEvents(Duration timeout) throws SamplingException {\n+    LOG.debug(\"Reading maintenance events.\");\n+    long eventReadPeriodEndMs = _kafkaCruiseControl.timeMs();\n+    if (refreshPartitionAssignment()) {\n+      _lastEventReadPeriodEndTimeMs = eventReadPeriodEndMs;\n+      return Collections.emptySet();\n+    }\n+\n+    long timeoutEndMs = eventReadPeriodEndMs + timeout.toMillis();\n+    Set<MaintenanceEvent> maintenanceEvents = new HashSet<>();\n+    try {\n+      Map<TopicPartition, Long> endOffsets = seekToRelevantOffsets();\n+      LOG.debug(\"Started to consume from maintenance event topic partitions {}.\", _currentPartitionAssignment);\n+      _consumer.resume(_consumer.paused());\n+      Set<TopicPartition> partitionsToPause = new HashSet<>();\n+\n+      do {\n+        ConsumerRecords<String, MaintenancePlan> records = _consumer.poll(timeout);\n+        for (ConsumerRecord<String, MaintenancePlan> record : records) {\n+          if (record == null) {\n+            // This means that the record cannot be parsed because the maintenance event type is not recognized.\n+            // It might happen when newer type of maintenance events have been added and the current code is still old.\n+            // We simply ignore that metric in this case (see MaintenancePlanSerde#fromBytes).\n+            LOG.warn(\"Cannot parse record, please update your Cruise Control version.\");\n+            continue;\n+          }\n+\n+          long planGenerationTimeMs = record.value().timeMs();\n+          if (planGenerationTimeMs + _maintenancePlanExpirationMs < eventReadPeriodEndMs) {\n+            LOG.warn(\"Discarding the expired plan {}. (Expired: {} Evaluated: {}).\", record.value(),\n+                     planGenerationTimeMs + _maintenancePlanExpirationMs, eventReadPeriodEndMs);\n+          } else if (planGenerationTimeMs >= eventReadPeriodEndMs) {\n+            TopicPartition tp = new TopicPartition(record.topic(), record.partition());", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MTk2OQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497761969", "bodyText": "As (arguably) typically assumed across the participants of Kafka ecosystem, we also assume clocks are loosely synchronized. For example, we know that secure communication between brokers and clients can fail due to certificate expiration issues caused by clock skew. Similarly, if a producer has a clock that is significantly behind the others, then its plan can expire \"faster\" than it expects.\nPlease also see the comment above, which discusses that the delay can be introduced by Kafka producers:\n  // A maintenance event has a certain validity period after which it expires and becomes invalid. A delay in handling\n  // could be introduced by the Kafka producer that generates the plan, the consumer that retrieves the plan, or the\n  // network that connects these clients to the Kafka cluster. Maintenance event topic reader discards the expired events.", "author": "efeg", "createdAt": "2020-09-30T19:52:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjQyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0OTM5Nw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497849397", "bodyText": "I saw that comment.  My point is that this introduces additional latency and that one broker with a bad clock can block all maintenance events from being processed.", "author": "smccauliff", "createdAt": "2020-09-30T23:07:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjQyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzkzMTEzNA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497931134", "bodyText": "one broker with a bad clock can block all maintenance events from being processed.\n\nI am a little confused. Unless that broker is either (1) the only one in the system that generates maintenance events or (2) generates maintenance events every MaintenanceEventDetector#READ_EVENTS_TIMEOUT, it will not block the other maintenance events. Am I missing something?", "author": "efeg", "createdAt": "2020-10-01T01:53:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjQyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjkzMQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497246931", "bodyText": "What does it mean to rebalance a particular broker?", "author": "smccauliff", "createdAt": "2020-09-30T05:12:39Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/RebalancePlan.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+\n+\n+public class RebalancePlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+\n+  public RebalancePlan(long timeMs, int brokerId) {\n+    super(MaintenanceEventType.REBALANCE, timeMs, brokerId);\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  /**\n+   * Deserialize given byte buffer to a {@link RebalancePlan}.\n+   *\n+   * @param headerSize The header size of the buffer.\n+   * @param buffer buffer to deserialize.\n+   * @return The {@link RebalancePlan} corresponding to the deserialized buffer.\n+   */\n+  public static RebalancePlan fromBuffer(int headerSize, ByteBuffer buffer) throws UnknownVersionException {\n+    verifyCrc(headerSize, buffer);\n+    byte version = buffer.get();\n+    if (version > PLAN_VERSION) {\n+      throw new UnknownVersionException(\"Cannot deserialize the plan for version \" + version + \". Current version: \" + PLAN_VERSION);\n+    }\n+\n+    long timeMs = buffer.getLong();\n+    int brokerId = buffer.getInt();\n+\n+    return new RebalancePlan(timeMs, brokerId);", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MjAxMg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497762012", "bodyText": "This is to audit provenance of the maintenance plan.\nExcerpt from JavaDoc of MaintenancePlan#brokerId(): The id of the broker that reported this maintenance event.", "author": "efeg", "createdAt": "2020-09-30T19:52:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjkzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0OTg5Nw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497849897", "bodyText": "Got it.", "author": "smccauliff", "createdAt": "2020-09-30T23:09:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NjkzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NzU5Nw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497247597", "bodyText": "Do you want to validate the regex here?", "author": "smccauliff", "createdAt": "2020-09-30T05:15:12Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/TopicReplicationFactorPlan.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+public class TopicReplicationFactorPlan extends MaintenancePlan {\n+  public static final byte PLAN_VERSION = 0;\n+  // A map containing the regex of topics by the corresponding desired replication factor\n+  private final Map<Short, String> _topicRegexWithRFUpdate;\n+\n+  public TopicReplicationFactorPlan(long timeMs, int brokerId, Map<Short, String> topicRegexWithRFUpdate) {\n+    super(MaintenanceEventType.TOPIC_REPLICATION_FACTOR, timeMs, brokerId);\n+    if (topicRegexWithRFUpdate == null || topicRegexWithRFUpdate.isEmpty()) {\n+      throw new IllegalArgumentException(\"Missing replication factor updates for the plan.\");\n+    }\n+    // Sanity check: the number of bulk updates for replication factor of topics.\n+    if (topicRegexWithRFUpdate.size() > Byte.MAX_VALUE) {\n+      throw new IllegalArgumentException(String.format(\"Cannot update more than %d different replication factor (attempt: %d).\",\n+                                                       Byte.MAX_VALUE, topicRegexWithRFUpdate.size()));\n+    }\n+    // Sanity check: Each regex must have some value.\n+    for (String regex : topicRegexWithRFUpdate.values()) {\n+      if (regex == null || regex.isEmpty()) {\n+        throw new IllegalArgumentException(\"Missing topics of the replication factor update for the plan.\");\n+      }\n+    }\n+    _topicRegexWithRFUpdate = topicRegexWithRFUpdate;\n+  }\n+\n+  @Override\n+  public byte planVersion() {\n+    return PLAN_VERSION;\n+  }\n+\n+  public Map<Short, String> topicRegexWithRFUpdate() {\n+    return _topicRegexWithRFUpdate;\n+  }\n+\n+  @Override\n+  public ByteBuffer toBuffer(int headerSize) {\n+    byte numRFUpdateEntries = (byte) _topicRegexWithRFUpdate.size();\n+    int requiredCapacityForRFEntries = 0;\n+    for (Map.Entry<Short, String> entry : _topicRegexWithRFUpdate.entrySet()) {\n+      requiredCapacityForRFEntries += (Short.BYTES /* replication factor */ + Integer.BYTES /* regex length */\n+                                       + entry.getValue().getBytes(StandardCharsets.UTF_8).length /* regex */);\n+    }\n+    int contentSize = (Byte.BYTES /* plan version */\n+                       + Long.BYTES /* timeMs */\n+                       + Integer.BYTES /* broker id */\n+                       + Byte.BYTES /* number of replication factor update entries */\n+                       + requiredCapacityForRFEntries /* total capacity for all entries */);\n+    ByteBuffer buffer = ByteBuffer.allocate(headerSize + Long.BYTES /* crc */ + contentSize);\n+    buffer.position(headerSize + Long.BYTES);\n+    buffer.put(planVersion());\n+    buffer.putLong(timeMs());\n+    buffer.putInt(brokerId());\n+    buffer.put(numRFUpdateEntries);\n+    for (Map.Entry<Short, String> entry : _topicRegexWithRFUpdate.entrySet()) {\n+      buffer.putShort(entry.getKey());\n+      byte[] regex = entry.getValue().getBytes(StandardCharsets.UTF_8);\n+      buffer.putInt(regex.length);\n+      buffer.put(regex);\n+    }\n+    putCrc(headerSize, buffer, contentSize);\n+    return buffer;\n+  }\n+\n+  /**\n+   * Deserialize given byte buffer to an {@link TopicReplicationFactorPlan}.\n+   *\n+   * @param headerSize The header size of the buffer.\n+   * @param buffer buffer to deserialize.\n+   * @return The {@link TopicReplicationFactorPlan} corresponding to the deserialized buffer.\n+   */\n+  public static TopicReplicationFactorPlan fromBuffer(int headerSize, ByteBuffer buffer) throws UnknownVersionException {\n+    verifyCrc(headerSize, buffer);\n+    byte version = buffer.get();\n+    if (version > PLAN_VERSION) {\n+      throw new UnknownVersionException(\"Cannot deserialize the plan for version \" + version + \". Current version: \" + PLAN_VERSION);\n+    }\n+\n+    long timeMs = buffer.getLong();\n+    int brokerId = buffer.getInt();\n+    byte numRFUpdateEntries = buffer.get();\n+    Map<Short, String> topicRegexWithRFUpdate = new HashMap<>(numRFUpdateEntries);\n+\n+    for (int i = 0; i < numRFUpdateEntries; i++) {\n+      short replicationFactor = buffer.getShort();\n+      int regexLength = buffer.getInt();\n+      String regex = new String(buffer.array(), buffer.arrayOffset() + buffer.position(), regexLength, StandardCharsets.UTF_8);", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MjA2Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497762066", "bodyText": "The validity of the expression's syntax is verified in MaintenanceEvent#topicPatternByReplicationFactor(Map<String, ?>) -- i.e. upon creation of a MaintenanceEvent from TopicReplicationFactorPlan.", "author": "efeg", "createdAt": "2020-09-30T19:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NzU5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg1MDE4Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497850186", "bodyText": "That's assuming your code is going to be generating these events which it may not be.", "author": "smccauliff", "createdAt": "2020-09-30T23:10:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NzU5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzkzNDM3MA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497934370", "bodyText": "Yes, because the semantic interpretation of this String, called regex, is relevant to MaintenanceEvent.", "author": "efeg", "createdAt": "2020-10-01T01:59:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0NzU5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0Nzk0OA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497247948", "bodyText": "Is this duplicated elsewhere?", "author": "smccauliff", "createdAt": "2020-09-30T05:16:38Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/monitor/sampling/SamplingUtils.java", "diffHunk": "@@ -346,124 +313,70 @@ private static boolean skipBuildingBrokerMetricSample(BrokerLoad brokerLoad, int\n   }\n \n   /**\n-   * Build a wrapper around the topic with the given desired properties and {@link #DEFAULT_CLEANUP_POLICY}.\n+   * Create a Kafka consumer for retrieving reported Cruise Control metrics.\n+   * The consumer uses {@link String} for keys and {@link CruiseControlMetric} for values.\n+   *\n+   * This consumer is not intended to use (1) the group management functionality by using subscribe(topic) or (2) the Kafka-based\n+   * offset management strategy. Hence, the {@link ConsumerConfig#GROUP_ID_CONFIG} config is irrelevant to it.\n    *\n-   * @param topic The name of the topic.\n-   * @param partitionCount Desired partition count.\n-   * @param replicationFactor Desired replication factor.\n-   * @param retentionMs Desired retention in milliseconds.\n-   * @return A wrapper around the topic with the given desired properties.\n+   * @param configs The configurations for Cruise Control.\n+   * @param clientIdPrefix Client id prefix.\n+   * @return A new Kafka consumer\n    */\n-  public static NewTopic wrapTopic(String topic, int partitionCount, short replicationFactor, long retentionMs) {\n-    if (partitionCount <= 0 || replicationFactor <= 0 || retentionMs <= 0) {\n-      throw new IllegalArgumentException(String.format(\"Partition count (%d), replication factor (%d), and retention ms (%d)\"\n-                                                       + \" must be positive for the topic (%s).\", partitionCount,\n-                                                       replicationFactor, retentionMs, topic));\n+  public static Consumer<String, CruiseControlMetric> createMetricConsumer(Map<String, ?> configs, String clientIdPrefix) {\n+    // Get bootstrap servers\n+    String bootstrapServers = (String) configs.get(METRIC_REPORTER_SAMPLER_BOOTSTRAP_SERVERS);\n+    if (bootstrapServers == null) {\n+      bootstrapServers = bootstrapServers(configs);\n     }\n \n-    NewTopic newTopic = new NewTopic(topic, partitionCount, replicationFactor);\n-    Map<String, String> config = new HashMap<>(2);\n-    config.put(RetentionMsProp(), Long.toString(retentionMs));\n-    config.put(CleanupPolicyProp(), DEFAULT_CLEANUP_POLICY);\n-    newTopic.configs(config);\n-\n-    return newTopic;\n+    // Create consumer\n+    long randomToken = RANDOM.nextLong();\n+    Properties consumerProps = new Properties();\n+    consumerProps.putAll(configs);\n+    consumerProps.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n+    consumerProps.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, clientIdPrefix + \"-consumer-\" + randomToken);\n+    consumerProps.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\");\n+    consumerProps.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n+    consumerProps.setProperty(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, Integer.toString(Integer.MAX_VALUE));\n+    consumerProps.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());\n+    consumerProps.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, MetricSerde.class.getName());\n+    consumerProps.setProperty(ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG, configs.get(RECONNECT_BACKOFF_MS_CONFIG).toString());\n+    return new KafkaConsumer<>(consumerProps);\n   }\n \n-  /**\n-   * Add config altering operations to the given configs to alter for configs that differ between current and desired.\n-   *\n-   * @param configsToAlter A set of config altering operations to be populated.\n-   * @param desiredConfig Desired config value by name.\n-   * @param currentConfig Current config.\n-   */\n-  private static void maybeUpdateConfig(Set<AlterConfigOp> configsToAlter, Map<String, String> desiredConfig, Config currentConfig) {\n-    for (Map.Entry<String, String> entry : desiredConfig.entrySet()) {\n-      String configName = entry.getKey();\n-      String targetConfigValue = entry.getValue();\n-      ConfigEntry currentConfigEntry = currentConfig.get(configName);\n-      if (currentConfigEntry == null || !currentConfigEntry.value().equals(targetConfigValue)) {\n-        configsToAlter.add(new AlterConfigOp(new ConfigEntry(configName, targetConfigValue), AlterConfigOp.OpType.SET));\n-      }\n+  private static String bootstrapServers(Map<String, ?> configs) {\n+    String bootstrapServers = configs.get(MonitorConfig.BOOTSTRAP_SERVERS_CONFIG).toString();\n+    // Trim the brackets in List's String representation.\n+    if (bootstrapServers.length() > 2) {\n+      bootstrapServers = bootstrapServers.substring(1, bootstrapServers.length() - 1);\n     }\n+    return bootstrapServers;", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc2MjA5Mg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497762092", "bodyText": "Moved the bootstrapServers extraction to a util function.", "author": "efeg", "createdAt": "2020-09-30T19:52:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI0Nzk0OA=="}], "type": "inlineReview"}, {"oid": "4ffcf0ca2ef5f1e29a428047bac3f63b1a2c414e", "url": "https://github.com/linkedin/cruise-control/commit/4ffcf0ca2ef5f1e29a428047bac3f63b1a2c414e", "message": "Address the feedback.", "committedDate": "2020-09-30T19:51:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMjIwNg==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497732206", "bodyText": "I don't understand what this plan is supposed to do.  Can there be some javadoc here to explain the purpose of this class.", "author": "smccauliff", "createdAt": "2020-09-30T18:57:07Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/detector/TopicReplicationFactorPlan.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.metricsreporter.exception.UnknownVersionException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+\n+public class TopicReplicationFactorPlan extends MaintenancePlan {", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk0NjI0Mw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497946243", "bodyText": "Added JavaDoc to each plan to clarify its purpose.", "author": "efeg", "createdAt": "2020-10-01T02:17:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMjIwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MjAzNA==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497982034", "bodyText": "This was the only one which was confusing to me, but thanks for updating everything.", "author": "smccauliff", "createdAt": "2020-10-01T04:50:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMjIwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMzg3MQ==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497733871", "bodyText": "Do admin clients actually use the broker specified in bootstrap servers or do they refresh metadata from bootstrap server and then choose one of the brokers specified in the metadata?", "author": "smccauliff", "createdAt": "2020-09-30T19:00:02Z", "path": "cruise-control/src/test/java/com/linkedin/kafka/cruisecontrol/detector/MaintenanceEventTopicReaderTest.java", "diffHunk": "@@ -0,0 +1,213 @@\n+/*\n+ * Copyright 2020 LinkedIn Corp. Licensed under the BSD 2-Clause License (the \"License\"). See License in the project root for license information.\n+ */\n+\n+package com.linkedin.kafka.cruisecontrol.detector;\n+\n+import com.linkedin.kafka.cruisecontrol.CruiseControlIntegrationTestHarness;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControl;\n+import com.linkedin.kafka.cruisecontrol.KafkaCruiseControlUtils;\n+import com.linkedin.kafka.cruisecontrol.config.constants.AnomalyDetectorConfig;\n+import com.linkedin.kafka.cruisecontrol.exception.SamplingException;\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.AdminClientConfig;\n+import org.apache.kafka.clients.admin.Config;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.config.ConfigResource;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.easymock.EasyMock;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static com.linkedin.kafka.cruisecontrol.detector.AnomalyDetectorUtils.KAFKA_CRUISE_CONTROL_OBJECT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.DEFAULT_MAINTENANCE_PLAN_EXPIRATION_MS;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventTopicReader.MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG;\n+import static com.linkedin.kafka.cruisecontrol.detector.MaintenanceEventType.REBALANCE;\n+import static com.linkedin.kafka.cruisecontrol.detector.notifier.KafkaAnomalyType.MAINTENANCE_EVENT;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+\n+\n+public class MaintenanceEventTopicReaderTest extends CruiseControlIntegrationTestHarness {\n+  public static final String TEST_TOPIC = \"__CloudMaintenanceEvent\";\n+  public static final String TEST_TOPIC_REPLICATION_FACTOR = \"1\";\n+  public static final String TEST_TOPIC_PARTITION_COUNT = \"8\";\n+  public static final String TEST_TOPIC_RETENTION_TIME_MS = \"3600000\";\n+  public static final String RETENTION_MS_CONFIG = \"retention.ms\";\n+  public static final long TEST_REBALANCE_PLAN_TIME = 1601089200000L;\n+  public static final long TEST_EXPIRED_PLAN_TIME = TEST_REBALANCE_PLAN_TIME - 1L;\n+  public static final int TEST_BROKER_ID = 42;\n+  public static final Duration TEST_TIMEOUT = Duration.ofSeconds(5);\n+  public static final Map<Short, String> TEST_TOPIC_REGEX_WITH_RF_UPDATE = Collections.singletonMap((short) 2, \"T1\");\n+  private TopicDescription _topicDescription;\n+  private Config _topicConfig;\n+\n+  /**\n+   * Setup the unit test and produce maintenance plans to the maintenance topic.\n+   */\n+  @Before\n+  public void setup() throws Exception {\n+    super.start();\n+    produceMaintenancePlans();\n+  }\n+\n+  /**\n+   * Produces plans to the {@link #TEST_TOPIC}.\n+   * All plans except {@link RebalancePlan} are expired.\n+   */\n+  private void produceMaintenancePlans() {\n+    Properties props = new Properties();\n+    props.setProperty(ProducerConfig.ACKS_CONFIG, \"-1\");\n+    try (Producer<String, MaintenancePlan> producer = createMaintenancePlanProducer(props)) {\n+      sendPlan(producer, new RebalancePlan(TEST_REBALANCE_PLAN_TIME, TEST_BROKER_ID));\n+      sendPlan(producer, new FixOfflineReplicasPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID));\n+      sendPlan(producer, new DemoteBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new AddBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new RemoveBrokerPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, Collections.singleton(TEST_BROKER_ID)));\n+      sendPlan(producer, new TopicReplicationFactorPlan(TEST_EXPIRED_PLAN_TIME, TEST_BROKER_ID, TEST_TOPIC_REGEX_WITH_RF_UPDATE));\n+    }\n+  }\n+\n+  @javax.annotation.Nonnull\n+  protected Producer<String, MaintenancePlan> createMaintenancePlanProducer(Properties overrides) {\n+    Properties props = new Properties();\n+    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers());\n+    props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getCanonicalName());\n+    props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, MaintenancePlanSerde.class.getCanonicalName());\n+    //apply overrides\n+    if (overrides != null) {\n+      props.putAll(overrides);\n+    }\n+    return new KafkaProducer<>(props);\n+  }\n+\n+  private void sendPlan(Producer<String, MaintenancePlan> producer, MaintenancePlan maintenancePlan) {\n+    producer.send(new ProducerRecord<>(TEST_TOPIC, maintenancePlan), (recordMetadata, e) -> {\n+      if (e != null) {\n+        fail(\"Failed to produce maintenance plan\");\n+      }\n+    });\n+  }\n+\n+  @After\n+  public void teardown() {\n+    super.stop();\n+  }\n+\n+  @Override\n+  protected Map<String, Object> withConfigs() {\n+    Map<String, Object> configs = new HashMap<>(5);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_CONFIG, TEST_TOPIC);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_REPLICATION_FACTOR_CONFIG, TEST_TOPIC_REPLICATION_FACTOR);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_PARTITION_COUNT_CONFIG, TEST_TOPIC_PARTITION_COUNT);\n+    configs.put(MAINTENANCE_EVENT_TOPIC_RETENTION_MS_CONFIG, TEST_TOPIC_RETENTION_TIME_MS);\n+    configs.put(AnomalyDetectorConfig.MAINTENANCE_EVENT_READER_CLASS_CONFIG, MaintenanceEventTopicReader.class.getName());\n+    return configs;\n+  }\n+\n+  /**\n+   * Retrieve the latest metadata for {@link #_topicDescription} and {@link #_topicConfig} topics.\n+   * To ensure the latest metadata update, admin clients retrieve the metadata from both brokers and ensure that they\n+   * have the same metadata.\n+   */\n+  private void retrieveLatestMetadata() throws InterruptedException, ExecutionException {\n+    TopicDescription description0;\n+    TopicDescription description1;\n+    Config topicConfig0;\n+    Config topicConfig1;\n+    ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, TEST_TOPIC);\n+    AdminClient adminClient0 = KafkaCruiseControlUtils.createAdminClient(Collections.singletonMap(\n+        AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, broker(0).plaintextAddr()));\n+    AdminClient adminClient1 = KafkaCruiseControlUtils.createAdminClient(Collections.singletonMap(\n+        AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, broker(1).plaintextAddr()));", "originalCommit": "174414248d750a9b9bd7ebe449348efb0ae7921d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk0NTc4Mw==", "url": "https://github.com/linkedin/cruise-control/pull/1332#discussion_r497945783", "bodyText": "If my understanding is correct, they will prefer a node with an existing connection, which will be the one specified as the bootstrap server.\nIn practice, not using this trick definitely leads to reproducible failure with sufficient repetition of this test case.", "author": "efeg", "createdAt": "2020-10-01T02:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzczMzg3MQ=="}], "type": "inlineReview"}, {"oid": "5b394034cd2064f253e70cfc74b4f699cffd6a68", "url": "https://github.com/linkedin/cruise-control/commit/5b394034cd2064f253e70cfc74b4f699cffd6a68", "message": "Use Gson for de/serialization of maintenance plans.", "committedDate": "2020-10-01T01:39:24Z", "type": "commit"}, {"oid": "5b394034cd2064f253e70cfc74b4f699cffd6a68", "url": "https://github.com/linkedin/cruise-control/commit/5b394034cd2064f253e70cfc74b4f699cffd6a68", "message": "Use Gson for de/serialization of maintenance plans.", "committedDate": "2020-10-01T01:39:24Z", "type": "forcePushed"}, {"oid": "88605563058cd02e08d152a1b6798a960b979f96", "url": "https://github.com/linkedin/cruise-control/commit/88605563058cd02e08d152a1b6798a960b979f96", "message": "Resolve encoding issue.", "committedDate": "2020-10-01T01:47:04Z", "type": "commit"}, {"oid": "d0315af0c8b19b6f23d6727040f7ced52a415eb9", "url": "https://github.com/linkedin/cruise-control/commit/d0315af0c8b19b6f23d6727040f7ced52a415eb9", "message": "Add missing JavaDoc to plans.", "committedDate": "2020-10-01T02:17:29Z", "type": "commit"}]}