{"pr_number": 1289, "pr_title": "Automate replica reassignment concurrency adjustment based on broker metrics", "pr_createdAt": "2020-07-29T20:35:31Z", "pr_url": "https://github.com/linkedin/cruise-control/pull/1289", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYxNTEyNA==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462615124", "bodyText": "I don't see a lock or other synchronization mechanisms in these functions. But since they are accessing the some shared members, will there by any race condition?", "author": "hzxa21", "createdAt": "2020-07-29T22:01:00Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/executor/Executor.java", "diffHunk": "@@ -288,6 +295,50 @@ public void run() {\n     }\n   }\n \n+  /**\n+   * A runnable class to auto-adjust the allowed inter-broker partition reassignment concurrency for ongoing executions\n+   * using selected broker metrics and based on additive-increase/multiplicative-decrease (AIMD) feedback control algorithm.\n+   * Skips concurrency adjustment for demote operations.\n+   */\n+  private class ConcurrencyAdjuster implements Runnable {\n+    private final int _maxPartitionMovementsPerBroker;\n+    private LoadMonitor _loadMonitor;\n+\n+    public ConcurrencyAdjuster() {\n+      _maxPartitionMovementsPerBroker = _config.getInt(ExecutorConfig.CONCURRENCY_ADJUSTER_MAX_PARTITION_MOVEMENTS_PER_BROKER_CONFIG);\n+      _loadMonitor = null;\n+    }\n+\n+    public void setLoadMonitor(LoadMonitor loadMonitor) {\n+      _loadMonitor = loadMonitor;\n+    }\n+\n+    private boolean canRefreshConcurrency() {\n+      return _concurrencyAdjusterEnabled && _executorState.state() == ExecutorState.State.INTER_BROKER_REPLICA_MOVEMENT_TASK_IN_PROGRESS\n+             && !_isLatestExecutionDemote && _loadMonitor != null;\n+    }\n+\n+    private void refreshConcurrency() {\n+      if (canRefreshConcurrency()) {\n+        Integer recommendedConcurrency = ExecutionUtils.recommendedConcurrency(_loadMonitor.currentBrokerMetricValues(),\n+                                                                               _executionTaskManager.interBrokerPartitionMovementConcurrency(),\n+                                                                               _maxPartitionMovementsPerBroker);\n+        if (recommendedConcurrency != null) {\n+          setRequestedInterBrokerPartitionMovementConcurrency(recommendedConcurrency);\n+        }\n+      }\n+    }", "originalCommit": "b6c27d990e37bca8e99bcc19b0caa2f1d1d4b16c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY5NjIxOQ==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462696219", "bodyText": "There is indeed a case that may potentially override the initial request concurrency by ConcurrencyAdjuster thread. Updated the code to prevent this -- thanks!\nNote that the check on _executorState.state() is just an optimization to minimize updates to inter-broker partition movement concurrency. It has no affect to set it in any other execution state. Hence, no strict synchronization is needed.", "author": "efeg", "createdAt": "2020-07-30T02:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYxNTEyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMzEwNg==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462623106", "bodyText": "I noticed that the metrics used by the adjuster are all broker-side metrics related to consumption and production. It may not reflect problems related to delayed leadership changes. For example, there can be a long queue of LeaderAndIsrRequest or UpdateMetadataRequest causing some partitions to become leaderless while the fetch/produce local time metrics still look good because there is less I/O if partitions are leaderless.\nHave you considered also looking at the controller metrics as well? The ones I am thinking about are:\n\nController to per broker request queue time: https://github.com/linkedin/kafka/blob/2.3-li-1/core/src/main/scala/kafka/controller/ControllerChannelManager.scala#L184\nController to per broker request queue size: https://github.com/linkedin/kafka/blob/2.3-li-1/core/src/main/scala/kafka/controller/ControllerChannelManager.scala#L192\n\nThis patch is already a big step forward and I don't think we should address this comment in this patch but it is something worth thinking.", "author": "hzxa21", "createdAt": "2020-07-29T22:20:01Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/executor/ExecutionUtils.java", "diffHunk": "@@ -30,10 +42,119 @@\n   public static final String GAUGE_EXECUTION_STOPPED_BY_USER = EXECUTION_STOPPED + \"-by-user\";\n   public static final String GAUGE_EXECUTION_STARTED_IN_KAFKA_ASSIGNER_MODE = EXECUTION_STARTED + \"-\" + KAFKA_ASSIGNER_MODE;\n   public static final String GAUGE_EXECUTION_STARTED_IN_NON_KAFKA_ASSIGNER_MODE = EXECUTION_STARTED + \"-non-\" + KAFKA_ASSIGNER_MODE;\n+  public static final long EXECUTION_HISTORY_SCANNER_PERIOD_SECONDS = 5;\n+  public static final long EXECUTION_HISTORY_SCANNER_INITIAL_DELAY_SECONDS = 0;\n+  public static final int ADDITIVE_INCREASE_PARAM = 1;\n+  public static final int MULTIPLICATIVE_DECREASE_PARAM = 2;\n+  static final Map<String, Double> CONCURRENCY_ADJUSTER_LIMIT_BY_METRIC_NAME = new HashMap<>(4);\n \n \n   private ExecutionUtils() { }\n \n+  /**\n+   * Initialize the concurrency adjuster limits.\n+   *\n+   * @param config The configurations for Cruise Control.\n+   */\n+  static void init(KafkaCruiseControlConfig config) {\n+    CONCURRENCY_ADJUSTER_LIMIT_BY_METRIC_NAME.put(BROKER_LOG_FLUSH_TIME_MS_999TH.name(),\n+                                                  config.getDouble(ExecutorConfig.CONCURRENCY_ADJUSTER_LIMIT_LOG_FLUSH_TIME_MS_CONFIG));\n+    CONCURRENCY_ADJUSTER_LIMIT_BY_METRIC_NAME.put(BROKER_FOLLOWER_FETCH_LOCAL_TIME_MS_999TH.name(),\n+                                                  config.getDouble(ExecutorConfig.CONCURRENCY_ADJUSTER_LIMIT_FOLLOWER_FETCH_LOCAL_TIME_MS_CONFIG));\n+    CONCURRENCY_ADJUSTER_LIMIT_BY_METRIC_NAME.put(BROKER_PRODUCE_LOCAL_TIME_MS_999TH.name(),\n+                                                  config.getDouble(ExecutorConfig.CONCURRENCY_ADJUSTER_LIMIT_PRODUCE_LOCAL_TIME_MS_CONFIG));\n+    CONCURRENCY_ADJUSTER_LIMIT_BY_METRIC_NAME.put(BROKER_CONSUMER_FETCH_LOCAL_TIME_MS_999TH.name(),\n+                                                  config.getDouble(ExecutorConfig.CONCURRENCY_ADJUSTER_LIMIT_CONSUMER_FETCH_LOCAL_TIME_MS_CONFIG));", "originalCommit": "b6c27d990e37bca8e99bcc19b0caa2f1d1d4b16c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY5NjE5Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462696196", "bodyText": "Thanks for the feedback -- We can definitely look into enriching the set of tracked broker metrics.\nOverall, I feel that bursty requests might lead to an increase in the queue size, but the growing size would be acceptable as long as the per-request time spent on the queue does not increase beyond \"some acceptable\" limit. That being said, as a safety net, we can track RequestQueueSize with a reasonable limit (updated the code to add this).\nController to per broker request queue time metric is interesting, but it breaks CC's metric reporting/sampling assumptions on 1-1 mapping between metrics and brokers -- i.e. all other broker metrics are reported per broker, however, in this case controller reports as many metrics as the number of brokers in the cluster.", "author": "efeg", "createdAt": "2020-07-30T02:24:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMzEwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxMzgwMQ==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462713801", "bodyText": "Some scenarios may not be reflected in RequestQueueSize on the broker side because controller is doing blocking send and receive for the controller request, i.e. it waits for the broker response before sending out the next request in queue.\nOne example can be:\n\nController receives a lot of operations (e.g. partition reassignment, preferred leader election) at the same time, which results in a large number of requests enqueued in the controller broker request queue. Let's say there are 2001 requests needed to be sent out per broker.\nThe broker is processing the controller request at a normal speed. There is no excessive load on the broker so broker side metrics look fine, including RequestQueueSize and Request LocalTime metrics. Let's say the average latency for processing each request is 50ms.\nDue to the natural of the blocking send and receive for controller request, the 2001th request in the controller broker queue can take 2000 * 50ms = 100s to reach the broker since the request is generated. If the controller broker request queue is imbalance across different brokers, it can easily lead to prolong leadership transition time for some partitions.", "author": "hzxa21", "createdAt": "2020-07-30T03:32:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMzEwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIwNDk3Ng==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r463204976", "bodyText": "The controller indeed has a blocking send on a separate thread for each broker -- i.e. https://github.com/linkedin/kafka/blob/2.3-li-1/core/src/main/scala/kafka/controller/ControllerChannelManager.scala#L274.\nIn arguably the typical case with more data plane requests (e.g. produce/consume), a backlogged requestQueue on the broker side would cause the relevant per-broker controller thread to be blocked  (i.e. controller cannot send out more requests to that broker) for a longer period of time on the broker response w/o the KIP-291 support.\nI believe the scenario you exemplified can happen if the controller receives a burst of control plane requests (i.e. LeaderAndIsr, StopReplica, or UpdateMetadata). This is more likely to happen during a\n\nDemote operation,\nPLE,\nReplication factor change, or\nA leadership transfer phase of the other goal-based CC executions, which are configured with a high \"leadership\" movement concurrency (i.e. leadership movement concurrency also applies to replica reorder concurrency during the demote operation -- this generates UpdateMetadata requests) and/or low \"execution progress check\" interval.\n\nExcept for the replication factor changes, which should now be addressed with the latest commit, changes introduced in this PR were already not relevant to these operations. Also note that Concurrency Adjuster comes with a concurrency.adjuster.max.partition.movements.per.broker config, which puts a bound on the extent of concurrency increase. Hence, even if this becomes an issue, there are means to workaround it.\nHowever, once we want to add support to cover auto-concurrency adjustment for the listed cases above, controller metrics will be more relevant. Hence, I created #1292 to address those cases.", "author": "efeg", "createdAt": "2020-07-30T18:55:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjYyMzEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxODM5NQ==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r462718395", "bodyText": "setRequestedInterBrokerPartitionMovementConcurrency is a public method in the Executor class. Can it be called elsewhere? My concern is that we only synchronize the methods in ConcurrencyAdjuster but they are calling methods in the outer class. If the outer class method is invoked without going through ConcurrencyAdjuster, there still can be a race condition.", "author": "hzxa21", "createdAt": "2020-07-30T03:51:16Z", "path": "cruise-control/src/main/java/com/linkedin/kafka/cruisecontrol/executor/Executor.java", "diffHunk": "@@ -288,6 +295,59 @@ public void run() {\n     }\n   }\n \n+  /**\n+   * A runnable class to auto-adjust the allowed inter-broker partition reassignment concurrency for ongoing executions\n+   * using selected broker metrics and based on additive-increase/multiplicative-decrease (AIMD) feedback control algorithm.\n+   * Skips concurrency adjustment for demote operations.\n+   */\n+  private class ConcurrencyAdjuster implements Runnable {\n+    private final int _maxPartitionMovementsPerBroker;\n+    private LoadMonitor _loadMonitor;\n+\n+    public ConcurrencyAdjuster() {\n+      _maxPartitionMovementsPerBroker = _config.getInt(ExecutorConfig.CONCURRENCY_ADJUSTER_MAX_PARTITION_MOVEMENTS_PER_BROKER_CONFIG);\n+      _loadMonitor = null;\n+    }\n+\n+    /**\n+     * Initialize the inter-broker partition reassignment concurrency adjustment with the load monitor and the initially\n+     * requested inter-broker partition reassignment concurrency.\n+     *\n+     * @param loadMonitor Load monitor.\n+     * @param requestedInterBrokerPartitionMovementConcurrency The maximum number of concurrent inter-broker partition movements\n+     *                                                         per broker(if null, use num.concurrent.partition.movements.per.broker).\n+     */\n+    public synchronized void initAdjustment(LoadMonitor loadMonitor, Integer requestedInterBrokerPartitionMovementConcurrency) {\n+      _loadMonitor = loadMonitor;\n+      setRequestedInterBrokerPartitionMovementConcurrency(requestedInterBrokerPartitionMovementConcurrency);", "originalCommit": "eeafad8d1e8eca5cd47febe98e8cde151bb47bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIwNDk0Mw==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r463204943", "bodyText": "The public method Executor#setRequestedInterBrokerPartitionMovementConcurrency can be called to handle Admin endpoint requests, which just calls the synchronized ExecutionTaskManager#setRequestedInterBrokerPartitionMovementConcurrency method to update the concurrency. Why is this a concern -- or is there some other race condition that I am missing?", "author": "efeg", "createdAt": "2020-07-30T18:55:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxODM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM5MzYyNQ==", "url": "https://github.com/linkedin/cruise-control/pull/1289#discussion_r463393625", "bodyText": "Oh I see. I think the missing part is I don't know ExecutionTaskManager#setRequestedInterBrokerPartitionMovementConcurrency is already synchronized. Thanks for the explanation. I don't have a concern then.", "author": "hzxa21", "createdAt": "2020-07-31T04:02:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjcxODM5NQ=="}], "type": "inlineReview"}, {"oid": "e466cfbcb615ddeabfde63fef2be1e76d6f3f7b0", "url": "https://github.com/linkedin/cruise-control/commit/e466cfbcb615ddeabfde63fef2be1e76d6f3f7b0", "message": "Automate replica reassignment concurrency adjustment based on broker metrics", "committedDate": "2020-07-30T18:13:50Z", "type": "commit"}, {"oid": "c9c75d86276099bb1a2c600c6552bc312f9c5f26", "url": "https://github.com/linkedin/cruise-control/commit/c9c75d86276099bb1a2c600c6552bc312f9c5f26", "message": "Address the feedback.", "committedDate": "2020-07-30T18:13:50Z", "type": "commit"}, {"oid": "ce9c98cff81771ae3aff986ccc4592ab4d1ec777", "url": "https://github.com/linkedin/cruise-control/commit/ce9c98cff81771ae3aff986ccc4592ab4d1ec777", "message": "Skip auto refresh concurrency for replication factor changes.", "committedDate": "2020-07-30T18:55:21Z", "type": "commit"}, {"oid": "ce9c98cff81771ae3aff986ccc4592ab4d1ec777", "url": "https://github.com/linkedin/cruise-control/commit/ce9c98cff81771ae3aff986ccc4592ab4d1ec777", "message": "Skip auto refresh concurrency for replication factor changes.", "committedDate": "2020-07-30T18:55:21Z", "type": "forcePushed"}]}