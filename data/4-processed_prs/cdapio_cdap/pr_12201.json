{"pr_number": 12201, "pr_title": "CDAP-16709 batch spark auto-join implementation", "pr_createdAt": "2020-05-20T23:12:28Z", "pr_url": "https://github.com/cdapio/cdap/pull/12201", "timeline": [{"oid": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "url": "https://github.com/cdapio/cdap/commit/57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-21T21:47:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NDQ3MQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430674471", "bodyText": "So the submitterPlugin can be null after the following ifs checks? Is it being handled already? I don't think the logic can this value being null. It is probably better to have a else that throw if the joiner plugin is not of one of the supported classes.", "author": "chtyim", "createdAt": "2020-05-26T20:00:52Z", "path": "cdap-app-templates/cdap-etl/cdap-etl-core/src/main/java/io/cdap/cdap/etl/common/submit/PipelinePhasePreparer.java", "diffHunk": "@@ -89,7 +90,7 @@ public PipelinePhasePreparer(PluginContext pluginContext, Metrics metrics, Macro\n       boolean isConnectorSink =\n         Constants.Connector.PLUGIN_TYPE.equals(pluginType) && phase.getSinks().contains(stageName);\n \n-      SubmitterPlugin submitterPlugin;\n+      SubmitterPlugin submitterPlugin = null;", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NjMwNA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430676304", "bodyText": "Use method reference .filter(JoinStage::isRequired)", "author": "chtyim", "createdAt": "2020-05-26T20:04:30Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3OTM5MA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430679390", "bodyText": "This list can be obtained by sorting the list directly.\nList<JoinStage> orderedStages = new ArrayList<>(joinDefinition.getStages())\norderedStages.sort((s1, s2) -> s1.isRequired() ? (s2.isRequired() ? 0 : -1) : s2.isRequired ? 1 : 0));", "author": "chtyim", "createdAt": "2020-05-26T20:10:41Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MDg5OA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430680898", "bodyText": "Use method reference instead of lambda toMap(JoinKey::getStageName, JoinKey::getFields)", "author": "chtyim", "createdAt": "2020-05-26T20:13:31Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);\n+\n+    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    JoinStage left = stageIter.next();\n+    SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n+    Schema leftSchema = left.getSchema();\n+\n+    JoinCondition condition = joinDefinition.getCondition();\n+    // currently this is the only operation, but check this for future changes\n+    if (condition.getOp() != JoinCondition.Op.KEY_EQUALITY) {\n+      throw new IllegalStateException(\"Unsupport join condition operation \" + condition.getOp());\n+    }\n+    JoinCondition.OnKeys onKeys = (JoinCondition.OnKeys) condition;\n+    Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n+      .collect(Collectors.toMap(j -> j.getStageName(), j -> j.getFields()));", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NjgzMw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430676833", "bodyText": "nit - extra line", "author": "yaojiefeng", "createdAt": "2020-05-26T20:05:39Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());\n+    orderedStages.addAll(requiredStages);\n+    orderedStages.addAll(optionalStages);\n+\n+    Iterator<JoinStage> stageIter = orderedStages.iterator();\n+    JoinStage left = stageIter.next();\n+    SparkCollection<Object> leftCollection = inputDataCollections.get(left.getStageName());\n+    Schema leftSchema = left.getSchema();\n+\n+    JoinCondition condition = joinDefinition.getCondition();\n+    // currently this is the only operation, but check this for future changes\n+    if (condition.getOp() != JoinCondition.Op.KEY_EQUALITY) {\n+      throw new IllegalStateException(\"Unsupport join condition operation \" + condition.getOp());\n+    }\n+    JoinCondition.OnKeys onKeys = (JoinCondition.OnKeys) condition;\n+    Map<String, List<String>> stageKeys = onKeys.getKeys().stream()\n+      .collect(Collectors.toMap(j -> j.getStageName(), j -> j.getFields()));\n+", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY3NzE3Mw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430677173", "bodyText": "This method and the handleJoin method is non-trivial logic, it is better to add some javadoc about the logic", "author": "yaojiefeng", "createdAt": "2020-05-26T20:06:18Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430681752", "bodyText": "Can you explain a bit why we want an ordered stages? From the logic below, we are basically going through the list one by one, and then determine the join type based on left is required or right is required. Why do we want to loop through required stages first?", "author": "yaojiefeng", "createdAt": "2020-05-26T20:15:06Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -378,6 +356,123 @@ public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n     }\n   }\n \n+  private SparkCollection<Object> handleAutoJoin(AutoJoiner autoJoiner, AutoJoinerContext autoJoinerContext,\n+                                                 Map<String, SparkCollection<Object>> inputDataCollections) {\n+    JoinDefinition joinDefinition = autoJoiner.define(autoJoinerContext);\n+\n+    // join required stages first to cut down the data as much as possible\n+    List<JoinStage> requiredStages = joinDefinition.getStages().stream()\n+      .filter(s -> s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> optionalStages = joinDefinition.getStages().stream()\n+      .filter(s -> !s.isRequired())\n+      .collect(Collectors.toList());\n+    List<JoinStage> orderedStages = new ArrayList<>(requiredStages.size() + optionalStages.size());", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcxNTY5OA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430715698", "bodyText": "there's a comment above but I can expand on it. If there is an outer join and an inner join, it is more performant to do the inner join first because it will generate less intermediate data.", "author": "albertshau", "createdAt": "2020-05-26T21:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczODEyNQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430738125", "bodyText": "though now that I think more about it, it may be better not to do this, and just follow the order provided by the plugin. Can add this back in if it helps once we experiment a bit more on uneven joins.", "author": "albertshau", "createdAt": "2020-05-26T22:17:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTc1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MjI4MA==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430682280", "bodyText": "Is this used anywhere? The getter doesn't seem to get called. Also it is good to add some comment on what this means.", "author": "yaojiefeng", "createdAt": "2020-05-26T20:16:09Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/join/JoinCollection.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.join;\n+\n+import io.cdap.cdap.api.data.schema.Schema;\n+import io.cdap.cdap.etl.spark.SparkCollection;\n+\n+import java.util.List;\n+\n+/**\n+ * Data to join.\n+ */\n+public class JoinCollection {\n+  private final String stage;\n+  private final String type;\n+  private final SparkCollection<?> data;\n+  private final Schema schema;\n+  private final List<String> key;\n+  private final boolean broadcast;", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc0MTk5Mw==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430741993", "bodyText": "not currently, was going to implement it later.", "author": "albertshau", "createdAt": "2020-05-26T22:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MjI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NTgxMQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430685811", "bodyText": "I feel it is good to add some comment on why we need different implementations for spark 1 and 2. Sometimes it takes some time for me to understand the difference. Some javadoc will help understand the compatibility or implementation difference between spark 1 and 2", "author": "yaojiefeng", "createdAt": "2020-05-26T20:22:54Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core2_2.11/src/main/java/io/cdap/cdap/etl/spark/batch/RDDCollection.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.batch;\n+\n+import io.cdap.cdap.api.data.DatasetContext;\n+import io.cdap.cdap.api.data.format.StructuredRecord;\n+import io.cdap.cdap.api.data.schema.Schema;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.api.spark.sql.DataFrames;\n+import io.cdap.cdap.etl.api.join.JoinField;\n+import io.cdap.cdap.etl.spark.SparkCollection;\n+import io.cdap.cdap.etl.spark.join.JoinCollection;\n+import io.cdap.cdap.etl.spark.join.JoinRequest;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n+import org.apache.spark.sql.types.StructType;\n+import scala.collection.JavaConversions;\n+import scala.collection.Seq;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Spark2 RDD collection.\n+ *\n+ * @param <T> type of object in the collection\n+ */\n+public class RDDCollection<T> extends BaseRDDCollection<T> {", "originalCommit": "57d102bc7d18a8cbe52f1b1ebdeeddae67555d96", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1NDk1NQ==", "url": "https://github.com/cdapio/cdap/pull/12201#discussion_r430754955", "bodyText": "is a just -> is just", "author": "yaojiefeng", "createdAt": "2020-05-26T23:05:49Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/BaseRDDCollection.java", "diffHunk": "@@ -54,29 +54,35 @@\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.api.java.function.FlatMapFunction;\n import org.apache.spark.api.java.function.PairFlatMapFunction;\n+import org.apache.spark.sql.SQLContext;\n import org.apache.spark.storage.StorageLevel;\n import scala.Tuple2;\n \n import javax.annotation.Nullable;\n \n \n /**\n- * Implementation of {@link SparkCollection} that is backed by a JavaRDD.\n+ * Implementation of {@link SparkCollection} that is backed by a JavaRDD. Spark1 and Spark2 implementations need to be\n+ * separate because DataFrames are not compatible between Spark1 and Spark2. In Spark2, DataFrame is a just a", "originalCommit": "b7f16f4f3acbd1b1437b84e6e4841a902744da55", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "url": "https://github.com/cdapio/cdap/commit/e31279cbb49f69a3b78d3482f7a0c792cd55a65c", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T00:46:41Z", "type": "forcePushed"}, {"oid": "d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "url": "https://github.com/cdapio/cdap/commit/d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T03:14:27Z", "type": "commit"}, {"oid": "d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "url": "https://github.com/cdapio/cdap/commit/d50f0ea4aa9c968e8b9e5645ea5adb348ec721e4", "message": "CDAP-16709 batch spark auto-join implementation\n\nImplemented auto join for batch spark pipelines.\n\nAdded a join method to SparkCollection that takes in the list of\nother SparkCollections that it should be joined to.\nRDDCollection converts RDDs into Datasets and uses the Dataset\njoin method to implement the join. This allows Spark to broadcast\nsmall datasets automatically, and to use sort merge join instead\nof shuffle hash join, which has better memory characteristics.\n\nAs part of this, added a separate RDDCollection implementation for\nSpark1 and Spark2, since the Spark API for joins is not compatible.", "committedDate": "2020-05-27T03:14:27Z", "type": "forcePushed"}]}