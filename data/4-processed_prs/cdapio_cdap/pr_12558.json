{"pr_number": 12558, "pr_title": "CDAP-17078 consolidate stages within a group", "pr_createdAt": "2020-08-03T23:44:11Z", "pr_url": "https://github.com/cdapio/cdap/pull/12558", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTMyNjY0NQ==", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465326645", "bodyText": "isGroup is never used", "author": "MEseifan", "createdAt": "2020-08-04T20:58:51Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/SparkPipelineRunner.java", "diffHunk": "@@ -107,28 +118,52 @@\n     SparkPairCollection<Object, List<JoinElement<Object>>> joinedInputs,\n     StageStatisticsCollector collector) throws Exception;\n \n-  public void runPipeline(PipelinePhase pipelinePhase, String sourcePluginType,\n+  public void runPipeline(PhaseSpec phaseSpec, String sourcePluginType,\n                           JavaSparkExecutionContext sec,\n                           Map<String, Integer> stagePartitions,\n                           PluginContext pluginContext,\n-                          Map<String, StageStatisticsCollector> collectors) throws Exception {\n-\n+                          Map<String, StageStatisticsCollector> collectors,\n+                          Set<String> uncombinableSinks,\n+                          boolean consolidateStages) throws Exception {\n+    PipelinePhase pipelinePhase = phaseSpec.getPhase();\n+    BasicArguments arguments = new BasicArguments(sec);\n     MacroEvaluator macroEvaluator =\n-      new DefaultMacroEvaluator(new BasicArguments(sec),\n-                                sec.getLogicalStartTime(), sec,\n-                                sec.getNamespace());\n+      new DefaultMacroEvaluator(arguments, sec.getLogicalStartTime(), sec, sec.getNamespace());\n     Map<String, EmittedRecords> emittedRecords = new HashMap<>();\n \n     // should never happen, but removes warning\n     if (pipelinePhase.getDag() == null) {\n       throw new IllegalStateException(\"Pipeline phase has no connections.\");\n     }\n \n+    Set<String> uncombinableStages = new HashSet<>(uncombinableSinks);\n+    for (String uncombinableType : UNCOMBINABLE_PLUGIN_TYPES) {\n+      pipelinePhase.getStagesOfType(uncombinableType).stream()\n+        .map(StageSpec::getName)\n+        .forEach(s -> uncombinableStages.add(s));\n+    }\n+\n+    CombinerDag groupedDag = new CombinerDag(pipelinePhase.getDag(), uncombinableStages);\n+    Map<String, Set<String>> groups = consolidateStages ? groupedDag.groupNodes() : Collections.emptyMap();\n+    if (!groups.isEmpty()) {\n+      LOG.debug(\"Stage consolidation is on.\");\n+      int groupNum = 1;\n+      for (Set<String> group : groups.values()) {\n+        LOG.debug(\"Group{}: {}\", groupNum, group);\n+        groupNum++;\n+      }\n+    }\n+\n     Collection<Runnable> sinkRunnables = new ArrayList<>();\n-    for (String stageName : pipelinePhase.getDag().getTopologicalOrder()) {\n+    for (String stageName : groupedDag.getTopologicalOrder()) {\n+      boolean isGroup = groups.containsKey(stageName);", "originalCommit": "5de8fd11d4bde8bc7d42149666495eaec485b1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MTY3NA==", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465351674", "bodyText": "This can be for (String outputName : sinkOutputNames)", "author": "MEseifan", "createdAt": "2020-08-04T21:53:32Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/batch/SparkBatchSinkFactory.java", "diffHunk": "@@ -76,18 +98,65 @@ private void addOutput(String stageName, String datasetName, String alias, Map<S\n     addStageOutput(stageName, alias);\n   }\n \n+  /**\n+   * Writes a combined RDD using multiple OutputFormatProviders.\n+   * Returns the set of output names that were written, which still require dataset lineage to be recorded.\n+   */\n+  public <K, V> Set<String> writeCombinedRDD(JavaPairRDD<String, KeyValue<K, V>> combinedRDD,\n+                                             JavaSparkExecutionContext sec, Set<String> sinkNames) {\n+    Map<String, OutputFormatProvider> outputs = new HashMap<>();\n+    Set<String> lineageNames = new HashSet<>();\n+    for (String sinkName : sinkNames) {\n+      Set<String> sinkOutputNames = sinkOutputs.get(sinkName);\n+      if (sinkOutputNames == null || sinkOutputNames.isEmpty()) {\n+        // should never happen if validation happened correctly at pipeline configure time\n+        throw new IllegalStateException(sinkName + \" has no outputs. \" +\n+                                          \"Please check that the sink calls addOutput at some point.\");\n+      }\n+\n+      for (String outputName : sinkOutputs.get(sinkName)) {", "originalCommit": "5de8fd11d4bde8bc7d42149666495eaec485b1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MjU0Mw==", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465352543", "bodyText": "unfinished comment?", "author": "MEseifan", "createdAt": "2020-08-04T21:55:36Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.function;\n+\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.dataset.lib.KeyValue;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.preview.DataTracer;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.ErrorTransform;\n+import io.cdap.cdap.etl.batch.PipelinePluginInstantiator;\n+import io.cdap.cdap.etl.batch.connector.SingleConnectorFactory;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.Constants;\n+import io.cdap.cdap.etl.common.DefaultEmitter;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.RecordType;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.exec.PipeTransformExecutor;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.SparkTransformExecutorFactory;\n+import scala.Tuple2;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Executes transforms leading into a grouped sink.\n+ * With a pipeline like:\n+ *\n+ *      |--> t1 --> k1\n+ * s1 --|\n+ *      |--> k2\n+ *           ^\n+ *     s2 ---|\n+ *\n+ * the group will include stages t1, k1, and k2:\n+ *\n+ *  s1 --|\n+ *       |--> group1\n+ *  s2 --|\n+ *\n+ * note that although s1 and s2 are both inputs to the group, the records for s2 should only be sent to\n+ * k2 and not to t1. Thus, every input record must be tagged with the stage that it came from, so that they can\n+ * be sent to the right underlying stages. This is why the input is a Tuple2. The key is the name of the stage\n+ * that emitted it and the value is the actual record.\n+ *\n+ * This function is meant to be executed right before saving the Spark collection using a Multi OutputFormat that\n+ * delegates to underlying output formats. This is", "originalCommit": "5de8fd11d4bde8bc7d42149666495eaec485b1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1ODE4Ng==", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465358186", "bodyText": "Should this say \"(t1 and k2 in the example above).\"? Since k1 has t1 as an input which means it shouldnt be a source given the definition provided", "author": "MEseifan", "createdAt": "2020-08-04T22:09:46Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/function/MultiSinkFunction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.cdap.etl.spark.function;\n+\n+import com.google.common.collect.Sets;\n+import io.cdap.cdap.api.dataset.lib.KeyValue;\n+import io.cdap.cdap.api.macro.MacroEvaluator;\n+import io.cdap.cdap.api.preview.DataTracer;\n+import io.cdap.cdap.api.spark.JavaSparkExecutionContext;\n+import io.cdap.cdap.etl.api.ErrorTransform;\n+import io.cdap.cdap.etl.batch.PipelinePluginInstantiator;\n+import io.cdap.cdap.etl.batch.connector.SingleConnectorFactory;\n+import io.cdap.cdap.etl.common.BasicArguments;\n+import io.cdap.cdap.etl.common.Constants;\n+import io.cdap.cdap.etl.common.DefaultEmitter;\n+import io.cdap.cdap.etl.common.DefaultMacroEvaluator;\n+import io.cdap.cdap.etl.common.PhaseSpec;\n+import io.cdap.cdap.etl.common.PipelinePhase;\n+import io.cdap.cdap.etl.common.PipelineRuntime;\n+import io.cdap.cdap.etl.common.RecordInfo;\n+import io.cdap.cdap.etl.common.RecordType;\n+import io.cdap.cdap.etl.common.StageStatisticsCollector;\n+import io.cdap.cdap.etl.exec.PipeTransformExecutor;\n+import io.cdap.cdap.etl.proto.v2.spec.StageSpec;\n+import io.cdap.cdap.etl.spark.SparkTransformExecutorFactory;\n+import scala.Tuple2;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Set;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * Executes transforms leading into a grouped sink.\n+ * With a pipeline like:\n+ *\n+ *      |--> t1 --> k1\n+ * s1 --|\n+ *      |--> k2\n+ *           ^\n+ *     s2 ---|\n+ *\n+ * the group will include stages t1, k1, and k2:\n+ *\n+ *  s1 --|\n+ *       |--> group1\n+ *  s2 --|\n+ *\n+ * note that although s1 and s2 are both inputs to the group, the records for s2 should only be sent to\n+ * k2 and not to t1. Thus, every input record must be tagged with the stage that it came from, so that they can\n+ * be sent to the right underlying stages. This is why the input is a Tuple2. The key is the name of the stage\n+ * that emitted it and the value is the actual record.\n+ *\n+ * This function is meant to be executed right before saving the Spark collection using a Multi OutputFormat that\n+ * delegates to underlying output formats. This is\n+ */\n+public class MultiSinkFunction implements PairFlatMapFunc<RecordInfo<Object>, String, KeyValue<Object, Object>> {\n+  private final PipelineRuntime pipelineRuntime;\n+  private final PhaseSpec phaseSpec;\n+  private final Set<String> group;\n+  private final Map<String, DataTracer> dataTracers;\n+  private final Map<String, StageStatisticsCollector> collectors;\n+  private transient DefaultEmitter<Tuple2<String, KeyValue<Object, Object>>> emitter;\n+  private transient SparkTransformExecutorFactory executorFactory;\n+  private transient Map<InputInfo, Set<String>> inputConnections;\n+  private transient Map<String, PipeTransformExecutor<Object>> branchExecutors;\n+\n+  public MultiSinkFunction(JavaSparkExecutionContext sec, PhaseSpec phaseSpec, Set<String> group,\n+                           Map<String, StageStatisticsCollector> collectors) {\n+    this.pipelineRuntime = new PipelineRuntime(\n+      sec.getNamespace(), sec.getApplicationSpecification().getName(), sec.getLogicalStartTime(),\n+      new BasicArguments(sec), sec.getMetrics(), sec.getPluginContext(), sec.getServiceDiscoverer(),\n+      sec.getSecureStore(), null, null);\n+    // create a copy because BatchPhaseSpec contains things that are not serializable while PhaseSpec does not\n+    this.phaseSpec = new PhaseSpec(phaseSpec.getPhaseName(), phaseSpec.getPhase(), phaseSpec.getConnectorDatasets(),\n+                                   phaseSpec.isStageLoggingEnabled(), phaseSpec.isProcessTimingEnabled());\n+    this.group = group;\n+    this.collectors = collectors;\n+    this.dataTracers = new HashMap<>();\n+    for (String stage : group) {\n+      dataTracers.put(stage, sec.getDataTracer(stage));\n+    }\n+  }\n+\n+  @Override\n+  public Iterable<Tuple2<String, KeyValue<Object, Object>>> call(RecordInfo<Object> input) throws Exception {\n+    if (branchExecutors == null) {\n+      // branch executors must be created lazily here instead of passed into the constructor to ensure that\n+      // they are not serialized in the function. This ensures that macros are evaluated each run instead of just for\n+      // the first run and then serialized.\n+      initializeBranchExecutors();\n+    }\n+\n+    /*\n+       Input records are a union of RecordInfo<Object> from all possible inputs to the group.\n+       For example, suppose the pipeline looks like:\n+\n+                              |port A --> agg --> k1\n+            s1 --> splitter --|\n+                      |       |port B --> k2\n+                      |\n+                      |--> error collector --> k3\n+\n+       In this scenario, k2, error collector, and k3 are grouped together.\n+       However, the input will contain all records output by the splitter.\n+       More specifically, it contains outputs for portA, outputs for portB, and errors.\n+       Records from portB need to be sent to the k2 branch, errors need to be sent to the error collector branch,\n+       and portA records need to be dropped.\n+     */\n+    InputInfo inputInfo = new InputInfo(input.getFromStage(), input.getType(), input.getFromPort());\n+    Object record = input.getValue();\n+    emitter.reset();\n+\n+    /*\n+        inputConnections contains a map from input source to the branch that should receive it.\n+        With the example pipeline above, it will look like:\n+          { stageName: splitter, port: B, type: output } -> [k2]\n+          { stageName: splitter, type: error } -> [error collector]\n+     */\n+    Set<String> groupSources = inputConnections.getOrDefault(inputInfo, Collections.emptySet());\n+    for (String groupSource : groupSources) {\n+      branchExecutors.get(groupSource).runOneIteration(record);\n+    }\n+\n+    return emitter.getEntries();\n+  }\n+\n+  private void initializeBranchExecutors() {\n+    emitter = new DefaultEmitter<>();\n+    PipelinePluginInstantiator pluginInstantiator =\n+      new PipelinePluginInstantiator(pipelineRuntime.getPluginContext(), pipelineRuntime.getMetrics(),\n+                                     phaseSpec, new SingleConnectorFactory());\n+    MacroEvaluator macroEvaluator = new DefaultMacroEvaluator(\n+      pipelineRuntime.getArguments(), pipelineRuntime.getLogicalStartTime(), pipelineRuntime.getSecureStore(),\n+      pipelineRuntime.getNamespace());\n+    executorFactory = new SparkTransformExecutorFactory(pluginInstantiator, macroEvaluator, null,\n+                                                        collectors, dataTracers, pipelineRuntime, emitter);\n+\n+    /*\n+       If the dag is:\n+\n+            |--> t1 --> k1\n+       s1 --|\n+            |--> k2\n+                 ^\n+           s2 ---|\n+\n+       the group is t1, k1, and k2.\n+     */\n+    PipelinePhase pipelinePhase = phaseSpec.getPhase();\n+    branchExecutors = new HashMap<>();\n+    inputConnections = new HashMap<>();\n+    for (String groupSource : group) {\n+      // start by finding the \"sources\" of the group (t1 and k1 in the example above).\n+      // group \"sources\" are stages in the group that don't have an input from another stage in the group.", "originalCommit": "5de8fd11d4bde8bc7d42149666495eaec485b1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM3NjYyOQ==", "url": "https://github.com/cdapio/cdap/pull/12558#discussion_r465376629", "bodyText": "Should we create a JIRA to track this and link it in this TODO?", "author": "MEseifan", "createdAt": "2020-08-04T23:00:16Z", "path": "cdap-app-templates/cdap-etl/hydrator-spark-core-base/src/main/java/io/cdap/cdap/etl/spark/streaming/DStreamCollection.java", "diffHunk": "@@ -168,6 +176,13 @@ public void run() {\n     };\n   }\n \n+  @Override\n+  public Runnable createMultiStoreTask(Set<String> sinkNames,\n+                                       PairFlatMapFunction<T, String, KeyValue<Object, Object>> multiSinkFunction) {\n+    // TODO: implement", "originalCommit": "5de8fd11d4bde8bc7d42149666495eaec485b1ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5995e061fc4e9b6b5d9ccb02235040665eb1f6cb", "url": "https://github.com/cdapio/cdap/commit/5995e061fc4e9b6b5d9ccb02235040665eb1f6cb", "message": "CDAP-17078 consolidate stages within a group\n\nChanged the SparkPipelineRunner to use a CombinerDag to group\nsinks and their preceding transforms together. These grouped\nstages are treated similarly to how a single sink is treated,\nwith flatMapToPair() called on the input RDD to transform it\ninto a PairRDD, then calling save() to write the RDD out.\nThis capability is off by default, but can be turned on by\nsetting a runtime argument.\n\nInstead of flatMapToPair() calling just the sink's transform\nmethod, a new MultiSinkFunction class is used to direct incoming\nrecords to the correct logical branches of the pipeline.\nThis requires that each input be tagged with which stage it\ncame from (stage and port), as well as its type (output, or error).\nIn order to do this, refactored the SparkPipelineRunner a bit\nto maintain the RDD<RecordInfo> for each stage rather than\nRDD<StructuredRecord>, as the RecordInfo class contains that\nextra information.\n\nAlso added a MultiOutputFormat that will take the output of the\nMultiSinkFunction and delegate writes to the correct underlying\nOutputFormat. Since the OutputFormat lives in the pipeline\napp, this approach means CDAP datasets cannot be combined.\nThis caused a problem with dataset lineage, since it is\nimplemented by implemented by wrapping OutputFormats into a hidden\nExternalDataset class in CDAP. Instead of doing this indirect\nwrapping, changed the SparkSinkFactory class to explicitly\nregister lineage through direct calls instead of hiding it\nunder several layers of abstraction.", "committedDate": "2020-08-05T18:08:00Z", "type": "forcePushed"}, {"oid": "7e4f1e1fe9cf01b9fa3efeb05b3c76865a0633bb", "url": "https://github.com/cdapio/cdap/commit/7e4f1e1fe9cf01b9fa3efeb05b3c76865a0633bb", "message": "CDAP-17078 consolidate stages within a group\n\nChanged the SparkPipelineRunner to use a CombinerDag to group\nsinks and their preceding transforms together. These grouped\nstages are treated similarly to how a single sink is treated,\nwith flatMapToPair() called on the input RDD to transform it\ninto a PairRDD, then calling save() to write the RDD out.\nThis capability is off by default, but can be turned on by\nsetting a runtime argument.\n\nInstead of flatMapToPair() calling just the sink's transform\nmethod, a new MultiSinkFunction class is used to direct incoming\nrecords to the correct logical branches of the pipeline.\nThis requires that each input be tagged with which stage it\ncame from (stage and port), as well as its type (output, or error).\nIn order to do this, refactored the SparkPipelineRunner a bit\nto maintain the RDD<RecordInfo> for each stage rather than\nRDD<StructuredRecord>, as the RecordInfo class contains that\nextra information.\n\nAlso added a MultiOutputFormat that will take the output of the\nMultiSinkFunction and delegate writes to the correct underlying\nOutputFormat. Since the OutputFormat lives in the pipeline\napp, this approach means CDAP datasets cannot be combined.\nThis caused a problem with dataset lineage, since it is\nimplemented by implemented by wrapping OutputFormats into a hidden\nExternalDataset class in CDAP. Instead of doing this indirect\nwrapping, changed the SparkSinkFactory class to explicitly\nregister lineage through direct calls instead of hiding it\nunder several layers of abstraction.", "committedDate": "2020-08-05T22:44:44Z", "type": "commit"}, {"oid": "7e4f1e1fe9cf01b9fa3efeb05b3c76865a0633bb", "url": "https://github.com/cdapio/cdap/commit/7e4f1e1fe9cf01b9fa3efeb05b3c76865a0633bb", "message": "CDAP-17078 consolidate stages within a group\n\nChanged the SparkPipelineRunner to use a CombinerDag to group\nsinks and their preceding transforms together. These grouped\nstages are treated similarly to how a single sink is treated,\nwith flatMapToPair() called on the input RDD to transform it\ninto a PairRDD, then calling save() to write the RDD out.\nThis capability is off by default, but can be turned on by\nsetting a runtime argument.\n\nInstead of flatMapToPair() calling just the sink's transform\nmethod, a new MultiSinkFunction class is used to direct incoming\nrecords to the correct logical branches of the pipeline.\nThis requires that each input be tagged with which stage it\ncame from (stage and port), as well as its type (output, or error).\nIn order to do this, refactored the SparkPipelineRunner a bit\nto maintain the RDD<RecordInfo> for each stage rather than\nRDD<StructuredRecord>, as the RecordInfo class contains that\nextra information.\n\nAlso added a MultiOutputFormat that will take the output of the\nMultiSinkFunction and delegate writes to the correct underlying\nOutputFormat. Since the OutputFormat lives in the pipeline\napp, this approach means CDAP datasets cannot be combined.\nThis caused a problem with dataset lineage, since it is\nimplemented by implemented by wrapping OutputFormats into a hidden\nExternalDataset class in CDAP. Instead of doing this indirect\nwrapping, changed the SparkSinkFactory class to explicitly\nregister lineage through direct calls instead of hiding it\nunder several layers of abstraction.", "committedDate": "2020-08-05T22:44:44Z", "type": "forcePushed"}]}