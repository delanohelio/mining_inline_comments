{"pr_number": 1918, "pr_title": "Moving 2 tests from KafkaMP to KafkaSE", "pr_createdAt": "2020-06-02T06:35:52Z", "pr_url": "https://github.com/oracle/helidon/pull/1918", "timeline": [{"oid": "bc08aced21d655809d4fc2fa95f914f930d27159", "url": "https://github.com/oracle/helidon/commit/bc08aced21d655809d4fc2fa95f914f930d27159", "message": "Moving 2 tests from KafkaMP to KafkaSE\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-06-01T15:53:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3OTIxOA==", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438779218", "bodyText": "This config seems not to be used anywhere", "author": "danielkec", "createdAt": "2020-06-11T13:27:06Z", "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "originalCommit": "bc08aced21d655809d4fc2fa95f914f930d27159", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTM4NDUzNQ==", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r439384535", "bodyText": "I forgot to remove it. Now that config is in AbstractKafkaTest.produceAndCheck", "author": "jbescos", "createdAt": "2020-06-12T12:19:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3OTIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc4MDIyMQ==", "url": "https://github.com/oracle/helidon/pull/1918#discussion_r438780221", "bodyText": "here too", "author": "danielkec", "createdAt": "2020-06-11T13:28:32Z", "path": "tests/integration/kafka/src/test/java/io/helidon/messaging/connectors/kafka/KafkaSeTest.java", "diffHunk": "@@ -334,4 +349,107 @@ void kafkaHeaderTest() throws InterruptedException {\n             messaging.stop();\n         }\n     }\n+\n+    @Test\n+    void someEventsNoAckWithOnePartition() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithOnePartition()\");\n+        final String GROUP = \"group_1\";\n+        final String TOPIC = TEST_SE_TOPIC_6;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        List<String> uncommit = new ArrayList<>();\n+        Channel6 kafkaConsumingBean = new Channel6();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());\n+        Messaging messaging = Messaging.builder().connector(KafkaConnector.create())\n+                .subscriber(fromKafka, ReactiveStreams.<KafkaMessage<Long, String>>builder()\n+                        .forEach(msg -> kafkaConsumingBean.onMsg(msg)))\n+                .build();\n+        try {\n+            messaging.start();\n+            // Push some messages that will ACK\n+            List<String> testData = IntStream.range(0, 20).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // Next message will not ACK\n+            testData = Arrays.asList(Channel6.NO_ACK);\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+            kafkaConsumingBean.restart();\n+            // As this topic only have one partition, next messages will not ACK because previous message wasn't\n+            testData = IntStream.range(100, 120).mapToObj(i -> Integer.toString(i)).collect(Collectors.toList());\n+            uncommit.addAll(testData);\n+            produceAndCheck(kafkaConsumingBean, testData, TOPIC, testData);\n+        } finally {\n+            messaging.stop();\n+        }\n+        // We receive uncommitted messages again\n+        List<String> events = readTopic(TOPIC, uncommit.size(), GROUP);\n+        Collections.sort(events);\n+        Collections.sort(uncommit);\n+        assertEquals(uncommit, events);\n+    }\n+\n+    @Test\n+    void someEventsNoAckWithDifferentPartitions() {\n+        LOGGER.fine(() -> \"==========> test someEventsNoAckWithDifferentPartitions()\");\n+        final long FROM = 2000;\n+        final long TO = FROM + Channel8.LIMIT;\n+        final String GROUP = \"group_2\";\n+        final String TOPIC = TEST_SE_TOPIC_7;\n+        Channel<String> fromKafka = Channel.<String>builder()\n+                .name(\"from-kafka\")\n+                .publisherConfig(KafkaConnector.configBuilder()\n+                        .bootstrapServers(KAFKA_SERVER)\n+                        .groupId(GROUP)\n+                        .topic(TOPIC)\n+                        .autoOffsetReset(KafkaConfigBuilder.AutoOffsetReset.EARLIEST)\n+                        .enableAutoCommit(false)\n+                        .keyDeserializer(LongDeserializer.class)\n+                        .valueDeserializer(StringDeserializer.class)\n+                        .build()\n+                )\n+                .build();\n+        // Send the message that will not ACK. This will make in one partition to not commit any new message\n+        Channel8 kafkaConsumingBean = new Channel8();\n+        Map<String, Object> config = new HashMap<>();\n+        config.put(\"bootstrap.servers\", KAFKA_SERVER);\n+        config.put(\"key.serializer\", LongSerializer.class.getName());\n+        config.put(\"value.serializer\", StringSerializer.class.getName());", "originalCommit": "bc08aced21d655809d4fc2fa95f914f930d27159", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "13a481e3ee88cbe091868eae00113b989adad54f", "url": "https://github.com/oracle/helidon/commit/13a481e3ee88cbe091868eae00113b989adad54f", "message": "Removed unused fields\n\nSigned-off-by: Jorge Bescos Gascon <jorge.bescos.gascon@oracle.com>", "committedDate": "2020-06-12T12:18:01Z", "type": "commit"}]}