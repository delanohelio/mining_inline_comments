{"pr_number": 333, "pr_title": "Batch Metrics API Design", "pr_createdAt": "2020-08-03T20:36:22Z", "pr_url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333", "timeline": [{"oid": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "message": "Add batch metrics api design doc", "committedDate": "2020-08-18T22:26:25Z", "type": "commit"}, {"oid": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "message": "Add batch metrics api design doc", "committedDate": "2020-08-18T22:26:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI1OTEwNg==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r478259106", "bodyText": "We should explain what samplingperiod means. I suppose it reflects the number of data points returned, e.g.  a sampling period of 30 will return 1 data point for every 30 seconds, i.e. 2 data points per minute. Would help if docs explain this in more detail along with an example.", "author": "vigyasharma", "createdAt": "2020-08-27T08:48:55Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,160 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.\n+\n+## Design\n+First, we would like to expand the retention period from beyond 5s. Currently, each cluster retains the past 5s of performance data in the form of a sqlite database (metricsdb files). We can naively expand the retention period by simply retaining more of these databases and allowing users to query from them.\n+\n+![Retaining more metricsdb files](/docs/images/batch-metrics-api-1.png)\n+\n+*Figure 1. Retaining more metricsdb files*\n+\n+The second issue is the limited granularity of the data. We can simply expand the granularity by allowing users to query the raw data for each metric rather than responding with an aggregate of that data. One drawback of this approach is that the response size will drastically increase. This prevents us from supplying a \u201cnodes=all\u201d parameter like with the metrics API, as it would require a single node to gather that data from all the other nodes and retain that data in memory before responding to the user. So, users will have to query individual nodes in order to gather performance metrics from those nodes.\n+\n+All the raw metrics data from a period of time may be too high granularity for some users. One approach to limiting this granularity might be to respond with some aggregate of that data, like with the metrics API. However, a more compute-efficient, practical, and useful approach would be to instead respond with a sample of that raw data. The available datapoints are currently in the form of 5s granularity bins (each metricsdb file collects 5s of data). We can efficiently sample from these bins by providing users with a \u201csampling-period\u201d parameter. We would partition the available data according to the sampling period and simply respond with the data from the first bin in each partition.\n+\n+![Sampling from bins](/docs/images/batch-metrics-api-2.png)\n+\n+*Figure 2. Sampling from bins*\n+\n+## API\n+\n+**performance-analyzer.properties**\n+* retention-period: The number of minutes worth of metrics data to collect. The configured retention period can be read via the \u201c/_opendistro/performanceanalyzer/batch/config\u201d api. Default=7 (7 minutes), min=1, max = 60.\n+  * Note, the default is 7 minutes because a typical use-case would be to query for 5 minutes worth of data from the node. In order to do this, a client would actually select a starttime of now-6min and an endtime of now-1min (this one minute offset will give sufficient time for the metrics in the time range to be available at the node). Atop this 6 minutes of retention, we need an extra 1 minute of retention to account for the time that would have passed by the time the query arrives at the node, and for the fact that starttime and endtime will be rounded down to the nearest sampling-period.\n+\n+**API**\n+\n+Queries:\n+\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/cluster/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* GET \\<endpoint\\>:9200/_opendistro/_performanceanalyzer/_agent/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+* GET \\<endpoint\\>:9600/_opendistro/_performanceanalyzer/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+\n+Parameters:\n+* metrics: list of metrics\n+* starttime: Unix timestamp (difference between the current time and midnight, January 1, 1970 UTC) determining the oldest data point to return. starttime is inclusive \u2014 data points from at or after the starttime will be returned. Note, the starttime and endtime supplied by the user with both be rounded down to the nearest sampling-period.\n+* endtime: Unix timestamp determining the freshest data point to return. endtime is exclusive \u2014 only datapoints from before the endtime will be returned.\n+* samplingperiod: The sampling period in seconds. Must be no less than 5, must be less than the retention period, and must be a multiple of 5. The default is 5s.", "originalCommit": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM2NTg0NA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483365844", "bodyText": "This was covered in design section of the doc, but will add it here also.", "author": "ricardolstephen", "createdAt": "2020-09-04T03:33:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI1OTEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU3MDU4Mw==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r478570583", "bodyText": "nit: fine granularity doesn't mean raw, it means smaller rollup windows like 1s vs 5s vs 30s for example", "author": "sidheart", "createdAt": "2020-08-27T17:09:56Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,160 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.", "originalCommit": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUwMzk0Nw==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r481503947", "bodyText": "It's finer granularity if you measure by datapoints rather than time.", "author": "ricardolstephen", "createdAt": "2020-09-02T00:24:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU3MDU4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5MDA4Ng==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r478590086", "bodyText": "This isn't more useful, it is more compute-efficient.", "author": "sidheart", "createdAt": "2020-08-27T17:43:57Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,160 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.\n+\n+## Design\n+First, we would like to expand the retention period from beyond 5s. Currently, each cluster retains the past 5s of performance data in the form of a sqlite database (metricsdb files). We can naively expand the retention period by simply retaining more of these databases and allowing users to query from them.\n+\n+![Retaining more metricsdb files](/docs/images/batch-metrics-api-1.png)\n+\n+*Figure 1. Retaining more metricsdb files*\n+\n+The second issue is the limited granularity of the data. We can simply expand the granularity by allowing users to query the raw data for each metric rather than responding with an aggregate of that data. One drawback of this approach is that the response size will drastically increase. This prevents us from supplying a \u201cnodes=all\u201d parameter like with the metrics API, as it would require a single node to gather that data from all the other nodes and retain that data in memory before responding to the user. So, users will have to query individual nodes in order to gather performance metrics from those nodes.\n+\n+All the raw metrics data from a period of time may be too high granularity for some users. One approach to limiting this granularity might be to respond with some aggregate of that data, like with the metrics API. However, a more compute-efficient, practical, and useful approach would be to instead respond with a sample of that raw data. The available datapoints are currently in the form of 5s granularity bins (each metricsdb file collects 5s of data). We can efficiently sample from these bins by providing users with a \u201csampling-period\u201d parameter. We would partition the available data according to the sampling period and simply respond with the data from the first bin in each partition.", "originalCommit": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTUwNTE1MA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r481505150", "bodyText": "I do believe sampled raw metrics is more useful than aggregated metrics. In this case, aggregation would tend to remove more useful data than sampling would.", "author": "ricardolstephen", "createdAt": "2020-09-02T00:29:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5MDA4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTU0MzI5Ng==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r481543296", "bodyText": "This isn't true. For example, if I care about max CPU, and CPU spikes after the first 5 seconds it'll be missed by the sampling but easily caught by a max aggregration. The sampling performance degrades with longer sampling period. You're making the trade off of throwing away data for compute efficiency.", "author": "sidheart", "createdAt": "2020-09-02T01:53:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5MDA4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzgzMzE5NA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483833194", "bodyText": "For certain interesting use cases, sampling is more useful. However, will just omit this for now.", "author": "ricardolstephen", "createdAt": "2020-09-04T20:33:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5MDA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5NDMzOA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r478594338", "bodyText": "This response should have 2 records right? Your query has a 15s window with a 5s period, and I guess in your particular example you're demonstrating that the final 5s sample may be dropped?", "author": "sidheart", "createdAt": "2020-08-27T17:51:28Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,160 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.\n+\n+## Design\n+First, we would like to expand the retention period from beyond 5s. Currently, each cluster retains the past 5s of performance data in the form of a sqlite database (metricsdb files). We can naively expand the retention period by simply retaining more of these databases and allowing users to query from them.\n+\n+![Retaining more metricsdb files](/docs/images/batch-metrics-api-1.png)\n+\n+*Figure 1. Retaining more metricsdb files*\n+\n+The second issue is the limited granularity of the data. We can simply expand the granularity by allowing users to query the raw data for each metric rather than responding with an aggregate of that data. One drawback of this approach is that the response size will drastically increase. This prevents us from supplying a \u201cnodes=all\u201d parameter like with the metrics API, as it would require a single node to gather that data from all the other nodes and retain that data in memory before responding to the user. So, users will have to query individual nodes in order to gather performance metrics from those nodes.\n+\n+All the raw metrics data from a period of time may be too high granularity for some users. One approach to limiting this granularity might be to respond with some aggregate of that data, like with the metrics API. However, a more compute-efficient, practical, and useful approach would be to instead respond with a sample of that raw data. The available datapoints are currently in the form of 5s granularity bins (each metricsdb file collects 5s of data). We can efficiently sample from these bins by providing users with a \u201csampling-period\u201d parameter. We would partition the available data according to the sampling period and simply respond with the data from the first bin in each partition.\n+\n+![Sampling from bins](/docs/images/batch-metrics-api-2.png)\n+\n+*Figure 2. Sampling from bins*\n+\n+## API\n+\n+**performance-analyzer.properties**\n+* retention-period: The number of minutes worth of metrics data to collect. The configured retention period can be read via the \u201c/_opendistro/performanceanalyzer/batch/config\u201d api. Default=7 (7 minutes), min=1, max = 60.\n+  * Note, the default is 7 minutes because a typical use-case would be to query for 5 minutes worth of data from the node. In order to do this, a client would actually select a starttime of now-6min and an endtime of now-1min (this one minute offset will give sufficient time for the metrics in the time range to be available at the node). Atop this 6 minutes of retention, we need an extra 1 minute of retention to account for the time that would have passed by the time the query arrives at the node, and for the fact that starttime and endtime will be rounded down to the nearest sampling-period.\n+\n+**API**\n+\n+Queries:\n+\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/cluster/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* GET \\<endpoint\\>:9200/_opendistro/_performanceanalyzer/_agent/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+* GET \\<endpoint\\>:9600/_opendistro/_performanceanalyzer/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+\n+Parameters:\n+* metrics: list of metrics\n+* starttime: Unix timestamp (difference between the current time and midnight, January 1, 1970 UTC) determining the oldest data point to return. starttime is inclusive \u2014 data points from at or after the starttime will be returned. Note, the starttime and endtime supplied by the user with both be rounded down to the nearest sampling-period.\n+* endtime: Unix timestamp determining the freshest data point to return. endtime is exclusive \u2014 only datapoints from before the endtime will be returned.\n+* samplingperiod: The sampling period in seconds. Must be no less than 5, must be less than the retention period, and must be a multiple of 5. The default is 5s.\n+\n+Note, the maximum number of datapoints that a single query can request for via API is capped at 100,800 datapoints. If a query exceeds this limit, an error is returned. Parameters like the starttime, endtime, and samplingperiod can be adjusted on such queries to request for fewer datapoints at a time.\n+\n+Sample Query:\n+\n+GET localhost:9600/_opendistro/_performanceanalyzer/batch?metrics=CPU_Utilization,IO_TotThroughput&starttime=1594412650000&endtime=1594412665000&samplingperiod=5\n+\n+Output\n+```\n+{\n+    \"1594412650000\": {\n+        \"CPU_Utilization\": {\n+            \"fields\": [\n+                {\n+                    \"name\": \"ShardID\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"IndexName\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"Operation\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"ShardRole\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"sum\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"avg\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"min\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"max\",\n+                    \"type\": \"DOUBLE\"\n+                }\n+            ],\n+            \"records\": [\n+                [\n+                    null,\n+                    null,\n+                    \"GC\",\n+                    null,\n+                    0.0039980247804126965,\n+                    0.0039980247804126965,\n+                    0.0039980247804126965,\n+                    0.0039980247804126965\n+                ],\n+                [\n+                    null,\n+                    null,\n+                    \"other\",\n+                    null,\n+                    0.0,\n+                    0.0,\n+                    0.0,\n+                    0.0\n+                ]\n+            ]\n+        },\n+        \"IO_TotThroughput\": {\n+            \"fields\": [\n+                {\n+                    \"name\": \"IndexName\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"ShardID\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"ShardRole\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"Operation\",\n+                    \"type\": \"VARCHAR\"\n+                },\n+                {\n+                    \"name\": \"sum\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"avg\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"min\",\n+                    \"type\": \"DOUBLE\"\n+                },\n+                {\n+                    \"name\": \"max\",\n+                    \"type\": \"DOUBLE\"\n+                }\n+            ],\n+            \"records\": [", "originalCommit": "98d12efe35ee23ca0ac35136829d15d8a7f4a9a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzM1NTI0OQ==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483355249", "bodyText": "Yes, but I think this may be misunderstood by the reader. Also, I don't know how hard the timestamps are to visually differentiate (1594412650000 vs 1594412665000). So, will update this.", "author": "ricardolstephen", "createdAt": "2020-09-04T02:47:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODU5NDMzOA=="}], "type": "inlineReview"}, {"oid": "a182e9079e2e01c30944bdffbeec566ced9c7a83", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/a182e9079e2e01c30944bdffbeec566ced9c7a83", "message": "Update batch metrics api sample query", "committedDate": "2020-09-04T03:01:20Z", "type": "commit"}, {"oid": "6c0bd9cfd598b454958ef522ee231af72b120688", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/6c0bd9cfd598b454958ef522ee231af72b120688", "message": "Add more documentation", "committedDate": "2020-09-04T03:31:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc3MDQ3Nw==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483770477", "bodyText": "We should add a line describing what this API does", "author": "yojs", "createdAt": "2020-09-04T17:52:11Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,247 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.\n+\n+## Design\n+First, we would like to expand the retention period from beyond 5s. Currently, each cluster retains the past 5s of performance data in the form of a sqlite database (metricsdb files). We can naively expand the retention period by simply retaining more of these databases and allowing users to query from them.\n+\n+![Retaining more metricsdb files](/docs/images/batch-metrics-api-1.png)\n+\n+*Figure 1. Retaining more metricsdb files*\n+\n+The second issue is the limited granularity of the data. We can simply expand the granularity by allowing users to query the raw data for each metric rather than responding with an aggregate of that data. One drawback of this approach is that the response size will drastically increase. This prevents us from supplying a \u201cnodes=all\u201d parameter like with the metrics API, as it would require a single node to gather that data from all the other nodes and retain that data in memory before responding to the user. So, users will have to query individual nodes in order to gather performance metrics from those nodes.\n+\n+All the raw metrics data from a period of time may be too high granularity for some users. One approach to limiting this granularity might be to respond with some aggregate of that data, like with the metrics API. However, a more compute-efficient, practical, and useful approach would be to instead respond with a sample of that raw data. The available datapoints are currently in the form of 5s granularity bins (each metricsdb file collects 5s of data). We can efficiently sample from these bins by providing users with a \u201csampling-period\u201d parameter. We would partition the available data according to the sampling period and simply respond with the data from the first bin in each partition.\n+\n+![Sampling from bins](/docs/images/batch-metrics-api-2.png)\n+\n+*Figure 2. Sampling from bins*\n+\n+## API\n+\n+**performance-analyzer.properties**\n+* retention-period: The number of minutes worth of metrics data to collect. The configured retention period can be read via the \u201c/_opendistro/performanceanalyzer/batch/config\u201d api. Default=7 (7 minutes), min=1, max = 60.\n+  * Note, the default is 7 minutes because a typical use-case would be to query for 5 minutes worth of data from the node. In order to do this, a client would actually select a starttime of now-6min and an endtime of now-1min (this one minute offset will give sufficient time for the metrics in the time range to be available at the node). Atop this 6 minutes of retention, we need an extra 1 minute of retention to account for the time that would have passed by the time the query arrives at the node, and for the fact that starttime and endtime will be rounded down to the nearest sampling-period.\n+\n+**API**\n+\n+Queries:\n+\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019", "originalCommit": "a182e9079e2e01c30944bdffbeec566ced9c7a83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzg0MjE3MA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483842170", "bodyText": "This information should be evident to someone who has read the design section of the document. Additionally, this is just the design doc rather than being a more heavily user-friendly document. However, such deficiencies in user-friendly documentation was noted in the main README file and rectified there (See opendistro-for-elasticsearch/performance-analyzer#159)", "author": "ricardolstephen", "createdAt": "2020-09-04T21:00:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc3MDQ3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTkxMzcwMg==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r485913702", "bodyText": "It's a good experience for the reader if a document is complete or refers to other documents for completeness, if not in its scope.", "author": "yojs", "createdAt": "2020-09-09T20:49:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc3MDQ3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc3Mjc1MQ==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483772751", "bodyText": "We should add a section about enabling and what it will cost and disabling (how to). Will they require restarts etc.", "author": "yojs", "createdAt": "2020-09-04T17:57:12Z", "path": "docs/batch-metrics-api.md", "diffHunk": "@@ -0,0 +1,247 @@\n+# Batch Metrics API\n+\n+## Background and Problem statement\n+The metrics API currently allows us to collect the past 5 seconds of performance data from an ODFE cluster. However, this data is aggregated, not raw, performance metrics. Additionally, it is restricting that we only have access to the past 5 seconds of data, rather than data from longer periods of time. Having fine granularity (raw) metrics from longer periods of time would allow users to learn a lot more about the performance of an AES cluster.\n+\n+## Design\n+First, we would like to expand the retention period from beyond 5s. Currently, each cluster retains the past 5s of performance data in the form of a sqlite database (metricsdb files). We can naively expand the retention period by simply retaining more of these databases and allowing users to query from them.\n+\n+![Retaining more metricsdb files](/docs/images/batch-metrics-api-1.png)\n+\n+*Figure 1. Retaining more metricsdb files*\n+\n+The second issue is the limited granularity of the data. We can simply expand the granularity by allowing users to query the raw data for each metric rather than responding with an aggregate of that data. One drawback of this approach is that the response size will drastically increase. This prevents us from supplying a \u201cnodes=all\u201d parameter like with the metrics API, as it would require a single node to gather that data from all the other nodes and retain that data in memory before responding to the user. So, users will have to query individual nodes in order to gather performance metrics from those nodes.\n+\n+All the raw metrics data from a period of time may be too high granularity for some users. One approach to limiting this granularity might be to respond with some aggregate of that data, like with the metrics API. However, a more compute-efficient, practical, and useful approach would be to instead respond with a sample of that raw data. The available datapoints are currently in the form of 5s granularity bins (each metricsdb file collects 5s of data). We can efficiently sample from these bins by providing users with a \u201csampling-period\u201d parameter. We would partition the available data according to the sampling period and simply respond with the data from the first bin in each partition.\n+\n+![Sampling from bins](/docs/images/batch-metrics-api-2.png)\n+\n+*Figure 2. Sampling from bins*\n+\n+## API\n+\n+**performance-analyzer.properties**\n+* retention-period: The number of minutes worth of metrics data to collect. The configured retention period can be read via the \u201c/_opendistro/performanceanalyzer/batch/config\u201d api. Default=7 (7 minutes), min=1, max = 60.\n+  * Note, the default is 7 minutes because a typical use-case would be to query for 5 minutes worth of data from the node. In order to do this, a client would actually select a starttime of now-6min and an endtime of now-1min (this one minute offset will give sufficient time for the metrics in the time range to be available at the node). Atop this 6 minutes of retention, we need an extra 1 minute of retention to account for the time that would have passed by the time the query arrives at the node, and for the fact that starttime and endtime will be rounded down to the nearest sampling-period.\n+\n+**API**\n+\n+Queries:\n+\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* POST \\<endpoint\\>:9200/_opendistro/performanceanalyzer/batch/cluster/config -H \u2018Content-Type: application/json\u2019 -d \u2018{\u201cenabled\u201d: true}\u2019\n+* GET \\<endpoint\\>:9200/_opendistro/_performanceanalyzer/_agent/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+* GET \\<endpoint\\>:9600/_opendistro/_performanceanalyzer/batch?metrics=\\<metrics\\>&starttime=\\<starttime\\>&endtime=\\<endtime\\>&samplingperiod=\\<samplingperiod\\>\n+", "originalCommit": "a182e9079e2e01c30944bdffbeec566ced9c7a83", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzg0MjIzOA==", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/pull/333#discussion_r483842238", "bodyText": "This information should be evident to someone who has read the design section of the document. Additionally, this is just the design doc rather than being a more heavily user-friendly document. However, such deficiencies in user-friendly documentation was noted in the main README file and rectified there (See opendistro-for-elasticsearch/performance-analyzer#159)", "author": "ricardolstephen", "createdAt": "2020-09-04T21:01:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mzc3Mjc1MQ=="}], "type": "inlineReview"}, {"oid": "06e5cd191768bd68dca5b7f45d8b698dc15bf479", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/06e5cd191768bd68dca5b7f45d8b698dc15bf479", "message": "Add minor fixes to batch metrics api design doc", "committedDate": "2020-09-04T20:57:38Z", "type": "commit"}, {"oid": "1a03861f1a7fc0f2e4d3dc731d514755350f3162", "url": "https://github.com/opendistro-for-elasticsearch/performance-analyzer-rca/commit/1a03861f1a7fc0f2e4d3dc731d514755350f3162", "message": "Note why max datapoints was capped", "committedDate": "2020-09-08T22:12:12Z", "type": "commit"}]}