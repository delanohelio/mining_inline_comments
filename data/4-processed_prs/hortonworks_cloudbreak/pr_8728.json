{"pr_number": 8728, "pr_title": "CB-8278: Backup using hdfs cli", "pr_createdAt": "2020-08-05T20:53:02Z", "pr_url": "https://github.com/hortonworks/cloudbreak/pull/8728", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466192991", "bodyText": "Why is the region removed?", "author": "keyki", "createdAt": "2020-08-06T07:13:38Z", "path": "core/src/main/java/com/sequenceiq/cloudbreak/reactor/handler/cluster/dr/BackupRestoreSaltConfigGenerator.java", "diffHunk": "@@ -34,7 +34,6 @@ public SaltConfig createSaltConfig(String location, String backupId, Stack stack\n \n         Map<String, String> disasterRecoveryValues = new HashMap<>();\n         disasterRecoveryValues.put(OBJECT_STORAGE_URL_KEY, fullLocation);\n-        disasterRecoveryValues.put(AWS_REGION_KEY, stack.getRegion());", "originalCommit": "6e43ad33e768122070f5c6c40dc0950b02664db3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyNTAxOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466525018", "bodyText": "We got the suggestion to use the hdfs cli instead of s3 to upload/download.\nIt was my understanding that we don't have to specify the region in that case, the hdfs configurations should take care of things?", "author": "brycederriso", "createdAt": "2020-08-06T16:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MTQxMQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467281411", "bodyText": "@keyki  AWS CLI would depend on permissions granted to the instance profile of the datalake node which is currently LOG_ROLE. Grating more permissions to LOG_ROLE was not a good idea.\nWith this change, we will be using hdfs client to write to S3. This uses S3a. All the services which to S3, internally use S3a. With the configuration we are using, S3a fetches the region information of the bucket and then constructs the request accordingly.", "author": "kkalvagadda1", "createdAt": "2020-08-07T21:31:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyNjczOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r468826739", "bodyText": "I'm resolving this conversation, given that we have a reason and there's been no further concern.", "author": "brycederriso", "createdAt": "2020-08-11T19:52:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5Mjk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1Mzk2MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466253961", "bodyText": "we can not really remove pillar property because then the older clusters will not work .", "author": "doktoric", "createdAt": "2020-08-06T09:02:17Z", "path": "orchestrator-salt/src/main/resources/salt/pillar/postgresql/disaster_recovery.sls", "diffHunk": "@@ -1,4 +1,2 @@\n disaster_recovery:\n   object_storage_url:\n-  aws_region: '' # empty string to prevent salt from passing the literal 'None'", "originalCommit": "6e43ad33e768122070f5c6c40dc0950b02664db3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU5OTYyOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466599629", "bodyText": "Ok, I'll readd it and make a comment that it is not used anywhere.\nThere's no way to remove a pillar property once it's been added? Not without breaking older clusters?", "author": "brycederriso", "createdAt": "2020-08-06T18:16:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1Mzk2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYwMjA2Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r466602062", "bodyText": "We can remove it now since you just introduced it on master and it is not deployed anywhere. It only gets problematic if we would have released it.", "author": "keyki", "createdAt": "2020-08-06T18:21:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1Mzk2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI4MzAyNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467283026", "bodyText": "as @keyki  mentioned, this functionality is not yet released to customers so it this be fine.", "author": "kkalvagadda1", "createdAt": "2020-08-07T21:33:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjI1Mzk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NjAzMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467296033", "bodyText": "It's not a HDFS location. It is the location of the object-store. Please rename it accordingly.", "author": "kkalvagadda1", "createdAt": "2020-08-07T21:50:42Z", "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -54,69 +42,50 @@ errorExit() {\n   exit 1\n }\n \n-dump_to_azure() {\n+kinit_as_hdfs() {\n+  # Use a new Kerberos credential cache, to keep from clobbering the default.\n+  export KRB5CCNAME=/tmp/krb5cc_cloudbreak_$EUID\n+  HDFS_KEYTAB=/run/cloudera-scm-agent/process/$(ls -t /run/cloudera-scm-agent/process/ | grep hdfs-NAMENODE$ | head -n 1)/hdfs.keytab\n+  doLog \"kinit as hdfs using Keytab: $HDFS_KEYTAB\"\n+  kinit -kt \"$HDFS_KEYTAB\" \"hdfs/$(hostname -f)\"\n+  if [[ $? -ne 0 ]]; then\n+    errorExit \"Couldn't kinit as hdfs\"\n+  fi\n+}\n+\n+move_backup_to_cloud () {\n+  LOCAL_BACKUP=\"$1\"\n+  kinit_as_hdfs\n+  doLog \"INFO Uploading to ${BACKUP_LOCATION}\"\n+\n+  hdfs dfs -mkdir -p \"$BACKUP_LOCATION\"\n+  HDFS_FILE_LOCATION=\"${BACKUP_LOCATION}/${SERVICE}_backup\"", "originalCommit": "f3dbea01996700c4e2361762a9bc8a516e4a232c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyNjA4MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r468826081", "bodyText": "Changed to OBJECT_STORE_PATH", "author": "brycederriso", "createdAt": "2020-08-11T19:50:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzI5NjAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDMwNw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467300307", "bodyText": "here are were are creating a directory and accessing directly. S3 provides an eventual guarantee.\nIn short, if you create a directory and try to write to it immediately, it might fail.\nwe use S3 guard to handle this issue with S3. S3 guard tracks all the operations and changes the metadata.\nI'm not sure if S3 guard is enabled by default. Is there anything that has to be done for it. Can you check with @risdenk  on this?", "author": "kkalvagadda1", "createdAt": "2020-08-07T21:56:21Z", "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -54,69 +42,50 @@ errorExit() {\n   exit 1\n }\n \n-dump_to_azure() {\n+kinit_as_hdfs() {\n+  # Use a new Kerberos credential cache, to keep from clobbering the default.\n+  export KRB5CCNAME=/tmp/krb5cc_cloudbreak_$EUID\n+  HDFS_KEYTAB=/run/cloudera-scm-agent/process/$(ls -t /run/cloudera-scm-agent/process/ | grep hdfs-NAMENODE$ | head -n 1)/hdfs.keytab\n+  doLog \"kinit as hdfs using Keytab: $HDFS_KEYTAB\"\n+  kinit -kt \"$HDFS_KEYTAB\" \"hdfs/$(hostname -f)\"\n+  if [[ $? -ne 0 ]]; then\n+    errorExit \"Couldn't kinit as hdfs\"\n+  fi\n+}\n+\n+move_backup_to_cloud () {\n+  LOCAL_BACKUP=\"$1\"\n+  kinit_as_hdfs\n+  doLog \"INFO Uploading to ${BACKUP_LOCATION}\"\n+\n+  hdfs dfs -mkdir -p \"$BACKUP_LOCATION\"", "originalCommit": "f3dbea01996700c4e2361762a9bc8a516e4a232c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0NDI0MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r467344241", "bodyText": "So there are potentially a few options here:\n\nretry on failure to upload\nUpload the whole folder instead of trying to create it then put files in it\nin theory s3guard is enabled so this isn't even something that needs to be worried about", "author": "risdenk", "createdAt": "2020-08-08T01:24:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDMwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE2NzQ3NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r468167475", "bodyText": "I'm going to be lazy and rely on S3 guard to protect me from the eventual consistency problems.", "author": "brycederriso", "createdAt": "2020-08-10T20:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwMDMwNw=="}], "type": "inlineReview"}, {"oid": "c33ebfc599ac204d10ff6b28d621d12691197804", "url": "https://github.com/hortonworks/cloudbreak/commit/c33ebfc599ac204d10ff6b28d621d12691197804", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.", "committedDate": "2020-08-10T20:53:37Z", "type": "forcePushed"}, {"oid": "1bd741d2e5e079087950a42dc2d977b15b3cf04a", "url": "https://github.com/hortonworks/cloudbreak/commit/1bd741d2e5e079087950a42dc2d977b15b3cf04a", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.", "committedDate": "2020-08-11T14:57:02Z", "type": "forcePushed"}, {"oid": "57f5ee7564cbcfacb75db6210665f0d561e05dd9", "url": "https://github.com/hortonworks/cloudbreak/commit/57f5ee7564cbcfacb75db6210665f0d561e05dd9", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-11T19:49:14Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MTcxMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469541713", "bodyText": "URL is not an HDFS URL . You can just say \" We retrieve the SQL files from a valid URL, ...\"", "author": "kkalvagadda1", "createdAt": "2020-08-12T20:56:08Z", "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/restore_db.sh", "diffHunk": "@@ -1,47 +1,37 @@\n #!/bin/bash\n # restore_db.sh\n # This script uses the 'psql' cli to drop hive and ranger databases, create them, then read in a plain SQL file to restore data.\n-# We use the `azcopy copy` and `aws s3 cp` commands to copy backups from Azure or AWS respectively.\n-# When using either cloud provider, the plaintext SQL files are first copied to the local filesystem, then fed into psql.\n+# We retrieve the SQL files from a valid HDFS URL, which should be an `s3a://` or `abfs://` for AWS or Azure clouds respectively.", "originalCommit": "57f5ee7564cbcfacb75db6210665f0d561e05dd9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1NjUyMg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469556522", "bodyText": "Done", "author": "brycederriso", "createdAt": "2020-08-12T21:26:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MTcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MzYyOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469543628", "bodyText": "URL is not an HDFS URL . You can just say \" .... the SQL file is uploaded to the cloud storage using the hdfs cli.\"", "author": "kkalvagadda1", "createdAt": "2020-08-12T20:59:43Z", "path": "orchestrator-salt/src/main/resources/salt/salt/postgresql/disaster_recovery/scripts/backup_db.sh", "diffHunk": "@@ -1,47 +1,36 @@\n #!/bin/bash\n # backup_db.sh\n # This script uses the 'pg_dump' utility to dump the contents of hive and ranger PostgreSQL databases as plain SQL.\n-# After PostgreSQL contents are dumped, the SQL is uploaded to AWS or Azure using their CLI clients, 'aws s3 cp' and 'azcopy copy' respectively.\n-# For the Azure upload, we must write the SQL commands to a local disk before running the `azcopy copy` command.\n-# For AWS, the cli supports piping file contents to stdin, so we do that and avoid writing to a local file.\n+# After PostgreSQL contents are dumped, the SQL file is uploaded to an HDFS URL using the hdfs cli.", "originalCommit": "57f5ee7564cbcfacb75db6210665f0d561e05dd9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1NjU3OA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8728#discussion_r469556578", "bodyText": "Done", "author": "brycederriso", "createdAt": "2020-08-12T21:26:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU0MzYyOA=="}], "type": "inlineReview"}, {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-12T21:27:32Z", "type": "commit"}, {"oid": "628aa1910dd13125e40108de5852e23a355f8a5b", "url": "https://github.com/hortonworks/cloudbreak/commit/628aa1910dd13125e40108de5852e23a355f8a5b", "message": "Backup and restore using HDFS cli agains object store.\n\n* Restore db using HDFS cli to transfer backups.\n* Hack together a fix for embedded database backups.\n* Adjust url construction and region passing to account for HDFS cli usage.\n* Use a new kerberos credential cache when kiniting.\n* Remove unused `region` pillar value.\n* Clean up after successful Ranger admin group substitution.", "committedDate": "2020-08-12T21:27:32Z", "type": "forcePushed"}]}