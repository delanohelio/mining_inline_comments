{"pr_number": 8194, "pr_title": "CB-7212: Add datalake service changes to perform database backup/restore.", "pr_createdAt": "2020-06-02T13:39:21Z", "pr_url": "https://github.com/hortonworks/cloudbreak/pull/8194", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg4NjAxOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r433886018", "bodyText": "nitpick: This should be DATABASE.\nThe same is true for all the following enums, and for the related @Bean(name = \"\") annotations in the action class.", "author": "brycederriso", "createdAt": "2020-06-02T13:44:38Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {\n+\n+    INIT_STATE,\n+    DATALAKE_DATABSE_BACKUP_START_STATE,", "originalCommit": "eeb0ac24348ecbbc4850b91fd35da885067a3cdd", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "url": "https://github.com/hortonworks/cloudbreak/commit/5b53f6a124117c9d051c0bc95aa76a71d4c6f3b7", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-11T04:10:41Z", "type": "forcePushed"}, {"oid": "193405603e46ccc9be38527729240556706c3fe6", "url": "https://github.com/hortonworks/cloudbreak/commit/193405603e46ccc9be38527729240556706c3fe6", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB.", "committedDate": "2020-06-15T12:32:59Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMzMjcxNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440332715", "bodyText": "Capitalization and grammer: \"Performs a backup of the database to a provided location\"", "author": "hreeve-cloudera", "createdAt": "2020-06-15T17:27:02Z", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/doc/OperationDescriptions.java", "diffHunk": "@@ -44,6 +44,8 @@\n         public static final String CHECK_STACK_UPGRADE = \"Checks for upgrade options by name\";\n         public static final String STACK_UPGRADE = \"Upgrades a cluster to the latest CM or CDH version\";\n         public static final String LIST_RETRYABLE_FLOWS = \"List retryable failed flows\";\n+        public static final String DATABASE_BACKUP = \"performs a backup of database to a provided location\";", "originalCommit": "193405603e46ccc9be38527729240556706c3fe6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440350769", "bodyText": "This is a general comment for the review about how the status works on the cloudbreak side. It might or might not have any affect on what you do with status in your code. But on the cloudbreak side, there is no separate states of requested/started and in-progress. By the time the flow advances to the point where it's updating the stack status, the operation is already in progress. So in my version of these status messages, I had:\nBACKUP_IN_PROGRESS(StatusKind.PROGRESS), BACKUP_FINISHED(StatusKind.FINAL), BACKUP_FAILED(StatusKind.FINAL), RESTORE_IN_PROGRESS(StatusKind.PROGRESS), RESTORE_FINISHED(StatusKind.FINAL), RESTORE_FAILED(StatusKind.FINAL);\nI see you have multiple places where you have a separate start state, and an in-progress state. For the datalake service I think that makes sense, since you can mark it as started as soon as you get the request, and in-progress when you get the flow identifier back from cloudbreak. I just wanted to give you a heads up that cloudbreak won't be using the REQUESTED stattus.", "author": "hreeve-cloudera", "createdAt": "2020-06-15T18:00:08Z", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/common/Status.java", "diffHunk": "@@ -12,7 +12,13 @@\n     AVAILABLE(StatusKind.FINAL),\n     UPDATE_IN_PROGRESS(StatusKind.PROGRESS),\n     UPDATE_REQUESTED(StatusKind.PROGRESS),\n+    BACKUP_REQUESTED(StatusKind.PROGRESS),", "originalCommit": "193405603e46ccc9be38527729240556706c3fe6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyNjY3Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440426673", "bodyText": "I just followed the states defined for UPDATE. UPDATE has UPDATE_REQUESTED and UPDATE_IN_PROGRESS etc.\nI can update the patch accordingly.", "author": "kkalvagadda1", "createdAt": "2020-06-15T20:25:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyODgxNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440428815", "bodyText": "I just followed the states defined for UPDATE. It has UPDATE_REQUESTED and UPDATE_IN_PROGRESS etc.\nIf your patch is not going to have the REQUESTED state, I will remove it.", "author": "kkalvagadda1", "createdAt": "2020-06-15T20:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDM1MDc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwODYwNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440408605", "bodyText": "To match REST conventions and StackV4Endpoint naming conventions:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @POST\n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_backup\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_BACKUP, nickname = \"databaseBackup\")\n          \n          \n            \n                BackupV4Response backupDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);\n          \n          \n            \n            \n          \n          \n            \n                @POST\n          \n          \n            \n                @Path(\"{name}/database_restore\")\n          \n          \n            \n                @Produces(MediaType.APPLICATION_JSON)\n          \n          \n            \n                @ApiOperation(value = DATABASE_RESTORE, nickname = \"databaseRestore\")\n          \n          \n            \n                RestoreV4Response restoreDatabaseByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name,\n          \n          \n            \n                        @QueryParam(\"location\") String location, @QueryParam(\"backupId\") String backupId);", "author": "hreeve-cloudera", "createdAt": "2020-06-15T19:49:35Z", "path": "core-api/src/main/java/com/sequenceiq/cloudbreak/api/endpoint/v4/stacks/StackV4Endpoint.java", "diffHunk": "@@ -269,4 +273,17 @@ FlowIdentifier setClusterMaintenanceMode(@PathParam(\"workspaceId\") Long workspac\n     @ApiOperation(value = UPDATE_SALT, nickname = \"updateSaltByName\")\n     FlowIdentifier updateSaltByName(@PathParam(\"workspaceId\") Long workspaceId, @PathParam(\"name\") String name);\n \n+    @POST", "originalCommit": "193405603e46ccc9be38527729240556706c3fe6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQyOTg4OA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r440429888", "bodyText": "Basically the suggestion is to accepted location and backup-id as query parameters, right.", "author": "kkalvagadda1", "createdAt": "2020-06-15T20:32:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQwODYwNQ=="}], "type": "inlineReview"}, {"oid": "0d0152ccd99c44623234b376e8701f5ef0328a1b", "url": "https://github.com/hortonworks/cloudbreak/commit/0d0152ccd99c44623234b376e8701f5ef0328a1b", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB.", "committedDate": "2020-06-17T12:14:26Z", "type": "forcePushed"}, {"oid": "4f2d9c115f912bb9ba9bc34022900a36c862928b", "url": "https://github.com/hortonworks/cloudbreak/commit/4f2d9c115f912bb9ba9bc34022900a36c862928b", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB.", "committedDate": "2020-06-17T15:11:03Z", "type": "forcePushed"}, {"oid": "878a27e569cf3d3847035d1e04140228710f8881", "url": "https://github.com/hortonworks/cloudbreak/commit/878a27e569cf3d3847035d1e04140228710f8881", "message": "CB-7212: Add datalake service changes to perform database backup/restore.\n\nAdded changes to accept the backup-id and pass it to CB.", "committedDate": "2020-06-17T15:50:16Z", "type": "forcePushed"}, {"oid": "c2b01428e259a1a1565101b57a98c9371b5fd157", "url": "https://github.com/hortonworks/cloudbreak/commit/c2b01428e259a1a1565101b57a98c9371b5fd157", "message": "fixed a flaky test(CloudStorageValidatorTest).", "committedDate": "2020-06-18T14:51:29Z", "type": "forcePushed"}, {"oid": "cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "url": "https://github.com/hortonworks/cloudbreak/commit/cc6825762f7475cf94b1e98379c24a3f7b5cd2c8", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-18T14:55:50Z", "type": "forcePushed"}, {"oid": "e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "url": "https://github.com/hortonworks/cloudbreak/commit/e2cf07a07d0db9bd403a8fc69fdb8a98e71270dc", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-18T14:58:38Z", "type": "forcePushed"}, {"oid": "22ed8587748ab6331641c647bad02daf3f52df0d", "url": "https://github.com/hortonworks/cloudbreak/commit/22ed8587748ab6331641c647bad02daf3f52df0d", "message": "Unit test fix", "committedDate": "2020-06-19T11:18:07Z", "type": "forcePushed"}, {"oid": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "url": "https://github.com/hortonworks/cloudbreak/commit/52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-22T03:24:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MzQ2Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073463", "bodyText": "this should be getBackupDatabaseStatusByName instead", "author": "pdarvasi", "createdAt": "2020-06-23T09:02:13Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMTQyOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444231429", "bodyText": "will fix.", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:39:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MzQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3Mzc4MA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444073780", "bodyText": "this should be getRestoreDatabaseStatusByName instead", "author": "pdarvasi", "createdAt": "2020-06-23T09:02:42Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/controller/sdx/SdxController.java", "diffHunk": "@@ -246,24 +250,31 @@ public FlowIdentifier stopByCrn(@ResourceCrn String crn) {\n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupResponse backupDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseBackup(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreResponse restoreDatabaseByName(@ResourceName String name, String backupId, String backupLocation) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.triggerDatabaseRestore(sdxCluster, backupId, backupLocation);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.BACKUP_DATALAKE)\n     public SdxDatabaseBackupStatusResponse backupDatabaseStatusByName(@ResourceName String name, String operationId) {\n-        return null;\n+        String userCrn = ThreadBasedUserCrnProvider.getUserCrn();\n+        SdxCluster sdxCluster = sdxService.getSdxByNameInAccount(userCrn, name);\n+        return sdxDrService.getDatabaseBackupStatus(sdxCluster, operationId);\n     }\n \n     @Override\n     @CheckPermissionByAccount(action = AuthorizationResourceAction.RESTORE_DATALAKE)\n     public SdxDatabaseRestoreStatusResponse restoreDatabaseStatusByName(@ResourceName String name, String operationId) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMTQ4MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444231481", "bodyText": "will fix.", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:39:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3Mzc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076722", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "author": "pdarvasi", "createdAt": "2020-06-23T09:07:41Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzMjk0MA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444232940", "bodyText": "Why should FlowIdentifier return when it is not used? API invoking triggerDatalakeDatabaseBackupFlow uses operationId instead of flowIdentifier.\nI can make changes to avoid the need for notifyDatabaseDrEvent. Does that address your concern?", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:41:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NDEwNA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444484104", "bodyText": "I have removed method \"notifyDatabaseDrEvent\" and re-used \"notify\".", "author": "kkalvagadda1", "createdAt": "2020-06-23T20:21:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3OTA2OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444979069", "bodyText": "will update.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:25:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjgxMg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444076812", "bodyText": "FlowIdentifier should be returned here, the operationId, should be passed as parameter so that notify() can be used instead of notifyDatabaseDrEvent()", "author": "pdarvasi", "createdAt": "2020-06-23T09:07:53Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -104,6 +109,18 @@ public FlowIdentifier triggerSdxStopFlow(SdxCluster cluster) {\n         return notify(selector, new SdxStartStopEvent(selector, cluster.getId(), userId));\n     }\n \n+    public String triggerDatalakeDatabaseBackupFlow(Long sdxId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        return notifyDatabaseDrEvent(selector, new DatalakeDatabaseBackupStartEvent(selector, sdxId, userId, backupId, backupLocation));\n+    }\n+\n+    public String triggerDatalakeDatabaseRestoreFlow(Long sdxId, String backupId, String backupLocation) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODk4NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978985", "bodyText": "will update.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:25:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NjgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NzA0Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444077043", "bodyText": "this is not needed if operationId is passed as parameter", "author": "pdarvasi", "createdAt": "2020-06-23T09:08:19Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/SdxReactorFlowManager.java", "diffHunk": "@@ -132,4 +149,30 @@ private FlowIdentifier notify(String selector, SdxEvent acceptable) {\n         }\n \n     }\n+\n+    String notifyDatabaseDrEvent(String selector,  DatalakeDatabaseDrStartBaseEvent acceptable) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4NDM1MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444484351", "bodyText": "will remove the method.", "author": "kkalvagadda1", "createdAt": "2020-06-23T20:22:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NzA0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4NTE0Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444085147", "bodyText": "shouldn't we set the status (sdxDrService.updateDatabaseStatusEntry() to INPROGRESS here?", "author": "pdarvasi", "createdAt": "2020-06-23T09:21:46Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDIzNzA3Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444237072", "bodyText": "yes, you are right. I will fix it.", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4NTE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTYzMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089630", "bodyText": "this doExecute() is the same as the backupCouldNotStart() one, could you pls. extract it to a common method?", "author": "pdarvasi", "createdAt": "2020-06-23T09:29:00Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQ4ODgwNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444488805", "bodyText": "I understand that the code is duplicate. In fact, I tried to do it before but there is an issue.\nThis code uses \"sentEvent\" which is an implementation in \"AbstractAction\". It is available in a private API in DatalakeDatabaseBackupActions.", "author": "kkalvagadda1", "createdAt": "2020-06-23T20:31:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTYzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444089836", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "author": "pdarvasi", "createdAt": "2020-06-23T09:29:24Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database backup has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseBackup(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakebackupInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database backup is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseBackupWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE\")\n+    public Action<?, ?> backupCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, payload.getException().getMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FINISHED_STATE\")\n+    public Action<?, ?> finishedBackupAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database backup is finalized with sdx id: {}\", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseBackupSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseBackupFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_FAILED_STATE\")\n+    public Action<?, ?> backupFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database backup failed for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT.event(), payload);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0Mzc5Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444243793", "bodyText": "Just for understanding sake, Is \"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\" used only in success cases?", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:56:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkwMDk0Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444900942", "bodyText": "Yes, we use it like that", "author": "pdarvasi", "createdAt": "2020-06-24T13:40:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA4OTgzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5MjczMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444092730", "bodyText": "Not DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT instead?", "author": "pdarvasi", "createdAt": "2020-06-23T09:34:23Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_BACKUP_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseBackupFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_START_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_BACKUP_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_BACKUP_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT).defaultFailureEvent()", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NTYwMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444095600", "bodyText": "Shouldn't we upgrade to INPROGRESS here also with updateDatabaseStatusEntry?", "author": "pdarvasi", "createdAt": "2020-06-23T09:39:10Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0NDA4NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444244084", "bodyText": "yes, you are right. I will fix it.", "author": "kkalvagadda1", "createdAt": "2020-06-23T13:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NTYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NjkxMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444096913", "bodyText": "How about setting the SDX status here to failed as well with sdxStatusService.setStatusForDatalakeAndNotify()?", "author": "pdarvasi", "createdAt": "2020-06-23T09:41:29Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0NzczMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444247730", "bodyText": "database backup/restore is one part of the datalake backup restore. There will be a separate patch to update the datalake status using SdxStatusService which will be invoked by datalakedr service which orchestrates the datalake backup and restore.", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:01:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5NjkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODAzNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098036", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "author": "pdarvasi", "createdAt": "2020-06-23T09:43:26Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreActions.java", "diffHunk": "@@ -0,0 +1,170 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseRestoreActions {\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseRestoreActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_START_STATE\")\n+    public Action<?, ?> datalakeRestore() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Datalake database restore has been started for {}\", payload.getResourceId());\n+                sdxDrService.databaseRestore(payload.getDrStatus(),\n+                        payload.getResourceId(),\n+                        payload.getBackupId(),\n+                        payload.getBackupLocation());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreCouldNotStartEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE\")\n+    public Action<?, ?> datalakeRestoreInProgress() {\n+        return new AbstractSdxAction<>(SdxEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext, SdxEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, SdxEvent payload, Map<Object, Object> variables) {\n+                LOGGER.info(\"Datalake database restore is in progress for {} \", payload.getResourceId());\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sendEvent(context, DatalakeDatabaseRestoreWaitRequest.from(context, operationId));\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(SdxEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE\")\n+    public Action<?, ?> restoreCouldNotStart() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreCouldNotStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreCouldNotStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreCouldNotStartEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreCouldNotStartEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FINISHED_STATE\")\n+    public Action<?, ?> finishedRestoreAction() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreSuccessEvent.class) {\n+\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreSuccessEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreSuccessEvent payload, Map<Object, Object> variables) throws Exception {\n+                LOGGER.info(\"Sdx database restore is finalized with sdx id: {}\", payload.getResourceId());\n+                sdxDrService.updateDatabaseStatusEntry(payload.getOperationId(), SdxDatabaseDrStatus.Status.SUCCEEDED, null);\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);\n+            }\n+\n+            @Override\n+            protected Object getFailurePayload(DatalakeDatabaseRestoreSuccessEvent payload, Optional<SdxContext> flowContext, Exception ex) {\n+                return DatalakeDatabaseRestoreFailedEvent.from(payload, ex);\n+            }\n+        };\n+    }\n+\n+    @Bean(name = \"DATALAKE_DATABASE_RESTORE_FAILED_STATE\")\n+    public Action<?, ?> restoreFailed() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseRestoreFailedEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseRestoreFailedEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseRestoreFailedEvent payload, Map<Object, Object> variables) throws Exception {\n+                Exception exception = payload.getException();\n+                LOGGER.error(\"Datalake database restore could not be started for datalake with id: {}\", payload.getResourceId(), exception);\n+                String operationId = (String) variables.get(OPERATION_ID);\n+                sdxDrService.updateDatabaseStatusEntry(operationId, SdxDatabaseDrStatus.Status.FAILED, exception.getLocalizedMessage());\n+                sendEvent(context, DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT.event(), payload);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA5ODE4OA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444098188", "bodyText": "this should be FAILED_HANDLED_EVENT instead", "author": "pdarvasi", "createdAt": "2020-06-23T09:43:42Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseRestoreFlowConfig.java", "diffHunk": "@@ -0,0 +1,98 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FAILED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_FINISHED_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.DATALAKE_DATABASE_RESTORE_START_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.FINAL_STATE;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrState.INIT_STATE;\n+\n+import java.util.List;\n+\n+import org.springframework.stereotype.Component;\n+\n+import com.sequenceiq.flow.core.config.AbstractFlowConfiguration;\n+import com.sequenceiq.flow.core.config.RetryableFlowConfiguration;\n+\n+@Component\n+public class DatalakeDatabaseRestoreFlowConfig extends AbstractFlowConfiguration<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>\n+        implements RetryableFlowConfiguration<DatalakeDatabaseDrEvent> {\n+\n+    private static final List<Transition<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>> TRANSITIONS =\n+            new Transition.Builder<DatalakeDatabaseDrState, DatalakeDatabaseDrEvent>()\n+                    .defaultFailureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(INIT_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_EVENT).noFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_START_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_IN_PROGRESS_STATE)\n+                    .to(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT)\n+                    .failureState(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .failureEvent(DATALAKE_DATABASE_RESTORE_FAILED_EVENT)\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FINISHED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()\n+\n+                    .from(DATALAKE_DATABASE_RESTORE_FAILED_STATE)\n+                    .to(FINAL_STATE)\n+                    .event(DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT).defaultFailureEvent()", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjMxOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102319", "bodyText": "We should include the FlowIdentifier in SdxDatabaseBackupResponse, too", "author": "pdarvasi", "createdAt": "2020-06-23T09:50:43Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODY4MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978681", "bodyText": "will update.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:25:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjMxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjQzNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444102436", "bodyText": "We should include the FlowIdentifier in SdxDatabaseRestoreResponse, too", "author": "pdarvasi", "createdAt": "2020-06-23T09:50:58Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODY0OA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978648", "bodyText": "will update.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:25:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMjQzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzM5MA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103390", "bodyText": "should return FlowIdentifier", "author": "pdarvasi", "createdAt": "2020-06-23T09:52:32Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwMzUwNw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444103507", "bodyText": "should return FlowIdentifier", "author": "pdarvasi", "createdAt": "2020-06-23T09:52:43Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNTYyNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444105625", "bodyText": "We might want to set SDX status to something to indicate the restore with sdxStatusService.setStatusForDatalakeAndNotify(), too", "author": "pdarvasi", "createdAt": "2020-06-23T09:56:13Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI1MDUxOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444250519", "bodyText": "database backup/restore is one part of the datalake backup restore. There will be a separate patch to update the datalake status using SdxStatusService which will be invoked by datalakedr service which orchestrates the datalake backup and restore.", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:05:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNTYyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNjIyMQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444106221", "bodyText": "these 3 lines have multiple occurrences pls extract to a separate method.", "author": "pdarvasi", "createdAt": "2020-06-23T09:57:17Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MTI1OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444261259", "bodyText": "will do.", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:20:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwNjIyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwODY5NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444108694", "bodyText": "typo: successful + doe -> for", "author": "pdarvasi", "createdAt": "2020-06-23T10:01:29Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MTU2Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444261562", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:20:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEwODY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMDUzNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444110536", "bodyText": "we might log this as it should not happen", "author": "pdarvasi", "createdAt": "2020-06-23T10:04:46Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2MjcyMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444262720", "bodyText": "will do.", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:21:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMDUzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjg3Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444112873", "bodyText": "the first 6 lines of getDatabaseBackupStatus and getDatabaseRestoreStatus are almost the same -> should be extracted to a common method with SdxDatabaseDrStatusTypeEnum as parameter", "author": "pdarvasi", "createdAt": "2020-06-23T10:09:02Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2NTM4NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444265385", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjg3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444116256", "bodyText": "you shouldn't specify the name here", "author": "lacikaaa", "createdAt": "2020-06-23T10:15:04Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2NzI2Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444267263", "bodyText": "This patch also has the sql file to create table \"sdxDatabaseDrstatus\". Without having the name here, can I assume that the entity would using the table with the name \"sdxDatabaseDrstatus\"?", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:27:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3NDEzNw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444974137", "bodyText": "will do.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:19:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNjI1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNzk3OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444117979", "bodyText": "please move them out from the entity", "author": "lacikaaa", "createdAt": "2020-06-23T10:18:09Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3NDAxNA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444374014", "bodyText": "will do.", "author": "kkalvagadda1", "createdAt": "2020-06-23T17:01:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExNzk3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120259", "bodyText": "please use converter", "author": "lacikaaa", "createdAt": "2020-06-23T10:22:20Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2ODg2Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444268866", "bodyText": "Can you elaborate?", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:29:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MDk3Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444380977", "bodyText": "Figured it out. will do.", "author": "kkalvagadda1", "createdAt": "2020-06-23T17:13:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyMDMwOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444120309", "bodyText": "here too", "author": "lacikaaa", "createdAt": "2020-06-23T10:22:27Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,102 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Entity;\n+import javax.persistence.EnumType;\n+import javax.persistence.Enumerated;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+@Entity\n+@Table(name = \"sdxDatabaseDrstatus\", uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {\n+    public enum SdxDatabaseDrStatusTypeEnum {\n+        BACKUP,\n+        RESTORE\n+    }\n+\n+    public enum Status {\n+        INIT,\n+        TRIGGERRED,\n+        INPROGRESS,\n+        SUCCEEDED,\n+        FAILED\n+    }\n+\n+    @Id\n+    @GeneratedValue(strategy = GenerationType.AUTO, generator = \"sdx_status_generator\")\n+    @SequenceGenerator(name = \"sdx_status_generator\", sequenceName = \"sdxstatus_id_seq\", allocationSize = 1)\n+    private Long id;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)\n+    private SdxDatabaseDrStatusTypeEnum operationType;\n+\n+    @NotNull\n+    private Long sdxClusterId;\n+\n+    private String operationId;\n+\n+    private String statusReason;\n+\n+    @NotNull\n+    @Enumerated(EnumType.STRING)", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyNjk4OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444126989", "bodyText": "throws Exception is not necessary. same for all the doExecute", "author": "lacikaaa", "createdAt": "2020-06-23T10:35:25Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseBackupActions.java", "diffHunk": "@@ -0,0 +1,173 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT;\n+\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.context.annotation.Bean;\n+import org.springframework.context.annotation.Configuration;\n+import org.springframework.statemachine.StateContext;\n+import org.springframework.statemachine.action.Action;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxContext;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupCouldNotStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.AbstractSdxAction;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDrService;\n+import com.sequenceiq.flow.core.FlowEvent;\n+import com.sequenceiq.flow.core.FlowParameters;\n+import com.sequenceiq.flow.core.FlowState;\n+\n+@Configuration\n+public class DatalakeDatabaseBackupActions {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupActions.class);\n+\n+    private static final String OPERATION_ID = \"OPERATION-ID\";\n+\n+    @Inject\n+    private SdxDrService sdxDrService;\n+\n+    @Bean(name = \"DATALAKE_DATABASE_BACKUP_START_STATE\")\n+    public Action<?, ?> datalakeBackup() {\n+        return new AbstractSdxAction<>(DatalakeDatabaseBackupStartEvent.class) {\n+            @Override\n+            protected SdxContext createFlowContext(FlowParameters flowParameters, StateContext<FlowState, FlowEvent> stateContext,\n+                    DatalakeDatabaseBackupStartEvent payload) {\n+                return SdxContext.from(flowParameters, payload);\n+            }\n+\n+            @Override\n+            protected void prepareExecution(DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) {\n+                super.prepareExecution(payload, variables);\n+                variables.put(OPERATION_ID, payload.getDrStatus().getOperationId());\n+            }\n+\n+            @Override\n+            protected void doExecute(SdxContext context, DatalakeDatabaseBackupStartEvent payload, Map<Object, Object> variables) throws Exception {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI3MDIzOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444270239", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyNjk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTA0NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129044", "bodyText": "I won't go through all of your request, but please check them all.\nThe parent class already has defined this with EventSelectorUtil so you should depend on that instead of overriding it with the same", "author": "lacikaaa", "createdAt": "2020-06-23T10:39:17Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseBackupCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseBackupCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseBackupCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseBackupCouldNotStartEvent\";", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI3MzYzOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444273638", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-23T14:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTA0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444129969", "bodyText": "please check my comment on one of your event, and use there and here also EventSelectorUtil instead of duplicating string everywhere", "author": "lacikaaa", "createdAt": "2020-06-23T10:41:03Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrEvent.java", "diffHunk": "@@ -0,0 +1,32 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowEvent;\n+\n+public enum DatalakeDatabaseDrEvent implements FlowEvent {\n+\n+    DATALAKE_DATABASE_BACKUP_EVENT(\"DATALAKE_DATABASE_BACKUP_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_BACKUP_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_BACKUP_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT(\"DATALAKE_DATABASE_BACKUP_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT(\"DATALAKE_DATABASE_BACKUP_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_BACKUP_FAILURE_HANDLED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_EVENT(\"DATALAKE_DATABASE_RESTORE_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT(\"DATALAKE_DATABASE_RESTORE_COULD_NOT_START_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT(\"DATALAKE_DATABASE_RESTORE_IN_PROGRESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT(\"DATALAKE_DATABASE_RESTORE_SUCCESS_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT(\"DATALAKE_DATABASE_RESTORE_FINALIZED_EVENT\"),\n+    DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT(\"DATALAKE_DATABASE_RESTORE_FAILURE_HANDLED_EVENT\");", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzNDIyNw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444334227", "bodyText": "I do not have classes defined for all the events. For the events for which classes are defined, i will make changes following your suggestion.", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:58:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MzA1Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444743052", "bodyText": "you don't have to create classes for all events. I didn't want to go through all of them to tag which should be replaced. Leaving those untouched which don't have class is fine", "author": "lacikaaa", "createdAt": "2020-06-24T08:50:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEyOTk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444131588", "bodyText": "you should use either plain String with concatenation or StringBuilder.\nalso final is unnecessary", "author": "lacikaaa", "createdAt": "2020-06-23T10:44:19Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreCouldNotStartEvent.java", "diffHunk": "@@ -0,0 +1,33 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseRestoreCouldNotStartEvent extends SdxEvent {\n+    private final Exception exception;\n+\n+    public DatalakeDatabaseRestoreCouldNotStartEvent(Long sdxId, String userId, Exception exception) {\n+        super(\"DatalakeDatabaseRestoreCouldNotStartEvent\", sdxId, userId);\n+        this.exception = exception;\n+    }\n+\n+    public static DatalakeDatabaseRestoreCouldNotStartEvent from(SdxEvent event, Exception exception) {\n+        return new DatalakeDatabaseRestoreCouldNotStartEvent(event.getResourceId(), event.getUserId(), exception);\n+    }\n+\n+    @Override\n+    public String selector() {\n+        return \"DatalakeDatabaseRestoreCouldNotStartEvent\";\n+    }\n+\n+    public Exception getException() {\n+        return exception;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        final StringBuffer sb = new StringBuffer(\"DatalakeDatabaseRestoreCouldNotStartEvent{\");", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzMzU5Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444333592", "bodyText": "I will remove the final.\nStringBuilder is not thread-safe by StringBuffer is. That is why I used StringBuffer.\nIs there a specific reason to use StringBuilder?", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:57:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc0MTkwNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444741905", "bodyText": "why would you need thread-safe solution here? it has a performance drawback\nalso simple string concatenation would do, I usually use that for toString", "author": "lacikaaa", "createdAt": "2020-06-24T08:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk3ODMwMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444978303", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMTU4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133419", "bodyText": "this class needs some identation fix, idea fixed 8 lines for me", "author": "lacikaaa", "createdAt": "2020-06-23T10:47:45Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNTQxMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444135413", "bodyText": "I think this class should be broken into 3\n\nbackup\nrestore\ncommon part\nwhat do you think?", "author": "lacikaaa", "createdAt": "2020-06-23T10:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMTk4Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444301983", "bodyText": "Fixed the indentation. What is the rationale to split the service into multiple parts?", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:13:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDczODc3NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444738774", "bodyText": "ideally one class does one thing, see: single-responsibility principle\nalso it would be easier to read as there would be less code in that class. so if you are looking for backup related stuff, you don't have to go through the restore part", "author": "lacikaaa", "createdAt": "2020-06-24T08:43:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4ODYyOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444988628", "bodyText": "Will create a sperate patch for it.", "author": "kkalvagadda1", "createdAt": "2020-06-24T15:39:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzQxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzYyNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133626", "bodyText": "if service is the name I think the annotation should be service", "author": "lacikaaa", "createdAt": "2020-06-23T10:48:09Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMjA3Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444302073", "bodyText": "will update", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:13:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzkxMQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444133911", "bodyText": "if we need this comment then the name of the class is wrong", "author": "lacikaaa", "createdAt": "2020-06-23T10:48:35Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMzEzOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444303139", "bodyText": "Will rename the class to SdxDatabaseDrService.", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:14:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzMzkxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDE0NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134145", "bodyText": "unused", "author": "lacikaaa", "createdAt": "2020-06-23T10:49:05Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDIwMg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444134202", "bodyText": "unused", "author": "lacikaaa", "createdAt": "2020-06-23T10:49:12Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMwMzM3NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444303374", "bodyText": "will update.", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:15:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNDIwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNjA3NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444136074", "bodyText": "this condition is a bit hard to read and also almost the same as in getDatabaseRestoreStatus\nit would worth to refactor them into a method", "author": "lacikaaa", "createdAt": "2020-06-23T10:53:05Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation doe SDX cluster {} is successfull\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    private boolean stackAndClusterAvailable(StackV4Response stackV4Response, ClusterV4Response cluster) {\n+        return stackV4Response.getStatus().isAvailable()\n+                && cluster != null\n+                && cluster.getStatus() != null\n+                && cluster.getStatus().isAvailable();\n+    }\n+\n+    /**\n+     * Updates the status of the database backup/restore operation.\n+     * @param operationId Operation Id\n+     * @param status Status of the operation\n+     * @param failedReason Failure reason, if any.\n+     */\n+    public void updateDatabaseStatusEntry(String operationId, SdxDatabaseDrStatus.Status status, String failedReason) {\n+        if (Strings.isNullOrEmpty(operationId)) {\n+            return;\n+        }\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        drStatus.setStatus(status);\n+        if (!Strings.isNullOrEmpty(failedReason)) {\n+            drStatus.setStatusReason(failedReason);\n+        }\n+        sdxDatabaseDrStatusRepository.save(drStatus);\n+    }\n+\n+    /**\n+     * Gets the status of the database backup operation.\n+     * @param sdxCluster Sdx cluster on which the backup operation is performed.\n+     * @param operationId Operation Id\n+     * @return Backup status\n+     */\n+    public SdxDatabaseBackupStatusResponse getDatabaseBackupStatus(SdxCluster sdxCluster, String operationId) {\n+        SdxDatabaseDrStatus drStatus = sdxDatabaseDrStatusRepository.findSdxDatabaseDrStatusByOperationId(operationId);\n+        if ((drStatus == null) || (!drStatus.getSdxClusterId().equals(sdxCluster.getId()))\n+                || (!drStatus.getOperationType().equals(SdxDatabaseDrStatus.SdxDatabaseDrStatusTypeEnum.BACKUP))) {", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MzYxMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444293613", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:01:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNjA3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzQ1NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137454", "bodyText": "this if-else thing got out of hand. please refactor parts of into separate method with meaningful names so it would be easier to follow what happens", "author": "lacikaaa", "createdAt": "2020-06-23T10:55:53Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in inmemory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (stackAndClusterAvailable(stackV4Response, cluster)) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else {\n+            if (Status.BACKUP_FAILED.equals(stackV4Response.getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getStatus())) {\n+                LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+            } else if (Status.BACKUP_FAILED.equals(stackV4Response.getCluster().getStatus()) ||\n+                    Status.RESTORE_FAILED.equals(stackV4Response.getCluster().getStatus())) {\n+                LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                        stackV4Response.getCluster().getStatus());\n+                return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+            } else {\n+                if (FINISHED.equals(flowState)) {\n+                    LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+                    return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+                } else {\n+                    return AttemptResults.justContinue();\n+                }\n+            }\n+        }", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNzQ4Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444327486", "bodyText": "will refactor it.", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:49:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzQ1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzkyMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444137920", "bodyText": "you might want to use WebApplicationExceptionMessageExtractor here", "author": "lacikaaa", "createdAt": "2020-06-23T10:56:55Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDrService.java", "diffHunk": "@@ -0,0 +1,273 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.status.SdxStatusService;\n+import com.sequenceiq.redbeams.api.endpoint.v4.databaseserver.DatabaseServerV4Endpoint;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Component\n+public class SdxDrService {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private SdxStatusService sdxStatusService;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private DatabaseServerV4Endpoint databaseServerV4Endpoint;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        String operationId = triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseBackupResponse(operationId);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        String operationId = triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+        return new SdxDatabaseRestoreResponse(operationId);\n+\n+    }\n+\n+    private String triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    private String triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        return sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(clusterId, backupId, backupLocation);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, backupV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                cloudbreakFlowService.saveLastCloudbreakFlowChainId(sdxCluster, restoreV4Response.getFlowIdentifier());\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.TRIGGERRED, null);\n+            }, () -> {\n+                updateDatabaseStatusEntry(drStatus.getOperationId(), SdxDatabaseDrStatus.Status.FAILED,\n+                        String.format(\"SDX cluster with Id %d not found\", clusterId));\n+                throw notFound(\"SDX cluster\", clusterId).get();\n+            });\n+        } catch (WebApplicationException e) {\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, e.getMessage());\n+            throw new CloudbreakApiException(message);\n+        }", "originalCommit": "52d0c5ba63baf481f6bd70ccb9ab3d5bc616f511", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyOTU0Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r444329543", "bodyText": "Will use it", "author": "kkalvagadda1", "createdAt": "2020-06-23T15:52:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDEzNzkyMA=="}], "type": "inlineReview"}, {"oid": "7bff4f9ec8d26fb09462b89b93099f341d595cef", "url": "https://github.com/hortonworks/cloudbreak/commit/7bff4f9ec8d26fb09462b89b93099f341d595cef", "message": "addressed review comments.", "committedDate": "2020-06-24T15:37:00Z", "type": "forcePushed"}, {"oid": "5f09eca14c5c14a0d91249d689024d88c8163506", "url": "https://github.com/hortonworks/cloudbreak/commit/5f09eca14c5c14a0d91249d689024d88c8163506", "message": "fixed issues observed in e2e testing.", "committedDate": "2020-06-25T12:06:49Z", "type": "forcePushed"}, {"oid": "5675e74f382ccd9d117a2c32c7a69a8ba9401d04", "url": "https://github.com/hortonworks/cloudbreak/commit/5675e74f382ccd9d117a2c32c7a69a8ba9401d04", "message": "fixed failure observed in e2e testing.", "committedDate": "2020-06-25T16:50:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzOTUxOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446139519", "bodyText": "why would you skip check in case of finished state?\nalso it looks like is changed in the other PR too", "author": "lacikaaa", "createdAt": "2020-06-26T11:58:04Z", "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -142,8 +142,10 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.EXTERNAL_DATABASE_CREATION_IN_PROGRESS,\n                 Status.BACKUP_IN_PROGRESS,\n                 Status.BACKUP_FAILED,\n+                Status.BACKUP_FINISHED,\n                 Status.RESTORE_IN_PROGRESS,\n-                Status.RESTORE_FAILED\n+                Status.RESTORE_FAILED,\n+                Status.RESTORE_FINISHED", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0Nzc0Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446247746", "bodyText": "will move the finished states to synchable states", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzOTUxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1MTg3Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446151872", "bodyText": "after having another glimpse at your pr, I'm confused why would you need this altogether.\nI mean we have flowid, with that you should be able to create a status on the fly, as this is also generated from the state of the flow.\nSo I think you should drop all the operationid stuff, return with the flowid, and the query endpoint should expect flowid instead of operationid. (right now your response also have flowid in it)\nI know in freeipa we have this, but back then we didn't have this for flow, and also usersync doesn't run in flow. But if we would like to go this way, then we should move from returning flowid on all endpoint and make operation general, but I'm not sure this would worth it right now", "author": "lacikaaa", "createdAt": "2020-06-26T12:25:44Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/SdxDatabaseDrStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxDatabaseDrStatus {", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDQ2MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446230461", "bodyText": "As per our conversation, I will generalize the classes and move them to a new package,", "author": "kkalvagadda1", "createdAt": "2020-06-26T14:48:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1MTg3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1NzA2Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446157067", "bodyText": "before the return line you should build the MDCContext using MdcBuilder", "author": "lacikaaa", "createdAt": "2020-06-26T12:36:59Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDY1Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446230656", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-26T14:48:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE1NzA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2MzEzNQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446163135", "bodyText": "as backup and restore 2 separate flow, it should have 2 separate state and event enum", "author": "lacikaaa", "createdAt": "2020-06-26T12:49:14Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/DatalakeDatabaseDrState.java", "diffHunk": "@@ -0,0 +1,35 @@\n+package com.sequenceiq.datalake.flow.dr;\n+\n+import com.sequenceiq.flow.core.FlowState;\n+import com.sequenceiq.flow.core.RestartAction;\n+import com.sequenceiq.flow.core.restart.DefaultRestartAction;\n+\n+public enum DatalakeDatabaseDrState implements FlowState {", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MjU2Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446242567", "bodyText": "will do.", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2MzEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NDk5NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446164994", "bodyText": "could be final", "author": "lacikaaa", "createdAt": "2020-06-26T12:53:07Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseBackupStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+\n+    private String backupId;\n+\n+    private String backupLocation;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzM4NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243384", "bodyText": "will fix", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:10:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NDk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTExOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165118", "bodyText": "could be final", "author": "lacikaaa", "createdAt": "2020-06-26T12:53:21Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseBackupSuccessEvent.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseBackupSuccessEvent extends SdxEvent {\n+    private String operationId;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzQ2Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243466", "bodyText": "will fix", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:10:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTM5NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165395", "bodyText": "coudl be final", "author": "lacikaaa", "createdAt": "2020-06-26T12:53:58Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseDrStartBaseEvent.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxEvent;\n+\n+public class DatalakeDatabaseDrStartBaseEvent extends SdxEvent  {\n+    private SdxDatabaseDrStatus drStatus;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzUwNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243506", "bodyText": "will fix", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:10:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTYwMA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165600", "bodyText": "final", "author": "lacikaaa", "createdAt": "2020-06-26T12:54:23Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/event/DatalakeDatabaseRestoreStartEvent.java", "diffHunk": "@@ -0,0 +1,25 @@\n+package com.sequenceiq.datalake.flow.dr.event;\n+\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+\n+public class DatalakeDatabaseRestoreStartEvent extends DatalakeDatabaseDrStartBaseEvent {\n+    private String backupId;\n+\n+    private String backupLocation;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0MzU1NQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446243555", "bodyText": "will fix", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:10:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446165952", "bodyText": "should be configurable", "author": "lacikaaa", "createdAt": "2020-06-26T12:55:02Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseBackupWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseBackupWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseBackupWaitRequest> {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(DatalakeDatabaseBackupWaitHandler.class);\n+\n+    private static final int SLEEP_TIME_IN_SEC = 20;\n+\n+    private static final int DURATION_IN_MINUTES = 90;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NDg5NA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446244894", "bodyText": "I did find a place where these values are configurable. Can you provide a reference?", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:12:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2MDQzMg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446260432", "bodyText": "look for @Value anywhere in the code", "author": "lacikaaa", "createdAt": "2020-06-26T15:40:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzM1Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446167352", "bodyText": "the 2 wait handler is almost the same, might be worth to refactor them into one", "author": "lacikaaa", "createdAt": "2020-06-26T12:57:41Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/flow/dr/handler/DatalakeDatabaseRestoreWaitHandler.java", "diffHunk": "@@ -0,0 +1,69 @@\n+package com.sequenceiq.datalake.flow.dr.handler;\n+\n+import java.util.concurrent.TimeUnit;\n+\n+import javax.inject.Inject;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Component;\n+\n+import com.dyngr.exception.PollerException;\n+import com.dyngr.exception.PollerStoppedException;\n+import com.dyngr.exception.UserBreakException;\n+import com.sequenceiq.cloudbreak.common.event.Selectable;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreFailedEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreSuccessEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreWaitRequest;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.datalake.service.sdx.dr.SdxDatabaseDrService;\n+import com.sequenceiq.flow.event.EventSelectorUtil;\n+import com.sequenceiq.flow.reactor.api.handler.ExceptionCatcherEventHandler;\n+\n+@Component\n+public class DatalakeDatabaseRestoreWaitHandler extends ExceptionCatcherEventHandler<DatalakeDatabaseRestoreWaitRequest> {", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NTYxOA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446245618", "bodyText": "They send different events on success and failure scenarios. I think it will be clean to have them separate.", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzM1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2OTYxOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446169619", "bodyText": "return Status.BACKUP_FINISHED.equals(status) ||\n                Status.RESTORE_FINISHED.equals(status);", "author": "lacikaaa", "createdAt": "2020-06-26T13:01:59Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */\n+    private boolean isStackOrClusterDrStatusComplete(Status status) {\n+        return (Status.BACKUP_FINISHED.equals(status) ||\n+                Status.RESTORE_FINISHED.equals(status)) ? true : false;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjMwNg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246306", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:15:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2OTYxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDA2Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170062", "bodyText": "return Status.BACKUP_FAILED.equals(status) ||\n                Status.RESTORE_FAILED.equals(status);", "author": "lacikaaa", "createdAt": "2020-06-26T13:02:57Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjM2OQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246369", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:15:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDA2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDM4Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170387", "bodyText": "drop the comment, method name is self explaining", "author": "lacikaaa", "createdAt": "2020-06-26T13:03:39Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */\n+    private boolean isStackOrClusterStatusFailed(Status status) {\n+        return (Status.BACKUP_FAILED.equals(status) ||\n+                Status.RESTORE_FAILED.equals(status)) ? true : false;\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrFailed(SdxCluster sdxCluster, String statusReason, String pollingMessage) {\n+        LOGGER.info(\"{} failed, statusReason: {}\", pollingMessage, statusReason);\n+        return AttemptResults.breakFor(\"SDX \" + pollingMessage + \" failed '\" + sdxCluster.getClusterName() + \"', \" + statusReason);\n+    }\n+\n+    private AttemptResult<StackV4Response> sdxDrSucceeded(StackV4Response stackV4Response) {\n+        LOGGER.info(\"Database DR operation does SDX cluster {} is successful\", stackV4Response.getCluster().getName());\n+        return AttemptResults.finishWith(stackV4Response);\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation is successful.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is successful, false otherwise.\n+     */", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0NjgyMw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246823", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:16:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDM4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDQ0Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446170446", "bodyText": "drop the comment", "author": "lacikaaa", "createdAt": "2020-06-26T13:03:47Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/service/sdx/dr/SdxDatabaseDrService.java", "diffHunk": "@@ -0,0 +1,309 @@\n+package com.sequenceiq.datalake.service.sdx.dr;\n+\n+import static com.sequenceiq.cloudbreak.exception.NotFoundException.notFound;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_BACKUP_EVENT;\n+import static com.sequenceiq.datalake.flow.dr.DatalakeDatabaseDrEvent.DATALAKE_DATABASE_RESTORE_EVENT;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.FINISHED;\n+import static com.sequenceiq.datalake.service.sdx.CloudbreakFlowService.FlowState.RUNNING;\n+\n+import java.util.Collections;\n+\n+import javax.inject.Inject;\n+import javax.ws.rs.WebApplicationException;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.stereotype.Service;\n+\n+import com.dyngr.Polling;\n+import com.dyngr.core.AttemptResult;\n+import com.dyngr.core.AttemptResults;\n+import com.fasterxml.jackson.core.JsonProcessingException;\n+import com.google.common.base.Strings;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.common.Status;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.StackV4Endpoint;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.StackV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.cluster.ClusterV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.BackupV4Response;\n+import com.sequenceiq.cloudbreak.api.endpoint.v4.stacks.response.dr.RestoreV4Response;\n+import com.sequenceiq.cloudbreak.auth.ThreadBasedUserCrnProvider;\n+import com.sequenceiq.cloudbreak.cloud.scheduler.PollGroup;\n+import com.sequenceiq.cloudbreak.common.exception.WebApplicationExceptionMessageExtractor;\n+import com.sequenceiq.cloudbreak.common.json.JsonUtil;\n+import com.sequenceiq.cloudbreak.exception.CloudbreakApiException;\n+import com.sequenceiq.datalake.entity.SdxCluster;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrOperationTypeEnum;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatus;\n+import com.sequenceiq.datalake.entity.SdxDatabaseDrStatusTypeEnum;\n+import com.sequenceiq.datalake.flow.SdxReactorFlowManager;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseBackupStartEvent;\n+import com.sequenceiq.datalake.flow.dr.event.DatalakeDatabaseRestoreStartEvent;\n+import com.sequenceiq.datalake.flow.statestore.DatalakeInMemoryStateStore;\n+import com.sequenceiq.datalake.repository.SDxDatabaseDrStatusRepository;\n+import com.sequenceiq.datalake.repository.SdxClusterRepository;\n+import com.sequenceiq.datalake.service.sdx.CloudbreakFlowService;\n+import com.sequenceiq.datalake.service.sdx.PollingConfig;\n+import com.sequenceiq.flow.api.model.FlowIdentifier;\n+import com.sequenceiq.sdx.api.model.DatalakeDatabaseDrStatus;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseBackupStatusResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreResponse;\n+import com.sequenceiq.sdx.api.model.SdxDatabaseRestoreStatusResponse;\n+\n+/**\n+ * Service to perform backup/restore of the database backing SDX.\n+ */\n+@Service\n+public class SdxDatabaseDrService {\n+\n+    private static final int MAX_SIZE_OF_FAILURE_REASON = 254;\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(SdxDatabaseDrService.class);\n+\n+    @Inject\n+    private SdxReactorFlowManager sdxReactorFlowManager;\n+\n+    @Inject\n+    private StackV4Endpoint stackV4Endpoint;\n+\n+    @Inject\n+    private WebApplicationExceptionMessageExtractor webApplicationExceptionMessageExtractor;\n+\n+    @Inject\n+    private CloudbreakFlowService cloudbreakFlowService;\n+\n+    @Inject\n+    private SdxClusterRepository sdxClusterRepository;\n+\n+    @Inject\n+    private SDxDatabaseDrStatusRepository sdxDatabaseDrStatusRepository;\n+\n+    public SdxDatabaseBackupResponse triggerDatabaseBackup(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+\n+        return triggerDatalakeDatabaseBackupFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    public SdxDatabaseRestoreResponse triggerDatabaseRestore(SdxCluster sdxCluster, String backupId, String backupLocation) {\n+        return triggerDatalakeDatabaseRestoreFlow(sdxCluster.getId(), backupId, backupLocation);\n+    }\n+\n+    private SdxDatabaseBackupResponse triggerDatalakeDatabaseBackupFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_BACKUP_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseBackupStartEvent startEvent = new DatalakeDatabaseBackupStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseBackupFlow(startEvent);\n+        return new SdxDatabaseBackupResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    private SdxDatabaseRestoreResponse triggerDatalakeDatabaseRestoreFlow(Long clusterId, String backupId, String backupLocation) {\n+        String selector = DATALAKE_DATABASE_RESTORE_EVENT.event();\n+        String userId = ThreadBasedUserCrnProvider.getUserCrn();\n+        DatalakeDatabaseRestoreStartEvent startEvent = new DatalakeDatabaseRestoreStartEvent(selector, clusterId, userId, backupId, backupLocation);\n+        FlowIdentifier flowIdentifier = sdxReactorFlowManager.triggerDatalakeDatabaseRestoreFlow(startEvent);\n+        return new SdxDatabaseRestoreResponse(startEvent.getDrStatus().getOperationId(), flowIdentifier);\n+    }\n+\n+    public void databaseBackup(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                BackupV4Response backupV4Response = stackV4Endpoint.backupDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, backupV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database backup failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void databaseRestore(SdxDatabaseDrStatus drStatus, Long clusterId, String backupId, String backupLocation) {\n+        try {\n+            sdxDatabaseDrStatusRepository.save(drStatus);\n+            sdxClusterRepository.findById(clusterId).ifPresentOrElse(sdxCluster -> {\n+                RestoreV4Response restoreV4Response = stackV4Endpoint.restoreDatabaseByName(0L, sdxCluster.getClusterName(),\n+                        backupLocation, backupId);\n+                updateSuccessStatus(drStatus.getOperationId(), sdxCluster, restoreV4Response.getFlowIdentifier(),\n+                        SdxDatabaseDrStatusTypeEnum.TRIGGERRED);\n+            }, () -> {\n+                updateFailureStatus(drStatus.getOperationId(), clusterId, String.format(\"SDX cluster with Id [%d] not found\", clusterId));\n+            });\n+        } catch (WebApplicationException e) {\n+            String errorMessage = webApplicationExceptionMessageExtractor.getErrorMessage(e);\n+            String message = String.format(\"Database restore failed for datalake-id: [%d]. Message: [%s]\", clusterId, errorMessage);\n+            throw new CloudbreakApiException(message);\n+        }\n+    }\n+\n+    public void waitCloudbreakFlow(Long id, PollingConfig pollingConfig, String pollingMessage) {\n+        SdxCluster sdxCluster = sdxClusterRepository.findById(id).orElseThrow(notFound(\"SDX cluster\", id));\n+        Polling.waitPeriodly(pollingConfig.getSleepTime(), pollingConfig.getSleepTimeUnit())\n+                .stopIfException(pollingConfig.getStopPollingIfExceptionOccured())\n+                .stopAfterDelay(pollingConfig.getDuration(), pollingConfig.getDurationTimeUnit())\n+                .run(() -> checkDatabaseDrStatus(sdxCluster, pollingMessage));\n+    }\n+\n+    private AttemptResult<StackV4Response> checkDatabaseDrStatus(SdxCluster sdxCluster, String pollingMessage) throws JsonProcessingException {\n+        LOGGER.info(\"{} polling cloudbreak for stack status: '{}' in '{}' env\", pollingMessage, sdxCluster.getClusterName(), sdxCluster.getEnvName());\n+        try {\n+            if (PollGroup.CANCELLED.equals(DatalakeInMemoryStateStore.get(sdxCluster.getId()))) {\n+                LOGGER.info(\"{} polling cancelled in in-memory store, id: {}\", pollingMessage, sdxCluster.getId());\n+                return AttemptResults.breakFor(pollingMessage + \" polling cancelled in inmemory store, id: \" + sdxCluster.getId());\n+            } else {\n+                CloudbreakFlowService.FlowState flowState = cloudbreakFlowService.getLastKnownFlowState(sdxCluster);\n+                if (RUNNING.equals(flowState)) {\n+                    LOGGER.info(\"{} polling will continue, cluster has an active flow in Cloudbreak, id: {}\", pollingMessage, sdxCluster.getId());\n+                    return AttemptResults.justContinue();\n+                } else {\n+                    return getStackResponseAttemptResult(sdxCluster, pollingMessage, flowState);\n+                }\n+            }\n+        } catch (javax.ws.rs.NotFoundException e) {\n+            LOGGER.debug(\"Stack not found on CB side \" + sdxCluster.getClusterName(), e);\n+            return AttemptResults.breakFor(\"Stack not found on CB side \" + sdxCluster.getClusterName());\n+        }\n+    }\n+\n+    private AttemptResult<StackV4Response> getStackResponseAttemptResult(SdxCluster sdxCluster, String pollingMessage, CloudbreakFlowService.FlowState flowState)\n+            throws JsonProcessingException {\n+        StackV4Response stackV4Response = stackV4Endpoint.get(0L, sdxCluster.getClusterName(), Collections.emptySet());\n+        LOGGER.info(\"Response from cloudbreak: {}\", JsonUtil.writeValueAsString(stackV4Response));\n+        ClusterV4Response cluster = stackV4Response.getCluster();\n+        if (isStackOrClusterDrStatusComplete(stackV4Response.getStatus()) && isStackOrClusterDrStatusComplete(cluster.getStatus())) {\n+            return sdxDrSucceeded(stackV4Response);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getStatus())) {\n+            LOGGER.info(\"{} failed for Stack {} with status {}\", pollingMessage, stackV4Response.getName(), stackV4Response.getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getStatusReason(), pollingMessage);\n+        } else if (isStackOrClusterStatusFailed(stackV4Response.getCluster().getStatus())) {\n+            LOGGER.info(\"{} failed for Cluster {} status {}\", pollingMessage, stackV4Response.getCluster().getName(),\n+                    stackV4Response.getCluster().getStatus());\n+            return sdxDrFailed(sdxCluster, stackV4Response.getCluster().getStatusReason(), pollingMessage);\n+        } else if (FINISHED.equals(flowState)) {\n+            LOGGER.info(\"Flow finished, but Backup/Restore is not complete: {}\", sdxCluster.getClusterName());\n+            return sdxDrFailed(sdxCluster, \"stack is in improper state\", pollingMessage);\n+        } else {\n+            LOGGER.info(\"Flow is unknown state\");\n+            return sdxDrFailed(sdxCluster, \"Flow is unknown state\", pollingMessage);\n+        }\n+    }\n+\n+    /**\n+     * Checks the status if the backup/restore operation failed.\n+     * @param status It could be stack/cluster status\n+     * @return true if status is failed, false otherwise.\n+     */", "originalCommit": "84c0a31f4e95900fb13de42d1db56c55e1c6bd7a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI0Njg5Nw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446246897", "bodyText": "will do", "author": "kkalvagadda1", "createdAt": "2020-06-26T15:16:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE3MDQ0Ng=="}], "type": "inlineReview"}, {"oid": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "url": "https://github.com/hortonworks/cloudbreak/commit/4592edaca637c73aebd18e9c87b2e62a1143aa11", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-26T15:25:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NTA4Ng==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446895086", "bodyText": "I think this class should be renamed also", "author": "lacikaaa", "createdAt": "2020-06-29T11:22:47Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/converter/SdxDatabaseDrStatusTypeEnumConverter.java", "diffHunk": "@@ -0,0 +1,12 @@\n+package com.sequenceiq.datalake.converter;\n+\n+import com.sequenceiq.cloudbreak.converter.DefaultEnumConverter;\n+import com.sequenceiq.datalake.entity.operation.SdxOperationStatusTypeEnum;\n+\n+public class SdxDatabaseDrStatusTypeEnumConverter extends DefaultEnumConverter<SdxOperationStatusTypeEnum> {", "originalCommit": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5MjQ5Mw==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446992493", "bodyText": "ok", "author": "kkalvagadda1", "createdAt": "2020-06-29T13:58:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NTA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NjY3MA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446896670", "bodyText": "I think SdxOperation would fit better, there is too much status around this class", "author": "lacikaaa", "createdAt": "2020-06-29T11:25:54Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatus.java", "diffHunk": "@@ -0,0 +1,92 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+import java.util.UUID;\n+\n+import javax.persistence.Convert;\n+import javax.persistence.Entity;\n+import javax.persistence.GeneratedValue;\n+import javax.persistence.GenerationType;\n+import javax.persistence.Id;\n+import javax.persistence.SequenceGenerator;\n+import javax.persistence.Table;\n+import javax.persistence.UniqueConstraint;\n+import javax.validation.constraints.NotNull;\n+\n+import com.sequenceiq.datalake.converter.SdxOperationTypeEnumConverter;\n+import com.sequenceiq.datalake.converter.SdxDatabaseDrStatusTypeEnumConverter;\n+\n+@Entity\n+@Table(uniqueConstraints = @UniqueConstraint(columnNames = {\"operationId\"}))\n+public class SdxOperationStatus {", "originalCommit": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5NzUzOQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446897539", "bodyText": "rename to SdxOperationStatus", "author": "lacikaaa", "createdAt": "2020-06-29T11:27:31Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationStatusTypeEnum.java", "diffHunk": "@@ -0,0 +1,19 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationStatusTypeEnum {", "originalCommit": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5ODIzNA==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446898234", "bodyText": "drop the Enum from the end", "author": "lacikaaa", "createdAt": "2020-06-29T11:28:57Z", "path": "datalake/src/main/java/com/sequenceiq/datalake/entity/operation/SdxOperationTypeEnum.java", "diffHunk": "@@ -0,0 +1,17 @@\n+package com.sequenceiq.datalake.entity.operation;\n+\n+public enum SdxOperationTypeEnum {", "originalCommit": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjkwMTI5MQ==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446901291", "bodyText": "I don't know which pr will contain the final setting for this, but the 2 FAILED statuses should be here also.\ncc @hreeve-cloudera", "author": "lacikaaa", "createdAt": "2020-06-29T11:34:49Z", "path": "core/src/main/java/com/sequenceiq/cloudbreak/job/StackStatusCheckerJob.java", "diffHunk": "@@ -156,7 +156,9 @@ protected void executeInternal(JobExecutionContext context) throws JobExecutionE\n                 Status.START_FAILED,\n                 Status.STOPPED,\n                 Status.STOP_FAILED,\n-                Status.AMBIGUOUS\n+                Status.AMBIGUOUS,\n+                Status.BACKUP_FINISHED,\n+                Status.RESTORE_FINISHED", "originalCommit": "4592edaca637c73aebd18e9c87b2e62a1143aa11", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5ODM2Mg==", "url": "https://github.com/hortonworks/cloudbreak/pull/8194#discussion_r446998362", "bodyText": "will take care of it.", "author": "kkalvagadda1", "createdAt": "2020-06-29T14:06:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjkwMTI5MQ=="}], "type": "inlineReview"}, {"oid": "f36e483bae35f9e39f0c958d3b73e7002981153a", "url": "https://github.com/hortonworks/cloudbreak/commit/f36e483bae35f9e39f0c958d3b73e7002981153a", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-29T13:32:05Z", "type": "forcePushed"}, {"oid": "cb474bc68b698a1fe3a72cafece971a6000dfa05", "url": "https://github.com/hortonworks/cloudbreak/commit/cb474bc68b698a1fe3a72cafece971a6000dfa05", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-29T14:31:49Z", "type": "forcePushed"}, {"oid": "b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "url": "https://github.com/hortonworks/cloudbreak/commit/b1a94bc7942abdb0d7e5ea324ba2eddcdba0c80c", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-29T14:47:13Z", "type": "forcePushed"}, {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f", "url": "https://github.com/hortonworks/cloudbreak/commit/87ffbcb7be03687ef4eae15f750eb4889195a22f", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-29T15:25:28Z", "type": "commit"}, {"oid": "87ffbcb7be03687ef4eae15f750eb4889195a22f", "url": "https://github.com/hortonworks/cloudbreak/commit/87ffbcb7be03687ef4eae15f750eb4889195a22f", "message": "CB-7212: Add datalake service changes to perform database backup/restore.", "committedDate": "2020-06-29T15:25:28Z", "type": "forcePushed"}]}