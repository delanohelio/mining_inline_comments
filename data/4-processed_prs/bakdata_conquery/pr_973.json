{"pr_number": 973, "pr_title": "imports by column name", "pr_createdAt": "2020-01-02T15:50:30Z", "pr_url": "https://github.com/bakdata/conquery/pull/973", "timeline": [{"oid": "51b3039b65f1b69b073c51a62e7355bf17fea516", "url": "https://github.com/bakdata/conquery/commit/51b3039b65f1b69b073c51a62e7355bf17fea516", "message": "moved error processing of filter out of try-catch block in Preprocessor", "committedDate": "2020-05-12T07:03:38Z", "type": "commit"}, {"oid": "a565ea3e16c2b0805b44f5aefb50bd7115fa33ad", "url": "https://github.com/bakdata/conquery/commit/a565ea3e16c2b0805b44f5aefb50bd7115fa33ad", "message": "fix assertion handling of PreprocessedHeader to not go out of bounds and try checking as much cols as possible", "committedDate": "2020-05-13T07:28:31Z", "type": "commit"}, {"oid": "797b63a909135bf6b1e4034ead415c9d1ce78c35", "url": "https://github.com/bakdata/conquery/commit/797b63a909135bf6b1e4034ead415c9d1ce78c35", "message": "upgrade select if big", "committedDate": "2020-05-13T14:37:05Z", "type": "commit"}, {"oid": "1eb95d008f0266adefae918629d655179a49a09b", "url": "https://github.com/bakdata/conquery/commit/1eb95d008f0266adefae918629d655179a49a09b", "message": "add: still send options argument.", "committedDate": "2020-05-13T15:34:08Z", "type": "commit"}, {"oid": "ae45b18cc240bc18ee220ff0da780bb82818191f", "url": "https://github.com/bakdata/conquery/commit/ae45b18cc240bc18ee220ff0da780bb82818191f", "message": "Add better error logging", "committedDate": "2020-05-13T16:01:08Z", "type": "commit"}, {"oid": "faf6c1aff54bc019c90d034c3f9fb9a59e2656aa", "url": "https://github.com/bakdata/conquery/commit/faf6c1aff54bc019c90d034c3f9fb9a59e2656aa", "message": "set defaultReturnValue to  -1", "committedDate": "2020-05-13T16:07:14Z", "type": "commit"}, {"oid": "a4c29f6a67c18e522db0287aab5628a947df504b", "url": "https://github.com/bakdata/conquery/commit/a4c29f6a67c18e522db0287aab5628a947df504b", "message": "fixed always returning false :/", "committedDate": "2020-05-13T16:13:13Z", "type": "commit"}, {"oid": "516f0dc44111de268d0f4bbf4ebfdc18116796c4", "url": "https://github.com/bakdata/conquery/commit/516f0dc44111de268d0f4bbf4ebfdc18116796c4", "message": "Simplify TableInputDescriptor; logging and headers map", "committedDate": "2020-05-14T07:24:28Z", "type": "commit"}, {"oid": "d16d27b19c2754ee2e6b864e28781de9b604cba7", "url": "https://github.com/bakdata/conquery/commit/d16d27b19c2754ee2e6b864e28781de9b604cba7", "message": "new exception logging", "committedDate": "2020-05-14T14:30:45Z", "type": "commit"}, {"oid": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "url": "https://github.com/bakdata/conquery/commit/5c5a11a46f77cf954df5040bdf01f3a34388a20c", "message": "fix NullPointerException and remove redundant exception throws", "committedDate": "2020-05-14T14:52:54Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTAzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675038", "bodyText": "Dieses und das n\u00e4chste if k\u00f6nnen weg wenn  ein max-Value == Integer.MAX_VALUE dann kannst einfach Math#min drauf aufrufen. Das ist dann auch egal", "author": "thoniTUB", "createdAt": "2020-05-18T14:38:46Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NTQ5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426675494", "bodyText": "Auch hier k\u00f6nnen dieses und das n\u00e4chste if  weg", "author": "thoniTUB", "createdAt": "2020-05-18T14:39:26Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -199,6 +195,74 @@ public CDateRange span(CDateRange other) {\n \t\treturn of(Math.min(getMinValue(), other.getMinValue()), Math.max(getMaxValue(), other.getMaxValue()));\n \t}\n \n+\t/**\n+\t * Create a span over ranges ignoring incoming open values, and favoring closed values.\n+\t *\n+\t * @param other Date range to span over, may be open.\n+\t * @return A new closed span.\n+\t */\n+\tpublic CDateRange spanClosed(CDateRange other) {\n+\t\tif(other == null){\n+\t\t\treturn this;\n+\t\t}\n+\n+\t\tint min = Integer.MAX_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMinValue() != Integer.MIN_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMinValue());\n+\t\t\t}\n+\n+\t\t\tif (getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (other.getMaxValue() != Integer.MAX_VALUE) {\n+\t\t\t\tmin = Math.min(min, other.getMaxValue());\n+\t\t\t}\n+\n+\t\t\tif (min == Integer.MAX_VALUE) {\n+\t\t\t\tmin = Integer.MIN_VALUE;\n+\t\t\t}\n+\t\t}\n+\n+\t\tint max = Integer.MIN_VALUE;\n+\t\t{\n+\t\t\tif (getMinValue() != Integer.MIN_VALUE) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODY4Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426698683", "bodyText": "Error hat mich etwas verwundert:\n\nAn Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch.", "author": "thoniTUB", "createdAt": "2020-05-18T15:11:20Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);\n+\n+\t\t\tfor (int col = 0; col < headers.length; col++) {\n+\t\t\t\tgroovy.setVariable(headers[col], col);\n+\t\t\t}\n+\n+\t\t\treturn  (GroovyPredicate) groovy.parse(filter);\n+\t\t} catch (Exception | Error e) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426701091", "bodyText": "Das sieht so aus, als k\u00f6nnte es auch statisch sein.", "author": "thoniTUB", "createdAt": "2020-05-18T15:14:39Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,141 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.Arrays;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.Data;\n+import lombok.extern.slf4j.Slf4j;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+@Slf4j\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"id\", MajorTypeId.STRING);\n+\t@Valid @NotEmpty\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")\n+\tpublic boolean isValidGroovyScript(){\n+\t\ttry{\n+\t\t\tcreateFilter(FAKE_HEADERS);\n+\t\t}\n+\t\tcatch (Exception ex) {\n+\t\t\tlog.error(\"Groovy script is not valid\",ex);\n+\t\t\treturn false;\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"Each column requires a unique name\")\n+\tpublic boolean isEachNameUnique() {\n+\t\tObject2IntMap<String> names = new Object2IntArrayMap<>(getWidth());\n+\t\tnames.defaultReturnValue(-1);\n+\n+\t\tfor (int index = 0; index < output.length; index++) {\n+\t\t\tint prev = names.put(output[index].getName(), index);\n+\t\t\tif(prev != -1){\n+\t\t\t\tlog.error(\"Duplicate Output to Column[{}] at indices {} and {}\", output[index].getName(), prev, index);\n+\t\t\t\treturn false;\n+\t\t\t}\n+\t\t}\n+\n+\t\treturn true;\n+\t}\n+\n+\t@JsonIgnore\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n+\tpublic boolean isPrimaryString() {\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n+\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\ttry {\n+\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n+\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n+\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n+\n+\t\t\tGroovyShell groovy = new GroovyShell(config);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NjQ5MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429856490", "bodyText": "Wir setzen \u00fcber die Skripte Properties der Shell, dann w\u00fcrden die sich gegenseitig reingr\u00e4tschen.", "author": "awildturtok", "createdAt": "2020-05-25T10:16:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTAyOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705028", "bodyText": "Ich sehe hier das Upgrade zum BigMultiSelect irgendwie nicht", "author": "thoniTUB", "createdAt": "2020-05-18T15:20:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/concepts/filters/specific/AbstractSelectFilter.java", "diffHunk": "@@ -58,20 +60,22 @@\n \tpublic void configureFrontend(FEFilter f) throws ConceptConfigurationException {\n \t\tf.setTemplate(getTemplate());\n \t\tf.setType(filterType);\n-\t\t// TODO: 20.11.2019 Upgrade to BigMultiSelect if more than maximumSize values are found.\n-\t\tif (values != null) {\n-\t\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n-\t\t\t\tthrow new ConceptConfigurationException(getConnector(),\n-\t\t\t\t\tString.format(\"Too many possible values (%d of %d in filter %s).\", values.size(), maximumSize, this.getId()));\n-\t\t\t}\n-\t\t\tif(this.filterType != FEFilterType.BIG_MULTI_SELECT) {\n-\t\t\t\tf.setOptions(\n-\t\t\t\t\tvalues\n-\t\t\t\t\t\t.stream()\n-\t\t\t\t\t\t.map(v->new FEValue(getLabelFor(v), v))\n-\t\t\t\t\t\t.collect(Collectors.toList())\n-\t\t\t\t);\n-\t\t\t}\n+\n+\t\tif (values == null) {\n+\t\t\treturn;\n+\t\t}\n+\n+\t\tif (maximumSize != -1 && values.size() > maximumSize) {\n+\t\t\tlog.warn(\"Too many possible values ({} of {} in Filter[{}]). Upgrading to BigMultiSelect\", values.size(), maximumSize, getId());", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTk5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426705994", "bodyText": "Das war die Syntax die nicht funktioniert", "author": "thoniTUB", "createdAt": "2020-05-18T15:21:51Z", "path": "backend/src/main/java/com/bakdata/conquery/models/jobs/UpdateMatchingStats.java", "diffHunk": "@@ -78,7 +77,7 @@ public void execute() throws Exception {\n \t\t\t\t}\n \t\t\t}\n \t\t\tcatch (Exception e) {\n-\t\t\t\tlog.error(\"Failed to collect the matching stats for CBlock \" + cBlock.getId(), e);\n+\t\t\t\tlog.error(\"Failed to collect the matching stats for {}\", cBlock, e);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzYwMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426707602", "bodyText": "Nice", "author": "thoniTUB", "createdAt": "2020-05-18T15:24:01Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "diffHunk": "@@ -1,41 +1,50 @@\n package com.bakdata.conquery.models.preproc;\n \n-import com.bakdata.conquery.models.common.daterange.CDateRange;\n+import java.util.StringJoiner;\n+\n import com.bakdata.conquery.models.datasets.Table;\n import lombok.AllArgsConstructor;\n-import lombok.Builder;\n import lombok.Data;\n import lombok.NoArgsConstructor;\n+import lombok.extern.slf4j.Slf4j;\n \n-@Data @Builder @NoArgsConstructor @AllArgsConstructor\n+@Data @NoArgsConstructor @AllArgsConstructor @Slf4j\n public class PreprocessedHeader {\n-\tprivate int validityHash;\n \tprivate String name;\n \tprivate String table;\n+\tprivate String suffix;\n+\n \tprivate long rows;\n \tprivate long groups;\n-\tprivate CDateRange eventRange;\n \tprivate PPColumn primaryColumn;\n \tprivate PPColumn[] columns;\n-\tprivate String suffix;\n+\n+\tprivate int validityHash;\n+\n \n \t/**\n \t * Verify that the supplied table matches the preprocessed' data in shape.\n \t */\n-\tpublic boolean matches(Table table) {\n-\t\tif(!table.getPrimaryColumn().matches(getPrimaryColumn())) {\n-\t\t\treturn false;\n+\tpublic void assertMatch(Table table) {", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwODg1Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426708853", "bodyText": "Den Reader k\u00f6nntest du schon mal statisch vorbereiten mit Jackson.BINARY_MAPPER.readFor(PreprocessedHeader.class)", "author": "thoniTUB", "createdAt": "2020-05-18T15:25:48Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426713146", "bodyText": "Hier w\u00fcrde doch auch ein int reichen", "author": "thoniTUB", "createdAt": "2020-05-18T15:31:59Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426714717", "bodyText": "Du z\u00e4hlst hier keine Errors vom GroovyScript mit, nur die von den Outputs", "author": "thoniTUB", "createdAt": "2020-05-18T15:34:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,298 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import it.unimi.dsi.fastutil.objects.Object2IntMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag, String extension) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceAll(Pattern.quote(extension) + \"$\", String.format(\".%s%s\", tag, extension)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n+\n+\t\t// Gather exception classes to get better overview of what kind of errors are happening.\n+\t\tObject2IntMap<Class<? extends Throwable>> exceptions = new Object2IntArrayMap<>();\n+\t\texceptions.defaultReturnValue(0);\n+\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTg1NzYyMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r429857621", "bodyText": "Ja, weil die skripte nicht abst\u00fcrzen d\u00fcrfen, und wenn sie es doch tun den ganzen preprocess der datei killen sollen", "author": "awildturtok", "createdAt": "2020-05-25T10:19:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDcxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDMxOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724319", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMaxValue() > maxValue) {\n          \n          \n            \n            \t\t\tmaxValue = v.getMaxValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t  maxValue = Math.max(maxValue,v.getMaxValue());", "author": "thoniTUB", "createdAt": "2020-05-18T15:48:16Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDgxMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426724811", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(v.getMinValue() < minValue) {\n          \n          \n            \n            \t\t\tminValue = v.getMinValue();\n          \n          \n            \n            \t\t}\n          \n          \n            \n                minValue = Math.min(minValue,v.getMinValue());", "author": "thoniTUB", "createdAt": "2020-05-18T15:48:58Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +32,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}\n+\n+\t\tif (onlyClosed && v.isOpen()) {\n+\t\t\tonlyClosed = false;\n+\t\t}\n+\n \t\tif(v.getMaxValue() > maxValue) {\n \t\t\tmaxValue = v.getMaxValue();\n \t\t}\n+\n \t\tif(v.getMinValue() < minValue) {\n \t\t\tminValue = v.getMinValue();\n \t\t}", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjczMjQ0OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r426732448", "bodyText": "Ich bin mir nicht ganz sicher warum die Map jetzt synchronisiert ist, aber m\u00fcsstest du  dann hier nicht explizit synchronisieren? Du \u00e4nderst du die Referenz von string.", "author": "thoniTUB", "createdAt": "2020-05-18T15:59:12Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/string/StringParser.java", "diffHunk": "@@ -94,8 +94,8 @@ public Boolean transform(@NonNull Integer value) {\n \t\t//remove prefix and suffix\n \t\tif(!StringUtils.isEmpty(prefix) || !StringUtils.isEmpty(suffix)) {\n \t\t\tlog.debug(\"Reduced strings by the '{}' prefix and '{}' suffix\", prefix, suffix);\n-\t\t\tLinkedHashMap<String, Integer> oldStrings = strings;\n-\t\t\tstrings = new LinkedHashMap<>(oldStrings.size());\n+\t\t\tMap<String, Integer> oldStrings = strings;\n+\t\t\tstrings = Collections.synchronizedMap(new LinkedHashMap<>(oldStrings.size()));", "originalCommit": "5c5a11a46f77cf954df5040bdf01f3a34388a20c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d1a3baba9a4e77fa65b38030bd7aee14bb739564", "url": "https://github.com/bakdata/conquery/commit/d1a3baba9a4e77fa65b38030bd7aee14bb739564", "message": "removes duplicates and adapts tests", "committedDate": "2020-05-20T12:48:24Z", "type": "commit"}, {"oid": "cb24d9914900201ea1cc0958fd15bd53ba53d9a5", "url": "https://github.com/bakdata/conquery/commit/cb24d9914900201ea1cc0958fd15bd53ba53d9a5", "message": "fix: resolves external queries prior to execution", "committedDate": "2020-05-20T12:59:43Z", "type": "commit"}, {"oid": "9bbf3c4dba3eea008544134385bf254dd0b4b4c6", "url": "https://github.com/bakdata/conquery/commit/9bbf3c4dba3eea008544134385bf254dd0b4b4c6", "message": "Temporary revert \"remove unused eventrange\"\n\nThis reverts commit 27edc0503ae00e70a8e133c620cc4a92f77fb746.\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/preproc/PreprocessedHeader.java", "committedDate": "2020-05-20T14:25:43Z", "type": "commit"}, {"oid": "d72bb14732ee48ee034a070ce4e9871188402558", "url": "https://github.com/bakdata/conquery/commit/d72bb14732ee48ee034a070ce4e9871188402558", "message": "review changes", "committedDate": "2020-05-25T12:47:50Z", "type": "commit"}, {"oid": "d586a356f123f11c49c47ceb59d89a320922719a", "url": "https://github.com/bakdata/conquery/commit/d586a356f123f11c49c47ceb59d89a320922719a", "message": "Merge branch 'develop' into feature/clean-up-duplicate-test-methods", "committedDate": "2020-05-25T12:49:47Z", "type": "commit"}, {"oid": "f3b67a64738c3d513fbe19977b7431f9c1700830", "url": "https://github.com/bakdata/conquery/commit/f3b67a64738c3d513fbe19977b7431f9c1700830", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/execution/ManagedExecution.java\n#\tbackend/src/main/java/com/bakdata/conquery/resources/api/ResultCSVResource.java\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_AGE_SPAN/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE_ERSTER.test.json\n#\tbackend/src/test/resources/tests/query/DATE_DISTANCE_NEGATION/DATE_DISTANCE_LETZTER.test.json\n#\tbackend/src/test/resources/tests/query/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY/DURATION_SUM_EMPTY_DATE_CONCEPT_QUERY.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING_NO_RESTRICTION/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_MISSING_NO_RESTRICTION/NUMBER2.test.json\n#\tbackend/src/test/resources/tests/query/NUMBER_NEGATION/NUMBER.test.json\n#\tbackend/src/test/resources/tests/query/SUM_EMPTY_DATE_CONCEPT_QUERY/SUM_EMPTY_DATE_CONCEPT_QUERY.test.json\n#\tdocs/Config JSON.md", "committedDate": "2020-05-25T13:27:47Z", "type": "commit"}, {"oid": "4c7b2561cc629064508c72322ecea18302f53f6e", "url": "https://github.com/bakdata/conquery/commit/4c7b2561cc629064508c72322ecea18302f53f6e", "message": "Merge f3b67a64738c3d513fbe19977b7431f9c1700830 into 4b9eaa6c72030991fea3d667e185aa14896ae906", "committedDate": "2020-05-25T13:33:22Z", "type": "commit"}, {"oid": "4286eb9d22a1fd0a08d76de0fdb06b79cb9fedb1", "url": "https://github.com/bakdata/conquery/commit/4286eb9d22a1fd0a08d76de0fdb06b79cb9fedb1", "message": "automatic update to docs", "committedDate": "2020-05-25T13:35:12Z", "type": "commit"}, {"oid": "c681575242f11596cea919ee061dd2683aecd607", "url": "https://github.com/bakdata/conquery/commit/c681575242f11596cea919ee061dd2683aecd607", "message": "Fail Preprocessing when too many faulty lines", "committedDate": "2020-05-27T09:29:43Z", "type": "commit"}, {"oid": "cd509a27e028eb6397401ede75bd083eedf63cf4", "url": "https://github.com/bakdata/conquery/commit/cd509a27e028eb6397401ede75bd083eedf63cf4", "message": "Merge c681575242f11596cea919ee061dd2683aecd607 into ba996c3747ed3bf0e4854acde49a6ac81a366eb5", "committedDate": "2020-05-27T09:29:53Z", "type": "commit"}, {"oid": "fcb9796773f792166b38080374221725657ab7f1", "url": "https://github.com/bakdata/conquery/commit/fcb9796773f792166b38080374221725657ab7f1", "message": "automatic update to docs", "committedDate": "2020-05-27T09:31:54Z", "type": "commit"}, {"oid": "e830ad2ac457b6c5ace1604ec31f3b19b5e02bbd", "url": "https://github.com/bakdata/conquery/commit/e830ad2ac457b6c5ace1604ec31f3b19b5e02bbd", "message": "adds login for parser type", "committedDate": "2020-05-28T08:59:53Z", "type": "commit"}, {"oid": "a1e25840a474553edcd1b70b9de32a31a98692be", "url": "https://github.com/bakdata/conquery/commit/a1e25840a474553edcd1b70b9de32a31a98692be", "message": "remove to string method from Stringparser", "committedDate": "2020-05-28T09:11:29Z", "type": "commit"}, {"oid": "f1a25206648978ad900c6473f1f3f1b09a9c48a9", "url": "https://github.com/bakdata/conquery/commit/f1a25206648978ad900c6473f1f3f1b09a9c48a9", "message": "revert wrongly commited changes to FormConfigProcessor", "committedDate": "2020-05-28T09:18:15Z", "type": "commit"}, {"oid": "7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e", "url": "https://github.com/bakdata/conquery/commit/7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e", "message": "very aggressive tracing for values throughout Preprocessor", "committedDate": "2020-05-28T09:30:13Z", "type": "commit"}, {"oid": "d74da46abf66d1262bf497b8f7268cd05716a1ef", "url": "https://github.com/bakdata/conquery/commit/d74da46abf66d1262bf497b8f7268cd05716a1ef", "message": "Merge 7279d9902a7cd790b452aa9fb9cc3e1d0e2cd35e into ba996c3747ed3bf0e4854acde49a6ac81a366eb5", "committedDate": "2020-05-28T09:31:22Z", "type": "commit"}, {"oid": "4bb6e10e0a82057caf193ad7480590b8907d1f04", "url": "https://github.com/bakdata/conquery/commit/4bb6e10e0a82057caf193ad7480590b8907d1f04", "message": "automatic update to docs", "committedDate": "2020-05-28T09:34:02Z", "type": "commit"}, {"oid": "ff0cffa41bc8cbb3094e3c327755fa59b20d4769", "url": "https://github.com/bakdata/conquery/commit/ff0cffa41bc8cbb3094e3c327755fa59b20d4769", "message": "fix trying to apply logic when no max was present in DecimalParser::decideType", "committedDate": "2020-05-28T12:12:58Z", "type": "commit"}, {"oid": "ccecaf4fa615bfdac801fac25f9ffe2afdc3df74", "url": "https://github.com/bakdata/conquery/commit/ccecaf4fa615bfdac801fac25f9ffe2afdc3df74", "message": "Fix percentage formatting", "committedDate": "2020-05-28T12:13:16Z", "type": "commit"}, {"oid": "762619079483413c915ae0f0ca8139292615685d", "url": "https://github.com/bakdata/conquery/commit/762619079483413c915ae0f0ca8139292615685d", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-05-28T12:13:47Z", "type": "commit"}, {"oid": "77e41f9384297e943c872c66386cda3fc38e70e6", "url": "https://github.com/bakdata/conquery/commit/77e41f9384297e943c872c66386cda3fc38e70e6", "message": "Merge remote-tracking branch 'origin/feature/clean-up-duplicate-test-methods' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/test/java/com/bakdata/conquery/integration/common/IntegrationUtils.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/common/LoadingUtil.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/ConceptPermissionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ConceptUpdateAndDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/DatasetDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java\n#\tbackend/src/test/java/com/bakdata/conquery/integration/tests/deletion/TableDeletionTest.java", "committedDate": "2020-05-28T13:26:43Z", "type": "commit"}, {"oid": "6b35000ad99188054a8449c7b47b3e9d7d004ce0", "url": "https://github.com/bakdata/conquery/commit/6b35000ad99188054a8449c7b47b3e9d7d004ce0", "message": "make tests pass again: Fix headers/tables and fix *Descriptor usage", "committedDate": "2020-05-28T14:29:54Z", "type": "commit"}, {"oid": "41cdf2dbaae4c6250d45354929cb3551ed873477", "url": "https://github.com/bakdata/conquery/commit/41cdf2dbaae4c6250d45354929cb3551ed873477", "message": "Add better error/success logging", "committedDate": "2020-05-29T10:02:10Z", "type": "commit"}, {"oid": "46ec8ec578fd70849a0725d832e9aecde16cbd36", "url": "https://github.com/bakdata/conquery/commit/46ec8ec578fd70849a0725d832e9aecde16cbd36", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-05-29T10:07:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553000", "bodyText": "onlyQuarters = onlyQuarters && v.isSingleQuarter()", "author": "thoniTUB", "createdAt": "2020-06-03T13:08:56Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -30,12 +34,19 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n-\t\tif(!v.isSingleQuarter()) {\n+\t\t// test if value is already set to avoid expensive computation.\n+\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n \t\t\tonlyQuarters = false;\n \t\t}", "originalCommit": "46ec8ec578fd70849a0725d832e9aecde16cbd36", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1Mzk5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434553994", "bodyText": "Ich wei\u00df nicht ob das assignment hier overhead machen w\u00fcrde", "author": "thoniTUB", "createdAt": "2020-06-03T13:10:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU2MzU5Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r434563597", "bodyText": "branching ist teurer als assignment vermutlich", "author": "awildturtok", "createdAt": "2020-06-03T13:25:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDU1MzAwMA=="}], "type": "inlineReview"}, {"oid": "258986dd7b129bd64aa06d1b9edc653f383d9565", "url": "https://github.com/bakdata/conquery/commit/258986dd7b129bd64aa06d1b9edc653f383d9565", "message": "process multiple tags at once", "committedDate": "2020-06-11T11:05:26Z", "type": "commit"}, {"oid": "ab31e6a098ddd9e959ee5a73559d6820007c971b", "url": "https://github.com/bakdata/conquery/commit/ab31e6a098ddd9e959ee5a73559d6820007c971b", "message": "fix using char instead of string", "committedDate": "2020-06-11T11:23:08Z", "type": "commit"}, {"oid": "0fcd7802a66998e96fa7783a5b8c2bc8eeac68a8", "url": "https://github.com/bakdata/conquery/commit/0fcd7802a66998e96fa7783a5b8c2bc8eeac68a8", "message": "collect missing jobs separately", "committedDate": "2020-06-11T11:54:50Z", "type": "commit"}, {"oid": "d14c730811590fc98d11f52f411ffb9ec912de42", "url": "https://github.com/bakdata/conquery/commit/d14c730811590fc98d11f52f411ffb9ec912de42", "message": "rework logging to be less verbose by default", "committedDate": "2020-06-11T11:57:05Z", "type": "commit"}, {"oid": "2af20e77c97ef6da68a55ac435ad2f1c33ab04d7", "url": "https://github.com/bakdata/conquery/commit/2af20e77c97ef6da68a55ac435ad2f1c33ab04d7", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-16T12:05:09Z", "type": "commit"}, {"oid": "a3f153ca43a9461869afd9ca92a149df12193046", "url": "https://github.com/bakdata/conquery/commit/a3f153ca43a9461869afd9ca92a149df12193046", "message": "handle no tags as well", "committedDate": "2020-06-16T12:25:11Z", "type": "commit"}, {"oid": "648519966f13f1eb3441e5a6e27ed6e719fa730d", "url": "https://github.com/bakdata/conquery/commit/648519966f13f1eb3441e5a6e27ed6e719fa730d", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-06-16T12:25:48Z", "type": "commit"}, {"oid": "05a9da70f9349fa9f7f3ff232f3880c7c735e755", "url": "https://github.com/bakdata/conquery/commit/05a9da70f9349fa9f7f3ff232f3880c7c735e755", "message": "fixed not Preprocessing", "committedDate": "2020-06-16T13:35:45Z", "type": "commit"}, {"oid": "b93bb899bb7b43dbe52b783770468e0f6e33dab4", "url": "https://github.com/bakdata/conquery/commit/b93bb899bb7b43dbe52b783770468e0f6e33dab4", "message": "cleanup of code, simplifying branching", "committedDate": "2020-06-17T13:31:38Z", "type": "commit"}, {"oid": "16b001d7dc638113768c3151094eb81631e001c8", "url": "https://github.com/bakdata/conquery/commit/16b001d7dc638113768c3151094eb81631e001c8", "message": "cleanup of code, simplifying branching", "committedDate": "2020-06-17T13:32:11Z", "type": "commit"}, {"oid": "3b136800091730f996af907ed8803f7cb5af4757", "url": "https://github.com/bakdata/conquery/commit/3b136800091730f996af907ed8803f7cb5af4757", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "committedDate": "2020-06-17T13:32:25Z", "type": "commit"}, {"oid": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "url": "https://github.com/bakdata/conquery/commit/43af35c470e844bb2e08e9c44f149e6cee56be4c", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-17T13:57:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYyOTg1NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441629854", "bodyText": "Autoformat?", "author": "thoniTUB", "createdAt": "2020-06-17T15:21:04Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -56,7 +57,10 @@ public PreprocessorCommand(ExecutorService pool) {\n \tpublic void configure(Subparser subparser) {\n \t\tsuper.configure(subparser);\n \n-\t\tfinal ArgumentGroup group = subparser.addArgumentGroup(\"Preprocessing CLI Config\").description(\"Optional arguments to do a single import step by hand. Overrides json configuration.\");\n+\t\tfinal ArgumentGroup\n+\t\t\t\tgroup =", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNTAyNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441635027", "bodyText": "Ist das ! richtig? mein Brain macht gerade nicht mehr so richtig mit", "author": "thoniTUB", "createdAt": "2020-06-17T15:28:13Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636232", "bodyText": "Und hier h\u00e4tte ich ein !v.isOpen() erwartet", "author": "thoniTUB", "createdAt": "2020-06-17T15:29:59Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -35,21 +35,12 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \t@Override\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n-\t\tif(onlyQuarters && !v.isSingleQuarter()) {\n-\t\t\tonlyQuarters = false;\n-\t\t}\n \n-\t\tif (onlyClosed && v.isOpen()) {\n-\t\t\tonlyClosed = false;\n-\t\t}\n+\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n+\t\tonlyClosed = onlyClosed && v.isOpen();", "originalCommit": "43af35c470e844bb2e08e9c44f149e6cee56be4c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjcwNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r441636707", "bodyText": "Hast du einen Test hier f\u00fcr?", "author": "thoniTUB", "createdAt": "2020-06-17T15:30:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA0MjY3Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444042672", "bodyText": "Gute Idee!", "author": "awildturtok", "createdAt": "2020-06-23T08:12:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTYzNjIzMg=="}], "type": "inlineReview"}, {"oid": "b753a8030b5154d6f083a70fd3d930b923a58c39", "url": "https://github.com/bakdata/conquery/commit/b753a8030b5154d6f083a70fd3d930b923a58c39", "message": "Tests for DateRangeParser verifying handling of  open ranges and packing", "committedDate": "2020-06-23T14:00:57Z", "type": "commit"}, {"oid": "dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "url": "https://github.com/bakdata/conquery/commit/dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-06-23T14:02:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI2OTYxNg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r444269616", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \t\tif(!v.isAtLeast()) {\n          \n          \n            \n            \t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \n          \n          \n            \n            \t\tif(!v.isAtMost()) {\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}\n          \n          \n            \n            \t\tif(!anyOpen) {\n          \n          \n            \n            \t\t        maxValue = Math.max(maxValue, v.getMaxValue());\n          \n          \n            \n            \t\t\tminValue = Math.min(minValue, v.getMinValue());\n          \n          \n            \n            \t\t}", "author": "thoniTUB", "createdAt": "2020-06-23T14:30:28Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -36,11 +36,16 @@ protected CDateRange parseValue(@Nonnull String value) throws ParsingException {\n \tprotected void registerValue(CDateRange v) {\n \t\t// test if value is already set to avoid expensive computation.\n \n-\t\tonlyQuarters = onlyQuarters && !v.isSingleQuarter();\n-\t\tonlyClosed = onlyClosed && v.isOpen();\n+\t\tonlyQuarters = onlyQuarters && v.isSingleQuarter();\n+\t\tanyOpen = anyOpen || v.isOpen();\n \n-\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n-\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\tif(!v.isAtLeast()) {\n+\t\t\tmaxValue = Math.max(maxValue, v.getMaxValue());\n+\t\t}\n+\n+\t\tif(!v.isAtMost()) {\n+\t\t\tminValue = Math.min(minValue, v.getMinValue());\n+\t\t}", "originalCommit": "dffa8a30d2be61dfbc16002904a701b3dc33b8ef", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b3895c68c9b581e00eae1fdb24581acfe77c5228", "url": "https://github.com/bakdata/conquery/commit/b3895c68c9b581e00eae1fdb24581acfe77c5228", "message": "cleanup", "committedDate": "2020-06-23T14:34:03Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5MzU0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363193546", "bodyText": "K\u00f6nntest du java importe nach vor javax machen?", "author": "thoniTUB", "createdAt": "2020-01-06T08:15:30Z", "path": "autodoc/src/main/java/com/bakdata/conquery/Constants.java", "diffHunk": "@@ -16,6 +8,14 @@\n import javax.ws.rs.core.Context;\n import javax.ws.rs.core.Response;\n \n+import java.net.InetAddress;", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5NDgyOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363194829", "bodyText": "umbenennen Jobs->Descriptiors", "author": "thoniTUB", "createdAt": "2020-01-06T08:20:48Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,8 +111,8 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<ImportDescriptor> findPreprocessingJobs(Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363198412", "bodyText": "Muss die Klasse ein Threadlocal sein?\nKann man\nprivate final Set<DateTimeFormatter> formats = new HashSet<>();\nprivate DateTimeFormatter lastFormat;\t\tprivate DateTimeFormatter lastFormat;\n\nnicht auch static und threadsafe machen und dann ein Singleton machen?", "author": "thoniTUB", "createdAt": "2020-01-06T08:34:03Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/DateFormats.java", "diffHunk": "@@ -11,27 +11,19 @@\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n \n+import com.bakdata.conquery.models.config.ConqueryConfig;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n \n public class DateFormats {\n \n-\tprivate static ThreadLocal<DateFormats> INSTANCE;\n-\tprivate static String[] ADDITIONAL_FORMATS;\n+\tprivate static ThreadLocal<DateFormats> INSTANCE = ThreadLocal.withInitial(() -> new DateFormats(ConqueryConfig.getInstance().getAdditionalFormats()));", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzY5OTMwMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363699302", "bodyText": "Ich habs \u00fcberarbeitet. Das Threadlocal ist schon sinnvoll, dass es keine Contention darauf gibt. Synchronisierung darauf w\u00e4re nicht gut, weil das extra daf\u00fcr da ist schnell zu sein und nicht pr\u00e4zise", "author": "awildturtok", "createdAt": "2020-01-07T11:08:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5ODQxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzE5OTg5OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363199898", "bodyText": "Doku und im namen verdeutlichen, dass die Klasse auf eine Column abzielt.", "author": "thoniTUB", "createdAt": "2020-01-06T08:39:05Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/OutputDescription.java", "diffHunk": "@@ -0,0 +1,56 @@\n+package com.bakdata.conquery.models.preproc.outputs;\n+\n+import java.io.Serializable;\n+import java.util.InputMismatchException;\n+import java.util.StringJoiner;\n+\n+import com.bakdata.conquery.io.cps.CPSBase;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.bakdata.conquery.models.preproc.ColumnDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.bakdata.conquery.models.types.parser.Parser;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonTypeInfo;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.hibernate.validator.constraints.NotEmpty;\n+\n+@Data", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDM1OQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200359", "bodyText": "Klammern", "author": "thoniTUB", "createdAt": "2020-01-06T08:40:42Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -69,39 +68,49 @@ public boolean checkAutoOutput() {\n \t}\n \n \t@JsonIgnore\n-\t@ValidationMethod(message=\"The primary column must be of type STRING\")\n+\t@ValidationMethod(message = \"The primary column must be of type STRING\")\n \tpublic boolean isPrimaryString() {\n-\t\treturn primary.getResultType()==MajorTypeId.STRING;\n+\t\treturn primary.getResultType() == MajorTypeId.STRING;\n \t}\n-\t\n-\tpublic boolean filter(String[] row) {\n-\t\tif(filter == null) {\n-\t\t\treturn true;\n-\t\t}\n-\t\telse {\n-\t\t\tif(script==null) {\n-\t\t\t\ttry {\n-\t\t\t\t\tCompilerConfiguration config = new CompilerConfiguration();\n-\t\t\t\t\tconfig.addCompilationCustomizers(new ImportCustomizer().addImports(AUTO_IMPORTS));\n-\t\t\t\t\tconfig.setScriptBaseClass(GroovyPredicate.class.getName());\n-\t\t\t\t\tGroovyShell groovy = new GroovyShell(config);\n-\t\t\t\t\t\n-\t\t\t\t\tscript = (GroovyPredicate) groovy.parse(filter);\n-\t\t\t\t} catch(Exception|Error e) {\n-\t\t\t\t\tthrow new RuntimeException(\"Failed to compile filter '\" + filter + \"'\", e);\n-\t\t\t\t}\n+\n+\tpublic GroovyPredicate createFilter(String[] headers){\n+\t\tif(filter == null)", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIwMDY3MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363200671", "bodyText": "InputTableDescriptor?", "author": "thoniTUB", "createdAt": "2020-01-06T08:41:46Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Input.java", "diffHunk": "@@ -1,60 +1,59 @@\n package com.bakdata.conquery.models.preproc;\n \n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n import java.io.File;\n import java.io.Serializable;\n import java.time.LocalDate;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n \n-import javax.validation.Valid;\n-import javax.validation.constraints.NotNull;\n-\n-import org.codehaus.groovy.control.CompilerConfiguration;\n-import org.codehaus.groovy.control.customizers.ImportCustomizer;\n-\n import com.bakdata.conquery.models.common.Range;\n import com.bakdata.conquery.models.exceptions.validators.ExistingFile;\n import com.bakdata.conquery.models.preproc.outputs.AutoOutput;\n-import com.bakdata.conquery.models.preproc.outputs.Output;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n import com.bakdata.conquery.models.types.MajorTypeId;\n import com.fasterxml.jackson.annotation.JsonIgnore;\n-\n import groovy.lang.GroovyShell;\n import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n \n @Data\n public class Input implements Serializable {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDAzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210039", "bodyText": "Schau mal ob die Checks nicht schon von der CDateRange Klasse gemacht werden", "author": "thoniTUB", "createdAt": "2020-01-06T09:13:23Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -1,59 +1,58 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"EPOCH_DATE_RANGE\", base=Output.class)\n-public class EpochDateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"EPOCH_DATE_RANGE\", base = OutputDescription.class)\n+public class EpochDateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363210493", "bodyText": "Wie werden offene ranges dargestellt?", "author": "thoniTUB", "createdAt": "2020-01-06T09:14:45Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/DateRangeOutput.java", "diffHunk": "@@ -1,60 +1,60 @@\n package com.bakdata.conquery.models.preproc.outputs;\n \n-import java.time.LocalDate;\n-import java.util.Collections;\n-import java.util.List;\n+import javax.validation.constraints.NotNull;\n \n-import javax.validation.constraints.Min;\n+import java.time.LocalDate;\n \n import com.bakdata.conquery.io.cps.CPSType;\n import com.bakdata.conquery.models.common.daterange.CDateRange;\n import com.bakdata.conquery.models.exceptions.ParsingException;\n import com.bakdata.conquery.models.preproc.DateFormats;\n import com.bakdata.conquery.models.types.MajorTypeId;\n-import com.bakdata.conquery.models.types.parser.Parser;\n-\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n import lombok.Getter;\n import lombok.Setter;\n import lombok.extern.slf4j.Slf4j;\n \n-@Slf4j @Getter @Setter @CPSType(id=\"DATE_RANGE\", base=Output.class)\n-public class DateRangeOutput extends Output {\n-\t\n+@Slf4j\n+@Getter\n+@Setter\n+@CPSType(id = \"DATE_RANGE\", base = OutputDescription.class)\n+public class DateRangeOutput extends OutputDescription {\n+\n \tprivate static final long serialVersionUID = 1L;\n-\t\n-\t@Min(0)\n-\tprivate int startColumn = -1;\n-\t@Min(0)\n-\tprivate int endColumn = -1;\n-\t\n+\n+\t@NotNull\n+\tprivate String startColumn, endColumn;\n+\n \t@Override\n-\tpublic List<Object> createOutput(Parser<?> type, String[] row, int source, long sourceLine) throws ParsingException {\n-\t\tif(row[startColumn]==null) {\n-\t\t\tif(row[endColumn]==null) {\n+\tpublic Output createForHeaders(Object2IntArrayMap<String> headers) {\n+\t\tassertRequiredHeaders(headers, startColumn, endColumn);\n+\n+\t\tint startIndex = headers.getInt(startColumn);\n+\t\tint endIndex = headers.getInt(endColumn);\n+\n+\t\treturn (type, row, source, sourceLine) -> {\n+\t\t\tif (row[startIndex] == null && row[endIndex] == null) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Mzc3MTM1NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363771354", "bodyText": "bisher gar nicht", "author": "awildturtok", "createdAt": "2020-01-07T14:26:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzIxMDQ5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0MzEzMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363343132", "bodyText": "copyOutput von QueryTest.java wiederverwenden", "author": "thoniTUB", "createdAt": "2020-01-06T15:31:10Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -142,9 +142,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\t}\n \t}\n \n-\tprivate Output copyOutput(int columnPosition, RequiredColumn column) {\n+\tprivate OutputDescription copyOutput(RequiredColumn column) {", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDUzMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344530", "bodyText": "Werden die settings gebraucht?", "author": "thoniTUB", "createdAt": "2020-01-06T15:34:04Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -135,7 +135,7 @@ public void importTableContents(StandaloneSupport support, Collection<RequiredTa\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2MzM0NDcxMg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r363344712", "bodyText": "Again: Werden die settings gebraucht?", "author": "thoniTUB", "createdAt": "2020-01-06T15:34:28Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -106,7 +104,9 @@ private void importTableContents(StandaloneSupport support) throws IOException,\n \t\tformat.setLineSeparator(\"\\n\");\n \t\tsettings.setFormat(format);\n \t\tsettings.setHeaderExtractionEnabled(true);", "originalCommit": "1dffd8e1322ddbbd03794727bbf0e2a81e6d8479", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "39d30b1d83a2323daf59774326ed09b684420983", "url": "https://github.com/bakdata/conquery/commit/39d30b1d83a2323daf59774326ed09b684420983", "message": "header based imports", "committedDate": "2020-01-27T13:42:00Z", "type": "commit"}, {"oid": "77ffaa048d4bb250c1ecb9ed057c777d91adce4c", "url": "https://github.com/bakdata/conquery/commit/77ffaa048d4bb250c1ecb9ed057c777d91adce4c", "message": "forced line endings to linux style", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "3614694bfa2ba1986834ab0481189e0441fc4052", "url": "https://github.com/bakdata/conquery/commit/3614694bfa2ba1986834ab0481189e0441fc4052", "message": "fixed headers for CQEXTERNAL test.", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "a5df13441070cee39dab3ea5a45dfafe74a67a6a", "url": "https://github.com/bakdata/conquery/commit/a5df13441070cee39dab3ea5a45dfafe74a67a6a", "message": "code cleanup", "committedDate": "2020-01-27T13:42:02Z", "type": "commit"}, {"oid": "bfad8cb99ea1e75c4ee0f53efc766e802bf7f605", "url": "https://github.com/bakdata/conquery/commit/bfad8cb99ea1e75c4ee0f53efc766e802bf7f605", "message": "made script column name based", "committedDate": "2020-01-27T13:42:03Z", "type": "commit"}, {"oid": "ce47c5fa49093522d1d068383567178c5f85946d", "url": "https://github.com/bakdata/conquery/commit/ce47c5fa49093522d1d068383567178c5f85946d", "message": "cleanup and decoupling of code.", "committedDate": "2020-01-27T13:42:39Z", "type": "commit"}, {"oid": "ee461aebe00bada7a7a3d683b4b753152e74134e", "url": "https://github.com/bakdata/conquery/commit/ee461aebe00bada7a7a3d683b4b753152e74134e", "message": "refactoring away from Preprocessor class", "committedDate": "2020-01-27T13:42:41Z", "type": "commit"}, {"oid": "33cc55cceed5ed1e54e312e3152f14d12ed158cf", "url": "https://github.com/bakdata/conquery/commit/33cc55cceed5ed1e54e312e3152f14d12ed158cf", "message": "fixed creation of InputFile", "committedDate": "2020-01-27T13:42:42Z", "type": "commit"}, {"oid": "66c643afddfa54fa8ba2117e1121c4990ae7f2ea", "url": "https://github.com/bakdata/conquery/commit/66c643afddfa54fa8ba2117e1121c4990ae7f2ea", "message": "refactoring of Output into OutputDescription (as API-layer class) and OutputDescription.Output as transformation.", "committedDate": "2020-01-27T13:42:57Z", "type": "commit"}, {"oid": "cc6f08647e13d7544296207b54ae91eada8cf942", "url": "https://github.com/bakdata/conquery/commit/cc6f08647e13d7544296207b54ae91eada8cf942", "message": "null tests for groovy predicate filter", "committedDate": "2020-01-27T13:42:59Z", "type": "commit"}, {"oid": "5a350e05beecbc9a7684e87a4b0dcf9c0bffeeb3", "url": "https://github.com/bakdata/conquery/commit/5a350e05beecbc9a7684e87a4b0dcf9c0bffeeb3", "message": "refactoring away from static initializer of DebugMode.java", "committedDate": "2020-01-27T13:43:17Z", "type": "commit"}, {"oid": "22bad57274ccce24730bf08a9514d05b3b8802e8", "url": "https://github.com/bakdata/conquery/commit/22bad57274ccce24730bf08a9514d05b3b8802e8", "message": "remove ExecutorService from Preprocessor and undo importing by upload", "committedDate": "2020-01-27T13:43:19Z", "type": "commit"}, {"oid": "ee9231e573f9ca58a2ca392403e3ab4476309d1e", "url": "https://github.com/bakdata/conquery/commit/ee9231e573f9ca58a2ca392403e3ab4476309d1e", "message": "automatic update to docs", "committedDate": "2020-01-27T13:43:19Z", "type": "commit"}, {"oid": "37863080692e07c0998b6f75187d9feb690aae0a", "url": "https://github.com/bakdata/conquery/commit/37863080692e07c0998b6f75187d9feb690aae0a", "message": "removed outputting of multiple lines as that is no longer used", "committedDate": "2020-01-27T13:43:20Z", "type": "commit"}, {"oid": "4252e36a083bbf94b7220574a93beff0312e0807", "url": "https://github.com/bakdata/conquery/commit/4252e36a083bbf94b7220574a93beff0312e0807", "message": "refactored DateFormats to be more atomic and concise", "committedDate": "2020-01-27T13:43:20Z", "type": "commit"}, {"oid": "f3e8bb7f79ac831afc37ad1501dc96128ab04984", "url": "https://github.com/bakdata/conquery/commit/f3e8bb7f79ac831afc37ad1501dc96128ab04984", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:24Z", "type": "commit"}, {"oid": "ff667a8e156aca392d61a921c3722167000de11c", "url": "https://github.com/bakdata/conquery/commit/ff667a8e156aca392d61a921c3722167000de11c", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:31Z", "type": "commit"}, {"oid": "069459f3e601cb7449b2840bec3ee7bac022ef70", "url": "https://github.com/bakdata/conquery/commit/069459f3e601cb7449b2840bec3ee7bac022ef70", "message": "more documentation for Preprocessing", "committedDate": "2020-01-27T13:44:31Z", "type": "commit"}, {"oid": "bc97453e6b3ac9f8fc6e4585aef4b69a581487d6", "url": "https://github.com/bakdata/conquery/commit/bc97453e6b3ac9f8fc6e4585aef4b69a581487d6", "message": "automatic update to docs", "committedDate": "2020-01-27T13:44:32Z", "type": "commit"}, {"oid": "401473392cd2052325606e77cb79bf44698c4c8d", "url": "https://github.com/bakdata/conquery/commit/401473392cd2052325606e77cb79bf44698c4c8d", "message": "removed unused settings", "committedDate": "2020-01-27T13:45:05Z", "type": "commit"}, {"oid": "d33c8227974e89420ad64e24c0a67e03a63a9b65", "url": "https://github.com/bakdata/conquery/commit/d33c8227974e89420ad64e24c0a67e03a63a9b65", "message": "automatic update to docs", "committedDate": "2020-01-27T13:45:08Z", "type": "commit"}, {"oid": "d4d973c49db79813d39f471ac9956e1d26460b9f", "url": "https://github.com/bakdata/conquery/commit/d4d973c49db79813d39f471ac9956e1d26460b9f", "message": "post rebase fixes", "committedDate": "2020-01-27T13:56:46Z", "type": "commit"}, {"oid": "d4d973c49db79813d39f471ac9956e1d26460b9f", "url": "https://github.com/bakdata/conquery/commit/d4d973c49db79813d39f471ac9956e1d26460b9f", "message": "post rebase fixes", "committedDate": "2020-01-27T13:56:46Z", "type": "forcePushed"}, {"oid": "67d14fd4cd27b777a5f42fa03e02eda5b1343d93", "url": "https://github.com/bakdata/conquery/commit/67d14fd4cd27b777a5f42fa03e02eda5b1343d93", "message": "dont track xodus files", "committedDate": "2020-01-27T17:01:33Z", "type": "commit"}, {"oid": "302066ec07be14a40eb16ad5eef5e7c4080f2aaa", "url": "https://github.com/bakdata/conquery/commit/302066ec07be14a40eb16ad5eef5e7c4080f2aaa", "message": "added removed required field", "committedDate": "2020-01-29T12:01:34Z", "type": "commit"}, {"oid": "336059e079bb7b9e808e9921b05b64644778024c", "url": "https://github.com/bakdata/conquery/commit/336059e079bb7b9e808e9921b05b64644778024c", "message": "handle required", "committedDate": "2020-01-29T13:52:53Z", "type": "commit"}, {"oid": "b60eb068989746776e1ef3d75290fd002c4a8c92", "url": "https://github.com/bakdata/conquery/commit/b60eb068989746776e1ef3d75290fd002c4a8c92", "message": "properly closing the CSV-Parser and better defaults", "committedDate": "2020-02-11T08:51:44Z", "type": "commit"}, {"oid": "4f5830654dac0305e11c7d769092be2cc3bb85db", "url": "https://github.com/bakdata/conquery/commit/4f5830654dac0305e11c7d769092be2cc3bb85db", "message": "Merge remote-tracking branch 'remotes/origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/apiv1/ResultCSVResource.java", "committedDate": "2020-02-11T08:53:23Z", "type": "commit"}, {"oid": "a4232cc294da2ae97b7b720c10003ffc1a6b7869", "url": "https://github.com/bakdata/conquery/commit/a4232cc294da2ae97b7b720c10003ffc1a6b7869", "message": "fixed constructors", "committedDate": "2020-02-11T09:12:21Z", "type": "commit"}, {"oid": "f182da3a6e35b983113639586c9417ec9a76d998", "url": "https://github.com/bakdata/conquery/commit/f182da3a6e35b983113639586c9417ec9a76d998", "message": "fixed name of column to inputColumn", "committedDate": "2020-02-11T10:57:19Z", "type": "commit"}, {"oid": "787c534a006b94e228e5a7db2a04f2a37880bea2", "url": "https://github.com/bakdata/conquery/commit/787c534a006b94e228e5a7db2a04f2a37880bea2", "message": "improved error logging for Preprocessor", "committedDate": "2020-02-24T15:53:20Z", "type": "commit"}, {"oid": "1463c3847447b8e9add88065548c0d4abfa28496", "url": "https://github.com/bakdata/conquery/commit/1463c3847447b8e9add88065548c0d4abfa28496", "message": "Added ToString annotations", "committedDate": "2020-02-24T16:12:56Z", "type": "commit"}, {"oid": "fca7139ec5deaefcfd527bd8f9fa389a6776fd5c", "url": "https://github.com/bakdata/conquery/commit/fca7139ec5deaefcfd527bd8f9fa389a6776fd5c", "message": "force error output", "committedDate": "2020-02-24T16:20:36Z", "type": "commit"}, {"oid": "9aae94c4ac29e0d43b8a5010522126181081793e", "url": "https://github.com/bakdata/conquery/commit/9aae94c4ac29e0d43b8a5010522126181081793e", "message": "fixed not giving out proper source", "committedDate": "2020-02-24T16:25:23Z", "type": "commit"}, {"oid": "54046deecbf1eccd4181a6adf726f5578422fbc2", "url": "https://github.com/bakdata/conquery/commit/54046deecbf1eccd4181a6adf726f5578422fbc2", "message": "allow open EpochDateRangeOutput", "committedDate": "2020-03-02T12:24:43Z", "type": "commit"}, {"oid": "d9994993f932a9aa1bb151813afd353989dda3e5", "url": "https://github.com/bakdata/conquery/commit/d9994993f932a9aa1bb151813afd353989dda3e5", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\t.gitignore\n#\tbackend/src/main/java/com/bakdata/conquery/models/config/ConqueryConfig.java\n#\tbackend/src/main/java/com/bakdata/conquery/models/query/concept/specific/CQExternal.java", "committedDate": "2020-03-02T12:33:57Z", "type": "commit"}, {"oid": "fa5d42a320ddda0b973ffafe5373cbfb298f4a8d", "url": "https://github.com/bakdata/conquery/commit/fa5d42a320ddda0b973ffafe5373cbfb298f4a8d", "message": "fixed merge", "committedDate": "2020-03-02T12:41:36Z", "type": "commit"}, {"oid": "4c27ba7933fec89b8c7910bdadbdf3ddd4215582", "url": "https://github.com/bakdata/conquery/commit/4c27ba7933fec89b8c7910bdadbdf3ddd4215582", "message": "added more info when failing", "committedDate": "2020-03-02T13:55:26Z", "type": "commit"}, {"oid": "079dd8d74aa8575c2b76ad3ac14ac524c3db8256", "url": "https://github.com/bakdata/conquery/commit/079dd8d74aa8575c2b76ad3ac14ac524c3db8256", "message": "delete now unused DateRangeOutput.java", "committedDate": "2020-03-02T14:01:41Z", "type": "commit"}, {"oid": "1aa304a6ef8a18e01af0cd0a1d88ec315a73b1a9", "url": "https://github.com/bakdata/conquery/commit/1aa304a6ef8a18e01af0cd0a1d88ec315a73b1a9", "message": "introduction of tags for imports as CLI param", "committedDate": "2020-03-02T15:19:43Z", "type": "commit"}, {"oid": "78aeb4800307d9bf533b831631c428d1bc05103d", "url": "https://github.com/bakdata/conquery/commit/78aeb4800307d9bf533b831631c428d1bc05103d", "message": "Merge remote-tracking branch 'origin/develop' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/config/ConqueryConfig.java", "committedDate": "2020-03-02T15:39:34Z", "type": "commit"}, {"oid": "6f924a958583af2253f6bd34602563139eb0443d", "url": "https://github.com/bakdata/conquery/commit/6f924a958583af2253f6bd34602563139eb0443d", "message": "Merge 78aeb4800307d9bf533b831631c428d1bc05103d into 242513d2a0a3f57200b7c97de7ea35866b6ae12f", "committedDate": "2020-03-02T15:39:50Z", "type": "commit"}, {"oid": "47260d5177b4a83f0461c3af23ad3ecdfb8c6011", "url": "https://github.com/bakdata/conquery/commit/47260d5177b4a83f0461c3af23ad3ecdfb8c6011", "message": "automatic update to docs", "committedDate": "2020-03-02T15:41:52Z", "type": "commit"}, {"oid": "56dc230c243df657a0273d83ecdb4f2bd6d12eda", "url": "https://github.com/bakdata/conquery/commit/56dc230c243df657a0273d83ecdb4f2bd6d12eda", "message": "changed mdc to contain csv file name instead of index.", "committedDate": "2020-03-05T14:53:14Z", "type": "commit"}, {"oid": "d1dcd98f15e3eec4e8198be7e28479667cf561f9", "url": "https://github.com/bakdata/conquery/commit/d1dcd98f15e3eec4e8198be7e28479667cf561f9", "message": "changed mdc to contain csv file name instead of index.", "committedDate": "2020-03-05T14:53:42Z", "type": "commit"}, {"oid": "1745a5e36cbd3cdb4b832c72ebae9d06c7fa3b62", "url": "https://github.com/bakdata/conquery/commit/1745a5e36cbd3cdb4b832c72ebae9d06c7fa3b62", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name\n\n# Conflicts:\n#\tbackend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "committedDate": "2020-03-05T14:54:35Z", "type": "commit"}, {"oid": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "url": "https://github.com/bakdata/conquery/commit/a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "message": "only file name not full path", "committedDate": "2020-03-05T16:53:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394912673", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen.\nOder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.", "author": "thoniTUB", "createdAt": "2020-03-19T10:03:43Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;\n \t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description, tag);\n \t\t\t\ttry {\n-\t\t\t\t\tImportDescriptor descr = file.readDescriptor(validator);\n+\t\t\t\t\tTableImportDescriptor descr = file.readDescriptor(validator, tag);\n \t\t\t\t\tdescr.setInputFile(file);\n-\t\t\t\t\tl.add(new Preprocessor(config, descr));\n-\t\t\t\t} catch (Exception e) {\n+\n+\t\t\t\t\t// Override name to tag if present\n+\t\t\t\t\tif (!Strings.isNullOrEmpty(tag)) {\n+\t\t\t\t\t\tdescr.setName(tag);\n+\t\t\t\t\t}\n+\n+\t\t\t\t\tout.add(descr);\n+\t\t\t\t}\n+\t\t\t\tcatch (Exception e) {\n \t\t\t\t\tlog.error(\"Failed to process \" + LogUtil.printPath(descriptionFile), e);", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMDk1MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395000950", "bodyText": "Oder auch ein fail-on-error Flag das man setzten kann. Dann k\u00f6nnte man in der CI auf auf das grep verzichten.\n\nGef\u00e4llt mir!", "author": "awildturtok", "createdAt": "2020-03-19T12:50:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMTUwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395001500", "bodyText": "Es w\u00e4re ganz nice am ende des Preprocess noch mal eine Zusammenfassung der Fehler zu bekommen nach categorie, da die log sehr lang sein k\u00f6nnen\n\nFrage ist wie man das kommuniziert, wir kriegen ja selber auch nicht so die guten Exceptions. Ich k\u00f6nnte eine Map machen, die einfach z\u00e4hlt wie of welche Exception kam und dann das loggen?", "author": "awildturtok", "createdAt": "2020-03-19T12:51:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxMjY3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394914288", "bodyText": "Dieses stille \u00dcberspringen kann zu sneaky Fehlern f\u00fchren.", "author": "thoniTUB", "createdAt": "2020-03-19T10:06:29Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,34 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n+\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();\n+\n+\t\t\tfor (File descriptionFile : inDir.listFiles()) {\n \t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n \t\t\t\t\tcontinue;", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMjQzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395002439", "bodyText": "Ja, das Problem ist so ein bisschen wie wir weiter gehen wollen. An und f\u00fcr sich ist das okay, weil das einfach ein contract ist. Aber ich w\u00fcnsche mir f\u00fcr diese stelle auch sowas wie globbing (wie oben angemerkt) dann k\u00f6nnen wir n\u00e4mlich in zukunft deutlich selektiver preprocessen ohne immer den ganzen stand ordentlich vorhalten zu m\u00fcssen etc. Bin mir noch nicht sicher ob das ne gute idee ist aber sieht man dann ja.", "author": "awildturtok", "createdAt": "2020-03-19T12:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDkxNDI4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394969993", "bodyText": "Das ist etwas gef\u00e4hrlich davon auszugehen, dass nicht schon vorher punkte drinne sind", "author": "thoniTUB", "createdAt": "2020-03-19T11:50:40Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTAwMzU0NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r395003544", "bodyText": "hm, was w\u00e4re dein vorschlag? Eigentlich sollte es das vorletzte sein\nalso table.tag.csv.gz oder table.dead.tag.csv.gz aber nicht so einfach.", "author": "awildturtok", "createdAt": "2020-03-19T12:55:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk2OTk5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3MzkyNg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394973926", "bodyText": "Macht ein Builder hier Sinn? Funktioniert das auch wenn man einen Member nicht setzen w\u00fcrde?", "author": "thoniTUB", "createdAt": "2020-03-19T11:58:45Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {\n+\t\t// Write content to file\n+\t\tImport imp = Import.createForPreprocessing(descriptor.getTable(), descriptor.getName(), columns);\n+\n+\t\ttry (Output out = new Output(outFile.writeContent())) {\n+\t\t\tfor(int entityId = 0; entityId < entries.size(); entityId++) {\n+\t\t\t\tList<Object[]> events = (List<Object[]>) entries.get(entityId);\n+\n+\t\t\t\tif(!events.isEmpty()) {\n+\t\t\t\t\twriteRowsToFile(out, imp, entityId, events);\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Then write headers.\n+\t\ttry (OutputStream out = outFile.writeHeader()) {\n+\t\t\tint hash = descriptor.calculateValidityHash();\n+\n+\t\t\tPreprocessedHeader header = PreprocessedHeader.builder()", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3ODYzOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394978639", "bodyText": "Wird das noch gebraucht?", "author": "thoniTUB", "createdAt": "2020-03-19T12:08:26Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessor.java", "diffHunk": "@@ -33,266 +29,278 @@\n import com.bakdata.conquery.util.io.ConqueryMDC;\n import com.bakdata.conquery.util.io.LogUtil;\n import com.bakdata.conquery.util.io.ProgressBar;\n+import com.google.common.base.Strings;\n import com.google.common.io.CountingInputStream;\n import com.jakewharton.byteunits.BinaryByteUnit;\n import com.univocity.parsers.csv.CsvParser;\n-import lombok.Getter;\n-import lombok.RequiredArgsConstructor;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.experimental.UtilityClass;\n import lombok.extern.slf4j.Slf4j;\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.lang3.ArrayUtils;\n \n @Slf4j\n-@RequiredArgsConstructor\n-@Getter\n+@UtilityClass\n public class Preprocessor {\n \n-\tprivate final ConqueryConfig config;\n-\tprivate final ImportDescriptor descriptor;\n-\tprivate final AtomicLong errorCounter = new AtomicLong(0L);\n-\tprivate long totalCsvSize;\n+\tpublic static long getTotalCsvSize(TableImportDescriptor descriptor) {\n+\t\tlong totalCsvSize = 0;\n+\t\tfor (TableInputDescriptor input : descriptor.getInputs()) {\n+\t\t\ttotalCsvSize += input.getSourceFile().length();\n+\t\t}\n+\n+\t\treturn totalCsvSize;\n+\t}\n \n-\tpublic boolean requiresProcessing() {\n+\tpublic static boolean requiresProcessing(TableImportDescriptor descriptor) {\n \t\tConqueryMDC.setLocation(descriptor.toString());\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\t\tif (descriptor.getInputFile().getPreprocessedFile().exists()) {\n+\n \t\t\tlog.info(\"EXISTS ALREADY\");\n+\n \t\t\tint currentHash = descriptor.calculateValidityHash();\n-\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false)) {\n-\t\t\t\ttry (InputStream is = outFile.readHeader()) {\n-\t\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n-\t\t\t\t\tif(header.getValidityHash()==currentHash) {\n-\t\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n-\t\t\t\t\t\treturn false;\n-\t\t\t\t\t}\n-\t\t\t\t\telse {\n-\t\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n-\t\t\t\t\t}\n+\n+\t\t\ttry (HCFile outFile = new HCFile(descriptor.getInputFile().getPreprocessedFile(), false);\n+\t\t\t\t InputStream is = outFile.readHeader()) {\n+\n+\t\t\t\tPreprocessedHeader header = Jackson.BINARY_MAPPER.readValue(is, PreprocessedHeader.class);\n+\n+\t\t\t\tif (header.getValidityHash() == currentHash) {\n+\t\t\t\t\tlog.info(\"\\tHASH STILL VALID\");\n+\t\t\t\t\treturn false;\n \t\t\t\t}\n-\t\t\t}\n-\t\t\tcatch(Exception e) {\n-\t\t\t\tlog.warn(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\telse {\n+\t\t\t\t\tlog.info(\"\\tHASH OUTDATED\");\n+\t\t\t\t}\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlog.error(\"\\tHEADER READING FAILED\", e);\n+\t\t\t\treturn false;\n \t\t\t}\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"DOES NOT EXIST\");\n \t\t}\n-\t\t\n-\t\tfor(Input input : descriptor.getInputs()) {\n-\t\t\ttotalCsvSize += input.getSourceFile().length();\n-\t\t}\n-\t\t\n+\n \t\treturn true;\n \t}\n-\t\n-\tpublic void preprocess(ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n-\t\tConqueryMDC.setLocation(descriptor.toString());\n+\n+\t/**\n+\t * Create version of file-name with tag.\n+\t */\n+\tpublic static File getTaggedVersion(File file, String tag) {\n+\t\tif(Strings.isNullOrEmpty(tag)) {\n+\t\t\treturn file;\n+\t\t}\n+\n+\t\treturn new File(file.getParentFile(), file.getName().replaceFirst(\"\\\\.\", String.format(\".%s.\", tag)));\n+\t}\n+\n+\n+\t/**\n+\t * Apply transformations in descriptor, then write them out to CQPP file for imports.\n+\t *\n+\t * Reads CSV file, per row extracts the primary key, then applies other transformations on each row, then compresses the data with {@link CType}.\n+\t */\n+\tpublic static void preprocess(TableImportDescriptor descriptor, ProgressBar totalProgress) throws IOException, JSONException, ParsingException {\n+\n \n \t\t//create temporary folders and check for correct permissions\n-\t\tFile tmp = ConqueryFileUtil.createTempFile(descriptor.getInputFile().getPreprocessedFile().getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n-\t\tif(!Files.isWritable(tmp.getParentFile().toPath())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(tmp.getParentFile()));\n+\t\tfinal File preprocessedFile = descriptor.getInputFile().getPreprocessedFile();\n+\t\tFile tmp = ConqueryFileUtil.createTempFile(preprocessedFile.getName(), ConqueryConstants.EXTENSION_PREPROCESSED.substring(1));\n+\n+\t\tif (!Files.isWritable(tmp.getParentFile().toPath())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(tmp.getParentFile()));\n \t\t}\n-\t\tif(!Files.isWritable(descriptor.getInputFile().getPreprocessedFile().toPath().getParent())) {\n-\t\t\tthrow new IllegalArgumentException(\"No write permission in \"+LogUtil.printPath(descriptor.getInputFile().getPreprocessedFile().toPath().getParent()));\n+\n+\t\tif (!Files.isWritable(preprocessedFile.toPath().getParent())) {\n+\t\t\tthrow new IllegalArgumentException(\"No write permission in \" + LogUtil.printPath(preprocessedFile\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .toPath()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   .getParent()));\n \t\t}\n+\n \t\t//delete target file if it exists\n-\t\tif(descriptor.getInputFile().getPreprocessedFile().exists()) {\n-\t\t\tFileUtils.forceDelete(descriptor.getInputFile().getPreprocessedFile());\n+\t\tif (preprocessedFile.exists()) {\n+\t\t\tFileUtils.forceDelete(preprocessedFile);\n \t\t}\n \n-\n \t\tlog.info(\"PREPROCESSING START in {}\", descriptor.getInputFile().getDescriptionFile());\n-\t\tPreprocessed result = new Preprocessed(config.getPreprocessor(), descriptor);\n-\t\tlong lineId = config.getCsv().isSkipHeader()?1:0;\n+\n+\t\tfinal AtomicLong errorCounter = new AtomicLong(0);\n+\n+\t\tfinal Preprocessed result = new Preprocessed(descriptor);\n+\n+\t\tlong lineId = 0;\n \n \t\ttry (HCFile outFile = new HCFile(tmp, true)) {\n-\t\t\tfor(int inputSource=0;inputSource<descriptor.getInputs().length;inputSource++) {\n-\t\t\t\tInput input = descriptor.getInputs()[inputSource];\n-\t\t\t\tfinal String name = descriptor.toString()+\":\"+descriptor.getTable()+\"[\"+inputSource+\"]\";\n+\t\t\tfor (int inputSource = 0; inputSource < descriptor.getInputs().length; inputSource++) {\n+\t\t\t\tfinal TableInputDescriptor input = descriptor.getInputs()[inputSource];\n+\t\t\t\tfinal File sourceFile = input.getSourceFile();\n+\n+\t\t\t\tfinal String name = String.format(\"%s:%s[%d/%s]\", descriptor.toString(), descriptor.getTable(), inputSource, sourceFile.getName());\n \t\t\t\tConqueryMDC.setLocation(name);\n \n-\t\t\t\ttry(CountingInputStream countingIn = new CountingInputStream(new FileInputStream(input.getSourceFile()))) {\n+\t\t\t\tCsvParser parser = null;\n+\n+\n+\t\t\t\ttry (CountingInputStream countingIn = new CountingInputStream(new FileInputStream(sourceFile))) {\n \t\t\t\t\tlong progress = 0;\n \n-\t\t\t\t\tfinal CsvParser parser = CsvIo.createParser();\n-\t\t\t\t\tfinal Iterator<String[]> it = new PrefetchingIterator<>(\n-\t\t\t\t\t\t\tparser.iterate(CsvIo.isGZipped(input.getSourceFile()) ? new GZIPInputStream(countingIn) : countingIn).iterator(),\n-\t\t\t\t\t\t\t1_000\n-\t\t\t\t\t);\n+\t\t\t\t\t// Create CSV parser according to config, but overriding some behaviour.\n+\t\t\t\t\tparser = new CsvParser(ConqueryConfig.getInstance().getCsv().withParseHeaders(true).withSkipHeader(false).createCsvParserSettings());\n \n-\t\t\t\t\twhile (it.hasNext()) {\n+\t\t\t\t\tparser.beginParsing(CsvIo.isGZipped(sourceFile) ? new GZIPInputStream(countingIn) : countingIn);\n \n-\t\t\t\t\t\tString[] row = it.next();\n+\t\t\t\t\tfinal String[] headers = parser.getContext().parsedHeaders();\n+\n+\t\t\t\t\tfinal Object2IntArrayMap<String> headerMap = TableInputDescriptor.buildHeaderMap(headers);\n+\n+\t\t\t\t\t// Compile filter.\n+\t\t\t\t\tfinal GroovyPredicate filter = input.createFilter(headers);\n \n-\t\t\t\t\t\tInteger primary = getPrimary((StringParser) result.getPrimaryColumn().getParser(), row, lineId, inputSource, input.getPrimary());\n-\t\t\t\t\t\tif (primary != null) {\n-\t\t\t\t\t\t\tint primaryId = result.addPrimary(primary);\n-\t\t\t\t\t\t\tparseRow(primaryId, result.getColumns(), row, input, lineId, result, inputSource);\n-\t\t\t\t\t\t}\n \n-\t\t\t\t\t\t//report progress\n-\t\t\t\t\t\ttotalProgress.addCurrentValue(countingIn.getCount() - progress);\n-\t\t\t\t\t\tprogress = countingIn.getCount();\n-\t\t\t\t\t\tlineId++;\n+\t\t\t\t\tfinal OutputDescription.Output primaryOut = input.getPrimary().createForHeaders(headerMap);\n+\t\t\t\t\tfinal List<OutputDescription.Output> outputs = new ArrayList<>();\n+\n+\t\t\t\t\t// Instantiate Outputs based on descriptors (apply header positions)\n+\t\t\t\t\tfor (OutputDescription op : input.getOutput()) {\n+\t\t\t\t\t\toutputs.add(op.createForHeaders(headerMap));\n \t\t\t\t\t}\n \n-\t\t\t\t\tif (input.checkAutoOutput()) {\n-\t\t\t\t\t\tList<AutoOutput.OutRow> outRows = input.getAutoOutput().finish();\n-\t\t\t\t\t\tfor (AutoOutput.OutRow outRow : outRows) {\n-\t\t\t\t\t\t\tresult.addRow(outRow.getPrimaryId(), outRow.getTypes(), outRow.getData());\n+\n+\t\t\t\t\tString[] row;\n+\n+\t\t\t\t\t// Read all CSV lines, apply Output transformations and add the to preprocessed.\n+\t\t\t\t\twhile ((row = parser.parseNext()) != null) {\n+\t\t\t\t\t\ttry {\n+\n+\t\t\t\t\t\t\tint primaryId = (int) Objects.requireNonNull(primaryOut.createOutput(row, result.getPrimaryColumn().getParser(), lineId), \"primaryId may not be null\");\n+\n+\t\t\t\t\t\t\tif (filter != null && !filter.filterRow(row)) {\n+\t\t\t\t\t\t\t\tcontinue;\n+\t\t\t\t\t\t\t}\n+\n+\t\t\t\t\t\t\tfinal int primary = result.addPrimary(primaryId);\n+\t\t\t\t\t\t\tfinal PPColumn[] columns = result.getColumns();\n+\n+\t\t\t\t\t\t\tresult.addRow(primary, columns, applyOutputs(outputs, columns, row, lineId));\n+\n \t\t\t\t\t\t}\n+\t\t\t\t\t\tcatch (OutputDescription.OutputException e) {\n+\n+\t\t\t\t\t\t\tlong errors = errorCounter.getAndIncrement();\n+\n+\t\t\t\t\t\t\tlog.warn(\"Failed to parse `{}` from line: {} content: {}. Errors={}\", e.getSource().getDescription(), lineId, Arrays.toString(row), errors, e.getCause());\n+\n+//\n+//\t\t\t\t\t\t\tif (log.isTraceEnabled() || errors < ConqueryConfig.getInstance().getPreprocessor().getMaximumPrintedErrors()) {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk3OTY5NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394979695", "bodyText": "Cool!", "author": "thoniTUB", "createdAt": "2020-03-19T12:10:37Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/TableInputDescriptor.java", "diffHunk": "@@ -0,0 +1,138 @@\n+package com.bakdata.conquery.models.preproc;\n+\n+import java.io.File;\n+import java.io.Serializable;\n+import java.time.LocalDate;\n+import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n+\n+import javax.validation.Valid;\n+import javax.validation.constraints.NotNull;\n+\n+import com.bakdata.conquery.models.common.Range;\n+import com.bakdata.conquery.models.preproc.outputs.CopyOutput;\n+import com.bakdata.conquery.models.preproc.outputs.OutputDescription;\n+import com.bakdata.conquery.models.types.MajorTypeId;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import groovy.lang.GroovyShell;\n+import io.dropwizard.validation.ValidationMethod;\n+import it.unimi.dsi.fastutil.objects.Object2IntArrayMap;\n+import lombok.Data;\n+import org.codehaus.groovy.control.CompilerConfiguration;\n+import org.codehaus.groovy.control.customizers.ImportCustomizer;\n+\n+/**\n+ * An input describes transformations on a single CSV file to be loaded into the table described in {@link TableImportDescriptor}.\n+ *\n+ * It requires a primary Output and at least one normal output.\n+ *\n+ * Input data can be filter using the field filter, which is evaluated as a groovy script on every row.\n+ */\n+@Data\n+public class TableInputDescriptor implements Serializable {\n+\n+\tprivate static final long serialVersionUID = 1L;\n+\tprivate static final String[] AUTO_IMPORTS = Stream.of(\n+\t\t\tLocalDate.class,\n+\t\t\tRange.class\n+\t).map(Class::getName).toArray(String[]::new);\n+\n+\t@NotNull\n+\tprivate File sourceFile;\n+\n+\tprivate String filter;\n+\n+\t/**\n+\t * Output producing the primary column. This should be the primary key across all tables.\n+\t * Default is `COPY(\"pid\", STRING)`\n+\t */\n+\t@NotNull\n+\t@Valid\n+\tprivate OutputDescription primary = new CopyOutput(\"pid\", \"pid\", MajorTypeId.STRING);\n+\t@Valid\n+\tprivate OutputDescription[] output;\n+\n+\t/**\n+\t * Empty array to be used only for validation of groovy script.\n+\t */\n+\tpublic static final String[] FAKE_HEADERS = new String[50];\n+\n+\t@JsonIgnore @ValidationMethod(message = \"Groovy script is not valid.\")", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394992077", "bodyText": "Nach Au\u00dfen sieht es aus als w\u00fcrde die Funktion immer erfolgreich sein. Ist das nicht ein Problem?", "author": "thoniTUB", "createdAt": "2020-03-19T12:34:14Z", "path": "backend/src/main/java/com/bakdata/conquery/util/DateFormats.java", "diffHunk": "@@ -0,0 +1,115 @@\n+package com.bakdata.conquery.util;\n+\n+import java.time.LocalDate;\n+import java.time.format.DateTimeFormatter;\n+import java.time.format.DateTimeFormatterBuilder;\n+import java.time.format.DateTimeParseException;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Locale;\n+import java.util.Set;\n+\n+import com.bakdata.conquery.models.config.ConqueryConfig;\n+import com.bakdata.conquery.models.exceptions.ParsingException;\n+import com.google.common.base.Strings;\n+import com.google.common.cache.CacheBuilder;\n+import com.google.common.cache.CacheLoader;\n+import com.google.common.cache.LoadingCache;\n+import lombok.experimental.UtilityClass;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Utility class for parsing multiple dateformats. Parsing is cached in two ways: First parsed values are cached. Second, the last used parser is cached since it's likely that it will be used again, we therefore try to use it first, then try all others.\n+ */\n+@UtilityClass\n+@Slf4j\n+public class DateFormats {\n+\n+\t/**\n+\t * All available formats for parsing.\n+\t */\n+\tprivate static Set<DateTimeFormatter> formats;\n+\n+\t/**\n+\t * Last successfully parsed dateformat.\n+\t */\n+\tprivate static ThreadLocal<DateTimeFormatter> lastFormat = new ThreadLocal<>();\n+\n+\tprivate static final LocalDate ERROR_DATE = LocalDate.MIN;\n+\n+\t/**\n+\t * Parsed values cache.\n+\t */\n+\tprivate static final LoadingCache<String, LocalDate> DATE_CACHE = CacheBuilder.newBuilder()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .weakKeys().weakValues()\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  // TODO: 07.01.2020 fk: Tweak this number?\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .concurrencyLevel(10)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .initialCapacity(64000)\n+\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .build(CacheLoader.from(DateFormats::tryParse));\n+\n+\t/**\n+\t * Try parsing the String value to a LocalDate.\n+\t */\n+\tpublic static LocalDate parseToLocalDate(String value) throws ParsingException {\n+\t\tif(Strings.isNullOrEmpty(value)) {\n+\t\t\treturn null;\n+\t\t}\n+\n+\t\treturn DATE_CACHE.getUnchecked(value);\n+\t}\n+\n+\t/**\n+\t * Try and parse with the last successful parser. If not successful try and parse with other parsers and update the last successful parser.\n+\t *\n+\t * Method is private as it is only directly accessed via the Cache.\n+\t */\n+\tprivate static LocalDate tryParse(String value) {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNTExNw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412225117", "bodyText": "Kein Kommentar von dir \ud83d\ude22", "author": "thoniTUB", "createdAt": "2020-04-21T14:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyNjgyMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412226820", "bodyText": "Was macht das ErrorDate in den Daten? Taucht das Event dann nur auf wenn man quasi keine Datumsbeschr\u00e4nkung gesetzt hat?", "author": "thoniTUB", "createdAt": "2020-04-21T14:19:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MjA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5MzY3NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394993675", "bodyText": "Die Funktionen sehen aus wie Util methoden. K\u00f6nnen die nicht in die LoadingUtil?", "author": "thoniTUB", "createdAt": "2020-03-19T12:37:10Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/QueryTest.java", "diffHunk": "@@ -59,8 +86,97 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \n \t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());\n \t\tsupport.waitUntilWorkDone();\n+\t\timportIdMapping(support);\n+\t\timportPreviousQueries(support);\n+\t}\n+\n+\tpublic void importIdMapping(StandaloneSupport support) throws JSONException, IOException {", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDk5NDE5NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r394994194", "bodyText": "Warum wird das gemacht?", "author": "thoniTUB", "createdAt": "2020-03-19T12:38:04Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/filter/FilterTest.java", "diffHunk": "@@ -99,12 +96,9 @@ public void importRequiredData(StandaloneSupport support) throws IOException, JS\n \t}\n \n \tprivate void importTableContents(StandaloneSupport support) throws IOException, JSONException {\n-\t\tCsvParserSettings settings = new CsvParserSettings();\n-\t\tCsvFormat format = new CsvFormat();\n-\t\tformat.setLineSeparator(\"\\n\");\n-\t\tsettings.setFormat(format);\n-\t\tsettings.setHeaderExtractionEnabled(true);\n-\t\tDateFormats.initialize(ArrayUtils.EMPTY_STRING_ARRAY);\n+\n+\t\tConqueryConfig.getInstance().setAdditionalFormats(ArrayUtils.EMPTY_STRING_ARRAY);", "originalCommit": "a111d9f65396af814aa7b52cb66ae3bfe74a5c68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "900c08629b3b0b56429980d5eda7ad080494f4ee", "url": "https://github.com/bakdata/conquery/commit/900c08629b3b0b56429980d5eda7ad080494f4ee", "message": "Gather exception classes into multiset", "committedDate": "2020-04-14T13:57:43Z", "type": "commit"}, {"oid": "06018a531e13216cc5d1031f0db48fe0f6c73402", "url": "https://github.com/bakdata/conquery/commit/06018a531e13216cc5d1031f0db48fe0f6c73402", "message": "refactor unused builder into AllArgsCtor", "committedDate": "2020-04-14T13:58:05Z", "type": "commit"}, {"oid": "0804ec1b82d1fc7a595e29cfc8c3a9667603a13a", "url": "https://github.com/bakdata/conquery/commit/0804ec1b82d1fc7a595e29cfc8c3a9667603a13a", "message": "Check for Error date", "committedDate": "2020-04-14T14:05:18Z", "type": "commit"}, {"oid": "cb17d3d551f3f7b6b5e022277586ff4e49d5c693", "url": "https://github.com/bakdata/conquery/commit/cb17d3d551f3f7b6b5e022277586ff4e49d5c693", "message": "refactor as static", "committedDate": "2020-04-14T14:23:16Z", "type": "commit"}, {"oid": "42f3deba7ca312dbe280dd8232a6710f1774be07", "url": "https://github.com/bakdata/conquery/commit/42f3deba7ca312dbe280dd8232a6710f1774be07", "message": "post merge fixes", "committedDate": "2020-04-16T09:00:02Z", "type": "commit"}, {"oid": "c6c360e678f33a51ce60ecc9b9eec2eee4907c5a", "url": "https://github.com/bakdata/conquery/commit/c6c360e678f33a51ce60ecc9b9eec2eee4907c5a", "message": "fix case sensitivity in headers.", "committedDate": "2020-04-20T14:15:36Z", "type": "commit"}, {"oid": "105fdd94585200e10fd4d48a5a0786dfda0705fb", "url": "https://github.com/bakdata/conquery/commit/105fdd94585200e10fd4d48a5a0786dfda0705fb", "message": "fix injection of content2.2.csv", "committedDate": "2020-04-20T14:17:28Z", "type": "commit"}, {"oid": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "url": "https://github.com/bakdata/conquery/commit/81ccda1cd3154f135b7f37dccfea58634c5138fc", "message": "fix usage of previousQuery Import", "committedDate": "2020-04-20T14:26:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ0NTk5Mw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411445993", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n          \n          \n            \n            \tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException {", "author": "thoniTUB", "createdAt": "2020-04-20T14:55:58Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411452497", "bodyText": "Hier w\u00fcrde ich den Member von description umbenennen.  Sodass man im code ein Hint welcher Dateityp dahinter steht: import/table ...", "author": "thoniTUB", "createdAt": "2020-04-20T15:04:01Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -111,25 +120,31 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n \t}\n \n-\tpublic static List<Preprocessor> findPreprocessingJobs(ConqueryConfig config, Validator validator, PreprocessingDirectories[] directories) throws IOException, JSONException {\n-\t\tList<Preprocessor> l = new ArrayList<>();\n+\tpublic static List<TableImportDescriptor> findPreprocessingDescriptions(Validator validator, PreprocessingDirectories[] directories, String tag) throws IOException, JSONException {\n+\t\tList<TableImportDescriptor> out = new ArrayList<>();\n \t\tfor (PreprocessingDirectories description : directories) {\n-\t\t\tFile in = description.getDescriptions().getAbsoluteFile();\n-\t\t\tfor (File descriptionFile : in.listFiles()) {\n-\t\t\t\tif (!descriptionFile.getName().endsWith(ConqueryConstants.EXTENSION_DESCRIPTION)) {\n-\t\t\t\t\tcontinue;\n-\t\t\t\t}\n \n-\t\t\t\tInputFile file = InputFile.fromDescriptionFile(descriptionFile, description);\n+\t\t\tFile inDir = description.getDescriptions().getAbsoluteFile();", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1NDYxOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r411454618", "bodyText": "Oder in dem fall, dass es ein Pfad ist", "author": "thoniTUB", "createdAt": "2020-04-20T15:06:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTQ1MjQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412212100", "bodyText": "Die Methoden Signatur ist hier unsauber, weil man der Methode auch ein HCFile im READ mode \u00fcbergeben kann.", "author": "thoniTUB", "createdAt": "2020-04-21T14:02:30Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -50,26 +50,69 @@ public Preprocessed(PreprocessingConfig config, ImportDescriptor descriptor) thr\n \t\tif(!(primaryColumn.getParser() instanceof StringParser)) {\n \t\t\tthrow new IllegalStateException(\"The primary column must be an ENTITY_ID or STRING column\");\n \t\t}\n+\n \t\tfor(int i=0;i<input.getWidth();i++) {\n \t\t\tColumnDescription columnDescription = input.getColumnDescription(i);\n \t\t\tcolumns[i] = new PPColumn(columnDescription.getName());\n \t\t\tcolumns[i].setParser(columnDescription.getType().createParser());\n \t\t}\n \t}\n-\t\n+\n+\tpublic void write(HCFile outFile) throws IOException {", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTcxMDE1MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415710151", "bodyText": "Ich hab hier einen Test eingebaut ob es in read ge\u00f6ffnet ist. Die alte variante war mMn noch bl\u00f6der aber ich sehe hier keinen trivialen Weg das zu garantieren.  Ich hab auch ein todo in die HCFile klasse geschrieben aber so wie sie aktuell ist gef\u00e4llt sie mir eigentlich nicht", "author": "awildturtok", "createdAt": "2020-04-27T10:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIxMjEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412220444", "bodyText": "Ich glaube hier ist Map<Integer, List<Object[]>> besser, da wir Die zweite Interpretation aus der Multimap-Doku benutzt wird und f\u00fcr diese eine Multimap nicht empfohlen wird", "author": "thoniTUB", "createdAt": "2020-04-21T14:12:07Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/Preprocessed.java", "diffHunk": "@@ -26,21 +27,20 @@\n \tprivate final String name;\n \tprivate final PPColumn primaryColumn;\n \tprivate final PPColumn[] columns;\n-\tprivate final ImportDescriptor descriptor;\n+\tprivate final TableImportDescriptor descriptor;\n \tprivate long rows = 0;\n \tprivate CDateRange eventRange;\n \tprivate long writtenGroups = 0;\n-\tprivate List<List<Object[]>> entries = new ArrayList<>();\n+\tprivate Multimap<Integer, Object[]> entries = MultimapBuilder.hashKeys().arrayListValues().build();", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMTE0Ng==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412221146", "bodyText": "A collection that maps keys to values, similar to Map, but in which each key may beassociated with multiple values. You can visualize the contents of a multimap either as amap from keys to nonempty collections of values:\n\u2022a \u00e2\u2020\u2019 1, 2\n\u2022b \u00e2\u2020\u2019 3\n... or as a single \"flattened\" collection of key-value pairs: \u2022a \u00e2\u2020\u2019 1\n\u2022a \u00e2\u2020\u2019 2\n\u2022b \u00e2\u2020\u2019 3\n\n\nImportant: although the first interpretation resembles how most multimaps are implemented, the design of the Multimap API is based on the second form.So, using the multimap shown above as an example, the size is 3, not 2,and the values collection is [1, 2, 3], not [[1, 2], [3]]. For thosetimes when the first style is more useful, use the multimap's asMap view (or create a Map<K, Collection> in the first place).", "author": "thoniTUB", "createdAt": "2020-04-21T14:13:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMjQ1NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412222455", "bodyText": "Dann sparst du dir auch den cast in Zeile 67", "author": "thoniTUB", "createdAt": "2020-04-21T14:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1NTgxNA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416455814", "bodyText": "Was sind deine Gedanken hierzu?", "author": "thoniTUB", "createdAt": "2020-04-28T09:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIyMDQ0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzMTUwOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412231509", "bodyText": "Mache ambesten aus dem table Array in RequiredData gleich eine Liste, dann musst du hier nichts wrappen", "author": "thoniTUB", "createdAt": "2020-04-21T14:24:52Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/json/FormTest.java", "diffHunk": "@@ -81,18 +82,18 @@\n \t@Override\n \tpublic void importRequiredData(StandaloneSupport support) throws Exception {\n \n-\t\tLoadingUtil.importTables(support, content);\n+\t\tIntegrationUtils.importTables(support, content);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT TABLES\", getLabel());\n \n \t\timportConcepts(support);\n \t\tsupport.waitUntilWorkDone();\n \t\tlog.info(\"{} IMPORT CONCEPTS\", getLabel());\n \n-\t\tLoadingUtil.importTableContents(support, content);\n+\t\tIntegrationUtils.importTableContents(support, Arrays.asList(content.getTables()), support.getDataset());", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r412234238", "bodyText": "Gibt es auch einen Test in dem tag != null ist?", "author": "thoniTUB", "createdAt": "2020-04-21T14:28:02Z", "path": "backend/src/test/java/com/bakdata/conquery/integration/tests/deletion/ImportDeletionTest.java", "diffHunk": "@@ -177,29 +176,13 @@ public void execute(String name, TestConquery testConquery) throws Exception {\n \t\t\tFileUtils.copyInputStreamToFile(In.resource(path.substring(0, path.lastIndexOf(\"/\")) + \"/\" + \"content2.2.csv\")\n \t\t\t\t\t\t\t\t\t\t\t  .asStream(), new File(conquery.getTmpDir(), import2Table.getCsv().getName()));\n \n-\t\t\t//create import descriptor\n-\t\t\tInputFile inputFile = InputFile.fromName(conquery.getConfig().getPreprocessor().getDirectories()[0], importId.getTag());\n-\t\t\tImportDescriptor desc = new ImportDescriptor();\n-\t\t\tdesc.setInputFile(inputFile);\n-\t\t\tdesc.setName(import2Table.getName() + \"_import\");\n-\t\t\tdesc.setTable(import2Table.getName());\n-\t\t\tInput input = new Input();\n-\t\t\t{\n-\t\t\t\tinput.setPrimary(IntegrationUtils.copyOutput(0, import2Table.getPrimaryColumn()));\n-\t\t\t\tinput.setSourceFile(new File(inputFile.getCsvDirectory(), import2Table.getCsv().getName()));\n-\t\t\t\tinput.setOutput(new Output[import2Table.getColumns().length]);\n-\t\t\t\tfor (int i = 0; i < import2Table.getColumns().length; i++) {\n-\t\t\t\t\tinput.getOutput()[i] = IntegrationUtils.copyOutput(i + 1, import2Table.getColumns()[i]);\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdesc.setInputs(new Input[]{input});\n-\t\t\tJackson.MAPPER.writeValue(inputFile.getDescriptionFile(), desc);\n \n \t\t\t//preprocess\n \t\t\tconquery.preprocessTmp();\n \n \t\t\t//import preprocessedFiles\n-\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), inputFile.getPreprocessedFile());\n+\n+\t\t\tconquery.getDatasetsProcessor().addImport(conquery.getDataset(), Preprocessor.getTaggedVersion(new File(conquery.getTmpDir(), import2Table.getCsv().getName().substring(0, import2Table.getCsv().getName().lastIndexOf('.')) + EXTENSION_PREPROCESSED), null));", "originalCommit": "81ccda1cd3154f135b7f37dccfea58634c5138fc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgwMzkwMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r415803900", "bodyText": "Daf\u00fcr m\u00fcssten wir so tief in die Tests eingreifen f\u00fcr ein convenience feature", "author": "awildturtok", "createdAt": "2020-04-27T13:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU2NDk1OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416564958", "bodyText": "aber ich sollte die methode selber unit testen", "author": "awildturtok", "createdAt": "2020-04-28T12:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjIzNDIzOA=="}], "type": "inlineReview"}, {"oid": "afa650605a2bf69184aeaa8749d4ddf26d18db18", "url": "https://github.com/bakdata/conquery/commit/afa650605a2bf69184aeaa8749d4ddf26d18db18", "message": "reimplement DateRangeOutput as Epoch dates were a stupid idea to begin with.", "committedDate": "2020-04-21T15:42:09Z", "type": "commit"}, {"oid": "e45d13d5419f4b53f208009fe5f8fa529521d7f0", "url": "https://github.com/bakdata/conquery/commit/e45d13d5419f4b53f208009fe5f8fa529521d7f0", "message": "removed weakKeys as they do not work properly with strings.", "committedDate": "2020-04-22T09:53:32Z", "type": "commit"}, {"oid": "e183e4bb19ec5b0b1f37e7ed1ce789f9cff00d48", "url": "https://github.com/bakdata/conquery/commit/e183e4bb19ec5b0b1f37e7ed1ce789f9cff00d48", "message": "add handling of normal exceptions.", "committedDate": "2020-04-22T14:45:42Z", "type": "commit"}, {"oid": "4fc24dd031a03eabf7cdc14eadfc469eca8d3916", "url": "https://github.com/bakdata/conquery/commit/4fc24dd031a03eabf7cdc14eadfc469eca8d3916", "message": "moved error output up so we have analysis earlier (becasue the code now below it is having problems)", "committedDate": "2020-04-22T14:52:41Z", "type": "commit"}, {"oid": "195d2186714d70bf8eea7d7e35c5b43681f545e2", "url": "https://github.com/bakdata/conquery/commit/195d2186714d70bf8eea7d7e35c5b43681f545e2", "message": "fixed illegal calls on open dateranges", "committedDate": "2020-04-22T15:03:44Z", "type": "commit"}, {"oid": "c0902acf94c0bdbce36020e649e45de6ba9a8459", "url": "https://github.com/bakdata/conquery/commit/c0902acf94c0bdbce36020e649e45de6ba9a8459", "message": "improve error logging", "committedDate": "2020-04-22T15:16:57Z", "type": "commit"}, {"oid": "2852d3928e10b199ec39955c2069eb4349a9ea30", "url": "https://github.com/bakdata/conquery/commit/2852d3928e10b199ec39955c2069eb4349a9ea30", "message": "more logging", "committedDate": "2020-04-22T15:32:40Z", "type": "commit"}, {"oid": "0513d63a2d8f91ae75f3521777b9617da5b18ba3", "url": "https://github.com/bakdata/conquery/commit/0513d63a2d8f91ae75f3521777b9617da5b18ba3", "message": "fixed an overflow bug", "committedDate": "2020-04-22T15:47:02Z", "type": "commit"}, {"oid": "dbdb4f7e50e2a6bf915cdd61ccdbdc8e9fc1be0a", "url": "https://github.com/bakdata/conquery/commit/dbdb4f7e50e2a6bf915cdd61ccdbdc8e9fc1be0a", "message": "add better checks for open DateRanges", "committedDate": "2020-04-27T10:06:47Z", "type": "commit"}, {"oid": "6f3ee9a108a4d02a3df7beb33dda7eb299e9a88e", "url": "https://github.com/bakdata/conquery/commit/6f3ee9a108a4d02a3df7beb33dda7eb299e9a88e", "message": "refactors and cleanups.", "committedDate": "2020-04-27T10:39:18Z", "type": "commit"}, {"oid": "ed8118001e5dea03abd73f2a5adabaf189d5a2e1", "url": "https://github.com/bakdata/conquery/commit/ed8118001e5dea03abd73f2a5adabaf189d5a2e1", "message": "create missing Storage directory if it does not yet exist.", "committedDate": "2020-04-27T10:47:37Z", "type": "commit"}, {"oid": "8e80bfb48ef8ec911dee911011ac10c136920f02", "url": "https://github.com/bakdata/conquery/commit/8e80bfb48ef8ec911dee911011ac10c136920f02", "message": "minor orphan cleanups", "committedDate": "2020-04-27T10:47:57Z", "type": "commit"}, {"oid": "2259374250d0edff244cd686130375cc0385ceb8", "url": "https://github.com/bakdata/conquery/commit/2259374250d0edff244cd686130375cc0385ceb8", "message": "renamed PreprocessingDirectories fields to better communicate that they are directories", "committedDate": "2020-04-27T10:49:30Z", "type": "commit"}, {"oid": "d8394cf443cfcf89f77459b8ad2e29a0b2d06754", "url": "https://github.com/bakdata/conquery/commit/d8394cf443cfcf89f77459b8ad2e29a0b2d06754", "message": "unify usage of IntegrationUtils as much as possible.", "committedDate": "2020-04-27T12:18:48Z", "type": "commit"}, {"oid": "5644bf9c6eef4fbbe8cab93c9effe408a2c5f8f8", "url": "https://github.com/bakdata/conquery/commit/5644bf9c6eef4fbbe8cab93c9effe408a2c5f8f8", "message": "fix usage of compute in intmap", "committedDate": "2020-04-27T12:28:42Z", "type": "commit"}, {"oid": "3331ab54b7613fefc92b80531e29bfb1a52b0d75", "url": "https://github.com/bakdata/conquery/commit/3331ab54b7613fefc92b80531e29bfb1a52b0d75", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-27T12:29:11Z", "type": "commit"}, {"oid": "dc9650da31a1e7f449d084051a7ca91cb4282b22", "url": "https://github.com/bakdata/conquery/commit/dc9650da31a1e7f449d084051a7ca91cb4282b22", "message": "Merge 3331ab54b7613fefc92b80531e29bfb1a52b0d75 into 6463a75e42e344e6a09a2eecb2b0a07c995b321d", "committedDate": "2020-04-27T12:29:14Z", "type": "commit"}, {"oid": "266f9e9efbae42b0c5cdc19e48a47f2b469c2e24", "url": "https://github.com/bakdata/conquery/commit/266f9e9efbae42b0c5cdc19e48a47f2b469c2e24", "message": "automatic update to docs", "committedDate": "2020-04-27T12:31:14Z", "type": "commit"}, {"oid": "b6edb442d3a611e075242b084189d990dd0449a9", "url": "https://github.com/bakdata/conquery/commit/b6edb442d3a611e075242b084189d990dd0449a9", "message": "better tag insertion via regex", "committedDate": "2020-04-27T12:37:26Z", "type": "commit"}, {"oid": "15cd72a8e06ce283a3bcb8617989a81d6af50223", "url": "https://github.com/bakdata/conquery/commit/15cd72a8e06ce283a3bcb8617989a81d6af50223", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-04-27T12:37:38Z", "type": "commit"}, {"oid": "eb45e9d2afed1a93203b6c10281e84d30e895409", "url": "https://github.com/bakdata/conquery/commit/eb45e9d2afed1a93203b6c10281e84d30e895409", "message": "better tag insertion via regex FIX", "committedDate": "2020-04-27T13:00:46Z", "type": "commit"}, {"oid": "f2e65b93a6e7abe39e4f40b7c124f90d7187ff1f", "url": "https://github.com/bakdata/conquery/commit/f2e65b93a6e7abe39e4f40b7c124f90d7187ff1f", "message": "add error output at the end of PreprocessorCommand", "committedDate": "2020-04-27T13:27:02Z", "type": "commit"}, {"oid": "a28e42e887c6e3741b83c2a0d5c3aedb2ce74b6a", "url": "https://github.com/bakdata/conquery/commit/a28e42e887c6e3741b83c2a0d5c3aedb2ce74b6a", "message": "more error reporting and fix output file naming", "committedDate": "2020-04-27T13:38:50Z", "type": "commit"}, {"oid": "fcc0903271d5a4965affcc1fe350946acc2ef9fa", "url": "https://github.com/bakdata/conquery/commit/fcc0903271d5a4965affcc1fe350946acc2ef9fa", "message": "fix replacement of file ext", "committedDate": "2020-04-27T13:44:29Z", "type": "commit"}, {"oid": "4a5c2ebadc6cddd1beba88212f9725e383f40eb9", "url": "https://github.com/bakdata/conquery/commit/4a5c2ebadc6cddd1beba88212f9725e383f40eb9", "message": "fix file naming again", "committedDate": "2020-04-27T13:58:38Z", "type": "commit"}, {"oid": "a93fe05c228c8a178feb08abac75743fe4b3b634", "url": "https://github.com/bakdata/conquery/commit/a93fe05c228c8a178feb08abac75743fe4b3b634", "message": "clear MDC after Preprocessing", "committedDate": "2020-04-27T14:17:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r413239901", "bodyText": "Der Cast macht nichts", "author": "thoniTUB", "createdAt": "2020-04-22T18:57:25Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -72,23 +71,27 @@ public Integer transform(CDateRange value) {\n \t\t\t\ttype\n \t\t\t);\n \t\t}\n-\t\tif(maxValue - minValue <PackedUnsigned1616.MAX_VALUE) {\n+\t\t// min or max can be Integer.MIN/MAX_VALUE when this happens, the left expression overflows causing it to be true when it is not.\n+\t\tif ((long) maxValue - (long) minValue < (long) PackedUnsigned1616.MAX_VALUE) {\n \t\t\tDateRangeTypePacked type = new DateRangeTypePacked();\n \t\t\ttype.setMinValue(minValue);\n \t\t\ttype.setMaxValue(maxValue);\n+\n+\t\t\tlog.debug(\"Decided for Packed: min={}, max={}\", minValue, maxValue);\n+\n \t\t\treturn new Decision<>(\n-\t\t\t\tnew Transformer<CDateRange, Integer>() {\n-\t\t\t\t\t@Override\n-\t\t\t\t\tpublic Integer transform(CDateRange value) {\n-\t\t\t\t\t\tCDateRange v = (CDateRange) value;\n-\t\t\t\t\t\tif(v.getMaxValue()>Integer.MAX_VALUE || v.getMinValue()<Integer.MIN_VALUE) {\n-\t\t\t\t\t\t\tthrow new IllegalArgumentException(value+\" is out of range\");\n+\t\t\t\t\tnew Transformer<CDateRange, Integer>() {\n+\t\t\t\t\t\t@Override\n+\t\t\t\t\t\tpublic Integer transform(CDateRange value) {\n+\t\t\t\t\t\t\tCDateRange v = (CDateRange) value;", "originalCommit": "0513d63a2d8f91ae75f3521777b9617da5b18ba3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NTQzMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416545431", "bodyText": "hast recht, das sollte auch weg.", "author": "awildturtok", "createdAt": "2020-04-28T11:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzIzOTkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0MzQ3MA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416443470", "bodyText": "Ist in dieser Datei einfach nur ein Import dazugekommen?", "author": "thoniTUB", "createdAt": "2020-04-28T08:51:39Z", "path": "backend/src/main/java/com/bakdata/conquery/apiv1/StoredQueriesProcessor.java", "diffHunk": "@@ -1,5 +1,7 @@\n package com.bakdata.conquery.apiv1;\n \n+import static com.bakdata.conquery.models.auth.AuthorizationHelper.*;", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0NDgwMQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416444801", "bodyText": "Ah nice! Insgeheim ein lang ersehntes Feature f\u00fcr mich ;)", "author": "thoniTUB", "createdAt": "2020-04-28T08:53:45Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/MasterCommand.java", "diffHunk": "@@ -91,6 +91,10 @@ public void run(ConqueryConfig config, Environment environment) {\n \t\t\n \t\tenvironment.lifecycle().manage(this);\n \n+\t\tif(config.getStorage().getDirectory().mkdirs()){\n+\t\t\tlog.warn(\"Had to create Storage Dir at `{}`\", config.getStorage().getDirectory());", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ0OTUwMw==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416449503", "bodyText": "Hier w\u00fcrde ich eine Message zusammenbauen und die als einzelnen Error absetzten. Das machst die Logs besser durchsuchbar und lesbarer, da die Zusammenfassung als ein Block erkennbar ist.", "author": "thoniTUB", "createdAt": "2020-04-28T09:00:49Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416450110", "bodyText": "Hier kollidierst du potentiell mit speziellen StatusCodes: http://tldp.org/LDP/abs/html/exitcodes.html", "author": "thoniTUB", "createdAt": "2020-04-28T09:01:38Z", "path": "backend/src/main/java/com/bakdata/conquery/commands/PreprocessorCommand.java", "diffHunk": "@@ -74,62 +81,81 @@ protected void run(Environment environment, Namespace namespace, ConqueryConfig\n \t\t\tpool = Executors.newFixedThreadPool(config.getPreprocessor().getThreads());\n \t\t}\n \n-\t\tCollection<Preprocessor> jobs = null;\n+\t\tfinal Collection<TableImportDescriptor> descriptors;\n+\n+\t\t// Tag if present is appended to input-file csvs, output-file cqpp and used as id of cqpps\n+\t\tfinal String tag = namespace.getString(\"tag\");\n \n \t\tif (namespace.get(\"in\") != null && namespace.get(\"desc\") != null && namespace.get(\"out\") != null) {\n \t\t\tlog.info(\"Preprocessing from command line config.\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), new PreprocessingDirectories[]{\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), new PreprocessingDirectories[]{\n \t\t\t\t\tnew PreprocessingDirectories(namespace.get(\"in\"), namespace.get(\"desc\"), namespace.get(\"out\"))\n-\t\t\t});\n+\t\t\t}, tag);\n \t\t}\n \t\telse {\n \t\t\tlog.info(\"Preprocessing from config.json\");\n-\t\t\tjobs = findPreprocessingJobs(config, environment.getValidator(), config.getPreprocessor().getDirectories());\n+\t\t\tdescriptors = findPreprocessingDescriptions(environment.getValidator(), config.getPreprocessor().getDirectories(), tag);\n \t\t}\n \n \n-\t\tjobs.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n+\t\tdescriptors.removeIf(Predicate.not(Preprocessor::requiresProcessing));\n \n-\t\tlong totalSize = jobs.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n+\t\tlong totalSize = descriptors.stream().mapToLong(Preprocessor::getTotalCsvSize).sum();\n \n \t\tlog.info(\"Required to preprocess {} in total\", BinaryByteUnit.format(totalSize));\n \n \t\tProgressBar totalProgress = new ProgressBar(totalSize, System.out);\n \n-\t\tfor (Preprocessor job : jobs) {\n+\t\tfor (TableImportDescriptor descriptor : descriptors) {\n \t\t\tpool.submit(() -> {\n-\t\t\t\tConqueryMDC.setLocation(job.getDescriptor().toString());\n+\t\t\t\tConqueryMDC.setLocation(descriptor.toString());\n \t\t\t\ttry {\n-\t\t\t\t\tjob.preprocess(totalProgress);\n+\t\t\t\t\tPreprocessor.preprocess(descriptor, totalProgress);\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(job.getDescriptor().getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tlog.error(\"Failed to preprocess \" + LogUtil.printPath(descriptor.getInputFile().getDescriptionFile()), e);\n+\t\t\t\t\tfailed.add(descriptor.toString());\n \t\t\t\t}\n \t\t\t});\n \t\t}\n \n \t\tpool.shutdown();\n \t\tpool.awaitTermination(24, TimeUnit.HOURS);\n+\n+\t\tConqueryMDC.clearLocation();\n+\n+\t\tif (!failed.isEmpty()) {\n+\t\t\tlog.error(\"Failed {} Preprocessing Jobs:\", failed.size());\n+\t\t\tfailed.forEach(desc -> log.error(\"\\tFailed Preprocessing for {}\", desc));\n+\t\t\tSystem.exit(Math.min(failed.size(), 255));", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MTQ3OA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416451478", "bodyText": "Zwar gilt das nur f\u00fcr bash, aber wo werden die StatusCodes sonst so eifrig benutzt?", "author": "thoniTUB", "createdAt": "2020-04-28T09:03:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODc1MQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468751", "bodyText": "https://www.gnu.org/software/parallel/man.html#EXIT-STATUS\nHabe mich hier von inspirieren lassen, das ist eher ein easter egg, es geht f\u00fcr mich nur um != 0 aber ichw erde es auf 100 truncaten so wie parallel die wissen was sie machen", "author": "awildturtok", "createdAt": "2020-04-28T09:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNzQyOQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417127429", "bodyText": "Okay :D so kann man es nat\u00fcrlich auch machen und einen Gro\u00dfteil der codes erhalten", "author": "thoniTUB", "createdAt": "2020-04-29T07:48:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MDExMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1MjkzOA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416452938", "bodyText": "\ud83d\udc4d", "author": "thoniTUB", "createdAt": "2020-04-28T09:06:00Z", "path": "backend/src/main/java/com/bakdata/conquery/models/config/PreprocessingConfig.java", "diffHunk": "@@ -3,15 +3,13 @@\n import javax.validation.Valid;\n import javax.validation.constraints.Min;\n \n-import org.hibernate.validator.constraints.NotEmpty;\n-\n import lombok.Getter;\n import lombok.Setter;\n import lombok.ToString;\n \n @Getter @Setter @ToString\n public class PreprocessingConfig {\n-\t@NotEmpty @Valid\n+\t@Valid", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416458552", "bodyText": "Hast du das mit Lennart abgesprochen?, ich wei\u00df nicht wann die OpenRanges auftreten aber potentiell k\u00f6nnen dadurch dann Daten in den Formularen fehlen.", "author": "thoniTUB", "createdAt": "2020-04-28T09:14:29Z", "path": "backend/src/main/java/com/bakdata/conquery/models/common/daterange/CDateRange.java", "diffHunk": "@@ -320,6 +320,11 @@ public boolean hasLowerBound() {\n \t * @return The years as date ranges, from the first date in range to the last in ascending order.\n \t */\n \tpublic List<CDateRange> getCoveredYears() {\n+\t\tif(isOpen()){\n+\t\t\t// TODO: 22.04.2020 throw exceptiopn?", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2ODIzMA==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416468230", "bodyText": "wenn wir es nicht testen ist das eine endlosschleife.", "author": "awildturtok", "createdAt": "2020-04-28T09:29:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEzMDA3NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417130075", "bodyText": "Ok, ich habe gerade auch nochmal nachgeschaut in dem Context, sollten eigentlich auch keine offnen Ranges auf treten", "author": "thoniTUB", "createdAt": "2020-04-29T07:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ1ODU1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416462485", "bodyText": "Angenommen es w\u00e4ren nur komplett offene DateRanges vorhanden, dann ist das ganz sch\u00f6n teuer die null abzuspeichern, aber ein Edgecase", "author": "thoniTUB", "createdAt": "2020-04-28T09:20:43Z", "path": "backend/src/main/java/com/bakdata/conquery/models/types/parser/specific/DateRangeParser.java", "diffHunk": "@@ -57,6 +67,15 @@ public static CDateRange parseISORange(String value) throws ParsingException {\n \t\n \t@Override\n \tprotected Decision<CDateRange, ?, ? extends CType<CDateRange, ?>> decideType() {\n+\t\t// We cannot yet do meaningful compression for open dateranges.\n+\t\t// TODO: 27.04.2020 consider packed compression with extra value as null value.\n+\t\tif(!onlyClosed) {", "originalCommit": "a93fe05c228c8a178feb08abac75743fe4b3b634", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjU0NDY3Mg==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r416544672", "bodyText": "ganz offene meinst du -inf/+inf, das werden wir nicht zulassen, das sind m\u00fcll daten. Bzw die werden wir so hoffentlich nie zulassen, da w\u00fcrden sehr viele komische sachen bei rauskommen.", "author": "awildturtok", "createdAt": "2020-04-28T11:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjQ2MjQ4NQ=="}], "type": "inlineReview"}, {"oid": "759a119f42b8ceec222d11216ef86abab75064a8", "url": "https://github.com/bakdata/conquery/commit/759a119f42b8ceec222d11216ef86abab75064a8", "message": "Update PreprocessorCommand.java", "committedDate": "2020-04-28T09:31:05Z", "type": "commit"}, {"oid": "2b67c79bbe626ee88390d45555ba4b5b3d4f2227", "url": "https://github.com/bakdata/conquery/commit/2b67c79bbe626ee88390d45555ba4b5b3d4f2227", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-28T11:41:46Z", "type": "commit"}, {"oid": "ae46979ca41566bf502b73186bf044a373101f8b", "url": "https://github.com/bakdata/conquery/commit/ae46979ca41566bf502b73186bf044a373101f8b", "message": "fix reading of empty ranges", "committedDate": "2020-04-28T13:46:30Z", "type": "commit"}, {"oid": "ee07107edc0e45f8020bf20406c2f4706edf7889", "url": "https://github.com/bakdata/conquery/commit/ee07107edc0e45f8020bf20406c2f4706edf7889", "message": "Merge remote-tracking branch 'origin/feature/imports-by-column-name' into feature/imports-by-column-name", "committedDate": "2020-04-28T13:46:42Z", "type": "commit"}, {"oid": "ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "url": "https://github.com/bakdata/conquery/commit/ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "message": "test for Preprocessor", "committedDate": "2020-04-28T14:32:54Z", "type": "commit"}, {"oid": "ad1a715ab9346a0f8eedd804d239400a6d3dc204", "url": "https://github.com/bakdata/conquery/commit/ad1a715ab9346a0f8eedd804d239400a6d3dc204", "message": "migrate usage of Preprocessed to Map of list as its not the appropriate use case for Multimaps.\nRemove redundant cast in DateRangeParser and remove code that does nothing.", "committedDate": "2020-04-29T07:29:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzEyNjAwNQ==", "url": "https://github.com/bakdata/conquery/pull/973#discussion_r417126005", "bodyText": "Hier verstehe ich die Logik gerade nicht.\nM\u00fcsstest du nicht erst pr\u00fcfen ob beides  NullorEmpty ist und offene ranges erlaubt sind und CDateRange.of() zur\u00fcckgeben anstatt null.\nDie nachfolgende If-Bedingung, sollte die nicht als erstes gepr\u00fcft werden?", "author": "thoniTUB", "createdAt": "2020-04-29T07:46:01Z", "path": "backend/src/main/java/com/bakdata/conquery/models/preproc/outputs/EpochDateRangeOutput.java", "diffHunk": "@@ -39,14 +40,13 @@ public Output createForHeaders(Object2IntArrayMap<String> headers) {\n \t\treturn new Output() {\n \t\t\t@Override\n \t\t\tprotected Object parseLine(String[] row, Parser<?> type, long sourceLine) throws ParsingException {\n-\t\t\t\tif (!allowOpen && (row[startIndex] == null || row[endIndex] == null)) {\n-\t\t\t\t\tthrow new IllegalArgumentException(\"Open Ranges are not allowed.\");\n-\t\t\t\t}\n-\n-\t\t\t\tif (row[startIndex] == null && row[endIndex] == null) {\n+\t\t\t\tif (Strings.isNullOrEmpty(row[startIndex]) && Strings.isNullOrEmpty(row[endIndex])) {", "originalCommit": "ed515d582a2f8a3d6709d52528d8836ea79bdbb4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7a28f358eaa03655debe83123a017db9d02ced5b", "url": "https://github.com/bakdata/conquery/commit/7a28f358eaa03655debe83123a017db9d02ced5b", "message": "new spanning logic that ignores +-Infinity in CDateRange. and accompanying tests", "committedDate": "2020-04-29T10:42:19Z", "type": "commit"}, {"oid": "27edc0503ae00e70a8e133c620cc4a92f77fb746", "url": "https://github.com/bakdata/conquery/commit/27edc0503ae00e70a8e133c620cc4a92f77fb746", "message": "remove unused eventrange", "committedDate": "2020-04-29T12:15:36Z", "type": "commit"}, {"oid": "4d6b95d1fd97ff69abc43dd7b9ea2bb6c4cba858", "url": "https://github.com/bakdata/conquery/commit/4d6b95d1fd97ff69abc43dd7b9ea2bb6c4cba858", "message": "minor code style fixes", "committedDate": "2020-04-29T13:27:00Z", "type": "commit"}, {"oid": "f55704328b1e32b5b579996cc615f186bbcba1b9", "url": "https://github.com/bakdata/conquery/commit/f55704328b1e32b5b579996cc615f186bbcba1b9", "message": "Merge branch 'develop' into feature/imports-by-column-name", "committedDate": "2020-04-29T13:27:49Z", "type": "commit"}, {"oid": "d2d80aae819809cbb5d3a959d774829dd6d26589", "url": "https://github.com/bakdata/conquery/commit/d2d80aae819809cbb5d3a959d774829dd6d26589", "message": "fix empty rows", "committedDate": "2020-04-30T10:02:09Z", "type": "commit"}, {"oid": "4d202cdc955a53ac75fff9544cfeec7bc67e0e45", "url": "https://github.com/bakdata/conquery/commit/4d202cdc955a53ac75fff9544cfeec7bc67e0e45", "message": "add better PreprocessedHeader validation for imports", "committedDate": "2020-04-30T11:24:04Z", "type": "commit"}, {"oid": "5c3b36775cc2bf1d10025493ace1f3d0e4ef8872", "url": "https://github.com/bakdata/conquery/commit/5c3b36775cc2bf1d10025493ace1f3d0e4ef8872", "message": "fixed trying to get cause where we didn't know there could be one.", "committedDate": "2020-05-06T14:38:11Z", "type": "commit"}]}