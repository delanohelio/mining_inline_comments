{"pr_number": 4418, "pr_title": "Add Kafka Avro encoder", "pr_createdAt": "2020-07-10T15:37:12Z", "pr_url": "https://github.com/trinodb/trino/pull/4418", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NjE5OQ==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452956199", "bodyText": "Extract making RowEncoder Closable to separate commit.", "author": "losipiuk", "createdAt": "2020-07-10T16:45:25Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/RowEncoder.java", "diffHunk": "@@ -15,7 +15,10 @@\n \n import io.prestosql.spi.block.Block;\n \n+import java.io.Closeable;\n+\n public interface RowEncoder\n+        extends Closeable", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIzMDE5Mg==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r454230192", "bodyText": "Sorry for not being precise.\nI meant extracting the interface chage to separate commit which is before one which add Avro encoder.\nThen AvroRowEncoder.close() can be added together with rest of Avro encoder code.", "author": "losipiuk", "createdAt": "2020-07-14T09:35:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1NjE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1Njk0OQ==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452956949", "bodyText": "rename to byteArrayOutputStream", "author": "losipiuk", "createdAt": "2020-07-10T16:46:54Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1ODU3NQ==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452958575", "bodyText": "do we need to call this one for each row?\nAnd if should we ignore DataFileWriter it returns?", "author": "losipiuk", "createdAt": "2020-07-10T16:50:19Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;\n+    private final Schema parsedSchema;\n+    private final DataFileWriter<GenericRecord> dataFileWriter;\n+    private final GenericData.Record record;\n+\n+    public AvroRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles, Schema parsedSchema)\n+    {\n+        super(session, columnHandles);\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"Unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        this.byteArrayOuts = new ByteArrayOutputStream();\n+        this.parsedSchema = requireNonNull(parsedSchema, \"parsedSchema is null\");\n+        this.dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(this.parsedSchema));\n+        this.record = new GenericData.Record(this.parsedSchema);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), null);\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        try {\n+            byteArrayOuts.reset();\n+            dataFileWriter.create(parsedSchema, byteArrayOuts);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2MzUzNg==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r453063536", "bodyText": "This is not a pretty API.  It looks like this call gets the writer ready to write to a new output stream with a new schema.  Feels like a factory method, but it just resets the instance.\nAnd without the schema registry integration, writing Avro messages are quite heavyweight (each message contains the schema).", "author": "aalbu", "createdAt": "2020-07-10T20:31:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1ODU3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4MTg5Ng==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r453781896", "bodyText": "It doesn't work without the create method call", "author": "charlesjmorgan", "createdAt": "2020-07-13T16:38:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1ODU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mjk1OTM0NQ==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r452959345", "bodyText": "can we write more than single row in the test?\n(this actually applies to other formats too)", "author": "losipiuk", "createdAt": "2020-07-10T16:51:52Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -141,6 +141,10 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n     private List<RoundTripTestCase> testRoundTripAllFormatsData()\n     {\n         return ImmutableList.<RoundTripTestCase>builder()\n+                .add(new RoundTripTestCase(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzA2NTE5OQ==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r453065199", "bodyText": "Nothing wrong with this, but it didn't seem necessary.  ByteArrayOutputStream.close() is really a no-op and I don't think the Avro writer holds on to any resources.", "author": "aalbu", "createdAt": "2020-07-10T20:35:27Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoder.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import com.google.common.collect.ImmutableSet;\n+import io.prestosql.plugin.kafka.encoder.AbstractRowEncoder;\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import io.prestosql.spi.type.Type;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.generic.GenericDatumWriter;\n+import org.apache.avro.generic.GenericRecord;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static io.prestosql.spi.type.BigintType.BIGINT;\n+import static io.prestosql.spi.type.BooleanType.BOOLEAN;\n+import static io.prestosql.spi.type.DoubleType.DOUBLE;\n+import static io.prestosql.spi.type.IntegerType.INTEGER;\n+import static io.prestosql.spi.type.RealType.REAL;\n+import static io.prestosql.spi.type.Varchars.isVarcharType;\n+import static java.lang.String.format;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoder\n+        extends AbstractRowEncoder\n+{\n+    private static final Set<Type> SUPPORTED_PRIMITIVE_TYPES = ImmutableSet.of(\n+            BOOLEAN, INTEGER, BIGINT, DOUBLE, REAL);\n+\n+    public static final String NAME = \"avro\";\n+\n+    private final ByteArrayOutputStream byteArrayOuts;\n+    private final Schema parsedSchema;\n+    private final DataFileWriter<GenericRecord> dataFileWriter;\n+    private final GenericData.Record record;\n+\n+    public AvroRowEncoder(ConnectorSession session, List<EncoderColumnHandle> columnHandles, Schema parsedSchema)\n+    {\n+        super(session, columnHandles);\n+        for (EncoderColumnHandle columnHandle : this.columnHandles) {\n+            checkArgument(columnHandle.getFormatHint() == null, \"Unexpected format hint '%s' defined for column '%s'\", columnHandle.getFormatHint(), columnHandle.getName());\n+            checkArgument(columnHandle.getDataFormat() == null, \"Unexpected data format '%s' defined for column '%s'\", columnHandle.getDataFormat(), columnHandle.getName());\n+\n+            checkArgument(isSupportedType(columnHandle.getType()), \"Unsupported column type '%s' for column '%s'\", columnHandle.getType(), columnHandle.getName());\n+        }\n+        this.byteArrayOuts = new ByteArrayOutputStream();\n+        this.parsedSchema = requireNonNull(parsedSchema, \"parsedSchema is null\");\n+        this.dataFileWriter = new DataFileWriter<>(new GenericDatumWriter<>(this.parsedSchema));\n+        this.record = new GenericData.Record(this.parsedSchema);\n+    }\n+\n+    private boolean isSupportedType(Type type)\n+    {\n+        return isVarcharType(type) || SUPPORTED_PRIMITIVE_TYPES.contains(type);\n+    }\n+\n+    @Override\n+    protected void appendNullValue()\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), null);\n+    }\n+\n+    @Override\n+    protected void appendLong(long value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendInt(int value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendShort(short value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendByte(byte value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendDouble(double value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendFloat(float value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendBoolean(boolean value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    protected void appendString(String value)\n+    {\n+        record.put(columnHandles.get(currentColumnIndex).getName(), value);\n+    }\n+\n+    @Override\n+    public byte[] toByteArray()\n+    {\n+        // make sure entire row has been updated with new values\n+        checkArgument(currentColumnIndex == columnHandles.size(), format(\"Missing %d columns\", columnHandles.size() - currentColumnIndex + 1));\n+\n+        try {\n+            byteArrayOuts.reset();\n+            dataFileWriter.create(parsedSchema, byteArrayOuts);\n+            dataFileWriter.append(record);\n+            dataFileWriter.flush();\n+\n+            resetColumnIndex(); // reset currentColumnIndex to prepare for next row\n+            return byteArrayOuts.toByteArray();\n+        }\n+        catch (IOException e) {\n+            throw new UncheckedIOException(\"Failed to append record\", e);\n+        }\n+    }\n+\n+    @Override\n+    public void close()\n+    {\n+        try {\n+            byteArrayOuts.close();\n+            dataFileWriter.close();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDIzMzA0Mg==", "url": "https://github.com/trinodb/trino/pull/4418#discussion_r454233042", "bodyText": "s/avro/Avro/", "author": "losipiuk", "createdAt": "2020-07-14T09:40:41Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/encoder/avro/AvroRowEncoderFactory.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package io.prestosql.plugin.kafka.encoder.avro;\n+\n+import io.prestosql.plugin.kafka.encoder.EncoderColumnHandle;\n+import io.prestosql.plugin.kafka.encoder.RowEncoder;\n+import io.prestosql.plugin.kafka.encoder.RowEncoderFactory;\n+import io.prestosql.spi.connector.ConnectorSession;\n+import org.apache.avro.Schema;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import static com.google.common.base.Preconditions.checkArgument;\n+import static java.util.Objects.requireNonNull;\n+\n+public class AvroRowEncoderFactory\n+        implements RowEncoderFactory\n+{\n+    @Override\n+    public RowEncoder create(ConnectorSession session, Optional<String> dataSchema, List<EncoderColumnHandle> columnHandles)\n+    {\n+        checkArgument(dataSchema.isPresent(), \"dataSchema for avro format is not present\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "88a95f1362f1135eb6a5eb9f9ab310472060ce22", "url": "https://github.com/trinodb/trino/commit/88a95f1362f1135eb6a5eb9f9ab310472060ce22", "message": "Make Kafka RowEncoder Closeable", "committedDate": "2020-07-14T14:24:48Z", "type": "commit"}, {"oid": "37ccd7bcd37d6e9c791459210f2c6eb9bd165b05", "url": "https://github.com/trinodb/trino/commit/37ccd7bcd37d6e9c791459210f2c6eb9bd165b05", "message": "Add Kafka Avro encoder", "committedDate": "2020-07-14T14:26:58Z", "type": "commit"}, {"oid": "7d2b2fde14166235feaaf37a661fd09633ff3372", "url": "https://github.com/trinodb/trino/commit/7d2b2fde14166235feaaf37a661fd09633ff3372", "message": "Add row to Kafka round trip test", "committedDate": "2020-07-14T14:27:04Z", "type": "commit"}, {"oid": "7d2b2fde14166235feaaf37a661fd09633ff3372", "url": "https://github.com/trinodb/trino/commit/7d2b2fde14166235feaaf37a661fd09633ff3372", "message": "Add row to Kafka round trip test", "committedDate": "2020-07-14T14:27:04Z", "type": "forcePushed"}]}