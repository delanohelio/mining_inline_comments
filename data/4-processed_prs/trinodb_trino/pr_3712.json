{"pr_number": 3712, "pr_title": "S3 streaming upload with multipart upload api", "pr_createdAt": "2020-05-13T03:23:13Z", "pr_url": "https://github.com/trinodb/trino/pull/3712", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA1OTczNw==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445059737", "bodyText": "Let's add a new config hive.s3.streaming.part-size for this instead of multiPartUploadMinFileSize", "author": "electrum", "createdAt": "2020-06-24T17:35:16Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {\n+            String uploadId = initMultipartUpload(bucketName, key).getUploadId();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StreamingOutputStream(s3, bucketName, key, uploadId, uploadExecutor, multiPartUploadMinPartSize),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MDM5MQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445080391", "bodyText": "We can use the new bucketName variable here", "author": "electrum", "createdAt": "2020-06-24T18:12:02Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {\n+            String uploadId = initMultipartUpload(bucketName, key).getUploadId();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StreamingOutputStream(s3, bucketName, key, uploadId, uploadExecutor, multiPartUploadMinPartSize),\n+                    statistics);\n+        }\n+        else {\n+            File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n+            return new FSDataOutputStream(\n+                    new PrestoS3StagingOutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MTUxMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445081512", "bodyText": "It might make sense to structure this like\nOutputStream out;\nif (streamingUploadEnabled) {\n    out = new PrestoS3StreamingOutputStream(...);\n}\nelse {\n    out = new PrestoS3StagingOutputStream(...);\n}\n\nreturn new FSDataOutputStream(out, statistics);", "author": "electrum", "createdAt": "2020-06-24T18:14:05Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -404,12 +427,22 @@ public FSDataOutputStream create(Path path, FsPermission permission, boolean ove\n         if (!stagingDirectory.isDirectory()) {\n             throw new IOException(\"Configured staging path is not a directory: \" + stagingDirectory);\n         }\n-        File tempFile = createTempFile(stagingDirectory.toPath(), \"presto-s3-\", \".tmp\").toFile();\n \n+        String bucketName = getBucketName(uri);\n         String key = keyFromPath(qualifiedPath(path));\n-        return new FSDataOutputStream(\n-                new PrestoS3OutputStream(s3, getBucketName(uri), key, tempFile, sseEnabled, sseType, sseKmsKeyId, multiPartUploadMinFileSize, multiPartUploadMinPartSize, s3AclType, requesterPaysEnabled, s3StorageClass),\n-                statistics);\n+\n+        if (streamingUploadEnabled) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4MzE5Nw==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445083197", "bodyText": "Add requireNonNull checks for all of these (not strictly required since this is a private class, but it's best practice and improves code readability)", "author": "electrum", "createdAt": "2020-06-24T18:17:07Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NDMyNA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445084324", "bodyText": "No need to make these synchronized. OutputStream is not thread safe and thus is expected to be used from a single thread (or used with external synchronization).", "author": "electrum", "createdAt": "2020-06-24T18:19:00Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NDQwMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445174402", "bodyText": "We should implement write(byte[], int, int) the variant for performance.", "author": "electrum", "createdAt": "2020-06-24T21:11:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NDMyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NDkxMA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445174910", "bodyText": "We avoid using increment in an expression. Make it a separate statement:\nbuf[count] = (byte) b;\ncount++;", "author": "electrum", "createdAt": "2020-06-24T21:12:44Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NTM3OQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445175379", "bodyText": "Make this\ncurrentPartNumber++;\nThen we can use it below.", "author": "electrum", "createdAt": "2020-06-24T21:13:46Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NjIxMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445176212", "bodyText": "Let's format this like\nList<PartETag> etags =  result.stream()\n        .map(UploadPartResult::getPartETag)\n        .collect(toList());\n\nCompleteMultipartUploadResult result = s3.completeMultipartUpload(\n        new CompleteMultipartUploadRequest(bucketName, key, uploadId, etags));", "author": "electrum", "createdAt": "2020-06-24T21:15:40Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))\n+                    .withPartSize(data.length);\n+\n+            return s3.uploadPart(uploadRequest);\n+        }\n+\n+        private CompleteMultipartUploadResult finishUpload(List<UploadPartResult> result)\n+        {\n+            CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3ODk5OA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445178998", "bodyText": "This will only a single upload for the entire server. We can use a cached pool since we will use exactly one thread per writer. We should also name the threads:\nprivate final ListeningExecutorService uploadExecutor = listeningDecorator(newCachedThreadPool(\"s3-upload-%s));", "author": "electrum", "createdAt": "2020-06-24T21:21:51Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -202,8 +220,11 @@\n     private PrestoS3AclType s3AclType;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean streamingUploadEnabled;\n     private PrestoS3StorageClass s3StorageClass;\n \n+    private final ListeningExecutorService uploadExecutor = listeningDecorator(newSingleThreadExecutor());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3OTUwMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445179502", "bodyText": "Change to a single future and wait for it if it exists (ensuring we only have one outstanding upload)", "author": "electrum", "createdAt": "2020-06-24T21:22:58Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MDExOQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445180119", "bodyText": "No need to copy the buffer. Make it non-final and create a new array for the next batch.", "author": "electrum", "createdAt": "2020-06-24T21:24:14Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MDM0Mw==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445180343", "bodyText": "No need to wrap this or the logging", "author": "electrum", "createdAt": "2020-06-24T21:24:43Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))\n+                    .withPartSize(data.length);\n+\n+            return s3.uploadPart(uploadRequest);\n+        }\n+\n+        private CompleteMultipartUploadResult finishUpload(List<UploadPartResult> result)\n+        {\n+            CompleteMultipartUploadRequest request = new CompleteMultipartUploadRequest(\n+                    bucketName,\n+                    key,\n+                    uploadId,\n+                    result.stream().map(UploadPartResult::getPartETag).collect(toList()));\n+\n+            CompleteMultipartUploadResult response = s3.completeMultipartUpload(request);\n+\n+            STATS.uploadSuccessful();\n+\n+            return response;\n+        }\n+\n+        private void abortUpload(boolean silent)\n+                throws IOException\n+        {\n+            STATS.uploadFailed();\n+\n+            try {\n+                s3.abortMultipartUpload(new AbortMultipartUploadRequest(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNDk3OA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445224978", "bodyText": "We should add the MD5 so that S3 can verify the integrity of the upload\n.withMD5Digest(getMd5AsBase64(data))\nprivate static String getMd5AsBase64(byte[] data)\n{\n    @SuppressWarnings(\"deprecation\")\n    byte[] md5 = md5().hashBytes(data).asBytes();\n    return Base64.getEncoder().encodeToString(md5);\n}", "author": "electrum", "createdAt": "2020-06-24T23:25:09Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);\n+\n+            if (!uploadFutures.isEmpty()) {\n+                ListenableFuture<List<UploadPartResult>> partsFuture = allAsList(uploadFutures);\n+                ListenableFuture<?> finishFuture = transform(partsFuture, this::finishUpload, uploadExecutor);\n+\n+                try {\n+                    // Sync call to block presto from doing any other thing before the file is uploaded.\n+                    finishFuture.get();\n+                }\n+                catch (InterruptedException e) {\n+                    abortUpload(true);\n+                    Thread.currentThread().interrupt();\n+                    throw new InterruptedIOException();\n+                }\n+                catch (ExecutionException e) {\n+                    abortUpload(true);\n+                    throw new IOException(e.getCause());\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = Arrays.copyOf(buf, count);\n+                ListenableFuture<UploadPartResult> future = uploadExecutor.submit(\n+                        () -> uploadPage(data));\n+                uploadFutures.add(future);\n+                count = 0;\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data)\n+        {\n+            int partNumber = ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(partNumber)\n+                    .withInputStream(new ByteArrayInputStream(data))", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNjEyNw==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r445226127", "bodyText": "If this throws, we won't clean up the upload.", "author": "electrum", "createdAt": "2020-06-24T23:28:41Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1330,6 +1391,153 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final byte[] buf;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private int count;\n+        private final List<ListenableFuture<UploadPartResult>> uploadFutures;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = s3;\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.uploadFutures = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public synchronized void write(int b)\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+            buf[count++] = (byte) b;\n+        }\n+\n+        @Override\n+        public synchronized void flush()\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            flushBuffer(true);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMTQ0Ng==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533021446", "bodyText": "I'm thinking we should make this larger by default, say 16MB or 32MB, since this controls the maximum file size (part size * 10,000 max parts). Using 16MB would increase the max from 50GB to 160GB, while still being acceptable from a memory standpoint (writers can use substantial memory for buffers).", "author": "electrum", "createdAt": "2020-12-01T01:54:51Z", "path": "presto-docs/src/main/sphinx/connector/hive-s3.rst", "diffHunk": "@@ -81,6 +81,11 @@ Property Name                                Description\n ``hive.s3.skip-glacier-objects``             Ignore Glacier objects rather than failing the query. This\n                                              skips data that may be expected to be part of the table\n                                              or partition. Defaults to ``false``.\n+\n+``hive.s3.streaming.enabled``                Use S3 multipart upload API to upload file in streaming way,\n+                                             without staging file to be created in the local file system.\n+\n+``hive.s3.streaming.part-size``              The part size for S3 streaming upload. Defaults to ``5MB``.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyMzIwMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533023202", "bodyText": "Nit: move this method below, to be directly before PrestoS3InputStream. The methods in this section are related to credentials and creating the S3 client.", "author": "electrum", "createdAt": "2020-12-01T02:00:22Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -835,6 +869,34 @@ private AmazonS3 createAmazonS3Client(Configuration hadoopConfig, ClientConfigur\n         return clientBuilder.build();\n     }\n \n+    private InitiateMultipartUploadResult initMultipartUpload(String bucket, String key)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDczMA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024730", "bodyText": "Nit: we avoid abbreviations. Name this buffer", "author": "electrum", "createdAt": "2020-12-01T02:05:09Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDg2Mg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024862", "bodyText": "This initialization can move to the field declaration", "author": "electrum", "createdAt": "2020-12-01T02:05:33Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNDk5Mg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533024992", "bodyText": "Add requireNonNull for all of these\n(this is a private class, so not strictly necessary, but best to be consistent)", "author": "electrum", "createdAt": "2020-12-01T02:06:01Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNTg0MA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533025840", "bodyText": "Shorten this name to minPartSize and change to int", "author": "electrum", "createdAt": "2020-12-01T02:08:56Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjA1OA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026058", "bodyText": "You can remove this cast and inline this after changing the parameter to an int", "author": "electrum", "createdAt": "2020-12-01T02:09:33Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjM1Ng==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026356", "bodyText": "Change streamingUploadPartSize to be an int and use Math.toIntExact()\nthis.streamingUploadPartSize = toIntExact(conf.getLong(S3_STREAMING_UPLOAD_PART_SIZE, defaults.getS3StreamingPartSize().toBytes()));", "author": "electrum", "createdAt": "2020-12-01T02:10:29Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -254,6 +278,8 @@ public void initialize(URI uri, Configuration conf)\n         String userAgentPrefix = conf.get(S3_USER_AGENT_PREFIX, defaults.getS3UserAgentPrefix());\n         this.skipGlacierObjects = conf.getBoolean(S3_SKIP_GLACIER_OBJECTS, defaults.isSkipGlacierObjects());\n         this.requesterPaysEnabled = conf.getBoolean(S3_REQUESTER_PAYS_ENABLED, defaults.isRequesterPaysEnabled());\n+        this.streamingUploadEnabled = conf.getBoolean(S3_STREAMING_UPLOAD_ENABLED, defaults.isS3StreamingUploadEnabled());\n+        this.streamingUploadPartSize = conf.getLong(S3_STREAMING_UPLOAD_PART_SIZE, defaults.getS3StreamingPartSize().toBytes());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjU3MA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533026570", "bodyText": "Let's add a max size\n@MaxDataSize(\"256MB\")", "author": "electrum", "createdAt": "2020-12-01T02:11:08Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/HiveS3Config.java", "diffHunk": "@@ -462,4 +464,31 @@ public HiveS3Config setRequesterPaysEnabled(boolean requesterPaysEnabled)\n         this.requesterPaysEnabled = requesterPaysEnabled;\n         return this;\n     }\n+\n+    public boolean isS3StreamingUploadEnabled()\n+    {\n+        return s3StreamingUploadEnabled;\n+    }\n+\n+    @Config(\"hive.s3.streaming.enabled\")\n+    public HiveS3Config setS3StreamingUploadEnabled(boolean s3StreamingUploadEnabled)\n+    {\n+        this.s3StreamingUploadEnabled = s3StreamingUploadEnabled;\n+        return this;\n+    }\n+\n+    @NotNull\n+    @MinDataSize(\"5MB\")", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNzE3MA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533027170", "bodyText": "Let's call this bufferSize so that it's paired with buffer", "author": "electrum", "createdAt": "2020-12-01T02:12:54Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNzkwNg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533027906", "bodyText": "Let's name the parameters offset and length, then remove these local variables. It's confusing to have a copy of the input parameters (I had to check if the originals were also used below)", "author": "electrum", "createdAt": "2020-12-01T02:15:14Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDI2Mg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533030262", "bodyText": "This can unconditionally call flushBuffer(false) since that method checks if the buffer is full -- no need to check here", "author": "electrum", "createdAt": "2020-12-01T02:23:01Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDk5NA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533030994", "bodyText": "I think we can simplify this as follows:\nwhile (remain > 0) {\n    int copied = min(buf.length - count, remain);\n    ...\n}\nSince flushBuffer(false) will flush if the buffer is full, there's no need to avoid calling it when not full, or for the check/copy after the loop. Each loop iteration will copy the least of the available buffer space or the remaining bytes.", "author": "electrum", "createdAt": "2020-12-01T02:25:25Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTk2OQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533031969", "bodyText": "This can catch RuntimeException since finishUpload() doesn't throw checked exceptions", "author": "electrum", "createdAt": "2020-12-01T02:28:30Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMjE2OQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533032169", "bodyText": "This probably needs to catch IOException | RuntimeException so that we always cleanup", "author": "electrum", "createdAt": "2020-12-01T02:29:12Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMjk3NA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533032974", "bodyText": "I think we can make this just ExecutorService (and remove listeningDecorator) since we only call get() on the future and don't use the listening part.", "author": "electrum", "createdAt": "2020-12-01T02:31:37Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -210,8 +230,12 @@\n     private PrestoS3AclType s3AclType;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean streamingUploadEnabled;\n+    private long streamingUploadPartSize;\n     private PrestoS3StorageClass s3StorageClass;\n \n+    private final ListeningExecutorService uploadExecutor = listeningDecorator(newCachedThreadPool(threadsNamed(\"s3-upload-%s\")));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMzE1NA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533033154", "bodyText": "Nit: use post increment\ncurrentPartNumber++;", "author": "electrum", "createdAt": "2020-12-01T02:32:17Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data, int length)\n+        {\n+            ++currentPartNumber;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMzcyMg==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533033722", "bodyText": "It's better to throw e here so that we retain the full stack trace. While the ExecutionException is not really relevant to the underlying exception, it contains the Future.get() call in the stack. If you discard it, you see a stack trace from a different thread, which can be confusing.\nSee https://github.com/google/guava/wiki/Why-we-deprecated-Throwables.propagate#encourages-unwrapping-exceptions-from-other-threads which changed my opinion on this", "author": "electrum", "createdAt": "2020-12-01T02:34:23Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzNTAzOA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533035038", "bodyText": "Nit: no need to wrap here", "author": "electrum", "createdAt": "2020-12-01T02:38:27Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/PrestoS3FileSystem.java", "diffHunk": "@@ -1400,6 +1462,195 @@ public synchronized void progressChanged(ProgressEvent progressEvent)\n         }\n     }\n \n+    private static class PrestoS3StreamingOutputStream\n+            extends OutputStream\n+    {\n+        private final AmazonS3 s3;\n+        private final String bucketName;\n+        private final String key;\n+        private final String uploadId;\n+\n+        private final ListeningExecutorService uploadExecutor;\n+\n+        private int currentPartNumber;\n+        private byte[] buf;\n+        private int count;\n+\n+        private ListenableFuture<UploadPartResult> inProgressUploadFuture;\n+        private final List<UploadPartResult> parts;\n+\n+        public PrestoS3StreamingOutputStream(\n+                AmazonS3 s3,\n+                String bucketName,\n+                String key,\n+                String uploadId,\n+                ListeningExecutorService uploadExecutor,\n+                long multiPartUploadMinPartSize)\n+        {\n+            STATS.uploadStarted();\n+\n+            this.s3 = requireNonNull(s3, \"s3 is null\");\n+\n+            int bufferSize = (int) multiPartUploadMinPartSize;\n+            this.buf = new byte[bufferSize];\n+\n+            this.bucketName = bucketName;\n+            this.key = key;\n+            this.uploadId = uploadId;\n+\n+            this.uploadExecutor = uploadExecutor;\n+\n+            this.parts = new ArrayList<>();\n+        }\n+\n+        @Override\n+        public void write(int b)\n+                throws IOException\n+        {\n+            if (count >= buf.length) {\n+                flushBuffer(false);\n+            }\n+\n+            buf[count] = (byte) b;\n+            count++;\n+        }\n+\n+        @Override\n+        public void write(byte[] bytes, int off, int len)\n+                throws IOException\n+        {\n+            int offset = off;\n+            int remain = len;\n+            while ((buf.length - count) <= remain) {\n+                int copied = buf.length - count;\n+                arraycopy(bytes, offset, buf, count, copied);\n+                count += copied;\n+\n+                flushBuffer(false);\n+\n+                offset += copied;\n+                remain -= copied;\n+            }\n+\n+            if (remain > 0) {\n+                arraycopy(bytes, offset, buf, count, remain);\n+                count += remain;\n+            }\n+        }\n+\n+        @Override\n+        public void flush()\n+                throws IOException\n+        {\n+            flushBuffer(false);\n+        }\n+\n+        @Override\n+        public void close()\n+                throws IOException\n+        {\n+            try {\n+                flushBuffer(true);\n+                waitForPreviousUploadFinish();\n+            }\n+            catch (IOException e) {\n+                abortUpload(true);\n+                throw e;\n+            }\n+\n+            if (!parts.isEmpty()) {\n+                try {\n+                    finishUpload(parts);\n+                }\n+                catch (Exception e) {\n+                    abortUpload(true);\n+                    throw new IOException(e);\n+                }\n+            }\n+            else {\n+                abortUpload(false);\n+            }\n+        }\n+\n+        private void flushBuffer(boolean finished)\n+                throws IOException\n+        {\n+            // The multipart upload API only accept the last part to be less than 5MB\n+            if (count == buf.length || (finished && count > 0)) {\n+                byte[] data = buf;\n+                int length = count;\n+                this.buf = new byte[buf.length];\n+                count = 0;\n+\n+                waitForPreviousUploadFinish();\n+\n+                inProgressUploadFuture = uploadExecutor.submit(() -> uploadPage(data, length));\n+            }\n+        }\n+\n+        private void waitForPreviousUploadFinish()\n+                throws IOException\n+        {\n+            if (inProgressUploadFuture == null) {\n+                return;\n+            }\n+\n+            try {\n+                inProgressUploadFuture.get();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new InterruptedIOException();\n+            }\n+            catch (ExecutionException e) {\n+                throw new IOException(\"Streaming upload failed\", e.getCause());\n+            }\n+        }\n+\n+        private UploadPartResult uploadPage(byte[] data, int length)\n+        {\n+            ++currentPartNumber;\n+            UploadPartRequest uploadRequest = new UploadPartRequest()\n+                    .withBucketName(bucketName)\n+                    .withKey(key)\n+                    .withUploadId(uploadId)\n+                    .withPartNumber(currentPartNumber)\n+                    .withInputStream(new ByteArrayInputStream(data, 0, length))\n+                    .withPartSize(length)\n+                    .withMD5Digest(getMd5AsBase64(data, 0, length));\n+\n+            UploadPartResult partResult = s3.uploadPart(uploadRequest);\n+            parts.add(partResult);\n+            return partResult;\n+        }\n+\n+        private void finishUpload(List<UploadPartResult> result)\n+        {\n+            List<PartETag> etags = result.stream()\n+                    .map(UploadPartResult::getPartETag)\n+                    .collect(toList());\n+            s3.completeMultipartUpload(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzODU0NA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r533038544", "bodyText": "Should we enable this by default? Are you using this change in production already?", "author": "electrum", "createdAt": "2020-12-01T02:49:46Z", "path": "presto-hive/src/main/java/io/prestosql/plugin/hive/s3/HiveS3Config.java", "diffHunk": "@@ -65,6 +65,8 @@\n     private PrestoS3AclType s3AclType = PrestoS3AclType.PRIVATE;\n     private boolean skipGlacierObjects;\n     private boolean requesterPaysEnabled;\n+    private boolean s3StreamingUploadEnabled;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDYxMzcyMw==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r534613723", "bodyText": "I prefer not enable this feature by default.\nWe are using this feature in production already, but we are using it without large loading.\nAnd there is concern that user should create a AbortIncompleteMultipartUpload policy, so they don't get charged if any upload is neither uploaded successfully nor never aborted (might caused by server crashed during upload).\nOur company create the policy automatically when the S3FileSystem was initialized.", "author": "chhsiao90", "createdAt": "2020-12-03T02:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzODU0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4OTQzMQ==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r535689431", "bodyText": "We already use multipart uploads via TransferManager, so the policy would be needed in either case. It's unfortunate that they don't have a default policy that aborts after some number of days. It seems ridiculous that the default is to keep them around forever.\nI'm ok with not enabling by default for now. We can do that in a future release, after we have confirmation that it is working well for multiple users.\nOne thing I just realized is that we have hive.s3.multipart.min-file-size for the existing behavior. We could add similar but simpler logic for the streaming approach to not use multipart if there would only be one part (i.e. if we are flushing when finished and there are no parts yet). This can be a follow up.", "author": "electrum", "createdAt": "2020-12-03T22:27:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzODU0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTgwNTA0NA==", "url": "https://github.com/trinodb/trino/pull/3712#discussion_r535805044", "bodyText": "I created a PR to implement this: #6201\nIt also improves the abort logic. I realized your original implementation of aborting immediately was good:\n\nensure we cleanup even if the output stream is not closed for some reason (bugs elsewhere)\nsince close() is called after a failure, we need to ensure that we don't finish the upload\n\nI added a failed flag to track the fact that a previous part upload failed, since the abort might fail but the finish could succeed.", "author": "electrum", "createdAt": "2020-12-04T03:16:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzODU0NA=="}], "type": "inlineReview"}, {"oid": "d2baed788647c0be4eaf8b535a77c32f199462cb", "url": "https://github.com/trinodb/trino/commit/d2baed788647c0be4eaf8b535a77c32f199462cb", "message": "Implement s3 streaming upload", "committedDate": "2020-12-03T03:08:17Z", "type": "commit"}]}