{"pr_number": 4294, "pr_title": "Relax task locking in SourcePartitionedScheduler", "pr_createdAt": "2020-07-01T11:50:45Z", "pr_url": "https://github.com/trinodb/trino/pull/4294", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQzODkyOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451438929", "bodyText": "commit message is too long", "author": "sopel39", "createdAt": "2020-07-08T10:22:19Z", "path": "presto-main/src/main/java/io/prestosql/execution/TaskStatus.java", "diffHunk": "@@ -247,6 +247,11 @@ public String toString()\n     }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ0MDQzMg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451440432", "bodyText": "Move this method to TestSourcePartitionedScheduler as this method is used only in tests. Rename this method to taskStatusWithOutputBufferUtilization as it's odd to have initial task status with overutilized buffer.", "author": "sopel39", "createdAt": "2020-07-08T10:25:11Z", "path": "presto-main/src/main/java/io/prestosql/execution/TaskStatus.java", "diffHunk": "@@ -247,6 +247,11 @@ public String toString()\n     }\n \n     public static TaskStatus initialTaskStatus(TaskId taskId, URI location, String nodeId)\n+    {\n+        return initialTaskStatusWithOutputBufferUtilization(taskId, location, nodeId, false);\n+    }\n+\n+    public static TaskStatus initialTaskStatusWithOutputBufferUtilization(TaskId taskId, URI location, String nodeId, boolean isOutputBufferOverutilized)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ0Njk2MA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451446960", "bodyText": "We don't have to pass AtomicReference explicitly. It's enough to pass Supplier<Collection<TaskStatus>>. AtomicReference can be handled in supplier itself.", "author": "sopel39", "createdAt": "2020-07-08T10:38:26Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -133,9 +140,10 @@ public static StageScheduler newSourcePartitionedSchedulerAsStageScheduler(\n             PlanNodeId partitionedNode,\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n-            int splitBatchSize)\n+            int splitBatchSize,\n+            AtomicReference<Supplier<Collection<TaskStatus>>> childTaskStatusSupplier)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ0ODAxMQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451448011", "bodyText": "A better name might be childStageTaskStatusSupplier", "author": "sopel39", "createdAt": "2020-07-08T10:40:39Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -133,9 +140,10 @@ public static StageScheduler newSourcePartitionedSchedulerAsStageScheduler(\n             PlanNodeId partitionedNode,\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n-            int splitBatchSize)\n+            int splitBatchSize,\n+            AtomicReference<Supplier<Collection<TaskStatus>>> childTaskStatusSupplier)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1MDAxNg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451450016", "bodyText": "We should flip the condition and unblock if any undone task get buffer overutilzed. Otherwise it might happen that n-1 child tasks are blocked and just one is progressing reducing concurrency to 1.\nFor example it might happen that a new build side table scan task was added. However, it only read last 10 remaining rows. It won't be done (as it's in FLUSHING state) nor it will be overutilized. In such case we would get a deadlock as shouldLockdownTasks would be false even though no task could progress.", "author": "sopel39", "createdAt": "2020-07-08T10:44:36Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -357,7 +365,20 @@ else if (pendingSplits.isEmpty()) {\n             // The build side blocks due to a full output buffer.\n             // In the meantime the probe side split cannot be consumed since\n             // builder side hash table construction has not finished.\n-            overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n+            boolean shouldLockdownTasks = true;\n+            if (anyBlockedOnPlacements && !groupedExecution) {\n+                Supplier<Collection<TaskStatus>> childTaskStatus = childTaskStatusSupplier.get();\n+                if (childTaskStatus != null) {\n+                    boolean anyChildTaskOutputBufferOverutilized = childTaskStatus.get().stream()\n+                            .anyMatch(task -> !task.getState().isDone() && task.isOutputBufferOverutilized());\n+                    if (!anyChildTaskOutputBufferOverutilized) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkzMzcwMQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r458933701", "bodyText": "Do you mean that in such cases we should lockdown the tasks? I have tried to do the similar thing itself by setting shouldLockdownTasks to false only if none of the task buffers are over-utilized. Did I miss something?", "author": "rohangarg", "createdAt": "2020-07-22T16:42:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1MDAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1NDQ0MQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451454441", "bodyText": "let's extract groupedExection alltogether:\nif (groupedExecution) {\n  // The fix is to finish task creation when grouped execution is enabled.\n  // This prevents deadlock when in case of NO_ACTIVE_DRIVER_GROUP and\n  // when partitions on the probe side table are small.\n  overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n} else if (anyBlockedOnPlacements) {\n  ...\n}", "author": "sopel39", "createdAt": "2020-07-08T10:53:44Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -357,7 +365,20 @@ else if (pendingSplits.isEmpty()) {\n             // The build side blocks due to a full output buffer.\n             // In the meantime the probe side split cannot be consumed since\n             // builder side hash table construction has not finished.\n-            overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n+            boolean shouldLockdownTasks = true;\n+            if (anyBlockedOnPlacements && !groupedExecution) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1ODIzOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451458239", "bodyText": "task.getState().isDone() seems redundant. Task with FLUSHING state won't be done, so it seems that only finished tasks are done, which don't have any buffer anymore.", "author": "sopel39", "createdAt": "2020-07-08T11:01:09Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -357,7 +365,20 @@ else if (pendingSplits.isEmpty()) {\n             // The build side blocks due to a full output buffer.\n             // In the meantime the probe side split cannot be consumed since\n             // builder side hash table construction has not finished.\n-            overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n+            boolean shouldLockdownTasks = true;\n+            if (anyBlockedOnPlacements && !groupedExecution) {\n+                Supplier<Collection<TaskStatus>> childTaskStatus = childTaskStatusSupplier.get();\n+                if (childTaskStatus != null) {\n+                    boolean anyChildTaskOutputBufferOverutilized = childTaskStatus.get().stream()\n+                            .anyMatch(task -> !task.getState().isDone() && task.isOutputBufferOverutilized());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU1MTg1Mw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r461551853", "bodyText": "ping. Is task.getState().isDone() redundant?", "author": "sopel39", "createdAt": "2020-07-28T12:47:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1ODIzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk4MTQxMA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462981410", "bodyText": "I think it is redundant (we also can get tasks which are in RUNNING state too along with FLUSHING ones).\nI had put it initially to avoid any miscalculations due to incorrect flag setting in finished task - ideally that should never happen. is it ok to assume that for all finished tasks the output buffer should come to be under utilized?", "author": "rohangarg", "createdAt": "2020-07-30T13:06:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1ODIzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk5NTYxNQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462995615", "bodyText": "is it ok to assume that for all finished tasks the output buffer should come to be under utilized?\n\nI think so.", "author": "sopel39", "createdAt": "2020-07-30T13:28:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1ODIzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ1OTk4Nw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451459987", "bodyText": "use toImmutableList", "author": "sopel39", "createdAt": "2020-07-08T11:04:33Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -427,13 +429,14 @@ else if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n \n         stageLinkages.put(stageId, new StageLinkage(plan.getFragment().getId(), parent, childStages));\n \n-        if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n-            Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()\n-                    .map(SqlStageExecution::getAllTasks)\n-                    .flatMap(Collection::stream)\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n+        Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()\n+                .map(SqlStageExecution::getAllTasks)\n+                .flatMap(Collection::stream)\n+                .map(RemoteTask::getTaskStatus)\n+                .collect(toList());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MDg2MA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451460860", "bodyText": "We could avoid AtomicReference at all if child stages were created before SOURCE_DISTRIBUTION parent stage. This can be done because only dependency seems to be bucketToPartition which gets initialized to Optional.of(new int[1]); in case of SOURCE_DISTRIBUTION.\nPerhaps you could extract createChildStages(...) method.", "author": "sopel39", "createdAt": "2020-07-08T11:06:19Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -310,6 +311,7 @@ private static void updateQueryOutputLocations(QueryStateMachine queryStateMachi\n \n         Optional<int[]> bucketToPartition;\n         PartitioningHandle partitioningHandle = plan.getFragment().getPartitioning();\n+        AtomicReference<Supplier<Collection<TaskStatus>>> childTaskStatusSupplier = new AtomicReference<>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MTkwNQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451461905", "bodyText": "make this package private", "author": "sopel39", "createdAt": "2020-07-08T11:08:33Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -75,16 +77,33 @@ public UniformNodeSelectorFactory(\n         checkArgument(maxSplitsPerNode >= maxPendingSplitsPerTask, \"maxSplitsPerNode must be > maxPendingSplitsPerTask\");\n     }\n \n+    @VisibleForTesting\n+    public UniformNodeSelectorFactory(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MzAzNQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451463035", "bodyText": "Let's make this method initialize all fields and the one that has @Inject call this(...).\nLet's make this method accept memoization timeout (with 0 meaning memoization disabled)", "author": "sopel39", "createdAt": "2020-07-08T11:10:56Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -75,16 +77,33 @@ public UniformNodeSelectorFactory(\n         checkArgument(maxSplitsPerNode >= maxPendingSplitsPerTask, \"maxSplitsPerNode must be > maxPendingSplitsPerTask\");\n     }\n \n+    @VisibleForTesting\n+    public UniformNodeSelectorFactory(\n+            InternalNodeManager nodeManager,\n+            NodeSchedulerConfig config,\n+            NodeTaskMap nodeTaskMap,\n+            boolean memoizeNodeMap)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MzQyOA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451463428", "bodyText": "squash this commit", "author": "sopel39", "createdAt": "2020-07-08T11:11:46Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -19,7 +19,6 @@\n import io.prestosql.client.NodeVersion;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxNzAwNQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462917005", "bodyText": "ping", "author": "sopel39", "createdAt": "2020-07-30T10:58:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MzQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MzcyNw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451463727", "bodyText": "add a comment with rationale.\nWhy check for canAddPages. If we can't add pages, it seems even more overutilzed", "author": "sopel39", "createdAt": "2020-07-08T11:12:23Z", "path": "presto-main/src/main/java/io/prestosql/execution/buffer/BroadcastOutputBuffer.java", "diffHunk": "@@ -104,7 +104,7 @@ public double getUtilization()\n     @Override\n     public boolean isOverutilized()\n     {\n-        return memoryManager.isOverutilized();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODkyNjU4MA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r458926580", "bodyText": "I think that canAddPages is different from BUFFER_FULL. Since the setNoMorePages is done only when all the drivers are completed in a task, indicating that the buffer will not get any new pages from now.", "author": "rohangarg", "createdAt": "2020-07-22T16:31:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ2MzcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ3MzEzMA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451473130", "bodyText": "rename to testNewTaskScheduledWhenBufferIsUnderutilized", "author": "sopel39", "createdAt": "2020-07-08T11:31:45Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -406,6 +412,80 @@ public void testBlockCausesFullSchedule()\n         secondStage.abort();\n     }\n \n+    @Test\n+    public void testNewTaskWhenNodeAdded() throws Exception", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTQ3MzQ3Ng==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r451473476", "bodyText": "rename to testNoNewTaskScheduledWhenBufferIsOverutilized", "author": "sopel39", "createdAt": "2020-07-08T11:32:28Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -406,6 +412,80 @@ public void testBlockCausesFullSchedule()\n         secondStage.abort();\n     }\n \n+    @Test\n+    public void testNewTaskWhenNodeAdded() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, false));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(400, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy task with under utilized output buffer\n+        TaskStatus dummyTaskStatus = initialTaskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", false);\n+        StageScheduler scheduler = newSourcePartitionedSchedulerAsStageScheduler(\n+                stage,\n+                Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n+                Iterables.getOnlyElement(plan.getSplitSources().values()),\n+                new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n+                400,\n+                new AtomicReference<>(() -> singletonList(dummyTaskStatus)));\n+\n+        // the queues of 3 running nodes should be full\n+        ScheduleResult scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getNewTasks().size(), 3);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 300);\n+\n+        // new node added - the pending splits should go to it since the child tasks are not blocked\n+        nodeManager.addNode(CONNECTOR_ID, new InternalNode(\"other4\", URI.create(\"http://127.0.0.4:14\"), NodeVersion.UNKNOWN, false));\n+        scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getNewTasks().size(), 1);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 100);\n+    }\n+\n+    @Test\n+    public void testNoNewTaskWhenNodeAdded() throws Exception", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU1NzMyMQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r461557321", "bodyText": "could we have 1 underutilized and 1 overutilized task?", "author": "sopel39", "createdAt": "2020-07-28T12:56:24Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -406,6 +412,80 @@ public void testBlockCausesFullSchedule()\n         secondStage.abort();\n     }\n \n+    @Test\n+    public void testNewTaskScheduledWhenChildStageBufferIsUnderutilized() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, 0));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(400, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy task with under utilized output buffer\n+        TaskStatus dummyTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", false);\n+        StageScheduler scheduler = newSourcePartitionedSchedulerAsStageScheduler(\n+                stage,\n+                Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n+                Iterables.getOnlyElement(plan.getSplitSources().values()),\n+                new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n+                400,\n+                () -> singletonList(dummyTaskStatus));\n+\n+        // the queues of 3 running nodes should be full\n+        ScheduleResult scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getNewTasks().size(), 3);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 300);\n+\n+        // new node added - the pending splits should go to it since the child tasks are not blocked\n+        nodeManager.addNode(CONNECTOR_ID, new InternalNode(\"other4\", URI.create(\"http://127.0.0.4:14\"), NodeVersion.UNKNOWN, false));\n+        scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getNewTasks().size(), 1);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 100);\n+    }\n+\n+    @Test\n+    public void testNoNewTaskScheduledWhenChildStageBufferIsOverutilized() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, 0));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(400, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy task with over utilized output buffer\n+        TaskStatus dummyTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", true);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxNzA3Nw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462917077", "bodyText": "ping", "author": "sopel39", "createdAt": "2020-07-30T10:58:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU1NzMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU2NzE2MA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r461567160", "bodyText": "remove this check", "author": "sopel39", "createdAt": "2020-07-28T13:11:40Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,32 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            boolean shouldLockdownTasks = true;\n+            if (sourceTasksProvider != null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxNjkwOA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462916908", "bodyText": "ping", "author": "sopel39", "createdAt": "2020-07-30T10:57:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU2NzE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk3Nzg5Mg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462977892", "bodyText": "the check is still needed because in FixedSourcePartitionedScheduler we set the supplier as null currently.", "author": "rohangarg", "createdAt": "2020-07-30T13:00:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTU2NzE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxNzY1NQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462917655", "bodyText": "rename to sourceTasksSupplier", "author": "sopel39", "createdAt": "2020-07-30T10:59:19Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -90,6 +93,7 @@\n     private final int splitBatchSize;\n     private final PlanNodeId partitionedNode;\n     private final boolean groupedExecution;\n+    private final Supplier<Collection<TaskStatus>> sourceTasksProvider;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxNzcyNA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462917724", "bodyText": "rename to sourceTasksSupplier", "author": "sopel39", "createdAt": "2020-07-30T10:59:29Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -103,12 +107,14 @@ private SourcePartitionedScheduler(\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n             int splitBatchSize,\n-            boolean groupedExecution)\n+            boolean groupedExecution,\n+            Supplier<Collection<TaskStatus>> sourceTasksProvider)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxODIyOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462918229", "bodyText": "change to:\nthis.sourceTasksSupplier = requireNonNull(sourceTasksSupplier, \"sourceTasksSupplier is null\");", "author": "sopel39", "createdAt": "2020-07-30T11:00:30Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -103,12 +107,14 @@ private SourcePartitionedScheduler(\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n             int splitBatchSize,\n-            boolean groupedExecution)\n+            boolean groupedExecution,\n+            Supplier<Collection<TaskStatus>> sourceTasksProvider)\n     {\n         this.stage = requireNonNull(stage, \"stage is null\");\n         this.partitionedNode = requireNonNull(partitionedNode, \"partitionedNode is null\");\n         this.splitSource = requireNonNull(splitSource, \"splitSource is null\");\n         this.splitPlacementPolicy = requireNonNull(splitPlacementPolicy, \"splitPlacementPolicy is null\");\n+        this.sourceTasksProvider = sourceTasksProvider;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxODMzOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462918339", "bodyText": "rename to sourceTasksSupplier", "author": "sopel39", "createdAt": "2020-07-30T11:00:48Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -133,9 +139,16 @@ public static StageScheduler newSourcePartitionedSchedulerAsStageScheduler(\n             PlanNodeId partitionedNode,\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n-            int splitBatchSize)\n+            int splitBatchSize,\n+            Supplier<Collection<TaskStatus>> sourceTasksProvider)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxODk0Mw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462918943", "bodyText": "use () -> ImmutableList.of() instead of null", "author": "sopel39", "createdAt": "2020-07-30T11:02:03Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -176,7 +189,7 @@ public static SourceScheduler newSourcePartitionedSchedulerAsSourceScheduler(\n             int splitBatchSize,\n             boolean groupedExecution)\n     {\n-        return new SourcePartitionedScheduler(stage, partitionedNode, splitSource, splitPlacementPolicy, splitBatchSize, groupedExecution);\n+        return new SourcePartitionedScheduler(stage, partitionedNode, splitSource, splitPlacementPolicy, splitBatchSize, groupedExecution, null);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkyNTMzMg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462925332", "bodyText": "please simplify:\n // EXPLANATION COMMENT\n boolean shouldLockdownTasks = sourceTasksProvider.get().stream()\n  .anyMatch(TaskStatus::isOutputBufferOverutilized());", "author": "sopel39", "createdAt": "2020-07-30T11:15:54Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,32 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            boolean shouldLockdownTasks = true;\n+            if (sourceTasksProvider != null) {\n+                boolean anySourceTaskOutputBufferOverutilized = sourceTasksProvider.get().stream()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkyNzU4OQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462927589", "bodyText": "just move that to separate function:\nSet<SqlStageExecution> createChildStages(Optional<int[]> bucketToPartition, ...)\n\npreferably as a separate commit", "author": "sopel39", "createdAt": "2020-07-30T11:20:47Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -305,10 +304,37 @@ private static void updateQueryOutputLocations(QueryStateMachine queryStateMachi\n                 queryExecutor,\n                 failureDetector,\n                 schedulerStats);\n-\n         stages.add(stage);\n \n-        Optional<int[]> bucketToPartition;\n+        // function to create child stages recursively by supplying the bucket partitioning (according to parent's partitioning)\n+        Function<Optional<int[]>, Set<SqlStageExecution>> getChildStages = bucketToPartition -> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkzMDU4OA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462930588", "bodyText": "extract this to separate method:\nprivate static Collection<TaskStatus> getChildTaskStatuses(Set<SqlStageExecution> childStages) {\n  return childStages.stream()\n    .map(SqlStageExecution::getAllTasks)\n    .flatMap(Collection::stream)\n    .map(RemoteTask::getTaskStatus)\n    .collect(toImmutableList());\n}", "author": "sopel39", "createdAt": "2020-07-30T11:27:02Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -321,13 +347,39 @@ private static void updateQueryOutputLocations(QueryStateMachine queryStateMachi\n             SplitPlacementPolicy placementPolicy = new DynamicSplitPlacementPolicy(nodeSelector, stage::getAllTasks);\n \n             checkArgument(!plan.getFragment().getStageExecutionDescriptor().isStageGroupedExecution());\n-            stageSchedulers.put(stageId, newSourcePartitionedSchedulerAsStageScheduler(stage, planNodeId, splitSource, placementPolicy, splitBatchSize));\n-            bucketToPartition = Optional.of(new int[1]);\n+\n+            childStages = getChildStages.apply(Optional.of(new int[1]));\n+            Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk4ODMxNw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462988317", "bodyText": "use airlift Duration instead", "author": "sopel39", "createdAt": "2020-07-30T13:17:16Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -54,12 +55,23 @@\n     private final int maxPendingSplitsPerTask;\n     private final boolean optimizedLocalScheduling;\n     private final NodeTaskMap nodeTaskMap;\n+    private final int nodeMapMemoizationTime;\n \n     @Inject\n     public UniformNodeSelectorFactory(\n             InternalNodeManager nodeManager,\n             NodeSchedulerConfig config,\n             NodeTaskMap nodeTaskMap)\n+    {\n+        this(nodeManager, config, nodeTaskMap, 5);\n+    }\n+\n+    @VisibleForTesting\n+    UniformNodeSelectorFactory(\n+            InternalNodeManager nodeManager,\n+            NodeSchedulerConfig config,\n+            NodeTaskMap nodeTaskMap,\n+            int nodeMapMemoizationTime)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk4ODk5Nw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462988997", "bodyText": "use () -> ImmutableList.of() instead", "author": "sopel39", "createdAt": "2020-07-30T13:18:23Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -318,7 +323,8 @@ public void testNoNodes()\n                     Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n                     Iterables.getOnlyElement(plan.getSplitSources().values()),\n                     new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n-                    2);\n+                    2,\n+                    null);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjk5OTk4NA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462999984", "bodyText": "let's rename it to createChildStages", "author": "sopel39", "createdAt": "2020-07-30T13:35:05Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -305,10 +304,37 @@ private static void updateQueryOutputLocations(QueryStateMachine queryStateMachi\n                 queryExecutor,\n                 failureDetector,\n                 schedulerStats);\n-\n         stages.add(stage);\n \n-        Optional<int[]> bucketToPartition;\n+        // function to create child stages recursively by supplying the bucket partitioning (according to parent's partitioning)\n+        Function<Optional<int[]>, Set<SqlStageExecution>> getChildStages = bucketToPartition -> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0OTQ2NA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462649464", "bodyText": "I would place the comment above the statement.", "author": "dain", "createdAt": "2020-07-29T23:36:52Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,32 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            boolean shouldLockdownTasks = true;\n+            if (sourceTasksProvider != null) {\n+                boolean anySourceTaskOutputBufferOverutilized = sourceTasksProvider.get().stream()\n+                        .anyMatch(task -> !task.getState().isDone() && task.isOutputBufferOverutilized());\n+                if (!anySourceTaskOutputBufferOverutilized) {\n+                    shouldLockdownTasks = false; // the child stage is not blocked right now. can add more tasks to the stage", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0OTg5Mg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462649892", "bodyText": "What if there are only done tasks?", "author": "dain", "createdAt": "2020-07-29T23:38:21Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,32 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            boolean shouldLockdownTasks = true;\n+            if (sourceTasksProvider != null) {\n+                boolean anySourceTaskOutputBufferOverutilized = sourceTasksProvider.get().stream()\n+                        .anyMatch(task -> !task.getState().isDone() && task.isOutputBufferOverutilized());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA5NTY2OA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465095668", "bodyText": "moved the change to only task.isOutputBufferOverutilized().\nIf there are only finished source tasks, then we'll not not block the current task generation.", "author": "rohangarg", "createdAt": "2020-08-04T14:31:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY0OTg5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjY1MDc5MQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r462650791", "bodyText": "Variable should include time unit in the name or be a duration", "author": "dain", "createdAt": "2020-07-29T23:41:14Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -54,12 +55,23 @@\n     private final int maxPendingSplitsPerTask;\n     private final boolean optimizedLocalScheduling;\n     private final NodeTaskMap nodeTaskMap;\n+    private final int nodeMapMemoizationTime;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEwOTY2OQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r464109669", "bodyText": "Instead of exposing this code to the complexity of the status of the supplier tasks, it would be easier if we encapsulated this call into Predicate.", "author": "dain", "createdAt": "2020-08-02T18:39:35Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,28 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            // Lock task generation in current stage if any of the source tasks' output buffer is overutilized.\n+            // It helps to avoid potential indefinite blocking of the output buffer\n+            boolean shouldLockdownTasks = sourceTasksSupplier.get().stream()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA5ODEzMA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465098130", "bodyText": "moved to Supplier<Boolean> anySourceTaskBlocked", "author": "rohangarg", "createdAt": "2020-08-04T14:35:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEwOTY2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDEwOTg3OA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r464109878", "bodyText": "Can we move this Function to a method?", "author": "dain", "createdAt": "2020-08-02T18:41:29Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -306,10 +305,37 @@ private static void updateQueryOutputLocations(QueryStateMachine queryStateMachi\n                 queryExecutor,\n                 failureDetector,\n                 schedulerStats);\n-\n         stages.add(stage);\n \n-        Optional<int[]> bucketToPartition;\n+        // function to create child stages recursively by supplying the bucket partitioning (according to parent's partitioning)\n+        Function<Optional<int[]>, Set<SqlStageExecution>> createChildStages = bucketToPartition -> {\n+            ImmutableSet.Builder<SqlStageExecution> childStagesBuilder = ImmutableSet.builder();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY4MzExMg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465683112", "bodyText": "fix formatting each arg in newline", "author": "sopel39", "createdAt": "2020-08-05T12:15:57Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -133,9 +137,16 @@ public static StageScheduler newSourcePartitionedSchedulerAsStageScheduler(\n             PlanNodeId partitionedNode,\n             SplitSource splitSource,\n             SplitPlacementPolicy splitPlacementPolicy,\n-            int splitBatchSize)\n+            int splitBatchSize,\n+            Supplier<Boolean> anySourceTaskBlocked)\n     {\n-        SourcePartitionedScheduler sourcePartitionedScheduler = new SourcePartitionedScheduler(stage, partitionedNode, splitSource, splitPlacementPolicy, splitBatchSize, false);\n+        SourcePartitionedScheduler sourcePartitionedScheduler = new SourcePartitionedScheduler(stage,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0MTAzMw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465741033", "bodyText": "fixed", "author": "rohangarg", "createdAt": "2020-08-05T13:50:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY4MzExMg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczMTEyNg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465731126", "bodyText": "use java.util.function.BooleanSupplier instead", "author": "sopel39", "createdAt": "2020-08-05T13:36:00Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -90,6 +91,7 @@\n     private final int splitBatchSize;\n     private final PlanNodeId partitionedNode;\n     private final boolean groupedExecution;\n+    private final Supplier<Boolean> anySourceTaskBlocked;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczMTc0Mw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465731743", "bodyText": "make it accept Supplier<Boolean> anySourceTaskBlocked and move TODO to FixedSourcePartitionedScheduler", "author": "sopel39", "createdAt": "2020-08-05T13:36:54Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -176,7 +187,9 @@ public static SourceScheduler newSourcePartitionedSchedulerAsSourceScheduler(\n             int splitBatchSize,\n             boolean groupedExecution)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczMjIxOA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465732218", "bodyText": "please add an issue to prestosql and reference it from TODO", "author": "sopel39", "createdAt": "2020-08-05T13:37:42Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -176,7 +187,9 @@ public static SourceScheduler newSourcePartitionedSchedulerAsSourceScheduler(\n             int splitBatchSize,\n             boolean groupedExecution)\n     {\n-        return new SourcePartitionedScheduler(stage, partitionedNode, splitSource, splitPlacementPolicy, splitBatchSize, groupedExecution);\n+        // TODO : change anySourceTaskBlocked to accommodate the correct blocked status of source tasks", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNDA3OQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465734079", "bodyText": "move this large comment into\nelse if (anyBlockedOnPlacements) {\n  // COMMENT HERE", "author": "sopel39", "createdAt": "2020-08-05T13:40:29Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,26 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNDM4Mg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465734382", "bodyText": "change it to:\nelse if (anyBlockedOnPlacements && anySourceTaskBlocked.get()) {\n  overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n}", "author": "sopel39", "createdAt": "2020-08-05T13:40:56Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,26 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNTA5Mg==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465735092", "bodyText": "this comment is redundant to large comment above", "author": "sopel39", "createdAt": "2020-08-05T13:41:59Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SourcePartitionedScheduler.java", "diffHunk": "@@ -346,19 +359,26 @@ else if (pendingSplits.isEmpty()) {\n             return new ScheduleResult(false, overallNewTasks.build(), overallSplitAssignmentCount);\n         }\n \n-        if (anyBlockedOnPlacements || groupedExecution) {\n-            // In a broadcast join, output buffers of the tasks in build source stage have to\n-            // hold onto all data produced before probe side task scheduling finishes,\n-            // even if the data is acknowledged by all known consumers. This is because\n-            // new consumers may be added until the probe side task scheduling finishes.\n-            //\n-            // As a result, the following line is necessary to prevent deadlock\n-            // due to neither build nor probe can make any progress.\n-            // The build side blocks due to a full output buffer.\n-            // In the meantime the probe side split cannot be consumed since\n-            // builder side hash table construction has not finished.\n+        // In a broadcast join, output buffers of the tasks in build source stage have to\n+        // hold onto all data produced before probe side task scheduling finishes,\n+        // even if the data is acknowledged by all known consumers. This is because\n+        // new consumers may be added until the probe side task scheduling finishes.\n+        //\n+        // As a result, the following line is necessary to prevent deadlock\n+        // due to neither build nor probe can make any progress.\n+        // The build side blocks due to a full output buffer.\n+        // In the meantime the probe side split cannot be consumed since\n+        // builder side hash table construction has not finished.\n+        if (groupedExecution) {\n             overallNewTasks.addAll(finalizeTaskCreationIfNecessary());\n         }\n+        else if (anyBlockedOnPlacements) {\n+            // Lock task generation in current stage if any of the source tasks' output buffer is blocked", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNjAyOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465736029", "bodyText": "make it private", "author": "sopel39", "createdAt": "2020-08-05T13:43:18Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -428,30 +453,21 @@ else if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n \n         stageLinkages.put(stageId, new StageLinkage(plan.getFragment().getId(), parent, childStages));\n \n-        if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n-            Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()\n-                    .map(SqlStageExecution::getAllTasks)\n-                    .flatMap(Collection::stream)\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n-\n-            Supplier<Collection<TaskStatus>> writerTasksProvider = () -> stage.getAllTasks().stream()\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n+        return stages.build();\n+    }\n \n-            ScaledWriterScheduler scheduler = new ScaledWriterScheduler(\n-                    stage,\n-                    sourceTasksProvider,\n-                    writerTasksProvider,\n-                    nodeScheduler.createNodeSelector(Optional.empty()),\n-                    schedulerExecutor,\n-                    getWriterMinSize(session));\n-            whenAllStages(childStages, StageState::isDone)\n-                    .addListener(scheduler::finish, directExecutor());\n-            stageSchedulers.put(stageId, scheduler);\n-        }\n+    private static Collection<TaskStatus> getChildTaskStatuses(Set<SqlStageExecution> childStages)\n+    {\n+        return childStages.stream()\n+                .map(SqlStageExecution::getAllTasks)\n+                .flatMap(Collection::stream)\n+                .map(RemoteTask::getTaskStatus)\n+                .collect(toImmutableList());\n+    }\n \n-        return stages.build();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNzQxNw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465737417", "bodyText": "make this method\nprivate static boolean isAnyTaskBlocked(Set<SqlStageExecution> childStages) {\n  return getChildTaskStatuses(childStages).stream().anyMatch(TaskStatus::isOutputBufferOverutilized);\n}\n\nand move above getChildTaskStatuses", "author": "sopel39", "createdAt": "2020-08-05T13:45:17Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -428,30 +453,21 @@ else if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n \n         stageLinkages.put(stageId, new StageLinkage(plan.getFragment().getId(), parent, childStages));\n \n-        if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n-            Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()\n-                    .map(SqlStageExecution::getAllTasks)\n-                    .flatMap(Collection::stream)\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n-\n-            Supplier<Collection<TaskStatus>> writerTasksProvider = () -> stage.getAllTasks().stream()\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n+        return stages.build();\n+    }\n \n-            ScaledWriterScheduler scheduler = new ScaledWriterScheduler(\n-                    stage,\n-                    sourceTasksProvider,\n-                    writerTasksProvider,\n-                    nodeScheduler.createNodeSelector(Optional.empty()),\n-                    schedulerExecutor,\n-                    getWriterMinSize(session));\n-            whenAllStages(childStages, StageState::isDone)\n-                    .addListener(scheduler::finish, directExecutor());\n-            stageSchedulers.put(stageId, scheduler);\n-        }\n+    private static Collection<TaskStatus> getChildTaskStatuses(Set<SqlStageExecution> childStages)\n+    {\n+        return childStages.stream()\n+                .map(SqlStageExecution::getAllTasks)\n+                .flatMap(Collection::stream)\n+                .map(RemoteTask::getTaskStatus)\n+                .collect(toImmutableList());\n+    }\n \n-        return stages.build();\n+    static Supplier<Boolean> getAnyTaskBlockedSupplier(Supplier<Collection<TaskStatus>> taskStatusSupplier)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTczNzg3NA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465737874", "bodyText": "make it return List<TaskStatus>", "author": "sopel39", "createdAt": "2020-08-05T13:45:56Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/SqlQueryScheduler.java", "diffHunk": "@@ -428,30 +453,21 @@ else if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n \n         stageLinkages.put(stageId, new StageLinkage(plan.getFragment().getId(), parent, childStages));\n \n-        if (partitioningHandle.equals(SCALED_WRITER_DISTRIBUTION)) {\n-            Supplier<Collection<TaskStatus>> sourceTasksProvider = () -> childStages.stream()\n-                    .map(SqlStageExecution::getAllTasks)\n-                    .flatMap(Collection::stream)\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n-\n-            Supplier<Collection<TaskStatus>> writerTasksProvider = () -> stage.getAllTasks().stream()\n-                    .map(RemoteTask::getTaskStatus)\n-                    .collect(toList());\n+        return stages.build();\n+    }\n \n-            ScaledWriterScheduler scheduler = new ScaledWriterScheduler(\n-                    stage,\n-                    sourceTasksProvider,\n-                    writerTasksProvider,\n-                    nodeScheduler.createNodeSelector(Optional.empty()),\n-                    schedulerExecutor,\n-                    getWriterMinSize(session));\n-            whenAllStages(childStages, StageState::isDone)\n-                    .addListener(scheduler::finish, directExecutor());\n-            stageSchedulers.put(stageId, scheduler);\n-        }\n+    private static Collection<TaskStatus> getChildTaskStatuses(Set<SqlStageExecution> childStages)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NTUzMQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465745531", "bodyText": "use nodeMamMemoizationDuration.getValue() > 0", "author": "sopel39", "createdAt": "2020-08-05T13:57:07Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -82,9 +97,15 @@ public NodeSelector createNodeSelector(Optional<CatalogName> catalogName)\n \n         // this supplier is thread-safe. TODO: this logic should probably move to the scheduler since the choice of which node to run in should be\n         // done as close to when the the split is about to be scheduled\n-        Supplier<NodeMap> nodeMap = Suppliers.memoizeWithExpiration(\n-                () -> createNodeMap(catalogName),\n-                5, TimeUnit.SECONDS);\n+        Supplier<NodeMap> nodeMap;\n+        if (nodeMapMemoizationDuration.roundTo(SECONDS) > 0) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NjEwNA==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465746104", "bodyText": "use milliseconds instead. There is a build-in duration method: Duration#toMillis", "author": "sopel39", "createdAt": "2020-08-05T13:57:56Z", "path": "presto-main/src/main/java/io/prestosql/execution/scheduler/UniformNodeSelectorFactory.java", "diffHunk": "@@ -82,9 +97,15 @@ public NodeSelector createNodeSelector(Optional<CatalogName> catalogName)\n \n         // this supplier is thread-safe. TODO: this logic should probably move to the scheduler since the choice of which node to run in should be\n         // done as close to when the the split is about to be scheduled\n-        Supplier<NodeMap> nodeMap = Suppliers.memoizeWithExpiration(\n-                () -> createNodeMap(catalogName),\n-                5, TimeUnit.SECONDS);\n+        Supplier<NodeMap> nodeMap;\n+        if (nodeMapMemoizationDuration.roundTo(SECONDS) > 0) {\n+            nodeMap = Suppliers.memoizeWithExpiration(\n+                    () -> createNodeMap(catalogName),\n+                    nodeMapMemoizationDuration.roundTo(SECONDS), SECONDS);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NzkyNQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465747925", "bodyText": "just use:\n() -> false", "author": "sopel39", "createdAt": "2020-08-05T14:00:20Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -318,7 +325,8 @@ public void testNoNodes()\n                     Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n                     Iterables.getOnlyElement(plan.getSplitSources().values()),\n                     new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n-                    2);\n+                    2,\n+                    getAnyTaskBlockedSupplier(ImmutableList::of));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc2MDY3NQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465760675", "bodyText": "the test does not FullSchedule anymore. I think this test can be removed as we now cover this test by testNewTaskScheduledWhenChildStageBufferIsUnderutilized and testNoNewTaskScheduledWhenChildStageBufferIsOverutilized.\nThe only difference is that this this test was inducing blocked on placements via extra query", "author": "sopel39", "createdAt": "2020-08-05T14:17:55Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -396,8 +404,8 @@ public void testBlockCausesFullSchedule()\n         scheduleResult = secondScheduler.schedule();\n         assertFalse(scheduleResult.isFinished());\n         assertTrue(scheduleResult.getBlocked().isDone());\n-        assertEquals(scheduleResult.getNewTasks().size(), 3);\n-        assertEquals(secondStage.getAllTasks().size(), 3);\n+        assertEquals(scheduleResult.getNewTasks().size(), 0);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQwMzk2OQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r466403969", "bodyText": "Yes, that makes sense. testNoNewTaskScheduledWhenChildStageBufferIsOverutilized is using a new node to assert the full schedule.", "author": "rohangarg", "createdAt": "2020-08-06T13:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc2MDY3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc2MzY1Mw==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465763653", "bodyText": "just use () -> false here", "author": "sopel39", "createdAt": "2020-08-05T14:21:51Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -406,6 +414,85 @@ public void testBlockCausesFullSchedule()\n         secondStage.abort();\n     }\n \n+    @Test\n+    public void testNewTaskScheduledWhenChildStageBufferIsUnderutilized() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, new Duration(0, SECONDS)));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(500, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy task with under utilized output buffer\n+        TaskStatus dummyTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", false);\n+        StageScheduler scheduler = newSourcePartitionedSchedulerAsStageScheduler(\n+                stage,\n+                Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n+                Iterables.getOnlyElement(plan.getSplitSources().values()),\n+                new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n+                500,\n+                getAnyTaskBlockedSupplier(() -> ImmutableList.of(dummyTaskStatus)));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc2MzgwOQ==", "url": "https://github.com/trinodb/trino/pull/4294#discussion_r465763809", "bodyText": "just use () -> true here", "author": "sopel39", "createdAt": "2020-08-05T14:22:03Z", "path": "presto-main/src/test/java/io/prestosql/execution/scheduler/TestSourcePartitionedScheduler.java", "diffHunk": "@@ -406,6 +414,85 @@ public void testBlockCausesFullSchedule()\n         secondStage.abort();\n     }\n \n+    @Test\n+    public void testNewTaskScheduledWhenChildStageBufferIsUnderutilized() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, new Duration(0, SECONDS)));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(500, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy task with under utilized output buffer\n+        TaskStatus dummyTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", false);\n+        StageScheduler scheduler = newSourcePartitionedSchedulerAsStageScheduler(\n+                stage,\n+                Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n+                Iterables.getOnlyElement(plan.getSplitSources().values()),\n+                new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n+                500,\n+                getAnyTaskBlockedSupplier(() -> ImmutableList.of(dummyTaskStatus)));\n+\n+        // the queues of 3 running nodes should be full\n+        ScheduleResult scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getBlockedReason().get(), SPLIT_QUEUES_FULL);\n+        assertEquals(scheduleResult.getNewTasks().size(), 3);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 300);\n+\n+        // new node added - the pending splits should go to it since the child tasks are not blocked\n+        nodeManager.addNode(CONNECTOR_ID, new InternalNode(\"other4\", URI.create(\"http://127.0.0.4:14\"), NodeVersion.UNKNOWN, false));\n+        scheduleResult = scheduler.schedule();\n+        assertEquals(scheduleResult.getBlockedReason().get(), SPLIT_QUEUES_FULL); // split queue is full but still the source task creation isn't blocked\n+        assertEquals(scheduleResult.getNewTasks().size(), 1);\n+        assertEquals(scheduleResult.getSplitsScheduled(), 100);\n+    }\n+\n+    @Test\n+    public void testNoNewTaskScheduledWhenChildStageBufferIsOverutilized() throws Exception\n+    {\n+        NodeTaskMap nodeTaskMap = new NodeTaskMap(finalizerService);\n+        // use private node manager so we can add a node later\n+        InMemoryNodeManager nodeManager = new InMemoryNodeManager();\n+        nodeManager.addNode(CONNECTOR_ID,\n+                new InternalNode(\"other1\", URI.create(\"http://127.0.0.1:11\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other2\", URI.create(\"http://127.0.0.1:12\"), NodeVersion.UNKNOWN, false),\n+                new InternalNode(\"other3\", URI.create(\"http://127.0.0.1:13\"), NodeVersion.UNKNOWN, false));\n+        NodeScheduler nodeScheduler = new NodeScheduler(new UniformNodeSelectorFactory(nodeManager, new NodeSchedulerConfig().setIncludeCoordinator(false), nodeTaskMap, new Duration(0, SECONDS)));\n+\n+        StageExecutionPlan plan = createPlan(createFixedSplitSource(400, TestingSplit::createRemoteSplit));\n+        SqlStageExecution stage = createSqlStageExecution(plan, nodeTaskMap);\n+\n+        // dummy tasks with over and under utilized output buffers\n+        TaskStatus dummyOverUtilizedTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", true);\n+        TaskStatus dummyUnderUtilizedTaskStatus = taskStatusWithOutputBufferUtilization(new TaskId(\"\"), new URI(\"\"), \"\", false);\n+        StageScheduler scheduler = newSourcePartitionedSchedulerAsStageScheduler(\n+                stage,\n+                Iterables.getOnlyElement(plan.getSplitSources().keySet()),\n+                Iterables.getOnlyElement(plan.getSplitSources().values()),\n+                new DynamicSplitPlacementPolicy(nodeScheduler.createNodeSelector(Optional.of(CONNECTOR_ID)), stage::getAllTasks),\n+                400,\n+                getAnyTaskBlockedSupplier(() -> ImmutableList.of(dummyOverUtilizedTaskStatus, dummyUnderUtilizedTaskStatus)));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "2e8b27cd5a0470c95549ffe0d974c07f29a04d12", "url": "https://github.com/trinodb/trino/commit/2e8b27cd5a0470c95549ffe0d974c07f29a04d12", "message": "Relax task locking in SourcePartitionedScheduler", "committedDate": "2020-08-06T15:07:06Z", "type": "commit"}, {"oid": "365b111e580aa94dec23dc726c5b0f004f999e51", "url": "https://github.com/trinodb/trino/commit/365b111e580aa94dec23dc726c5b0f004f999e51", "message": "Make BroadcastOutputBuffer#isOverutilized aggressive", "committedDate": "2020-08-06T15:07:06Z", "type": "commit"}, {"oid": "365b111e580aa94dec23dc726c5b0f004f999e51", "url": "https://github.com/trinodb/trino/commit/365b111e580aa94dec23dc726c5b0f004f999e51", "message": "Make BroadcastOutputBuffer#isOverutilized aggressive", "committedDate": "2020-08-06T15:07:06Z", "type": "forcePushed"}]}