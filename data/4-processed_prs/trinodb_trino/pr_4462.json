{"pr_number": 4462, "pr_title": "Add Kafka headers as column", "pr_createdAt": "2020-07-15T19:53:48Z", "pr_url": "https://github.com/trinodb/trino/pull/4462", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2ODQ4MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456068481", "bodyText": "There is a convenience method you can use here and when writing the value: TypeUtils.writeNativeValue(keyType, builder, header.getKey())", "author": "aalbu", "createdAt": "2020-07-16T20:47:47Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +279,58 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)\n+    {\n+        final Type keyType = varcharMapType.getTypeParameters().get(0);\n+        final Type valueArrayType = varcharMapType.getTypeParameters().get(1);\n+        final Type valueType = valueArrayType.getTypeParameters().get(0);\n+\n+        final BlockBuilder mapBlockBuilder = varcharMapType.createBlockBuilder(null, 1);\n+        final BlockBuilder builder = mapBlockBuilder.beginBlockEntry();\n+\n+        // Group by keys and collect values as array.\n+        final Map<String, List<byte[]>> headerMap = new HashMap<>();\n+        for (Header header : headers) {\n+            final String key = header.key();\n+            if (headerMap.containsKey(key)) {\n+                headerMap.get(key).add(header.value());\n+            }\n+            else {\n+                final List<byte[]> list = new ArrayList<>();\n+                list.add(header.value());\n+                headerMap.put(key, list);\n+            }\n+        }\n+\n+        for (Map.Entry<String, List<byte[]>> header : headerMap.entrySet()) {\n+            keyType.writeSlice(builder, utf8Slice(header.getKey()));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDA2Ng==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360066", "bodyText": "Thanks, didn't know that.", "author": "0xE282B0", "createdAt": "2020-07-17T10:30:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA2ODQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MDE5MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456070191", "bodyText": "You could add an underscore after header_test to separate more clearly the random part.", "author": "aalbu", "createdAt": "2020-07-16T20:51:06Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -54,13 +55,16 @@\n {\n     private TestingKafka testingKafka;\n     private String rawFormatTopic;\n+    private String headersTopic;\n \n     @Override\n     protected QueryRunner createQueryRunner()\n             throws Exception\n     {\n         testingKafka = new TestingKafka();\n         rawFormatTopic = \"test_raw_\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");\n+        headersTopic = \"header_test\" + UUID.randomUUID().toString().replaceAll(\"-\", \"_\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDI1Mw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360253", "bodyText": "sure, missed to copy.", "author": "0xE282B0", "createdAt": "2020-07-17T10:30:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MDE5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MjIxOA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456072218", "bodyText": "I think it's more readable if you put the value on a different line, like the entry before.", "author": "aalbu", "createdAt": "2020-07-16T20:54:58Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -76,6 +80,7 @@ protected QueryRunner createQueryRunner()\n                                         createOneFieldDescription(\"boolean_int\", BooleanType.BOOLEAN, \"41\", \"INT\"),\n                                         createOneFieldDescription(\"boolean_short\", BooleanType.BOOLEAN, \"45\", \"SHORT\"),\n                                         createOneFieldDescription(\"boolean_byte\", BooleanType.BOOLEAN, \"47\", \"BYTE\")))))\n+                .put(new SchemaTableName(\"default\", headersTopic), new KafkaTopicDescription(headersTopic, Optional.empty(), headersTopic, Optional.empty(), Optional.empty()))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDMyOA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360328", "bodyText": "sure.", "author": "0xE282B0", "createdAt": "2020-07-17T10:30:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MjIxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MzI5Ng==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456073296", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        ProducerRecord<byte[], byte[]> record = new ProducerRecord<byte[], byte[]>(topicName, null, \"{}\".getBytes(Charset.forName(\"UTF-8\")));\n          \n          \n            \n                        ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topicName, null, \"{}\".getBytes(StandardCharsets.UTF_8));", "author": "aalbu", "createdAt": "2020-07-16T20:57:03Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -134,6 +139,22 @@ private void insertData(byte[] data)\n         }\n     }\n \n+    private void createMessagesWithHeader(String topicName)\n+    {\n+        try (KafkaProducer<byte[], byte[]> producer = createProducer()) {\n+            ProducerRecord<byte[], byte[]> record = new ProducerRecord<byte[], byte[]>(topicName, null, \"{}\".getBytes(Charset.forName(\"UTF-8\")));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMjQ0MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456232441", "bodyText": "this, + static import UTF_8", "author": "findepi", "createdAt": "2020-07-17T05:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MzI5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDQ4NA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360484", "bodyText": "better, thanks.", "author": "0xE282B0", "createdAt": "2020-07-17T10:30:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3MzI5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3NTg5Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456075897", "bodyText": "Can you change the value to something other than null?  As the test is written, it's not clear whether the null in the result comes from this header or the one a few lines below (.add(\"foo\", null)).", "author": "aalbu", "createdAt": "2020-07-16T21:02:08Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -134,6 +139,22 @@ private void insertData(byte[] data)\n         }\n     }\n \n+    private void createMessagesWithHeader(String topicName)\n+    {\n+        try (KafkaProducer<byte[], byte[]> producer = createProducer()) {\n+            ProducerRecord<byte[], byte[]> record = new ProducerRecord<byte[], byte[]>(topicName, null, \"{}\".getBytes(Charset.forName(\"UTF-8\")));\n+            record.headers()\n+                    .add(\"notfoo\", null);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDYwNA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360604", "bodyText": "good point.", "author": "0xE282B0", "createdAt": "2020-07-17T10:31:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA3NTg5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MDQyNA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456080424", "bodyText": "You don't need the cast.", "author": "aalbu", "createdAt": "2020-07-16T21:11:26Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +279,58 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)\n+    {\n+        final Type keyType = varcharMapType.getTypeParameters().get(0);\n+        final Type valueArrayType = varcharMapType.getTypeParameters().get(1);\n+        final Type valueType = valueArrayType.getTypeParameters().get(0);\n+\n+        final BlockBuilder mapBlockBuilder = varcharMapType.createBlockBuilder(null, 1);\n+        final BlockBuilder builder = mapBlockBuilder.beginBlockEntry();\n+\n+        // Group by keys and collect values as array.\n+        final Map<String, List<byte[]>> headerMap = new HashMap<>();\n+        for (Header header : headers) {\n+            final String key = header.key();\n+            if (headerMap.containsKey(key)) {\n+                headerMap.get(key).add(header.value());\n+            }\n+            else {\n+                final List<byte[]> list = new ArrayList<>();\n+                list.add(header.value());\n+                headerMap.put(key, list);\n+            }\n+        }\n+\n+        for (Map.Entry<String, List<byte[]>> header : headerMap.entrySet()) {\n+            keyType.writeSlice(builder, utf8Slice(header.getKey()));\n+            final BlockBuilder arrayBuilder = builder.beginBlockEntry();\n+            for (byte[] value : header.getValue()) {\n+                if (value == null) {\n+                    arrayBuilder.appendNull();\n+                }\n+                else {\n+                    valueType.writeSlice(arrayBuilder, wrappedBuffer(value));\n+                }\n+            }\n+            builder.closeEntry();\n+        }\n+\n+        mapBlockBuilder.closeEntry();\n+\n+        return new FieldValueProvider() {\n+            @Override\n+            public boolean isNull()\n+            {\n+                return !headers.iterator().hasNext();\n+            }\n+\n+            @Override\n+            public Block getBlock()\n+            {\n+                return (Block) varcharMapType.getObject(mapBlockBuilder, 0);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM1OTc5NA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456359794", "bodyText": "right.", "author": "0xE282B0", "createdAt": "2020-07-17T10:29:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MDQyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MzYyMQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456083621", "bodyText": "I don't know if this is fit for a Javadoc comment.", "author": "aalbu", "createdAt": "2020-07-16T21:18:23Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -13,126 +13,191 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n \n import java.util.Map;\n import java.util.Optional;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static io.prestosql.spi.type.TypeSignature.arrayType;\n+import static io.prestosql.spi.type.TypeSignature.mapType;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n import static java.util.Arrays.stream;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n-     */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n-    {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n-    }\n-\n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public enum InternalFields\n     {\n-        checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n-        this.columnName = columnName;\n-        this.type = requireNonNull(type, \"type is null\");\n-        this.comment = requireNonNull(comment, \"comment is null\");\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_headers</tt> - The header fields of the Kafka message. Key is a UTF-8 String and values an array of byte[].\n+         */\n+        HEADERS_FIELD(\"_headers\", \"Headers of the message as map.\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(\n+                String columnName,\n+                String comment)\n+        {\n+            checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n+            this.columnName = columnName;\n+            this.comment = requireNonNull(comment, \"comment is null\");\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n     }\n \n-    public String getColumnName()\n+    /**\n+     * Inner class that wraps the {@link InternalFields} enum and add {@link Type}.\n+     * With this bean a non static data structure with types from the {@link TypeManager} can be created.\n+     */", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3MDk5NQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456370995", "bodyText": "Should I write a non-doc comment instead or just remove it?", "author": "0xE282B0", "createdAt": "2020-07-17T10:55:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MzYyMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQwODAyOA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456408028", "bodyText": "I would opt for removal.", "author": "aalbu", "createdAt": "2020-07-17T12:24:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA4MzYyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE4ODczNw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456188737", "bodyText": "Can you also publish a message without headers?", "author": "aalbu", "createdAt": "2020-07-17T02:49:23Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -134,6 +139,22 @@ private void insertData(byte[] data)\n         }\n     }\n \n+    private void createMessagesWithHeader(String topicName)\n+    {\n+        try (KafkaProducer<byte[], byte[]> producer = createProducer()) {\n+            ProducerRecord<byte[], byte[]> record = new ProducerRecord<byte[], byte[]>(topicName, null, \"{}\".getBytes(Charset.forName(\"UTF-8\")));\n+            record.headers()\n+                    .add(\"notfoo\", null);\n+            producer.send(record);\n+            record = new ProducerRecord<byte[], byte[]>(topicName, null, \"{}\".getBytes(Charset.forName(\"UTF-8\")));\n+            record.headers()\n+                    .add(\"foo\", \"bar\".getBytes(Charset.forName(\"UTF-8\")))\n+                    .add(\"foo\", null)\n+                    .add(\"foo\", \"baz\".getBytes(Charset.forName(\"UTF-8\")));\n+            producer.send(record);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDY1Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360657", "bodyText": "good point.", "author": "0xE282B0", "createdAt": "2020-07-17T10:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjE4ODczNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMTY0OQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456231649", "bodyText": "Please make the table wider in a separate commit, as in https://github.com/prestosql/presto/pull/3771/commits", "author": "findepi", "createdAt": "2020-07-17T05:49:09Z", "path": "presto-docs/src/main/sphinx/connector/kafka.rst", "diffHunk": "@@ -146,21 +146,22 @@ Internal Columns\n \n For each defined table, the connector maintains the following columns:\n \n-======================= ========= =============================\n-Column name             Type      Description\n-======================= ========= =============================\n-``_partition_id``       BIGINT    ID of the Kafka partition which contains this row.\n-``_partition_offset``   BIGINT    Offset within the Kafka partition for this row.\n-``_segment_start``      BIGINT    Lowest offset in the segment (inclusive) which contains this row. This offset is partition specific.\n-``_segment_end``        BIGINT    Highest offset in the segment (exclusive) which contains this row. The offset is partition specific. This is the same value as ``_segment_start`` of the next segment (if it exists).\n-``_segment_count``      BIGINT    Running count for the current row within the segment. For an uncompacted topic, ``_segment_start + _segment_count`` is equal to ``_partition_offset``.\n-``_message_corrupt``    BOOLEAN   True if the decoder could not decode the message for this row. When true, data columns mapped from the message should be treated as invalid.\n-``_message``            VARCHAR   Message bytes as an UTF-8 encoded string. This is only useful for a text topic.\n-``_message_length``     BIGINT    Number of bytes in the message.\n-``_key_corrupt``        BOOLEAN   True if the key decoder could not decode the key for this row. When true, data columns mapped from the key should be treated as invalid.\n-``_key``                VARCHAR   Key bytes as an UTF-8 encoded string. This is only useful for textual keys.\n-``_key_length``         BIGINT    Number of bytes in the key.\n-======================= ========= =============================\n+======================= =============================== =============================\n+Column name             Type                            Description\n+======================= =============================== =============================", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM3NzI3Mw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456377273", "bodyText": "\ud83d\udc4d", "author": "0xE282B0", "createdAt": "2020-07-17T11:10:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMTY0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMTk4Mw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456231983", "bodyText": "Per code style, we do not use final for local variables.\nFor short methods, it doesn't add to readability, and we strive to avoid long methods.", "author": "findepi", "createdAt": "2020-07-17T05:50:17Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +279,58 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)\n+    {\n+        final Type keyType = varcharMapType.getTypeParameters().get(0);\n+        final Type valueArrayType = varcharMapType.getTypeParameters().get(1);\n+        final Type valueType = valueArrayType.getTypeParameters().get(0);\n+\n+        final BlockBuilder mapBlockBuilder = varcharMapType.createBlockBuilder(null, 1);\n+        final BlockBuilder builder = mapBlockBuilder.beginBlockEntry();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM1OTk1Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456359957", "bodyText": "Thats fine.", "author": "0xE282B0", "createdAt": "2020-07-17T10:29:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMTk4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMjI3MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456232271", "bodyText": "Is there a value in returning NULL when no headers?\nWhy not return empty map?", "author": "findepi", "createdAt": "2020-07-17T05:51:22Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +279,58 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)\n+    {\n+        final Type keyType = varcharMapType.getTypeParameters().get(0);\n+        final Type valueArrayType = varcharMapType.getTypeParameters().get(1);\n+        final Type valueType = valueArrayType.getTypeParameters().get(0);\n+\n+        final BlockBuilder mapBlockBuilder = varcharMapType.createBlockBuilder(null, 1);\n+        final BlockBuilder builder = mapBlockBuilder.beginBlockEntry();\n+\n+        // Group by keys and collect values as array.\n+        final Map<String, List<byte[]>> headerMap = new HashMap<>();\n+        for (Header header : headers) {\n+            final String key = header.key();\n+            if (headerMap.containsKey(key)) {\n+                headerMap.get(key).add(header.value());\n+            }\n+            else {\n+                final List<byte[]> list = new ArrayList<>();\n+                list.add(header.value());\n+                headerMap.put(key, list);\n+            }\n+        }\n+\n+        for (Map.Entry<String, List<byte[]>> header : headerMap.entrySet()) {\n+            keyType.writeSlice(builder, utf8Slice(header.getKey()));\n+            final BlockBuilder arrayBuilder = builder.beginBlockEntry();\n+            for (byte[] value : header.getValue()) {\n+                if (value == null) {\n+                    arrayBuilder.appendNull();\n+                }\n+                else {\n+                    valueType.writeSlice(arrayBuilder, wrappedBuffer(value));\n+                }\n+            }\n+            builder.closeEntry();\n+        }\n+\n+        mapBlockBuilder.closeEntry();\n+\n+        return new FieldValueProvider() {\n+            @Override\n+            public boolean isNull()\n+            {\n+                return !headers.iterator().hasNext();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDE1Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360157", "bodyText": "I just liked SELECT _headers FROM topic1 WHERE _headers IS NOT NULL; better than SELECT _headers FROM topic1 WHERE cardinality(_headers) > 0; but thats not a strong opinion. In fact you are right that the headers are empty and not absent.", "author": "0xE282B0", "createdAt": "2020-07-17T10:30:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMjI3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMjU2Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456232567", "bodyText": "nit\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    \" WHERE element_at(_headers,'foo') is not NULL\",\n          \n          \n            \n                                    \" WHERE element_at(_headers, 'foo') IS NOT NULL\",", "author": "findepi", "createdAt": "2020-07-17T05:52:20Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -171,6 +192,17 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                 \"VALUES (\" + testCase.getFieldValues() + \"), (\" + testCase.getFieldValues() + \")\");\n     }\n \n+    @Test\n+    public void testKafkaHeaders()\n+    {\n+        createMessagesWithHeader(headersTopic);\n+\n+        assertQuery(\"SELECT from_utf8(value) FROM default.\" + headersTopic +\n+                        \" CROSS JOIN UNNEST(_headers['foo']) as arr (value)\" +\n+                        \" WHERE element_at(_headers,'foo') is not NULL\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjM2MDczMA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456360730", "bodyText": "right.", "author": "0xE282B0", "createdAt": "2020-07-17T10:31:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjIzMjU2Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwNDEzMA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456504130", "bodyText": "Can be inline", "author": "charlesjmorgan", "createdAt": "2020-07-17T15:10:39Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -13,126 +13,187 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n \n import java.util.Map;\n import java.util.Optional;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static io.prestosql.spi.type.TypeSignature.arrayType;\n+import static io.prestosql.spi.type.TypeSignature.mapType;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n import static java.util.Arrays.stream;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n-     */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n+    public enum InternalFields\n     {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_headers</tt> - The header fields of the Kafka message. Key is a UTF-8 String and values an array of byte[].\n+         */\n+        HEADERS_FIELD(\"_headers\", \"Headers of the message as map.\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwNTIxMA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456505210", "bodyText": "Can be inline", "author": "charlesjmorgan", "createdAt": "2020-07-17T15:12:27Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -13,126 +13,187 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n \n import java.util.Map;\n import java.util.Optional;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static io.prestosql.spi.type.TypeSignature.arrayType;\n+import static io.prestosql.spi.type.TypeSignature.mapType;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n import static java.util.Arrays.stream;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n-     */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n+    public enum InternalFields\n     {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_headers</tt> - The header fields of the Kafka message. Key is a UTF-8 String and values an array of byte[].\n+         */\n+        HEADERS_FIELD(\"_headers\", \"Headers of the message as map.\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(\n+                String columnName,\n+                String comment)\n+        {\n+            checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n+            this.columnName = columnName;\n+            this.comment = requireNonNull(comment, \"comment is null\");\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n     }\n \n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public static class InternalFieldDescription\n     {\n-        checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n-        this.columnName = columnName;\n-        this.type = requireNonNull(type, \"type is null\");\n-        this.comment = requireNonNull(comment, \"comment is null\");\n+        private final String columnName;\n+        private final Type type;\n+        private final String comment;\n+\n+        InternalFieldDescription(", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwOTMzMg==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456509332", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    internalFields = new ImmutableMap.Builder<InternalFields, InternalFieldDescription>()\n          \n          \n            \n                    internalFields = ImmutableMap.<InternalFields, InternalFieldDescription>builder()", "author": "charlesjmorgan", "createdAt": "2020-07-17T15:19:09Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -13,126 +13,187 @@\n  */\n package io.prestosql.plugin.kafka;\n \n+import com.google.common.collect.ImmutableMap;\n+import com.google.inject.Inject;\n import io.prestosql.spi.connector.ColumnMetadata;\n import io.prestosql.spi.type.BigintType;\n import io.prestosql.spi.type.BooleanType;\n import io.prestosql.spi.type.Type;\n+import io.prestosql.spi.type.TypeManager;\n \n import java.util.Map;\n import java.util.Optional;\n \n import static com.google.common.base.Preconditions.checkArgument;\n import static com.google.common.base.Strings.isNullOrEmpty;\n import static com.google.common.collect.ImmutableMap.toImmutableMap;\n+import static io.prestosql.spi.type.TypeSignature.arrayType;\n+import static io.prestosql.spi.type.TypeSignature.mapType;\n+import static io.prestosql.spi.type.VarbinaryType.VARBINARY;\n+import static io.prestosql.spi.type.VarcharType.VARCHAR;\n import static io.prestosql.spi.type.VarcharType.createUnboundedVarcharType;\n import static java.util.Arrays.stream;\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n-     */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n+    public enum InternalFields\n     {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_headers</tt> - The header fields of the Kafka message. Key is a UTF-8 String and values an array of byte[].\n+         */\n+        HEADERS_FIELD(\"_headers\", \"Headers of the message as map.\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(\n+                String columnName,\n+                String comment)\n+        {\n+            checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n+            this.columnName = columnName;\n+            this.comment = requireNonNull(comment, \"comment is null\");\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n     }\n \n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public static class InternalFieldDescription\n     {\n-        checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n-        this.columnName = columnName;\n-        this.type = requireNonNull(type, \"type is null\");\n-        this.comment = requireNonNull(comment, \"comment is null\");\n+        private final String columnName;\n+        private final Type type;\n+        private final String comment;\n+\n+        InternalFieldDescription(\n+                InternalFields internalField,\n+                Type type)\n+        {\n+            this.columnName = internalField.getColumnName();\n+            this.type = requireNonNull(type, \"type is null\");\n+            this.comment = internalField.comment;\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n+\n+        private Type getType()\n+        {\n+            return type;\n+        }\n+\n+        KafkaColumnHandle getColumnHandle(int index, boolean hidden)\n+        {\n+            return new KafkaColumnHandle(\n+                    getColumnName(),\n+                    getType(),\n+                    null,\n+                    null,\n+                    null,\n+                    false,\n+                    hidden,\n+                    true);\n+        }\n+\n+        ColumnMetadata getColumnMetadata(boolean hidden)\n+        {\n+            return ColumnMetadata.builder()\n+                    .setName(columnName)\n+                    .setType(type)\n+                    .setComment(Optional.ofNullable(comment))\n+                    .setHidden(hidden)\n+                    .build();\n+        }\n     }\n \n-    public String getColumnName()\n-    {\n-        return columnName;\n-    }\n-\n-    private Type getType()\n-    {\n-        return type;\n-    }\n+    private final Map<InternalFields, InternalFieldDescription> internalFields;\n \n-    KafkaColumnHandle getColumnHandle(int index, boolean hidden)\n+    @Inject\n+    public KafkaInternalFieldDescription(TypeManager typeManager)\n     {\n-        return new KafkaColumnHandle(\n-                getColumnName(),\n-                getType(),\n-                null,\n-                null,\n-                null,\n-                false,\n-                hidden,\n-                true);\n+        Type varcharMapType = typeManager.getType(mapType(VARCHAR.getTypeSignature(), arrayType(VARBINARY.getTypeSignature())));\n+\n+        internalFields = new ImmutableMap.Builder<InternalFields, InternalFieldDescription>()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDE2NQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456590165", "bodyText": "thanks.", "author": "0xE282B0", "createdAt": "2020-07-17T17:54:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUwOTMzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjUyMzUyNw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r456523527", "bodyText": "Move this before testRoundTripAllFormats so that it doesn't split that test from its data provider", "author": "charlesjmorgan", "createdAt": "2020-07-17T15:43:51Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -171,6 +197,17 @@ public void testRoundTripAllFormats(RoundTripTestCase testCase)\n                 \"VALUES (\" + testCase.getFieldValues() + \"), (\" + testCase.getFieldValues() + \")\");\n     }\n \n+    @Test\n+    public void testKafkaHeaders()", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNDU2Mg==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462104562", "bodyText": "singular InternalField", "author": "findepi", "createdAt": "2020-07-29T07:46:56Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -29,110 +32,160 @@\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n-     */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n-    {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n-    }\n-\n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public enum InternalFields", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEwNTU1MA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462105550", "bodyText": "Make it easy to notice this is internal field + type:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    private final String columnName;\n          \n          \n            \n                    private final Type type;\n          \n          \n            \n                    private final String comment;\n          \n          \n            \n                    private final InternalField internalField;\n          \n          \n            \n                    private final Type type;\n          \n      \n    \n    \n  \n\nyou can still provide getColumnName accessor for convinience", "author": "findepi", "createdAt": "2020-07-29T07:48:48Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -29,110 +32,160 @@\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n-     */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n-    {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n-    }\n-\n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public enum InternalFields\n     {\n-        checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n-        this.columnName = columnName;\n-        this.type = requireNonNull(type, \"type is null\");\n-        this.comment = requireNonNull(comment, \"comment is null\");\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(\n+                String columnName,\n+                String comment)\n+        {\n+            checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n+            this.columnName = columnName;\n+            this.comment = requireNonNull(comment, \"comment is null\");\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n     }\n \n-    public String getColumnName()\n+    /**\n+     * Inner class that wraps the {@link InternalFields} enum and add {@link Type}.\n+     * With this bean a non static data structure with types from the {@link TypeManager} can be created.\n+     */\n+    public static class InternalFieldDescription\n     {\n-        return columnName;\n+        private final String columnName;\n+        private final Type type;\n+        private final String comment;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMTY1NQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462111655", "bodyText": "Instead of doing this mapping manually, delegate back to the enum:\ninternalFields = Stream.of(InternalField.values())\n            .map(toImmutableMap(\n              identity(), \n              internalField -> new InternalFieldDescription(internalField, internalField.getPrestoType(typeManager))));\n\nin the enum class, this can look like this:\n        InternalField(String columnName, String comment, Type type)\n        {\n            this(columnName, comment, typeManager -> type);\n        }\n\n        InternalField(String columnName, String comment, Function<TypeManager, Type> typeProvider)\n        {\n....\n\nthis way all the scalar fields remain simple in their definition (they just provide the type inline)\nand the headers field provides a type bearing function", "author": "findepi", "createdAt": "2020-07-29T07:59:37Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -29,110 +32,160 @@\n import static java.util.Objects.requireNonNull;\n import static java.util.function.Function.identity;\n \n-/**\n- * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n- * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n- * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n- * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n- */\n-public enum KafkaInternalFieldDescription\n+public class KafkaInternalFieldDescription\n {\n     /**\n-     * <tt>_partition_id</tt> - Kafka partition id.\n-     */\n-    PARTITION_ID_FIELD(\"_partition_id\", BigintType.BIGINT, \"Partition Id\"),\n-\n-    /**\n-     * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n-     */\n-    PARTITION_OFFSET_FIELD(\"_partition_offset\", BigintType.BIGINT, \"Offset for the message within the partition\"),\n-\n-    /**\n-     * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", BooleanType.BOOLEAN, \"Message data is corrupt\"),\n-\n-    /**\n-     * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n-     */\n-    MESSAGE_FIELD(\"_message\", createUnboundedVarcharType(), \"Message text\"),\n-\n-    /**\n-     * <tt>_message_length</tt> - length in bytes of the message.\n-     */\n-    MESSAGE_LENGTH_FIELD(\"_message_length\", BigintType.BIGINT, \"Total number of message bytes\"),\n-\n-    /**\n-     * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n-     */\n-    KEY_CORRUPT_FIELD(\"_key_corrupt\", BooleanType.BOOLEAN, \"Key data is corrupt\"),\n-\n-    /**\n-     * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+     * Describes an internal (managed by the connector) field which is added to each table row. The definition itself makes the row\n+     * show up in the tables (the columns are hidden by default, so they must be explicitly selected) but unless the field is hooked in using the\n+     * forBooleanValue/forLongValue/forBytesValue methods and the resulting FieldValueProvider is then passed into the appropriate row decoder, the fields\n+     * will be null. Most values are assigned in the {@link io.prestosql.plugin.kafka.KafkaRecordSet}.\n      */\n-    KEY_FIELD(\"_key\", createUnboundedVarcharType(), \"Key text\"),\n-\n-    /**\n-     * <tt>_key_length</tt> - length in bytes of the key.\n-     */\n-    KEY_LENGTH_FIELD(\"_key_length\", BigintType.BIGINT, \"Total number of key bytes\");\n-\n-    private static final Map<String, KafkaInternalFieldDescription> BY_COLUMN_NAME =\n-            stream(KafkaInternalFieldDescription.values())\n-                    .collect(toImmutableMap(KafkaInternalFieldDescription::getColumnName, identity()));\n-\n-    public static KafkaInternalFieldDescription forColumnName(String columnName)\n-    {\n-        KafkaInternalFieldDescription description = BY_COLUMN_NAME.get(columnName);\n-        checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n-        return description;\n-    }\n-\n-    private final String columnName;\n-    private final Type type;\n-    private final String comment;\n-\n-    KafkaInternalFieldDescription(\n-            String columnName,\n-            Type type,\n-            String comment)\n+    public enum InternalFields\n     {\n-        checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n-        this.columnName = columnName;\n-        this.type = requireNonNull(type, \"type is null\");\n-        this.comment = requireNonNull(comment, \"comment is null\");\n+        /**\n+         * <tt>_partition_id</tt> - Kafka partition id.\n+         */\n+        PARTITION_ID_FIELD(\"_partition_id\", \"Partition Id\"),\n+\n+        /**\n+         * <tt>_partition_offset</tt> - The current offset of the message in the partition.\n+         */\n+        PARTITION_OFFSET_FIELD(\"_partition_offset\", \"Offset for the message within the partition\"),\n+\n+        /**\n+         * <tt>_message_corrupt</tt> - True if the row converter could not read the a message. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        MESSAGE_CORRUPT_FIELD(\"_message_corrupt\", \"Message data is corrupt\"),\n+\n+        /**\n+         * <tt>_message</tt> - Represents the full topic as a text column. Format is UTF-8 which may be wrong for some topics. TODO: make charset configurable.\n+         */\n+        MESSAGE_FIELD(\"_message\", \"Message text\"),\n+\n+        /**\n+         * <tt>_message_length</tt> - length in bytes of the message.\n+         */\n+        MESSAGE_LENGTH_FIELD(\"_message_length\", \"Total number of message bytes\"),\n+\n+        /**\n+         * <tt>_key_corrupt</tt> - True if the row converter could not read the a key. May be null if the row converter does not set a value (e.g. the dummy row converter does not).\n+         */\n+        KEY_CORRUPT_FIELD(\"_key_corrupt\", \"Key data is corrupt\"),\n+\n+        /**\n+         * <tt>_key</tt> - Represents the key as a text column. Format is UTF-8 which may be wrong for topics. TODO: make charset configurable.\n+         */\n+        KEY_FIELD(\"_key\", \"Key text\"),\n+\n+        /**\n+         * <tt>_key_length</tt> - length in bytes of the key.\n+         */\n+        KEY_LENGTH_FIELD(\"_key_length\", \"Total number of key bytes\");\n+\n+        private static final Map<String, InternalFields> BY_COLUMN_NAME =\n+                stream(InternalFields.values())\n+                        .collect(toImmutableMap(InternalFields::getColumnName, identity()));\n+\n+        public static InternalFields forColumnName(String columnName)\n+        {\n+            InternalFields description = BY_COLUMN_NAME.get(columnName);\n+            checkArgument(description != null, \"Unknown internal column name %s\", columnName);\n+            return description;\n+        }\n+\n+        private final String columnName;\n+        private final String comment;\n+\n+        InternalFields(\n+                String columnName,\n+                String comment)\n+        {\n+            checkArgument(!isNullOrEmpty(columnName), \"name is null or is empty\");\n+            this.columnName = columnName;\n+            this.comment = requireNonNull(comment, \"comment is null\");\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n     }\n \n-    public String getColumnName()\n+    /**\n+     * Inner class that wraps the {@link InternalFields} enum and add {@link Type}.\n+     * With this bean a non static data structure with types from the {@link TypeManager} can be created.\n+     */\n+    public static class InternalFieldDescription\n     {\n-        return columnName;\n+        private final String columnName;\n+        private final Type type;\n+        private final String comment;\n+\n+        InternalFieldDescription(\n+                InternalFields internalField,\n+                Type type)\n+        {\n+            this.columnName = internalField.getColumnName();\n+            this.type = requireNonNull(type, \"type is null\");\n+            this.comment = internalField.comment;\n+        }\n+\n+        public String getColumnName()\n+        {\n+            return columnName;\n+        }\n+\n+        private Type getType()\n+        {\n+            return type;\n+        }\n+\n+        KafkaColumnHandle getColumnHandle(int index, boolean hidden)\n+        {\n+            return new KafkaColumnHandle(\n+                    getColumnName(),\n+                    getType(),\n+                    null,\n+                    null,\n+                    null,\n+                    false,\n+                    hidden,\n+                    true);\n+        }\n+\n+        ColumnMetadata getColumnMetadata(boolean hidden)\n+        {\n+            return ColumnMetadata.builder()\n+                    .setName(columnName)\n+                    .setType(type)\n+                    .setComment(Optional.ofNullable(comment))\n+                    .setHidden(hidden)\n+                    .build();\n+        }\n     }\n \n-    private Type getType()\n-    {\n-        return type;\n-    }\n+    private final Map<InternalFields, InternalFieldDescription> internalFields;\n \n-    KafkaColumnHandle getColumnHandle(int index, boolean hidden)\n+    @Inject\n+    public KafkaInternalFieldDescription()\n     {\n-        return new KafkaColumnHandle(\n-                getColumnName(),\n-                getType(),\n-                null,\n-                null,\n-                null,\n-                false,\n-                hidden,\n-                true);\n+        internalFields = new ImmutableMap.Builder<InternalFields, InternalFieldDescription>()\n+                .put(InternalFields.PARTITION_ID_FIELD, new InternalFieldDescription(InternalFields.PARTITION_ID_FIELD, BigintType.BIGINT))\n+                .put(InternalFields.PARTITION_OFFSET_FIELD, new InternalFieldDescription(InternalFields.PARTITION_OFFSET_FIELD, BigintType.BIGINT))\n+                .put(InternalFields.MESSAGE_CORRUPT_FIELD, new InternalFieldDescription(InternalFields.MESSAGE_CORRUPT_FIELD, BooleanType.BOOLEAN))\n+                .put(InternalFields.MESSAGE_FIELD, new InternalFieldDescription(InternalFields.MESSAGE_FIELD, createUnboundedVarcharType()))\n+                .put(InternalFields.MESSAGE_LENGTH_FIELD, new InternalFieldDescription(InternalFields.MESSAGE_LENGTH_FIELD, BigintType.BIGINT))\n+                .put(InternalFields.KEY_CORRUPT_FIELD, new InternalFieldDescription(InternalFields.KEY_CORRUPT_FIELD, BooleanType.BOOLEAN))\n+                .put(InternalFields.KEY_FIELD, new InternalFieldDescription(InternalFields.KEY_FIELD, createUnboundedVarcharType()))\n+                .put(InternalFields.KEY_LENGTH_FIELD, new InternalFieldDescription(InternalFields.KEY_LENGTH_FIELD, BigintType.BIGINT))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjU4Njg4Mw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462586883", "bodyText": "https://github.com/prestosql/presto/pull/4462/files#diff-a4d914e9d2a3a44c167509b820255365R108\n@findepi, did I get you right?", "author": "0xE282B0", "createdAt": "2020-07-29T21:02:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMTY1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExMjU4Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462112587", "bodyText": "Remove the comment in the previous commit, which was adding it", "author": "findepi", "createdAt": "2020-07-29T08:01:13Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldDescription.java", "diffHunk": "@@ -111,19 +118,13 @@ public String getColumnName()\n         }\n     }\n \n-    /**\n-     * Inner class that wraps the {@link InternalFields} enum and add {@link Type}.\n-     * With this bean a non static data structure with types from the {@link TypeManager} can be created.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNjE1MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462116151", "bodyText": "Use Multimap.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    Map<String, List<byte[]>> headerMap = new HashMap<>();\n          \n          \n            \n                    for (Header header : headers) {\n          \n          \n            \n                        String key = header.key();\n          \n          \n            \n                        if (headerMap.containsKey(key)) {\n          \n          \n            \n                            headerMap.get(key).add(header.value());\n          \n          \n            \n                        }\n          \n          \n            \n                        else {\n          \n          \n            \n                            List<byte[]> list = new ArrayList<>();\n          \n          \n            \n                            list.add(header.value());\n          \n          \n            \n                            headerMap.put(key, list);\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    Multimap<String, byte[]> headerMap = ArrayListMultimap.create();\n          \n          \n            \n                    for (Header header : headers) {\n          \n          \n            \n                        headerMap.put(header.key(), header.value());\n          \n          \n            \n                    }", "author": "findepi", "createdAt": "2020-07-29T08:07:38Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +278,53 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)\n+    {\n+        Type keyType = varcharMapType.getTypeParameters().get(0);\n+        Type valueArrayType = varcharMapType.getTypeParameters().get(1);\n+        Type valueType = valueArrayType.getTypeParameters().get(0);\n+\n+        BlockBuilder mapBlockBuilder = varcharMapType.createBlockBuilder(null, 1);\n+        BlockBuilder builder = mapBlockBuilder.beginBlockEntry();\n+\n+        // Group by keys and collect values as array.\n+        Map<String, List<byte[]>> headerMap = new HashMap<>();\n+        for (Header header : headers) {\n+            String key = header.key();\n+            if (headerMap.containsKey(key)) {\n+                headerMap.get(key).add(header.value());\n+            }\n+            else {\n+                List<byte[]> list = new ArrayList<>();\n+                list.add(header.value());\n+                headerMap.put(key, list);\n+            }\n+        }", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462117874", "bodyText": "I'd assume empty headers case is not a rare case, so we could want to cache a result for this (empty block).\n@dain wdyt?", "author": "findepi", "createdAt": "2020-07-29T08:10:44Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaRecordSet.java", "diffHunk": "@@ -269,4 +278,53 @@ public void close()\n             kafkaConsumer.close();\n         }\n     }\n+\n+    public static FieldValueProvider headerMapValueProvider(MapType varcharMapType, Headers headers)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc3NzU4MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r473777581", "bodyText": "@0xE282B0 can you address this one unless the assumption about the fact, that empty headers is a common thing, is false?", "author": "losipiuk", "createdAt": "2020-08-20T08:48:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTczODUzOQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r479738539", "bodyText": "sure.", "author": "0xE282B0", "createdAt": "2020-08-30T08:16:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTc0MDUwMw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r479740503", "bodyText": "Is there an easier way to get an empty MapBlock than creating a MapType and using a MapBlockBuilder?\nMapType mapType = new MapType(VarcharType.VARCHAR, new ArrayType(VarbinaryType.VARBINARY),\n        MethodHandles.empty(methodType(Boolean.class, Block.class, int.class, long.class)),\n        MethodHandles.empty(methodType(Boolean.class, Block.class, int.class, Block.class, int.class)),\n        MethodHandles.empty(methodType(long.class, Object.class)),\n        MethodHandles.empty(methodType(long.class, Object.class)));\nBlockBuilder mapBlockBuilder = new MapBlockBuilder(mapType, null, 0);\nmapBlockBuilder.beginBlockEntry();\nmapBlockBuilder.closeEntry();\nBlock emptyMapBlock = mapType.getObject(mapBlockBuilder, 0);\n\n414ddcb#diff-1938e94844afe6ab2ee40936951490d9R300-R308", "author": "0xE282B0", "createdAt": "2020-08-30T08:35:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDAxNzQ5MQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r480017491", "bodyText": "I am not aware. Looks good to me.\nNit: can you rename createEmptyMapBlockProvider to createEmptyHeadersFieldProvider\nand EMPTY_MAP_BLOCK_PROVIDER to EMPTY_HEADERS_FIELD_PROVIDER", "author": "losipiuk", "createdAt": "2020-08-31T09:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDAxODUwNA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r480018504", "bodyText": "Also please add a test when we are reading _headers column but headers map is empty.", "author": "losipiuk", "createdAt": "2020-08-31T09:50:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjcyMjEwNw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r482722107", "bodyText": "Naming sounds better - more specific.\nThe empty header test doesn't look as straightforward as I wanted it to be, but it works. Since the assertQuery does not work on maps and VARBINARY does not work with JSON, I converted the map type and then cast it to JSON to compare it with empty objects.\nSELECT cast(transform_values(_headers,(k,v)->transform(v,x->from_utf8(x))) AS JSON)\n\nRecomendations welcome \ud83d\ude09", "author": "0xE282B0", "createdAt": "2020-09-03T05:59:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjgzOTcyNw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r482839727", "bodyText": "I think the test could be a bit more readable if headers topic also defined named id field. It would make assertions simpler.\nThen if messages with empty headers has id equal to 1 and 2.\nThe assertion would look like:\n assertQuery(\"SELECT id FROM default.\" + headersTopic + \" WHERE cardinality(_headers) = 0\",\n                \"VALUES (1), (2)\");\n\nThe other test query could be more readable too if you used id in WHERE clause.\nWDYT?", "author": "losipiuk", "createdAt": "2020-09-03T09:28:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM4MjUxMQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r485382511", "bodyText": "Unfortunately we don't have an ID, but we can enumerate the messages based on the value of the message.\nassertQuery(\"SELECT _message FROM default.\" + headersTopic + \" WHERE cardinality(_headers) = 0\",\n               \"VALUES ('1'),('2')\");", "author": "0xE282B0", "createdAt": "2020-09-09T07:01:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQ3MTc1Nw==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r485471757", "bodyText": "I was thinking of extending the schema with id but using _message is totally fine too.", "author": "losipiuk", "createdAt": "2020-09-09T09:27:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExNzg3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExODMwNg==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462118306", "bodyText": "thanks for taking care of null values\ncan header key be null as well?", "author": "findepi", "createdAt": "2020-07-29T08:11:28Z", "path": "presto-kafka/src/test/java/io/prestosql/plugin/kafka/TestKafkaIntegrationSmokeTest.java", "diffHunk": "@@ -134,6 +139,27 @@ private void insertData(byte[] data)\n         }\n     }\n \n+    private void createMessagesWithHeader(String topicName)\n+    {\n+        try (KafkaProducer<byte[], byte[]> producer = createProducer()) {\n+            // Message without headers\n+            ProducerRecord<byte[], byte[]> record = new ProducerRecord<>(topicName, null, \"{}\".getBytes(UTF_8));\n+            producer.send(record);\n+            // Message with simple header\n+            record = new ProducerRecord<>(topicName, null, \"{}\".getBytes(UTF_8));\n+            record.headers()\n+                    .add(\"notfoo\", \"some value\".getBytes(UTF_8));\n+            producer.send(record);\n+            // Message with multiple same key headers\n+            record = new ProducerRecord<>(topicName, null, \"{}\".getBytes(UTF_8));\n+            record.headers()\n+                    .add(\"foo\", \"bar\".getBytes(UTF_8))\n+                    .add(\"foo\", null)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUwMjAxNA==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r462502014", "bodyText": "No, at least the Kafka Java client throws an exception if you try to create a Header with null as key.\nSee: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeader.java#L32", "author": "0xE282B0", "createdAt": "2020-07-29T18:28:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjExODMwNg=="}], "type": "inlineReview"}, {"oid": "80dc7cea55e1005eff07c0bbca20ae938c58c020", "url": "https://github.com/trinodb/trino/commit/80dc7cea55e1005eff07c0bbca20ae938c58c020", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-07-29T18:30:30Z", "type": "forcePushed"}, {"oid": "b58473aba6619d686d5e3f86a9d60cc717b63a0e", "url": "https://github.com/trinodb/trino/commit/b58473aba6619d686d5e3f86a9d60cc717b63a0e", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-07-29T20:54:55Z", "type": "forcePushed"}, {"oid": "a8d59c3ba863da2c7258d8fa8a1043e4c802fa67", "url": "https://github.com/trinodb/trino/commit/a8d59c3ba863da2c7258d8fa8a1043e4c802fa67", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-08-18T19:45:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mzc3NjEyMQ==", "url": "https://github.com/trinodb/trino/pull/4462#discussion_r473776121", "bodyText": "drop dot at the end :)", "author": "losipiuk", "createdAt": "2020-08-20T08:46:58Z", "path": "presto-kafka/src/main/java/io/prestosql/plugin/kafka/KafkaInternalFieldManager.java", "diffHunk": "@@ -138,6 +149,10 @@ public KafkaInternalFieldManager(TypeManager typeManager)\n                         MESSAGE_FIELD,\n                         \"Message text\",\n                         createUnboundedVarcharType()))\n+                .put(HEADERS_FIELD, new InternalField(\n+                        HEADERS_FIELD,\n+                        \"Headers of the message as map.\",", "originalCommit": "a8d59c3ba863da2c7258d8fa8a1043e4c802fa67", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "b3479f0af25757c8eca876fcef8f57ff47ebf7df", "url": "https://github.com/trinodb/trino/commit/b3479f0af25757c8eca876fcef8f57ff47ebf7df", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-08-31T15:27:01Z", "type": "forcePushed"}, {"oid": "3ddb4ae8ca8fdd25e1245a6f5a7542c25882e748", "url": "https://github.com/trinodb/trino/commit/3ddb4ae8ca8fdd25e1245a6f5a7542c25882e748", "message": "Make Type column wider in Kafka columns documentation\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-09-09T06:56:13Z", "type": "commit"}, {"oid": "0daf4b198fbd91386ae51255642a9fc6b2fa0b8f", "url": "https://github.com/trinodb/trino/commit/0daf4b198fbd91386ae51255642a9fc6b2fa0b8f", "message": "Refactor KafkaInternalFieldDescription to KafkaInternalFieldManager\n\nThe KafkaInternalFieldManager creates the internalFields map in the\nconsructor where the TypeManager can be used.\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-09-09T06:56:13Z", "type": "commit"}, {"oid": "228e08b2b943e057b9e510824b73194b82fbd909", "url": "https://github.com/trinodb/trino/commit/228e08b2b943e057b9e510824b73194b82fbd909", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-09-09T06:56:14Z", "type": "commit"}, {"oid": "228e08b2b943e057b9e510824b73194b82fbd909", "url": "https://github.com/trinodb/trino/commit/228e08b2b943e057b9e510824b73194b82fbd909", "message": "Add header column to Kafka Connector\n\nColumn definition has been added to KafkaInternalFieldDescription with\nmap(VARCHAR,array(VARBINARY)) type from TypeManager.\nValueProvider has been added to KafkaRecordSet\n\nSigned-off-by: Sven Pfennig <sven.pfennig@syncier.com>", "committedDate": "2020-09-09T06:56:14Z", "type": "forcePushed"}]}